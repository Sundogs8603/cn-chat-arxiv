<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#27604;&#20043;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2311.01655</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#35270;&#35273;&#27010;&#24565;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;AI&#29983;&#25104;&#22270;&#20687;&#20998;&#31867;&#20013;&#26816;&#27979;&#20266;&#30456;&#20851;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification. (arXiv:2311.01655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01655
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#27604;&#20043;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20542;&#21521;&#20110;&#33258;&#21160;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20851;&#32852;&#65292;&#32780;&#19981;&#20250;&#36136;&#30097;&#20854;&#26377;&#25928;&#24615;&#25110;&#36866;&#24403;&#24615;&#12290;&#36825;&#31181;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#26159;&#20266;&#30456;&#20851;&#20851;&#31995;&#30340;&#20307;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#19981;&#21487;&#38752;&#19988;&#23481;&#26131;&#22833;&#36133;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#35797;&#22270;&#32416;&#27491;&#20266;&#30456;&#20851;&#20851;&#31995;&#30340;&#26041;&#27861;&#20165;&#23545;&#27169;&#22411;&#24050;&#30693;&#30340;&#20266;&#20851;&#32852;&#26377;&#25928;&#12290;&#24403;&#21069;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#26816;&#27979;&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#65292;&#35201;&#20040;&#22312;&#20854;&#21046;&#23450;&#20013;&#22826;&#36807;&#38480;&#21046;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#23545;&#35270;&#35273;&#24037;&#20214;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#36825;&#22312;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#26631;&#20934;&#35268;&#33539;&#30340;&#20869;&#23481;&#32780;&#38395;&#21517;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#27604;&#20043;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often machine learning models tend to automatically learn associations present in the training data without questioning their validity or appropriateness. This undesirable property is the root cause of the manifestation of spurious correlations, which render models unreliable and prone to failure in the presence of distribution shifts. Research shows that most methods attempting to remedy spurious correlations are only effective for a model's known spurious associations. Current spurious correlation detection algorithms either rely on extensive human annotations or are too restrictive in their formulation. Moreover, they rely on strict definitions of visual artifacts that may not apply to data produced by generative models, as they are known to hallucinate contents that do not conform to standard specifications. In this work, we introduce a general-purpose method that efficiently detects potential spurious correlations, and requires significantly less human interference in comparison t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65288;rMCL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Voronoi tessellations&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#65292;&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#36755;&#20837;&#21487;&#33021;&#37319;&#26679;&#22810;&#20010;&#30446;&#26631;&#30340;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#22768;&#28304;&#23450;&#20301;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#39564;&#35777;&#21644;&#36827;&#19968;&#27493;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#30340;&#26377;&#29992;&#24615;&#21644;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01052</link><description>&lt;p&gt;
&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65306;&#29992;&#20110;&#38899;&#39057;&#22330;&#26223;&#20998;&#26512;&#30340;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#30340;&#24341;&#20837;
&lt;/p&gt;
&lt;p&gt;
Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis. (arXiv:2311.01052v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01052
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65288;rMCL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Voronoi tessellations&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#65292;&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#36755;&#20837;&#21487;&#33021;&#37319;&#26679;&#22810;&#20010;&#30446;&#26631;&#30340;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#22768;&#28304;&#23450;&#20301;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#39564;&#35777;&#21644;&#36827;&#19968;&#27493;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#30340;&#26377;&#29992;&#24615;&#21644;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65288;rMCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#36755;&#20837;&#21487;&#33021;&#37319;&#26679;&#22810;&#20010;&#30446;&#26631;&#30340;&#22238;&#24402;&#35774;&#32622;&#19979;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;MCL&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;&#22810;&#36873;&#23398;&#20064;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#65292;&#20351;&#29992;&#20102;&#19968;&#32452;&#20551;&#35774;&#30340;&#32988;&#32773;&#20840;&#25343;&#65288;WTA&#65289;&#25439;&#22833;&#12290;&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#29616;&#26377;&#30340;MCL&#21464;&#20307;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#24182;&#20551;&#35774;&#19978;&#65292;&#20174;&#32780;&#26368;&#32456;&#29306;&#29298;&#20102;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#22522;&#20110;Voronoi tessellations&#30340;&#36755;&#20986;&#31354;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#25903;&#25345;&#30340;&#26032;&#39062;&#30340;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#24471;&#20986;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;rMCL&#22312;&#22768;&#28304;&#23450;&#20301;&#38382;&#39064;&#19978;&#30340;&#20248;&#28857;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#30340;&#26377;&#29992;&#24615;&#21644;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00619</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#25439;&#22833;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#27491;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#27880;&#37322;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#20010;&#21035;&#27880;&#37322;&#32773;&#32463;&#24120;&#20250;&#25552;&#20379;&#25968;&#21315;&#20010;&#35780;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30130;&#21171;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27880;&#37322;&#36807;&#31243;&#21487;&#33021;&#20250;&#25345;&#32493;&#22810;&#22825;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#38543;&#26102;&#38388;&#30340;&#19981;&#20934;&#30830;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#22810;&#26679;&#24847;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#28165;&#26970;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20462;&#25913;&#21487;&#20197;&#25913;&#21892;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#24212;&#29992;&#20110;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.08176</link><description>&lt;p&gt;
&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#22238;&#24402;/&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27599;&#20010;&#20840;&#36830;&#25509;&#23618;&#30340;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#26159;&#23545;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30456;&#36830;&#25509;&#65292;&#21518;&#32773;&#37117;&#26159;&#20855;&#26377;&#24736;&#20037;&#20256;&#32479;&#21644;&#20016;&#23500;&#29702;&#35770;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#36739;&#23569;&#65292;&#21487;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20351;&#20854;&#22312;&#24212;&#29992;&#20013;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#36830;&#25509;&#30340;&#30740;&#31350;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#36235;&#21183;&#12290;&#23545;&#20110;&#22810;&#31181;&#26550;&#26500;&#65288;&#21253;&#25324;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#25512;&#23548;&#20986;&#20102;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#38381;&#24335;&#24418;&#24335;&#12290;&#23545;&#36825;&#20123;&#26550;&#26500;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26102;&#38388;&#20934;&#21017;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#27491;&#21017;&#21270;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#20248;&#21270;&#22120;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#35843;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(mRNN)&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#20102;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12620</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#27169;&#22411;&#22312;&#26102;&#38388;&#20934;&#21017;&#19979;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria. (arXiv:2309.12620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26102;&#38388;&#20934;&#21017;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#27491;&#21017;&#21270;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#20248;&#21270;&#22120;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#35843;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(mRNN)&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#20102;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26041;&#27861;&#30340;&#20986;&#29616;&#20419;&#36827;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26102;&#38388;&#20934;&#21017;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#24182;&#22312;&#27491;&#21017;&#21270;&#26694;&#26550;&#20869;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#21512;&#24182;&#22810;&#20010;&#21487;&#33021;&#36739;&#24369;&#30340;&#20248;&#21270;&#22120;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#39640;&#25928;&#25191;&#34892;&#27492;&#36807;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#24182;&#36866;&#24212;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#35843;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(mRNN)&#12290;&#23427;&#26088;&#22312;&#25429;&#25417;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#20559;&#22909;&#21160;&#24577;&#65292;&#21516;&#26102;&#20445;&#25345;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#22266;&#26377;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21253;&#25324;&#20934;&#21017;&#30340;&#39034;&#24207;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of predictive methodologies has catalyzed the emergence of data-driven decision support across various domains. However, developing models capable of effectively handling input time series data presents an enduring challenge. This study presents novel preference learning approaches to multiple criteria sorting problems in the presence of temporal criteria. We first formulate a convex quadratic programming model characterized by fixed time discount factors, operating within a regularization framework. Additionally, we propose an ensemble learning algorithm designed to consolidate the outputs of multiple, potentially weaker, optimizers, a process executed efficiently through parallel computation. To enhance scalability and accommodate learnable time discount factors, we introduce a novel monotonic Recurrent Neural Network (mRNN). It is designed to capture the evolving dynamics of preferences over time while upholding critical properties inherent to MCS problems, including crit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10923</link><description>&lt;p&gt;
&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;: &#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10923
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#37319;&#38598;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#30340; SuperCon &#25968;&#25454;&#24211;&#30340;&#20998;&#21306;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#26356;&#26032; SuperCon &#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#32452;&#25104;&#30340;&#24037;&#20316;&#27969;&#39537;&#21160;&#30340;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#25968;&#25454;&#24211;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#26657;&#39564;&#21644;&#32416;&#38169;&#12290;&#24322;&#24120;&#26816;&#27979;&#33258;&#21160;&#36807;&#31243;&#29992;&#20110;&#39044;&#20808;&#31579;&#36873;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23450;&#21046;&#30340;&#29992;&#25143;&#30028;&#38754;&#22312;&#21407;&#22987; PDF &#25991;&#26723;&#19978;&#36827;&#34892;&#25968;&#25454;&#39564;&#35777;&#21644;&#32416;&#38169;&#12290;&#27492;&#22806;&#65292;&#24403;&#35760;&#24405;&#34987;&#32416;&#38169;&#26102;&#65292;&#20854;&#21407;&#22987;&#25968;&#25454;&#34987;&#25910;&#38598;&#24182;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#30028;&#38754;&#19982;&#20256;&#32479;&#30340;&#25163;&#21160;&#38405;&#35835; PDF &#25991;&#26723;&#24182;&#22312; Excel &#25991;&#26723;&#20013;&#35760;&#24405;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07383</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#36817;&#20284;&#30340;&#26576;&#20123;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#32452;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;$H(\Omega)$&#20013;&#20986;&#29616;&#30340;&#19968;&#20123;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#31867;&#30340;&#26412;&#22320;&#31354;&#38388;&#20013;&#26500;&#24314;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31163;&#32447;&#36817;&#20284;&#30340;&#31639;&#23376;&#26041;&#31243;&#30340;&#24378;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#20010;&#31639;&#23376;&#26041;&#31243;&#20986;&#29616;&#22312;&#31574;&#30053;&#36845;&#20195;&#20013;&#12290;&#21033;&#29992;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;$H_N$&#22312;&#26412;&#22320;&#31354;&#38388;$H(\Omega)$&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;$\Pwr_{H,N}$&#65292;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#26174;&#24335;&#19978;&#30028;&#12290;&#36825;&#20123;&#19978;&#30028;&#20855;&#26377;&#20960;&#20309;&#24615;&#36136;&#65292;&#24182;&#23545;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#26377;&#20102;&#19968;&#20123;&#25913;&#36827;&#21644;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20849;&#21516;&#22686;&#38271;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2307.00534</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20849;&#21516;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation. (arXiv:2307.00534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20849;&#21516;&#22686;&#38271;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24615;&#33021;&#26377;&#25928;&#65292;&#20854;&#20013;&#20856;&#22411;&#30446;&#26631;&#26159;&#23558;&#28145;&#23618;&#25945;&#24072;GNN&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#27973;&#30340;&#23398;&#29983;GNN&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#21442;&#25968;&#21270;&#21644;&#36807;&#24179;&#28369;&#38382;&#39064;&#65292;&#35757;&#32451;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#28145;&#23618;GNN&#36890;&#24120;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#33258;&#30001;&#26041;&#21521;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;FreeKD&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#26356;&#28145;&#23618;&#27425;&#30340;&#20248;&#21270;&#33391;&#22909;&#30340;&#25945;&#24072;GNN&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21327;&#21516;&#23398;&#20064;&#20004;&#20010;&#36739;&#27973;&#30340;GNN&#65292;&#20197;&#20415;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#12290;&#30001;&#20110;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#20856;&#22411;&#30340;GNN&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#19981;&#21516;&#33410;&#28857;&#34920;&#29616;&#20986;&#26356;&#22909;&#21644;&#26356;&#24046;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#21644;&#33258;&#30001;&#26041;&#21521;&#30340;&#30693;&#35782;&#20256;&#36882;&#31574;&#30053;&#65292;&#23427;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00310</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20284;&#65306;&#25935;&#24863;&#24230;&#32463;&#24120;&#34987;&#36807;&#39640;&#20272;&#35745;&#22312;DP-SGD&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#31169;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#31639;&#27861;&#12290;&#34429;&#28982;&#24050;&#30693;&#20854;&#38544;&#31169;&#20998;&#26512;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#26159;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;DP-SGD&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25429;&#25417;&#21040;&#22312;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#30456;&#20284;&#37051;&#23621;&#30340;&#28857;&#20139;&#21463;&#26356;&#22909;&#38544;&#31169;&#24615;&#30340;&#30452;&#35273;&#12290;&#24418;&#24335;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#36890;&#36807;&#20462;&#25913;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#35745;&#31639;&#24471;&#21040;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#27599;&#27493;&#38544;&#31169;&#24615;&#20998;&#26512;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#23450;&#29702;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#27599;&#27493;&#20998;&#26512;&#26469;&#25512;&#29702;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;DP-SGD&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#24335;&#22320;&#26174;&#31034;DP-SGD&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) is the canonical algorithm for private deep learning. While it is known that its privacy analysis is tight in the worst-case, several empirical results suggest that when training on common benchmark datasets, the models obtained leak significantly less privacy for many datapoints. In this paper, we develop a new analysis for DP-SGD that captures the intuition that points with similar neighbors in the dataset enjoy better privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints. In particular, we ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31454;&#20105;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#25552;&#39640;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;&#20379;&#24212;&#21830;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#38477;&#20302;&#31038;&#20250;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2306.14670</link><description>&lt;p&gt;
&#31454;&#20105;&#29615;&#22659;&#19979;&#36125;&#21494;&#26031;&#39118;&#38505;&#30340;&#25552;&#39640;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#31119;&#21033;&#30340;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition. (arXiv:2306.14670v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31454;&#20105;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#25552;&#39640;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;&#20379;&#24212;&#21830;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#38477;&#20302;&#31038;&#20250;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#32553;&#25918;&#23450;&#24459;&#31561;&#36235;&#21183;&#39044;&#35745;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36235;&#21183;&#21482;&#32771;&#34385;&#20102;&#21333;&#20010;&#27169;&#22411;&#20379;&#24212;&#21830;&#30340;&#35270;&#35282;&#65292;&#32780;&#23454;&#38469;&#19978;&#20379;&#24212;&#21830;&#20043;&#38388;&#24120;&#24120;&#31454;&#20105;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#31454;&#20105;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#36825;&#20123;&#32553;&#25918;&#36235;&#21183;&#30340;&#34892;&#20026;&#65292;&#29978;&#33267;&#21487;&#33021;&#36896;&#25104;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#32780;&#38750;&#21333;&#35843;&#25110;&#38477;&#20302;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#31454;&#20105;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#34920;&#31034;&#20316;&#20026;&#30740;&#31350;&#35268;&#27169;&#22686;&#21152;&#30340;&#24433;&#21709;&#30340;&#38236;&#22836;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19968;&#23478;&#24066;&#22330;&#19978;&#65292;&#25913;&#21892;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#65288;&#25353;&#36125;&#21494;&#26031;&#39118;&#38505;&#35745;&#37327;&#65289;&#21487;&#33021;&#20250;&#38477;&#20302;&#31454;&#20105;&#27169;&#22411;&#20379;&#24212;&#21830;&#30340;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#21363;&#31038;&#20250;&#31119;&#21033;&#65289;&#12290;&#25105;&#20204;&#30340;&#20363;&#23376;&#28085;&#30422;&#20102;&#31616;&#21333;&#35774;&#32622;&#20013;&#30340;&#23553;&#38381;&#24335;&#20844;&#24335;&#21040;&#39044;&#35757;&#32451;&#30340; CIFAR-10 &#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the scale of machine learning models increases, trends such as scaling laws anticipate consistent downstream improvements in predictive accuracy. However, these trends take the perspective of a single model-provider in isolation, while in reality providers often compete with each other for users. In this work, we demonstrate that competition can fundamentally alter the behavior of these scaling trends, even causing overall predictive accuracy across users to be non-monotonic or decreasing with scale. We define a model of competition for classification tasks, and use data representations as a lens for studying the impact of increases in scale. We find many settings where improving data representation quality (as measured by Bayes risk) decreases the overall predictive accuracy across users (i.e., social welfare) for a marketplace of competing model-providers. Our examples range from closed-form formulas in simple settings to simulations with pretrained representations on CIFAR-10. At
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#38598;&#20013;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21592;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#35780;&#20272;&#35774;&#32622;&#19981;&#33021;&#24456;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25104;&#21592;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.12983</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#26356;&#30495;&#23454;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Towards More Realistic Membership Inference Attacks on Large Diffusion Models. (arXiv:2306.12983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#38598;&#20013;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21592;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#35780;&#20272;&#35774;&#32622;&#19981;&#33021;&#24456;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25104;&#21592;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#31283;&#23450;&#25193;&#25955;&#21644;Midjourney&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#21560;&#24341;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#25968;&#21313;&#20159;&#20010;&#20114;&#32852;&#32593;&#26469;&#28304;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#28508;&#22312;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#30340;&#37325;&#35201;&#25285;&#24551;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#30830;&#23450;&#29305;&#23450;&#22270;&#20687;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#20102;&#65292;&#36825;&#22312;&#32593;&#32476;&#23433;&#20840;&#31038;&#21306;&#20013;&#34987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#31283;&#23450;&#25193;&#25955;&#65292;&#24182;&#35299;&#20915;&#20102;&#35774;&#35745;&#19968;&#20010;&#20844;&#24179;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#22238;&#31572;&#36825;&#20010;&#25104;&#21592;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24314;&#31435;&#19968;&#20010;&#20844;&#24179;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#31283;&#23450;&#25193;&#25955;&#65292;&#20351;&#28508;&#22312;&#30340;&#25193;&#23637;&#21040;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#21033;&#29992;&#36825;&#20010;&#35780;&#20272;&#35774;&#32622;&#65292;&#25105;&#20204;&#25191;&#34892;&#25104;&#21592;&#25915;&#20987;&#65288;&#21253;&#25324;&#24050;&#30693;&#21644;&#26032;&#24341;&#20837;&#30340;&#25915;&#20987;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#35780;&#20272;&#35774;&#32622;&#19981;&#33021;&#24456;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25104;&#21592;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not
&lt;/p&gt;</description></item><item><title>GADBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#21457;&#29616;&#20102;&#26641;&#38598;&#25104;&#21644;&#31616;&#21333;&#37051;&#22495;&#27719;&#32858;&#26041;&#27861;&#32988;&#36807;&#25152;&#26377;23&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#38024;&#23545;GAD&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;GNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12251</link><description>&lt;p&gt;
GADBench&#65306;&#37325;&#26032;&#23457;&#35270;&#21644;&#23545;&#30417;&#30563;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection. (arXiv:2306.12251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12251
&lt;/p&gt;
&lt;p&gt;
GADBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#21457;&#29616;&#20102;&#26641;&#38598;&#25104;&#21644;&#31616;&#21333;&#37051;&#22495;&#27719;&#32858;&#26041;&#27861;&#32988;&#36807;&#25152;&#26377;23&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#38024;&#23545;GAD&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20256;&#32479;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#31639;&#27861;&#21644;&#26368;&#36817;&#27969;&#34892;&#30340;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#19968;&#30452;&#23384;&#22312;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;&#26631;&#20934;&#32508;&#21512;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#22914;&#20309;&#65292;GNN&#26159;&#21542;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#65288;&#22914;&#26641;&#38598;&#25104;&#65289;&#20197;&#21450;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#30340;&#25928;&#29575;&#22914;&#20309;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GADBench - &#19968;&#20010;&#38745;&#24577;&#22270;&#24418;&#30417;&#30563;&#24322;&#24120;&#33410;&#28857;&#26816;&#27979;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;GADBench&#22312;&#20174;&#25968;&#21315;&#21040;&#25968;&#30334;&#19975;&#33410;&#28857;&#65288;&#32422;6M&#65289;&#30340;&#21313;&#20010;&#30495;&#23454;GAD&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;23&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20855;&#26377;&#31616;&#21333;&#37051;&#22495;&#27719;&#32858;&#30340;&#26641;&#38598;&#25104;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#38024;&#23545;GAD&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;GNN&#12290;&#36890;&#36807;&#23558;GADBench&#20316;&#20026;&#24320;&#28304;&#24037;&#20855;&#25552;&#20379;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;GAD&#24403;&#21069;&#36827;&#23637;&#30340;&#20851;&#38190;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs outperform traditional algorithms such as tree ensembles, and (3) their efficiency on large-scale graphs. In response, we present GADBench -- a comprehensive benchmark for supervised anomalous node detection on static graphs. GADBench provides a thorough comparison across 23 distinct models on ten real-world GAD datasets ranging from thousands to millions of nodes ($\sim$6M). Our main finding is that tree ensembles with simple neighborhood aggregation outperform all other baselines, including the latest GNNs tailored for the GAD task. By making GADBench available as an open-source tool, we offer pivotal insights into the current advancements of GAD and establish a solid foundation for future research. Our code is available at https://github.com/squareRoot3/GADBen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Corrector&#25805;&#20316;&#31526;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#26696;&#23545;&#20110;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#36824;&#22312;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2306.12047</link><description>&lt;p&gt;
&#20351;&#29992;Corrector&#25805;&#20316;&#31526;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems. (arXiv:2306.12047v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Corrector&#25805;&#20316;&#31526;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#26696;&#23545;&#20110;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#36824;&#22312;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31867;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#31526;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#24335;&#12290;&#31070;&#32463;&#31639;&#23376;&#26377;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#25104;&#26412;-&#20934;&#30830;&#24230;&#26435;&#34913;&#21644;&#38750;&#24179;&#20961;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#12290;&#31070;&#32463;&#31639;&#23376;&#20934;&#30830;&#24230;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#25512;&#29702;&#12289;&#20248;&#21270;&#21644;&#25511;&#21046;&#31561;&#21518;&#32493;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#21464;&#20998;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#32473;&#20986;&#20102;&#31070;&#32463;&#31639;&#23376;&#39044;&#27979;&#32467;&#26524;&#30340;&#26657;&#27491;&#20540;&#12290;&#19982;&#26657;&#27491;&#38382;&#39064;&#30456;&#20851;&#30340;&#31639;&#23376;&#31216;&#20026;&#26657;&#27491;&#31639;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#26041;&#26696;&#23545;&#37319;&#29992;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20351;&#29992;&#26657;&#27491;&#26041;&#27861;&#23545;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#26657;&#27491;&#26102;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20063;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on developing methods for approximating the solution operators of a class of parametric partial differential equations via neural operators. Neural operators have several challenges, including the issue of generating appropriate training data, cost-accuracy trade-offs, and nontrivial hyperparameter tuning. The unpredictability of the accuracy of neural operators impacts their applications in downstream problems of inference, optimization, and control. A framework is proposed based on the linear variational problem that gives the correction to the prediction furnished by neural operators. The operator associated with the corrector problem is referred to as the corrector operator. Numerical results involving a nonlinear diffusion model in two dimensions with PCANet-type neural operators show almost two orders of increase in the accuracy of approximations when neural operators are corrected using the proposed scheme. Further, topology optimization involving a nonlinear d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27979;&#24230;&#19978;&#32534;&#20889;&#21464;&#20998;&#30446;&#26631;&#30340;&#21160;&#26426;&#65292;&#25552;&#20986;&#36890;&#36807;&#27492;&#31867;&#30446;&#26631;&#25512;&#23548;&#23454;&#29992;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#31561;&#38382;&#39064;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11928</link><description>&lt;p&gt;
&#24320;&#25918;&#38382;&#39064;&#65306;&#22522;&#20110;&#21464;&#20998;&#30446;&#26631;&#30340;&#27979;&#24230;&#23398;&#20064; (arXiv:2306.11928v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Open Problem: Learning with Variational Objectives on Measures. (arXiv:2306.11928v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27979;&#24230;&#19978;&#32534;&#20889;&#21464;&#20998;&#30446;&#26631;&#30340;&#21160;&#26426;&#65292;&#25552;&#20986;&#36890;&#36807;&#27492;&#31867;&#30446;&#26631;&#25512;&#23548;&#23454;&#29992;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#31561;&#38382;&#39064;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20851;&#27880;&#30340;&#26159;&#22522;&#20110;&#20989;&#25968;&#30340;&#21464;&#20998;&#30446;&#26631;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#27979;&#24230;&#19978;&#32534;&#20889;&#31867;&#20284;&#30446;&#26631;&#30340;&#21160;&#26426;&#65292;&#29305;&#21035;&#26159;&#35752;&#35770;&#20102;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#33021;&#21542;&#23558;&#36890;&#24120;&#30340;&#32479;&#35745;&#23398;&#20064;&#32467;&#26524;&#36716;&#21270;&#20026;&#22522;&#20110;&#27979;&#37327;&#34920;&#36798;&#30340;&#30446;&#26631;&#65311;&#32467;&#26524;&#26500;&#24314;&#26159;&#21542;&#20250;&#23548;&#33268;&#26032;&#30340;&#23454;&#29992;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
The theory of statistical learning has focused on variational objectives expressed on functions. In this note, we discuss motivations to write similar objectives on measures, in particular to discuss out-of-distribution generalization and weakly-supervised learning. It raises a natural question: can one cast usual statistical learning results to objectives expressed on measures? Does the resulting construction lead to new algorithms of practical interest?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#35299;&#20915;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#21464;&#37327;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31934;&#24230;&#30697;&#38453;&#30340;$l_p$&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#39057;&#29575;&#23398;&#27966;&#21644;&#36125;&#21494;&#26031;&#23398;&#27966;&#30340;&#20248;&#28857;&#34701;&#21512;&#22312;&#21464;&#20998;&#25512;&#29702;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#30697;&#38453;&#21464;&#37327;&#26631;&#20934;&#21270;&#27969;&#31243;&#26469;&#36924;&#36817;&#21518;&#39564;&#12290;</title><link>http://arxiv.org/abs/2306.07255</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#26465;&#20214;&#30697;&#38453;&#27969;
&lt;/p&gt;
&lt;p&gt;
Conditional Matrix Flows for Gaussian Graphical Models. (arXiv:2306.07255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#35299;&#20915;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#21464;&#37327;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31934;&#24230;&#30697;&#38453;&#30340;$l_p$&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#39057;&#29575;&#23398;&#27966;&#21644;&#36125;&#21494;&#26031;&#23398;&#27966;&#30340;&#20248;&#28857;&#34701;&#21512;&#22312;&#21464;&#20998;&#25512;&#29702;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#30697;&#38453;&#21464;&#37327;&#26631;&#20934;&#21270;&#27969;&#31243;&#26469;&#36924;&#36817;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#25968;&#35266;&#27979;&#21464;&#37327;&#20013;&#30740;&#31350;&#35768;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#39640;&#26031;&#22270;&#27169;&#22411;&#36890;&#36807;&#22312;$l_p$&#27491;&#21017;&#21270;&#20013;&#40723;&#21169;&#31934;&#24230;&#30697;&#38453;&#30340;&#31232;&#30095;&#24615;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20854;&#20013;$p \leq1$&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20122;-$l_1$&#20266;&#33539;&#25968;&#20351;&#30446;&#26631;&#39640;&#24230;&#38750;&#20984;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;$l_1$&#33539;&#25968;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#20801;&#35768;&#20248;&#38597;&#22320;&#35745;&#31639;&#20316;&#20026;&#25910;&#32553;&#21442;&#25968;$\lambda$&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#12290;&#36125;&#21494;&#26031;&#20844;&#24335;&#20026;&#31934;&#24230;&#30697;&#38453;&#24341;&#20837;&#20102;&#25289;&#26222;&#25289;&#26031;&#20808;&#39564;&#65292;&#20294;&#26159;&#19981;&#21516;$\lambda$&#20540;&#30340;&#21518;&#39564;&#25512;&#26029;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#26114;&#36149;&#30340;&#21513;&#24067;&#26031;&#37319;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;GGM&#30340;&#21464;&#20998;&#25512;&#29702;&#65292;&#23427;&#32479;&#19968;&#20102;&#39057;&#29575;&#23398;&#27966;&#21644;&#36125;&#21494;&#26031;&#23398;&#27966;&#30340;&#20248;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#29992;&#23450;&#20041;&#22312;s&#31354;&#38388;&#19978;&#30340;&#30697;&#38453;&#21464;&#37327;&#26631;&#20934;&#21270;&#27969;&#31243;&#26469;&#36924;&#36817;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying conditional independence structure among many variables with few observations is a challenging task. Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through an $l_p$ regularization with $p\leq1$. However, since the objective is highly non-convex for sub-$l_1$ pseudo-norms, most approaches rely on the $l_1$ norm. In this case frequentist approaches allow to elegantly compute the solution path as a function of the shrinkage parameter $\lambda$. Instead of optimizing the penalized likelihood, the Bayesian formulation introduces a Laplace prior on the precision matrix. However, posterior inference for different $\lambda$ values requires repeated runs of expensive Gibbs samplers. We propose a very general framework for variational inference in GGMs that unifies the benefits of frequentist and Bayesian frameworks. Specifically, we propose to approximate the posterior with a matrix-variate Normalizing Flow defined on the space of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;transformer&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#27491;&#38754;&#35828;&#26126;&#20102;transformer&#22312;&#31232;&#30095;&#24179;&#22343;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#27604;&#24490;&#29615;&#32593;&#32476;&#21644;&#21069;&#39304;&#32593;&#32476;&#26356;&#39640;&#65292;&#24182;&#23637;&#31034;&#20102;&#22823;&#23884;&#20837;&#32500;&#24230;&#22312;transformer&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#20316;&#29992;&#65307;&#36127;&#38754;&#35828;&#26126;&#20102;&#27880;&#24847;&#21147;&#23618;&#30340;&#22797;&#26434;&#24230;&#38543;&#36755;&#20837;&#22823;&#23567;&#32447;&#24615;&#32553;&#25918;&#65292;&#20294;&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21457;&#29983;&#65292;&#21487;&#20197;&#20351;&#29992;&#26367;&#20195;&#30340;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2306.02896</link><description>&lt;p&gt;
Transformer&#30340;&#20195;&#34920;&#24615;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Representational Strengths and Limitations of Transformers. (arXiv:2306.02896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;transformer&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#27491;&#38754;&#35828;&#26126;&#20102;transformer&#22312;&#31232;&#30095;&#24179;&#22343;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#27604;&#24490;&#29615;&#32593;&#32476;&#21644;&#21069;&#39304;&#32593;&#32476;&#26356;&#39640;&#65292;&#24182;&#23637;&#31034;&#20102;&#22823;&#23884;&#20837;&#32500;&#24230;&#22312;transformer&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#20316;&#29992;&#65307;&#36127;&#38754;&#35828;&#26126;&#20102;&#27880;&#24847;&#21147;&#23618;&#30340;&#22797;&#26434;&#24230;&#38543;&#36755;&#20837;&#22823;&#23567;&#32447;&#24615;&#32553;&#25918;&#65292;&#20294;&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21457;&#29983;&#65292;&#21487;&#20197;&#20351;&#29992;&#26367;&#20195;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#23618;&#24120;&#29992;&#20110;transformer&#20013;&#65292;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#25903;&#26609;&#20043;&#19968;&#65292;&#20294;&#19982;&#20854;&#20182;&#32593;&#32476;&#32467;&#26500;&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#22909;&#22788;&#21644;&#32570;&#38519;&#27809;&#26377;&#25968;&#23398;&#25551;&#36848;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#27880;&#24847;&#21147;&#23618;&#30340;&#34920;&#31034;&#33021;&#21147;&#36827;&#34892;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#32858;&#28966;&#20110;&#20869;&#22312;&#22797;&#26434;&#24230;&#21442;&#25968;&#65292;&#22914;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#23884;&#20837;&#32500;&#24230;&#12290;&#22312;&#27491;&#38754;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#31232;&#30095;&#24179;&#22343;&#20219;&#21153;&#65292;&#20854;&#20013;&#24490;&#29615;&#32593;&#32476;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#37117;&#38543;&#36755;&#20837;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#32553;&#25918;&#65292;&#32780;transformer&#20165;&#21576;&#23545;&#25968;&#32553;&#25918;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26500;&#36896;&#26469;&#23637;&#31034;transformer&#20013;&#22823;&#23884;&#20837;&#32500;&#24230;&#30340;&#24517;&#35201;&#24615;&#21644;&#20316;&#29992;&#12290;&#22312;&#36127;&#38754;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20803;&#26816;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#27880;&#24847;&#21147;&#23618;&#30340;&#22797;&#26434;&#24230;&#38543;&#36755;&#20837;&#22823;&#23567;&#21576;&#32447;&#24615;&#32553;&#25918;&#65307;&#30001;&#20110;&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#36341;&#20013;&#20284;&#20046;&#24456;&#23569;&#21457;&#29983;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21487;&#20197;&#26367;&#20195;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention layers, as commonly used in transformers, form the backbone of modern deep learning, yet there is no mathematical description of their benefits and deficiencies as compared with other architectures. In this work we establish both positive and negative results on the representation power of attention layers, with a focus on intrinsic complexity parameters such as width, depth, and embedding dimension. On the positive side, we present a sparse averaging task, where recurrent networks and feedforward networks all have complexity scaling polynomially in the input size, whereas transformers scale merely logarithmically in the input size; furthermore, we use the same construction to show the necessity and role of a large embedding dimension in a transformer. On the negative side, we present a triple detection task, where attention layers in turn have complexity scaling linearly in the input size; as this scenario seems rare in practice, we also present natural variants that can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#38388;&#20855;&#26377;&#29305;&#24449;&#26354;&#29575;&#21078;&#38754;&#65292;&#26354;&#29575;&#24046;&#24322;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#19988;mixup&#31561;&#24120;&#35265;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20135;&#29983;&#26356;&#24179;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.19730</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#20687;&#27969;&#24418;&#30340;&#25968;&#25454;&#34920;&#31034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Representations' Study of Latent Image Manifolds. (arXiv:2305.19730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#38388;&#20855;&#26377;&#29305;&#24449;&#26354;&#29575;&#21078;&#38754;&#65292;&#26354;&#29575;&#24046;&#24322;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#19988;mixup&#31561;&#24120;&#35265;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20135;&#29983;&#26356;&#24179;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#20869;&#22312;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#21363;&#22312;&#20854;&#20027;&#26041;&#21521;&#19978;&#30340;&#27969;&#24418;&#20559;&#31163;&#24179;&#22374;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#38388;&#20855;&#26377;&#29305;&#24449;&#26354;&#29575;&#21078;&#38754;&#65306;&#19968;&#20010;&#21021;&#22987;&#24613;&#21095;&#22686;&#21152;&#65292;&#25509;&#30528;&#26159;&#38271;&#26102;&#38388;&#30340;&#24179;&#21488;&#26399;&#65292;&#28982;&#21518;&#26159;&#21478;&#19968;&#20010;&#22686;&#21152;&#12290;&#30456;&#21453;&#65292;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#19981;&#20986;&#29616;&#36825;&#31181;&#34892;&#20026;&#65292;&#20854;&#20013;&#26354;&#29575;&#21464;&#24179;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#26368;&#21518;&#20004;&#23618;&#20043;&#38388;&#30340;&#26354;&#29575;&#24046;&#24322;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#28508;&#22312;&#32534;&#30721;&#30340;&#20869;&#22312;&#32500;&#24230;&#24182;&#38750;&#24517;&#28982;&#34920;&#24449;&#26354;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;mixup&#31561;&#24120;&#35265;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#22312;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#26102;&#20135;&#29983;&#26356;&#24179;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. Moreover, we find that the intrinsic dimension of latent codes is not necessarily indicative of curvature. Finally, we observe that common regularization methods such as mixup yield flatter representations when compared to other m
&lt;/p&gt;</description></item><item><title>UMD&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#65292;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#26032;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#36873;&#25321;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.18651</link><description>&lt;p&gt;
UMD: &#26080;&#30417;&#30563;&#27169;&#22411;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (arXiv:2305.18651v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18651
&lt;/p&gt;
&lt;p&gt;
UMD&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#65292;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#26032;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#36873;&#25321;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#65288;&#29305;&#27931;&#20234;&#65289;&#25915;&#20987;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#24120;&#35265;&#23041;&#32961;&#65292;&#20854;&#20013;&#23884;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#31867;&#21035;&#30340;&#26679;&#26412;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#23545;&#25239;&#30446;&#26631;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#20998;&#31867;&#22120;&#26159;&#21542;&#36973;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#38024;&#23545;&#21333;&#19968;&#23545;&#25239;&#30446;&#26631;&#30340;&#25915;&#20987;&#35774;&#35745;&#30340;&#65288;&#20363;&#22914;&#65292;&#20840;&#23545;&#19968;&#25915;&#20987;&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#30340;&#28304;&#31867;&#21035;&#30340;&#26356;&#26222;&#36941;&#30340;X2X&#25915;&#20987;&#65292;&#27599;&#20010;&#28304;&#31867;&#21035;&#37117;&#19982;&#20219;&#24847;&#30446;&#26631;&#31867;&#21035;&#37197;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UMD&#65292;&#31532;&#19968;&#20010;&#36890;&#36807;&#32852;&#21512;&#25512;&#26029;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#26469;&#26377;&#25928;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#37327;&#24230;&#21644;&#36873;&#25321;&#19968;&#32452;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#36873;&#25321;&#30340;&#31867;&#21035;&#23545;&#26159;&#22522;&#20110;&#32852;&#21512;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15798</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#21387;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Architectural Compression of Text-to-Image Diffusion Models. (arXiv:2305.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SDMs&#65289;&#20013;&#20986;&#33394;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#32467;&#26524;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#26399;&#20851;&#20110;&#39640;&#25928;SDMs&#30340;&#30740;&#31350;&#23558;&#37325;&#28857;&#25918;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#19978;&#12290;&#19982;&#36825;&#20123;&#26041;&#21521;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#65292;&#24378;&#35843;&#20102;&#32463;&#20856;&#26550;&#26500;&#21387;&#32553;&#22312;&#36890;&#29992;T2I&#21512;&#25104;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;SDMs&#30340;U-Net&#20013;&#21024;&#38500;&#20102;&#20960;&#20010;&#27531;&#24046;&#21644;&#27880;&#24847;&#21147;&#22359;&#65292;&#20351;&#21442;&#25968;&#25968;&#37327;&#12289;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;MAC&#21644;&#24310;&#36831;&#20943;&#23569;&#20102;&#36229;&#36807;30&#65285;&#12290;&#25105;&#20204;&#22312;&#21333;&#20010;A100 GPU&#19978;&#20165;&#20351;&#29992;0.22M LAION&#23545;&#36827;&#34892;&#33976;&#39311;&#39044;&#35757;&#32451;&#65288;&#23569;&#20110;&#20840;&#20307;&#35757;&#32451;&#23545;&#30340;0.1&#65285;&#65289;&#12290;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20256;&#36882;&#30340;&#30693;&#35782;&#27169;&#20223;&#21407;&#22987;SDM&#65292;&#24182;&#22312;&#23545;&#25239;&#36739;&#22823;&#30340;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized reducing the number of sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.12025</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#29992;&#20110;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient memcapacitive physical reservoir computing system for temporal data processing. (arXiv:2305.12025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#26469;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#12290;&#29289;&#29702;&#20648;&#23618;&#21487;&#20351;&#29992;&#30913;&#26059;&#30005;&#23376;&#12289;&#21407;&#23376;&#24320;&#20851;&#32593;&#32476;&#12289;&#30789;&#20809;&#23398;&#27169;&#22359;&#12289;&#38081;&#30005;&#26230;&#20307;&#31649;&#21644;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#22791;&#30001;&#20110;&#20854;&#30005;&#38459;&#24615;&#36136;&#26412;&#36136;&#19978;&#23384;&#22312;&#33021;&#37327;&#32791;&#25955;&#38382;&#39064;&#65292;&#23548;&#33268;&#21151;&#32791;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#30005;&#23481;&#23384;&#20648;&#22120;&#35774;&#22791;&#21487;&#25552;&#20379;&#26356;&#20026;&#33021;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#36817;&#20284;&#26576;&#20123;&#30701;&#26399;&#31361;&#35302;&#21487;&#22609;&#24615;&#21151;&#33021;&#30340;&#26131;&#22833;&#29983;&#29289;&#33180;&#22522;&#36136;&#37327;&#20316;&#20026;&#20648;&#23618;&#65292;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21475;&#38899;&#25968;&#23383;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;98&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20108;&#38454;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;0.0012&#30340;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing is a highly efficient machine learning framework for processing temporal data by extracting features from the input signal and mapping them into higher dimensional spaces. Physical reservoir layers have been realized using spintronic oscillators, atomic switch networks, silicon photonic modules, ferroelectric transistors, and volatile memristors. However, these devices are intrinsically energy-dissipative due to their resistive nature, which leads to increased power consumption. Therefore, capacitive memory devices can provide a more energy-efficient approach. Here, we leverage volatile biomembrane-based memcapacitors that closely mimic certain short-term synaptic plasticity functions as reservoirs to solve classification tasks and analyze time-series data in simulation and experimentally. Our system achieves a 98% accuracy rate for spoken digit classification and a normalized mean square error of 0.0012 in a second-order non-linear regression task. Further, to demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#25506;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33104;&#36133;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#21644;&#24418;&#24335;&#31283;&#20581;&#24615;&#39564;&#35777;&#39046;&#22495;&#20013;&#65292;&#31283;&#20581;&#24615;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#22312;Lp&#33539;&#25968;&#36317;&#31163;&#20869;&#23545;&#25152;&#26377;&#36755;&#20837;&#21464;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#36890;&#24120;&#36890;&#36807;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#26469;&#25913;&#36827;&#21644;&#35780;&#20272;&#65292;&#32780;&#24456;&#23569;&#32771;&#34385;&#25968;&#23398;&#23450;&#20041;&#30340;Lp&#33539;&#25968;&#22833;&#30495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#26469;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#25239;&#31283;&#20581;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;Lp&#33539;&#25968;&#20043;&#38388;&#31283;&#20581;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21738;&#20123;Lp&#33539;&#25968;&#30340;&#22833;&#30495;&#24212;&#35813;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;FES&#24247;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35814;&#32454;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#23547;&#25214;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30340;&#33258;&#34892;&#36710;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#21487;&#30001;&#38750;&#25216;&#26415;&#20154;&#21592;&#20351;&#29992;&#32780;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#20256;&#24863;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.09986</link><description>&lt;p&gt;
&#23454;&#29616;AI&#25511;&#21046;&#30340;FES&#36816;&#21160;&#24247;&#22797;&#65306;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards AI-controlled FES-restoration of movements: Learning cycling stimulation pattern with reinforcement learning. (arXiv:2303.09986v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;FES&#24247;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35814;&#32454;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#23547;&#25214;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30340;&#33258;&#34892;&#36710;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#21487;&#30001;&#38750;&#25216;&#26415;&#20154;&#21592;&#20351;&#29992;&#32780;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30005;&#21050;&#28608;&#65288;FES&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#20854;&#20182;&#24247;&#22797;&#35774;&#22791;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290; FES&#24490;&#29615;&#26159;&#24247;&#22797;&#27835;&#30103;&#20013;&#24120;&#29992;&#30340;FES&#24212;&#29992;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#21050;&#28608;&#33151;&#37096;&#32908;&#32905;&#20197;&#29305;&#23450;&#27169;&#24335;&#36827;&#34892;&#12290; &#36866;&#24403;&#30340;&#27169;&#24335;&#22240;&#20154;&#32780;&#24322;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#24182;&#23545;&#20010;&#20307;&#29992;&#25143;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#23547;&#25214;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30828;&#20214;&#25110;&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65292;&#39318;&#20808;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#35814;&#32454;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#25214;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#24335;&#65292;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;&#26500;&#24314;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#33258;&#21160;&#33050;&#26412;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#30001;&#38750;&#25216;&#26415;&#20154;&#21592;&#20351;&#29992;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#36153;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30495;&#23454;&#30340;&#33258;&#34892;&#36710;&#25968;&#25454;&#23545;&#27169;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290; &#25105;&#20204;&#22312;&#38745;&#27490;&#19977;&#36718;&#36710;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27169;&#25311;&#27979;&#35797;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;&#22312;&#27169;&#25311;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Functional electrical stimulation (FES) has been increasingly integrated with other rehabilitation devices, including robots. FES cycling is one of the common FES applications in rehabilitation, which is performed by stimulating leg muscles in a certain pattern. The appropriate pattern varies across individuals and requires manual tuning which can be time-consuming and challenging for the individual user. Here, we present an AI-based method for finding the patterns, which requires no extra hardware or sensors. Our method has two phases, starting with finding model-based patterns using reinforcement learning and detailed musculoskeletal models. The models, built using open-source software, can be customised through our automated script and can be therefore used by non-technical individuals without extra cost. Next, our method fine-tunes the pattern using real cycling data. We test our both in simulation and experimentally on a stationary tricycle. In the simulation test, our method can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SSBM&#65292;&#23427;&#21482;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25506;&#32034;&#20102;&#20174;&#19981;&#23436;&#25972;&#30340;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#36825;&#20026;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#24674;&#22797;&#20449;&#21495;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SSBM&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.08691</link><description>&lt;p&gt;
&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#23398;&#20064;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning to Reconstruct Signals From Binary Measurements. (arXiv:2303.08691v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SSBM&#65292;&#23427;&#21482;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25506;&#32034;&#20102;&#20174;&#19981;&#23436;&#25972;&#30340;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#36825;&#20026;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#24674;&#22797;&#20449;&#21495;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SSBM&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#20165;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#23398;&#20064;&#20449;&#21495;&#37325;&#26500;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#21644;&#31185;&#23398;&#25104;&#20687;&#20197;&#21450;&#20256;&#24863;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#32463;&#24120;&#31232;&#32570;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#27979;&#37327;&#19981;&#20165;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#65292;&#32780;&#19988;&#36824;&#34987;&#37327;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20174;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20174;&#19981;&#23436;&#25972;&#20108;&#36827;&#21046;&#25968;&#25454;&#20013;&#35782;&#21035;&#19968;&#32452;&#20449;&#21495;&#25152;&#38656;&#30340;&#27979;&#37327;&#25968;&#37327;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#23545;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#20449;&#21495;&#24674;&#22797;&#29616;&#26377;&#30028;&#38480;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;SSBM&#8221;&#65292;&#23427;&#20165;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;SSBM&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#24182;&#20248;&#20110;&#31232;&#30095;&#37325;&#26500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice, measurements are not only noisy and incomplete but also quantized. Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach, which we name SSBM, that only requires binary data for training. We demonstrate in a series of experiments with real datasets that SSBM performs on par with supervised learning and outperforms sparse reconstruction methods wit
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>ForkMerge&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#24110;&#21161;&#32531;&#35299;&#20102;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;ATL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12618</link><description>&lt;p&gt;
ForkMerge: &#32531;&#35299;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning. (arXiv:2301.12618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12618
&lt;/p&gt;
&lt;p&gt;
ForkMerge&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#24110;&#21161;&#32531;&#35299;&#20102;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;ATL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#65288;ATL&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#20250;&#23548;&#33268;&#27604;&#20165;&#23398;&#20064;&#30446;&#26631;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#26356;&#20302;&#65292;&#36825;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#24402;&#22240;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#65292;&#24182;&#19988;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24120;&#24120;&#36890;&#36807;&#21327;&#35843;&#20219;&#21153;&#26799;&#24230;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#36741;&#21161;&#30446;&#26631;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36127;&#36801;&#31227;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#25105;&#20204;&#20174;&#20248;&#21270;&#21644;&#27867;&#21270;&#35282;&#24230;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ForkMerge&#65292;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#20250;&#23450;&#26399;&#23558;&#27169;&#22411;&#20998;&#20026;&#22810;&#20010;&#20998;&#25903;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#39564;&#35777;&#38169;&#35823;&#33258;&#21160;&#25628;&#32034;&#19981;&#21516;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;&#24182;&#21160;&#24577;&#22320;&#21512;&#24182;&#25152;&#26377;&#20998;&#25903;&#26469;&#36807;&#28388;&#26377;&#23475;&#30340;&#20219;&#21153;&#21442;&#25968;&#26356;&#26032;&#12290;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;ForkMerge&#20248;&#20110;&#29616;&#26377;&#30340;ATL&#26041;&#27861;&#65292;&#24182;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce ForkMerge, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral2Spectral&#30340;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22270;&#20687;-&#20809;&#35889;&#39046;&#22495;&#20869;&#30340;&#30456;&#20284;&#24615;&#20808;&#39564;&#36890;&#36807;&#26080;&#21442;&#32771;&#26041;&#24335;&#26469;&#36741;&#21161;&#20809;&#35889;CT&#30340;&#28145;&#24230;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2210.01125</link><description>&lt;p&gt;
Spectral2Spectral: &#26080;&#21442;&#32771;&#30340;&#22270;&#20687;&#20809;&#35889;&#30456;&#20284;&#24615;&#36741;&#21161;&#20809;&#35889;CT&#28145;&#24230;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep Reconstruction without Reference. (arXiv:2210.01125v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral2Spectral&#30340;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22270;&#20687;-&#20809;&#35889;&#39046;&#22495;&#20869;&#30340;&#30456;&#20284;&#24615;&#20808;&#39564;&#36890;&#36807;&#26080;&#21442;&#32771;&#26041;&#24335;&#26469;&#36741;&#21161;&#20809;&#35889;CT&#30340;&#28145;&#24230;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#65288;PCD&#65289;&#30340;&#20809;&#35889;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#20026;&#29983;&#29289;&#21307;&#23398;&#26448;&#26009;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#29421;&#31364;&#33021;&#37327;&#21306;&#38388;&#20869;&#26377;&#38480;&#30340;&#20809;&#23376;&#25968;&#37327;&#23548;&#33268;&#22270;&#20687;&#32467;&#26524;&#30340;&#20449;&#22122;&#27604;&#36739;&#20302;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;CT&#37325;&#24314;&#30340;&#30417;&#30563;&#24335;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#24456;&#38590;&#35299;&#20915;&#65292;&#22240;&#20026;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#24102;&#26377;&#28165;&#26224;&#32467;&#26500;&#30340;&#26080;&#22122;&#22768;&#20020;&#24202;&#22270;&#20687;&#20316;&#20026;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#28145;&#24230;&#37325;&#24314;&#32593;&#32476;&#65292;&#23558;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#25968;&#25454;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#21629;&#21517;&#20026;Spectral2Spectral&#12290;&#25105;&#20204;&#30340;Spectral2Spectral&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20013;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#22270;&#20687;-&#20809;&#35889;&#39046;&#22495;&#20869;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#20808;&#39564;&#34987;&#25913;&#36827;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#36827;&#19968;&#27493;&#32422;&#26463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral computed tomography based on a photon-counting detector (PCD) attracts more and more attentions since it has the capability to provide more accurate identification and quantitative analysis for biomedical materials. The limited number of photons within narrow energy bins leads to imaging results of low signal-noise ratio. The existing supervised deep reconstruction networks for CT reconstruction are difficult to address these challenges because it is usually impossible to acquire noise-free clinical images with clear structures as references. In this paper, we propose an iterative deep reconstruction network to synergize unsupervised method and data priors into a unified framework, named as Spectral2Spectral. Our Spectral2Spectral employs an unsupervised deep training strategy to obtain high-quality images from noisy data in an end-to-end fashion. The structural similarity prior within image-spectral domain is refined as a regularization term to further constrain the network t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.04876</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Spiking Neural Networks via Minimum Description Length and Structural Stability. (arXiv:2207.04876v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#32463;&#39564;&#31639;&#27861;&#21644;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#35757;&#32451;&#21518;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#65292;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#31283;&#23450;&#24615;&#23454;&#26045;&#20102;SNN&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#23558;&#22312;SNN&#20013;&#30830;&#23450;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#25361;&#25112;&#36716;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#23450;&#37327;&#29305;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decades have witnessed an increasing interest in spiking neural networks due to their great potential of modeling time-dependent data. Many empirical algorithms and techniques have been developed. However, theoretically, it remains unknown whether and to what extent a trained spiking neural network performs well on unseen data. This work takes one step in this direction by exploiting the minimum description length principle and thus, presents an explicit generalization bound for spiking neural networks. Further, we implement the description length of SNNs through structural stability and specify the lower and upper bounds of the maximum number of stable bifurcation solutions, which convert the challenge of qualifying structural stability in SNNs into a mathematical problem with quantitative properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#21452;&#23618;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#38656;warm-start&#20063;&#21487;&#23454;&#29616;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.03397</link><description>&lt;p&gt;
&#26377;&#19979;&#23618;&#21387;&#32553;&#30340;&#21452;&#23618;&#20248;&#21270;: &#26080;warm-start&#24773;&#20917;&#19979;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start. (arXiv:2202.03397v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#21452;&#23618;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#38656;warm-start&#20063;&#21487;&#23454;&#29616;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31867;&#19968;&#33324;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#23618;&#38382;&#39064;&#26159;&#23558;&#19968;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#65292;&#19979;&#23618;&#38382;&#39064;&#26159;&#23547;&#25214;&#19968;&#20809;&#28369;&#25910;&#32553;&#26144;&#23556;&#30340;&#19981;&#21160;&#28857;&#12290;&#36825;&#31867;&#38382;&#39064;&#21253;&#25324;&#20803;&#23398;&#20064;&#12289;&#22343;&#34913;&#27169;&#22411;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#25968;&#25454;&#27745;&#26579;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#27809;&#26377;warm-start&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22914;&#20803;&#23398;&#20064;&#21644;&#22343;&#34913;&#27169;&#22411;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39034;&#24207;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse a general class of bilevel problems, in which the upper-level problem consists in the minimization of a smooth objective function and the lower-level problem is to find the fixed point of a smooth contraction map. This type of problems include instances of meta-learning, equilibrium models, hyperparameter optimization and data poisoning adversarial attacks. Several recent works have proposed algorithms which warm-start the lower-level problem, i.e. they use the previous lower-level approximate solution as a staring point for the lower-level solver. This warm-start procedure allows one to improve the sample complexity in both the stochastic and deterministic settings, achieving in some cases the order-wise optimal sample complexity. However, there are situations, e.g., meta learning and equilibrium models, in which the warm-start procedure is not well-suited or ineffective. In this work we show that without warm-start, it is still possible to achieve order-wise (near) optimal
&lt;/p&gt;</description></item></channel></rss>