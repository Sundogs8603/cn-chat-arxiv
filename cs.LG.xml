<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01103</link><description>&lt;p&gt;
&#32452;&#21512;&#29983;&#25104;&#24314;&#27169;&#65306;&#21333;&#19968;&#27169;&#22411;&#24182;&#19981;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Modeling: A Single Model is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#24040;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#20027;&#27969;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#22914;&#20309;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#20998;&#24067;&#65292;&#20351;&#24471;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#20998;&#24067;&#37096;&#20998;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#35757;&#32451;&#26102;&#23436;&#20840;&#26410;&#35265;&#30340;&#20219;&#21153;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29420;&#31435;&#30340;&#32452;&#21512;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01032</link><description>&lt;p&gt;
&#36319;&#30528;&#25105;&#37325;&#22797;&#65306;Transformer&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#27604;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Repeat After Me: Transformers are Better than State Space Models at Copying
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#22266;&#23450;&#22823;&#23567;&#28508;&#22312;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;"&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;" (GSSMs)&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;GSSMs&#22312;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#19978;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#38656;&#35201;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#22797;&#21046;&#30340;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30456;&#23545;&#20110;transformer&#27169;&#22411;&#26469;&#35828;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#23545;&#31616;&#21333;&#30340;&#23383;&#31526;&#20018;&#22797;&#21046;&#20219;&#21153;&#30340;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#30340;transformer&#21487;&#20197;&#22797;&#21046;&#25351;&#25968;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#65292;&#32780;GSSMs&#30001;&#20110;&#20854;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#29366;&#24577;&#22312;&#26681;&#26412;&#19978;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;transformer&#22312;&#38656;&#35201;&#22797;&#21046;&#19978;&#19979;&#25991;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#20248;&#20110;GSSMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31383;&#21475;&#36807;&#28388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#35821;&#20041;&#25628;&#32034;&#38382;&#39064;&#20013;&#23454;&#29616;&#39640;&#36895;&#25628;&#32034;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00943</link><description>&lt;p&gt;
&#20351;&#29992;&#31383;&#21475;&#36807;&#28388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbor Search with Window Filters
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31383;&#21475;&#36807;&#28388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#35821;&#20041;&#25628;&#32034;&#38382;&#39064;&#20013;&#23454;&#29616;&#39640;&#36895;&#25628;&#32034;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23450;&#20041;&#24182;&#30740;&#31350;&#20102;$\textit{c-&#36817;&#20284;&#31383;&#21475;&#25628;&#32034;}$&#38382;&#39064;&#65306;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#20854;&#20013;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#28857;&#37117;&#26377;&#19968;&#20010;&#25968;&#20540;&#26631;&#31614;&#65292;&#30446;&#26631;&#26159;&#22312;&#20219;&#24847;&#26631;&#31614;&#33539;&#22260;&#20869;&#25214;&#21040;&#26597;&#35810;&#28857;&#30340;&#26368;&#36817;&#37051;&#12290;&#35768;&#22810;&#35821;&#20041;&#25628;&#32034;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26377;&#26102;&#38388;&#25139;&#36807;&#28388;&#22120;&#30340;&#22270;&#20687;&#21644;&#25991;&#26723;&#25628;&#32034;&#65292;&#25110;&#24102;&#26377;&#25104;&#26412;&#36807;&#28388;&#22120;&#30340;&#20135;&#21697;&#25628;&#32034;&#65292;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22359;&#21270;&#26641;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35299;&#20915;&#20256;&#32479;c-&#36817;&#20284;&#26368;&#36817;&#37051;&#38382;&#39064;&#30340;&#32034;&#24341;&#36716;&#21270;&#20026;&#35299;&#20915;&#31383;&#21475;&#25628;&#32034;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#30340;&#26368;&#36817;&#37051;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#37197;&#22791;&#20102;&#38543;&#26426;&#26631;&#31614;&#20540;&#12289;&#23545;&#25239;&#24615;&#26500;&#24314;&#30340;&#23884;&#20837;&#20197;&#21450;&#24102;&#26377;&#30495;&#23454;&#26102;&#38388;&#25139;&#30340;&#22270;&#20687;&#25628;&#32034;&#23884;&#20837;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#39640;&#36798;75&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define and investigate the problem of $\textit{c-approximate window search}$: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\times$ speedup over existing solutions at the same level of recall.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18063</link><description>&lt;p&gt;
&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65306;&#21327;&#35843;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;Transformer&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#32467;&#26500;&#36827;&#34892;&#20102;&#30740;&#31350; - &#22914;ViT&#12289;PVT&#21644;Swin&#12290;&#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#20854;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#20154;&#20204;&#24863;&#21463;&#21040;&#20102;&#21253;&#21547;&#23616;&#37096;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#36825;&#23548;&#33268;&#22312;Transformer&#20013;&#24341;&#20837;&#21367;&#31215;&#65292;&#22914;CPVT&#21644;CvT&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#20613;&#31435;&#21494;&#22522;&#30784;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;AFNO&#12289;GFNet&#21644;Spectformer&#23454;&#29616;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#25968;&#25454;&#30340;&#19977;&#31181;&#19981;&#21516;&#35270;&#22270; - &#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#23454;&#22495;&#20809;&#35889;&#34920;&#31034;&#30340;&#26368;&#31616;&#21333;&#20840;&#23616;&#34920;&#31034; - &#36890;&#36807;Hartley&#21464;&#25442;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#31639;&#23376;&#25429;&#25417;&#23616;&#37096;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#24182;&#33719;&#24471;&#19968;&#20010;&#25552;&#20379;&#25913;&#36827;&#24615;&#33021;&#30340;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65288;SCT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#19979;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#20869;&#26144;&#23556;&#30340;&#22806;&#26144;&#23556;&#22266;&#23450;&#28857;&#30340;&#38544;&#24335;&#23548;&#25968;&#30340;&#26032;&#26041;&#27861;NSID&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11687</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#38544;&#24335;&#24494;&#20998;&#65306;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11687
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#19979;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#20869;&#26144;&#23556;&#30340;&#22806;&#26144;&#23556;&#22266;&#23450;&#28857;&#30340;&#38544;&#24335;&#23548;&#25968;&#30340;&#26032;&#26041;&#27861;NSID&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#25928;&#35745;&#31639;&#21442;&#25968;&#21270;&#19981;&#21487;&#24494;&#25910;&#32553;&#26144;&#23556;&#22266;&#23450;&#28857;&#23548;&#25968;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65306;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#12290;&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38142;&#35268;&#21017;&#19981;&#20877;&#25104;&#31435;&#12290;&#22312;Bolte&#31561;&#20154;&#65288;2022&#65289;&#26368;&#36817;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#19981;&#21487;&#24494;&#20998;ITD&#30340;&#32447;&#24615;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;ITD&#21644;AID&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;NSID&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22266;&#23450;&#28857;&#34987;&#23450;&#20041;&#20026;&#21482;&#36890;&#36807;&#38543;&#26426;&#26080;&#20559;&#20272;&#35745;&#22120;&#35775;&#38382;&#30340;&#22806;&#26144;&#23556;&#21644;&#20869;&#26144;&#23556;&#30340;&#32452;&#21512;&#26102;&#35745;&#31639;&#38544;&#24335;&#23548;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09267</link><description>&lt;p&gt;
&#28145;&#24230;&#38480;&#20215;&#35746;&#21333;&#31807;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Limit Order Book Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#23574;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#22312;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#19978;&#20132;&#26131;&#30340;&#19968;&#32452;&#24322;&#36136;&#32929;&#31080;&#30340;&#39640;&#39057;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#8220;LOBFrame&#8221;&#65292;&#19968;&#20010;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#65292;&#24182;&#23450;&#37327;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#21452;&#37325;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#32929;&#31080;&#30340;&#24494;&#35266;&#32467;&#26500;&#29305;&#24449;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39640;&#39044;&#27979;&#33021;&#21147;&#19981;&#19968;&#23450;&#23545;&#24212;&#21487;&#25805;&#20316;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;&#38480;&#20215;&#35746;&#21333;&#31807;&#29615;&#22659;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#20934;&#30830;&#39044;&#27979;&#30340;&#27010;&#29575;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07134</link><description>&lt;p&gt;
COMQ: &#19968;&#31181;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07134
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#39640;&#24230;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#27169;&#22411;&#38477;&#33267;&#20302;&#27604;&#29305;&#34920;&#31034;&#32780;&#19981;&#25439;&#23475;&#21407;&#22987;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PTQ&#31639;&#27861;&#31216;&#20026;COMQ&#65292;&#23427;&#36890;&#36807;&#20381;&#27425;&#20943;&#23567;&#36880;&#23618;&#37325;&#26500;&#35823;&#24046;&#26469;&#36827;&#34892;&#22352;&#26631;&#26041;&#21521;&#19978;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#25972;&#25968;&#37327;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#37327;&#21270;&#26435;&#37325;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#28014;&#28857;&#26631;&#37327;&#21644;&#19968;&#20010;&#25972;&#25968;&#20301;&#32534;&#30721;&#12290;&#22312;&#22266;&#23450;&#23618;&#20869;&#65292;COMQ&#23558;&#25152;&#26377;&#32553;&#25918;&#22240;&#23376;&#21644;&#20301;&#32534;&#30721;&#35270;&#20026;&#37325;&#26500;&#35823;&#24046;&#30340;&#21464;&#37327;&#12290;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#27839;&#30528;&#19968;&#20010;&#22352;&#26631;&#36724;&#25913;&#36827;&#36825;&#20010;&#38169;&#35823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#24658;&#23450;&#12290;COMQ&#26131;&#20110;&#20351;&#29992;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23427;&#21482;&#28041;&#21450;&#28857;&#20056;&#21644;&#22235;&#33293;&#20116;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
&lt;/p&gt;</description></item><item><title>DARL&#20351;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#36827;&#34892;&#33258;&#22238;&#24402;&#39044;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#34917;&#19969;&#35299;&#30721;&#22120;&#21644;&#29305;&#23450;&#22122;&#22768;&#35745;&#21010;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#19982;&#33945;&#29256;&#39044;&#27979;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05196</link><description>&lt;p&gt;
&#38477;&#22122;&#33258;&#22238;&#24402;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Denoising Autoregressive Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05196
&lt;/p&gt;
&lt;p&gt;
DARL&#20351;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#36827;&#34892;&#33258;&#22238;&#24402;&#39044;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#34917;&#19969;&#35299;&#30721;&#22120;&#21644;&#29305;&#23450;&#22122;&#22768;&#35745;&#21010;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#19982;&#33945;&#29256;&#39044;&#27979;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#30340;&#26032;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DARL&#37319;&#29992;&#20102;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#26469;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#22270;&#20687;&#34917;&#19969;&#12290;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#36827;&#34892;&#35757;&#32451;&#20250;&#24471;&#21040;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#34917;&#19969;&#35299;&#30721;&#22120;&#23558;MSE&#25439;&#22833;&#26367;&#25442;&#20026;&#25193;&#25955;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;&#22122;&#22768;&#35745;&#21010;&#21644;&#26356;&#22823;&#27169;&#22411;&#30340;&#26356;&#38271;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#20339;&#35745;&#21010;&#19982;&#26631;&#20934;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#35745;&#21010;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#23613;&#31649;&#20854;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;DARL&#22312;&#24494;&#35843;&#21327;&#35758;&#19979;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#33945;&#29256;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26631;&#24535;&#30528;&#21521;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#24863;&#30693;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;&#27169;&#22411;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05196v1 Announce Type: new  Abstract: In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23558;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;&#30340;&#29702;&#35770;&#25193;&#23637;&#21040;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#34892;&#20026;&#31574;&#30053;&#25152;&#38656;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#23376;&#21160;&#24577;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2403.02741</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#36793;&#20449;&#24687;&#30340;&#29366;&#24577;&#32422;&#26463;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
State-Constrained Zero-Sum Differential Games with One-Sided Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02741
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;&#30340;&#29702;&#35770;&#25193;&#23637;&#21040;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#34892;&#20026;&#31574;&#30053;&#25152;&#38656;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#23376;&#21160;&#24577;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#21644;&#21333;&#36793;&#20449;&#24687;&#30340;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;&#65292;&#20854;&#20013;&#30693;&#24773;&#29609;&#23478;&#65288;&#29609;&#23478;1&#65289;&#20855;&#26377;&#23545;&#20110;&#26410;&#30693;&#20110;&#19981;&#30693;&#24773;&#29609;&#23478;&#65288;&#29609;&#23478;2&#65289;&#30340;&#20998;&#31867;&#25903;&#20184;&#31867;&#22411;&#12290;&#29609;&#23478;1&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#36829;&#21453;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20182;&#30340;&#25903;&#20184;&#65292;&#32780;&#29609;&#23478;2&#30340;&#30446;&#26631;&#26159;&#35201;&#20040;&#36829;&#21453;&#29366;&#24577;&#32422;&#26463;&#65292;&#35201;&#20040;&#26368;&#22823;&#21270;&#25903;&#20184;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#26159;&#23558;Cardaliaguet&#65288;2007&#65289;&#23545;&#20110;&#27809;&#26377;&#29366;&#24577;&#32422;&#26463;&#30340;&#31867;&#20284;&#21338;&#24328;&#20215;&#20540;&#23384;&#22312;&#24615;&#21644;&#23545;&#20110;&#29609;&#23478;&#30340;&#20849;&#21516;&#20449;&#20208;&#20984;&#24615;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#30340;&#24494;&#20998;&#21338;&#24328;&#65292;&#24182;&#25512;&#23548;&#20102;&#29992;&#20110;&#35745;&#31639;&#34892;&#20026;&#31574;&#30053;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#23376;&#21160;&#24577;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02741v1 Announce Type: cross  Abstract: We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies. Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on reveal
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01306</link><description>&lt;p&gt;
ICC&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#31579;&#36873;&#30340;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38024;&#23545;&#37197;&#23545;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#30340;Web&#35268;&#27169;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#25361;&#25112;&#22312;&#37326;&#22806;&#25968;&#25454;&#38598;&#30340;&#39640;&#22122;&#22768;&#29305;&#24615;&#12290;&#26631;&#20934;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#25104;&#21151;&#21435;&#38500;&#20102;&#19981;&#21305;&#37197;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#65292;&#20294;&#20801;&#35768;&#35821;&#20041;&#30456;&#20851;&#20294;&#38750;&#24120;&#25277;&#35937;&#25110;&#20027;&#35266;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#33021;&#21147;&#26469;&#38548;&#31163;&#25552;&#20379;&#22312;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#35780;&#20272;&#27809;&#26377;&#22270;&#20687;&#21442;&#32771;&#30340;&#26631;&#39064;&#25991;&#26412;&#20197;&#34913;&#37327;&#20854;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#20379;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#34913;&#37327;&#35270;&#35273;-&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#30340;&#24378;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19982;&#20154;&#31867;&#23545;&#21333;&#35789;&#21644;&#21477;&#23376;&#32423;&#25991;&#26412;&#20855;&#20307;&#24615;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#22411;Agent Dr. Strategy&#65292;&#37197;&#22791;&#20102;Dreaming Strategy&#65292;&#23454;&#29616;&#20102;&#22312;&#26790;&#22659;&#20013;&#23398;&#20064;&#19968;&#32452;&#28508;&#22312;&#22320;&#26631;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#22320;&#26631;&#23398;&#20064;&#22320;&#26631;&#26465;&#20214;&#30340;&#39640;&#36895;&#20844;&#36335;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.18866</link><description>&lt;p&gt;
Dr. Strategy: &#20855;&#26377;&#25112;&#30053;&#26790;&#24819;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#22411;Agent
&lt;/p&gt;
&lt;p&gt;
Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#22411;Agent Dr. Strategy&#65292;&#37197;&#22791;&#20102;Dreaming Strategy&#65292;&#23454;&#29616;&#20102;&#22312;&#26790;&#22659;&#20013;&#23398;&#20064;&#19968;&#32452;&#28508;&#22312;&#22320;&#26631;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#22320;&#26631;&#23398;&#20064;&#22320;&#26631;&#26465;&#20214;&#30340;&#39640;&#36895;&#20844;&#36335;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning&#65288;MBRL&#65289;&#19968;&#30452;&#26159;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#24182;&#21019;&#24314;&#36890;&#29992;&#22411;Agent&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22686;&#24378;&#26790;&#24819;&#31574;&#30053;&#26412;&#36523;&#30340;&#21162;&#21147;&#24182;&#19981;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#35748;&#30693;&#31185;&#23398;&#35266;&#23519;&#21040;&#20154;&#31867;&#22312;&#35268;&#21010;&#26102;&#20351;&#29992;&#31354;&#38388;&#20998;&#38548;&#19982;&#24449;&#26381;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MBRL agent&#65292;&#31216;&#20026;Dr. Strategy&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Dreaming Strategy&#12290;&#25152;&#25552;&#20986;&#30340;Agent&#22312;&#26790;&#22659;&#20013;&#23454;&#29616;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20998;&#38548;&#19982;&#24449;&#26381;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18866v1 Announce Type: new  Abstract: Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can "dream better" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;INLA&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#25512;&#26029;&#29366;&#24577;&#21644;&#21442;&#25968;&#65292;&#33021;&#22815;&#20445;&#30041;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.17036</link><description>&lt;p&gt;
&#36845;&#20195;INLA&#29992;&#20110;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;INLA&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#25512;&#26029;&#29366;&#24577;&#21644;&#21442;&#25968;&#65292;&#33021;&#22815;&#20445;&#30041;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#26041;&#27861;&#20351;&#29992;&#28304;&#33258;&#24494;&#20998;&#26041;&#31243;&#30340;&#20808;&#39564;&#26465;&#20214;&#26469;&#31283;&#20581;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#21644;&#22806;&#25512;&#12290;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#22914;&#22788;&#29702;&#39640;&#32500;&#38750;&#32447;&#24615;PDE&#20808;&#39564;&#26465;&#20214;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#29366;&#24577;&#20272;&#35745;&#65292;&#20294;&#21487;&#33021;&#20250;&#22312;&#23398;&#20064;&#21442;&#25968;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#23398;&#20064;&#29366;&#24577;&#21644;&#21442;&#25968;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#25110;&#32773;&#20135;&#29983;&#38590;&#20197;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21463;&#31354;&#38388;&#32479;&#35745;&#20013;&#38598;&#25104;&#23884;&#22871;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65288;INLA&#65289;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36845;&#20195;&#32447;&#24615;&#21270;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;DA&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20135;&#29983;&#19968;&#20010;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;INLA&#26469;&#25512;&#26029;&#29366;&#24577;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#34987;&#35777;&#26126;&#21487;&#20197;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17036v1 Announce Type: cross  Abstract: Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data. Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model. This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters. Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to o
&lt;/p&gt;</description></item><item><title>&#22312;AI&#36741;&#21161;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#12289;&#23454;&#29992;&#21644;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31574;&#30053;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#25552;&#20986;&#25968;&#23398;&#27169;&#22411;&#35299;&#20915;&#20102;&#35821;&#35328;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#20986;&#38169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16858</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#26377;&#25928;&#24615;&#36890;&#36947;&#38169;&#35823;&#30340;&#23454;&#29992;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16858
&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#36741;&#21161;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#12289;&#23454;&#29992;&#21644;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31574;&#30053;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#25552;&#20986;&#25968;&#23398;&#27169;&#22411;&#35299;&#20915;&#20102;&#35821;&#35328;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#20986;&#38169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21363;&#23558;&#21040;&#26469;&#30340;AI&#36741;&#21161;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#25972;&#21512;&#35821;&#20041;&#12289;&#23454;&#29992;&#21644;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31574;&#30053;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#25972;&#21512;&#23558;&#23454;&#29616;&#20165;&#28041;&#21450;&#30456;&#20851;&#20219;&#21153;&#25968;&#25454;&#30340;&#24863;&#30693;&#12289;&#20256;&#36755;&#21644;&#22788;&#29702;&#65292;&#30830;&#20445;&#20256;&#36798;&#30340;&#20449;&#24687;&#20855;&#26377;&#21487;&#29702;&#35299;&#30340;&#12289;&#23454;&#29992;&#30340;&#35821;&#20041;&#37325;&#35201;&#24615;&#65292;&#19982;&#30446;&#26631;&#21644;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#38500;&#20102;&#20856;&#22411;&#26080;&#32447;&#36890;&#20449;&#21160;&#24577;&#24341;&#36215;&#30340;&#38169;&#35823;&#22806;&#65292;&#30001;&#20110;&#35821;&#20041;&#22788;&#29702;&#33021;&#21147;&#30340;&#38480;&#21046;&#12289;&#21457;&#23556;&#26041;&#24847;&#22270;&#21644;&#25509;&#25910;&#26041;&#35299;&#37322;&#20043;&#38388;&#30340;&#24847;&#20041;&#24046;&#24322;&#20197;&#21450;&#21457;&#23556;&#26041;&#21644;&#25509;&#25910;&#26041;&#20043;&#38388;&#30340;&#35821;&#35328;&#21644;&#30693;&#35782;&#34920;&#24449;&#24046;&#24322;&#65292;&#36824;&#21487;&#33021;&#20986;&#29616;&#21457;&#23556;&#26041;&#24847;&#22270;&#21644;&#25509;&#25910;&#26041;&#35299;&#37322;&#20043;&#38388;&#30340;&#28508;&#22312;&#22833;&#30495;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#20986;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#28304;&#20110;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38169;&#35823;&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#26377;&#25928;&#24615;&#32423;&#21035;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16858v1 Announce Type: cross  Abstract: In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and goal-oriented communication strategies becomes imperative. This integration will enable sensing, transmission, and processing of exclusively pertinent task data, ensuring conveyed information possesses understandable, pragmatic semantic significance, aligning with destination needs and goals. Without doubt, no communication is error free. Within this context, besides errors stemming from typical wireless communication dynamics, potential distortions between transmitter-intended and receiver-interpreted meanings can emerge due to limitations in semantic processing capabilities, as well as language and knowledge representation disparities between transmitters and receivers. The main contribution of this paper is two-fold. First, it proposes and details a novel mathematical modeling of errors stemming from language mismatches at both semantic and effectiveness le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;</title><link>https://arxiv.org/abs/2402.16627</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#19979;&#30340;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#29983;&#25104;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#25991;&#26412;-&#35270;&#35273;&#20851;&#31995;&#29420;&#21344;&#22320;&#34701;&#20837;&#21040;&#36870;&#36807;&#31243;&#20013;&#65292;&#24448;&#24448;&#24573;&#30053;&#20102;&#23427;&#20204;&#22312;&#27491;&#21521;&#36807;&#31243;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#27491;&#21453;&#36807;&#31243;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#21487;&#33021;&#38480;&#21046;&#20102;&#22312;&#35270;&#35273;&#21512;&#25104;&#32467;&#26524;&#20013;&#31934;&#30830;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#21253;&#21547;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#34701;&#20837;&#21040;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#19978;&#19979;&#25991;&#20256;&#25773;&#21040;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26102;&#38388;&#27493;&#65292;&#20197;&#35843;&#25972;&#23427;&#20204;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#20419;&#36827;&#36328;&#27169;&#24577;&#26465;&#20214;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#25512;&#24191;&#21040;DDPMs&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16627v2 Announce Type: cross  Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.14202</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#27604;&#36739;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Graph Transformers via Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#25442;&#22120;&#30340;&#21306;&#20998;&#33021;&#21147;&#19982;&#20301;&#32622;&#32534;&#30721;&#30340;&#36873;&#25321;&#32039;&#23494;&#30456;&#20851;&#65306;&#29992;&#20110;&#22686;&#24378;&#22522;&#26412;&#21464;&#25442;&#22120;&#19982;&#22270;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#65306;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#12290;APEs&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#21464;&#25442;&#22120;&#30340;&#36755;&#20837;&#12290;&#32780;RPEs&#21017;&#20026;&#27599;&#23545;&#33410;&#28857;&#65288;&#20363;&#22914;&#65292;&#22270;&#36317;&#31163;&#65289;&#20998;&#37197;&#19968;&#20010;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#22686;&#24378;&#27880;&#24847;&#21147;&#22359;&#12290;&#20808;&#39564;&#19978;&#65292;&#30446;&#21069;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#26356;&#26377;&#21033;&#20110;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#20301;&#32622;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;APEs&#21644;RPEs&#30340;&#22270;&#21464;&#25442;&#22120;&#22312;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20854;&#21306;&#20998;&#33021;&#21147;&#30340;&#21516;&#26102;&#20132;&#25442;APEs&#21644;RPEs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#27599;&#20010;&#20010;&#20307;&#37096;&#20998;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;&#27979;&#35797;&#39118;&#38505;&#19978;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.14145</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#39046;&#22495;&#30340;&#26412;&#22320;&#20998;&#24067;&#20559;&#31227;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14145
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#27599;&#20010;&#20010;&#20307;&#37096;&#20998;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;&#27979;&#35797;&#39118;&#38505;&#19978;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#32473;&#22312;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#25968;&#25454;&#20998;&#24067;&#38543;&#25972;&#20010;&#24635;&#20307;&#30340;&#22810;&#20010;&#37096;&#20998;&#21464;&#21270;&#30340;&#24773;&#24418;&#65292;&#24182;&#20165;&#22312;&#27599;&#20010;&#37096;&#20998;&#20869;&#23545;&#35757;&#32451;&#19982;&#27979;&#35797;&#65288;&#37096;&#32626;&#65289;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#36827;&#34892;&#23616;&#37096;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#27599;&#20010;&#20010;&#20307;&#37096;&#20998;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#25311;&#21512;&#22522;&#20110;&#20174;&#22810;&#20010;&#37096;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#27169;&#22411;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#19982;&#24120;&#29992;&#30340;&#29616;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#36215;&#23454;&#26045;&#12290;&#25105;&#20204;&#22312;&#27979;&#35797;&#39118;&#38505;&#19978;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#27867;&#21270;&#30028;&#38480;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14145v1 Announce Type: cross  Abstract: Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.13551</link><description>&lt;p&gt;
&#21465;&#20107;&#32972;&#26223;&#30340;&#22270;&#34920;&#31034;&#65306;&#36890;&#36807;&#22238;&#39038;&#24615;&#38382;&#39064;&#30340;&#36830;&#36143;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36825;&#26159;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#21465;&#36848;&#20013;&#30340;&#20010;&#21035;&#27573;&#33853;&#36890;&#24120;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#19968;&#20010;&#21517;&#20026;NARCO&#30340;&#22270;&#65292;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#12290;&#29305;&#21035;&#26159;&#65292;NARCO&#20013;&#30340;&#36793;&#28085;&#30422;&#20102;&#20004;&#20010;&#19978;&#19979;&#25991;&#29255;&#27573;&#20043;&#38388;&#30340;&#33258;&#30001;&#24418;&#24335;&#22238;&#39038;&#24615;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#24863;&#30693;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#19981;&#26029;&#20174;&#20808;&#21069;&#32972;&#26223;&#20013;&#37325;&#30003;&#30456;&#20851;&#20107;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22270;&#26159;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;LLM&#25552;&#31034;&#23454;&#20363;&#21270;&#30340;&#65292;&#22240;&#27492;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#20010;&#20851;&#20110;&#20854;&#23454;&#38469;&#25928;&#29992;&#30340;&#29420;&#29305;&#30740;&#31350;&#65292;&#36890;&#36807;&#24635;&#32467;&#35782;&#21035;&#26816;&#39564;&#36793;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24773;&#33410;&#26816;&#32034;&#36827;&#34892;&#26412;&#22320;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#20197;&#21450;&#36890;&#36807;&#38271;&#25991;&#26723;&#38382;&#31572;&#31034;&#20363;&#21270;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13551v1 Announce Type: new  Abstract: This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Expe
&lt;/p&gt;</description></item><item><title>ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13542</link><description>&lt;p&gt;
ARL2: &#36890;&#36807;&#33258;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#24615;&#26631;&#35760;&#23558;&#26816;&#32034;&#22120;&#19982;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13542
&lt;/p&gt;
&lt;p&gt;
ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#30456;&#20851;&#20449;&#24687;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#20943;&#36731;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#24320;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;LLMs&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#19982;LLMs&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARL2&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#30340;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;ARL2&#21033;&#29992;LLMs&#27880;&#37322;&#21644;&#35780;&#20998;&#30456;&#20851;&#35777;&#25454;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#24378;&#22823;&#30340;LLM&#30417;&#30563;&#20013;&#23398;&#20064;&#26816;&#32034;&#22120;&#12290;&#27492;&#22806;&#65292;ARL2&#20351;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#26469;&#31574;&#21010;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30456;&#20851;&#24615;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ARL2&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;NQ&#19978;&#25552;&#39640;&#20102;5.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;MMLU&#19978;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#38544;&#31169;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38544;&#31169;&#20998;&#26512;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#19978;&#21017;&#19981;&#20877;&#25104;&#31435;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#23457;&#35745;&#25581;&#31034;&#20102;&#24403;&#21069;&#29702;&#35770;&#38544;&#31169;&#30028;&#19982;&#23454;&#35777;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.13087</link><description>&lt;p&gt;
&#36873;&#25321;&#22914;&#20309;&#27844;&#28431;&#38544;&#31169;&#65306;&#37325;&#26032;&#23457;&#35270;&#31169;&#26377;&#36873;&#25321;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25913;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#38544;&#31169;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38544;&#31169;&#20998;&#26512;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#19978;&#21017;&#19981;&#20877;&#25104;&#31435;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#23457;&#35745;&#25581;&#31034;&#20102;&#24403;&#21069;&#29702;&#35770;&#38544;&#31169;&#30028;&#19982;&#23454;&#35777;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;(DP)&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#28041;&#21450;&#20174;&#20960;&#20010;&#36816;&#34892;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#36807;&#31243;&#12290;&#19982;&#35768;&#22810;&#31169;&#26377;&#31639;&#27861;&#65288;&#21253;&#25324;&#26222;&#36941;&#23384;&#22312;&#30340;DP-SGD&#65289;&#19981;&#21516;&#65292;&#35843;&#25972;&#30340;&#38544;&#31169;&#24433;&#21709;&#20173;&#28982;&#19981;&#22815;&#20102;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31169;&#26377;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#35843;&#25972;&#36807;&#31243;&#65292;&#28982;&#32780;&#19968;&#20010;&#26681;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#31169;&#30028;&#26159;&#21542;&#32039;&#23494;&#65311;&#26412;&#25991;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#31215;&#26497;&#21644;&#28040;&#26497;&#30340;&#31572;&#26696;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#30740;&#31350;&#35777;&#23454;&#20102;&#24403;&#21069;&#30340;&#38544;&#31169;&#20998;&#26512;&#22312;&#19968;&#33324;&#24847;&#20041;&#19978;&#30830;&#23454;&#26159;&#32039;&#23494;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#19987;&#38376;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#26102;&#65292;&#36825;&#31181;&#32039;&#23494;&#24615;&#21017;&#19981;&#20877;&#25104;&#31435;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#35843;&#25972;&#36807;&#31243;&#36827;&#34892;&#38544;&#31169;&#23457;&#35745;&#26469;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#24403;&#21069;&#29702;&#35770;&#38544;&#31169;&#30028;&#19982;&#23454;&#35777;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13087v1 Announce Type: new  Abstract: We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirica
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13006</link><description>&lt;p&gt;
&#25506;&#31350;&#27169;&#22411;&#19981;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Instability on Explanations and Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13006
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#65292;&#28982;&#32780;&#65292;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#12289;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#25197;&#26354;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#22312;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#34987;&#20840;&#38754;&#35780;&#20272;&#65292;&#22240;&#27492;&#24456;&#38590;&#35780;&#20272;&#29305;&#23450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#23581;&#35797;&#20026;&#35299;&#37322;&#21019;&#24314;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#20294;&#27809;&#26377;&#20154;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#20043;&#38388;&#30340;&#29616;&#26377;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25512;&#26029;&#26102;&#24341;&#20837;&#22122;&#22768;&#26469;&#20154;&#20026;&#27169;&#25311;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25554;&#20837;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#24182;&#27979;&#37327;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#28982;&#32780;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#19981;&#30830;&#23450;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#35299;&#37322;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.12479</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20462;&#21098;&#32593;&#32476;&#26159;&#19968;&#20010;&#22909;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In deep reinforcement learning, a pruned network is a good network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26377;&#25928;&#21033;&#29992;&#20854;&#32593;&#32476;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#20248;&#21183;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#24182;&#35777;&#26126;&#36880;&#28176;&#21098;&#26525;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#12290;&#36825;&#23548;&#33268;&#32593;&#32476;&#27604;&#20256;&#32479;&#32593;&#32476;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#34920;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#65292;&#20165;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.12423</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#39046;&#22495;&#65292;Denoising Diffusion Models (DDMs) &#30340;&#24341;&#20837;&#26085;&#30410;&#22686;&#22810;&#65292;&#20026;&#21512;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#25552;&#20379;&#20102;&#24040;&#22823;&#20215;&#20540;&#12290;&#23613;&#31649;&#23427;&#20204;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#30340;&#35821;&#20041;&#33021;&#21147;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#25511;&#21046;&#21512;&#25104;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21463;&#22270;&#20687;&#21512;&#25104;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20923;&#32467;&#30340;TTS&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#30001;DDM&#21435;&#22122;&#22120;&#30340;&#28508;&#31354;&#38388;&#28608;&#27963;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#31354;&#38388;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#27010;&#36848;&#20102;&#33509;&#24178;&#26597;&#25214;&#20854;&#20013;&#35821;&#20041;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#29616;&#25104;&#38899;&#39057;&#32534;&#36753;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#32534;&#36753;&#21518;&#38899;&#39057;&#30340;&#35821;&#20041;&#21644;&#22768;&#23398;&#29305;&#36136;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#34917;&#20805;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10693</link><description>&lt;p&gt;
&#25506;&#32034;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20197;&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Precision and Recall to assess the quality and diversity of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama-2&#21644;Mistral&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#28857;&#26159;&#23558;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36827;&#34892;&#32454;&#33268;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24320;&#25918;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20934;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#27169;&#22411;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#22522;&#20110;&#20998;&#24067;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20026;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#26041;&#38754;&#38754;&#20020;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
&lt;/p&gt;</description></item><item><title>&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.10360</link><description>&lt;p&gt;
&#23398;&#20064;&#24615;&#26159;&#19968;&#31181;&#32039;&#20945;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learnability is a Compact Property
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10360
&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23398;&#20064;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65306;&#21508;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#25110;&#32773;&#19982;&#26631;&#20934;&#38598;&#21512;&#35770;ZFC&#20844;&#29702;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#19981;&#26159;&#20855;&#26377;&#26377;&#38480;&#29305;&#24615;&#30340;&#23646;&#24615;&#65306;&#38750;&#27491;&#24335;&#22320;&#35828;&#65292;&#23427;&#19981;&#33021;&#36890;&#36807;&#26816;&#26597;&#38382;&#39064;&#30340;&#26377;&#38480;&#25237;&#24433;&#26469;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;D-GD&#65289;&#25552;&#20986;&#20102;&#39318;&#20010;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#25143;&#37325;&#24314;&#20854;&#37051;&#22495;&#20043;&#22806;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10001</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Privacy Attacks in Decentralized Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;D-GD&#65289;&#25552;&#20986;&#20102;&#39318;&#20010;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#25143;&#37325;&#24314;&#20854;&#37051;&#22495;&#20043;&#22806;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;D-GD&#65289;&#20801;&#35768;&#19968;&#32452;&#29992;&#25143;&#22312;&#32593;&#32476;&#22270;&#20013;&#36890;&#36807;&#36845;&#20195;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#19982;&#20854;&#37051;&#23621;&#21512;&#20316;&#23398;&#20064;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#38750;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#30340;&#32570;&#22833;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#26080;&#27861;&#25512;&#26029;&#20986;&#20851;&#20110;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#30340;&#31934;&#30830;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;D-GD&#30340;&#25915;&#20987;&#65292;&#20351;&#19968;&#20010;&#29992;&#25143;&#65288;&#25110;&#19968;&#32452;&#29992;&#25143;&#65289;&#33021;&#22815;&#37325;&#24314;&#20854;&#37051;&#22495;&#20043;&#22806;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#20256;&#38395;&#24179;&#22343;&#21327;&#35758;&#30340;&#37325;&#24314;&#25915;&#20987;&#65292;&#28982;&#21518;&#23558;&#20854;&#25193;&#23637;&#20197;&#22788;&#29702;D-GD&#25552;&#20986;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#22270;&#21644;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#21333;&#20010;&#25110;&#23569;&#25968;&#25915;&#20987;&#32773;&#25152;&#23041;&#32961;&#21040;&#30340;&#29992;&#25143;&#25968;&#37327;&#36890;&#24120;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#26041;&#26696;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10001v1 Announce Type: new  Abstract: Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08845</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#36890;&#36807;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAMs&#65289;&#36890;&#36807;&#25200;&#21160;&#27979;&#35797;&#26469;&#27979;&#37327;&#27599;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#25200;&#21160;&#19979;&#30340;&#39044;&#27979;&#24046;&#24322;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#24449;&#30340;&#39044;&#27979;&#21464;&#21270;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25200;&#21160;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#22686;&#24378;FAMs&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#36215;&#21040;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65288;FANS&#65289;&#65292;&#36890;&#36807;&#28041;&#21450;&#20004;&#20010;&#38454;&#27573;&#65288;&#20107;&#23454;&#24615;&#21644;&#24178;&#39044;&#24615;&#65289;&#30340;&#25200;&#21160;&#27979;&#35797;&#35745;&#31639;PNS&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;&#20102;&#29983;&#25104;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08672</link><description>&lt;p&gt;
&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Assessment and Selection under Temporal Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#24403;&#21069;&#26102;&#26399;&#21644;&#21382;&#21490;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#30693;&#21644;&#21487;&#33021;&#20219;&#24847;&#30340;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#32473;&#23450;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#36825;&#31181;&#31574;&#30053;&#36824;&#36890;&#36807;&#20272;&#35745;&#20004;&#20010;&#20505;&#36873;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#35823;&#24046;&#24046;&#24322;&#26469;&#26041;&#20415;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20004;&#20004;&#27604;&#36739;&#25972;&#21512;&#21040;&#21333;&#22330;&#28120;&#27760;&#36187;&#20013;&#65292;&#20174;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#27169;&#22411;&#36873;&#25321;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#23545;&#25968;&#25454;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08609</link><description>&lt;p&gt;
&#19987;&#23478;&#32452;&#21512;&#35299;&#38145;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Experts Unlock Parameter Scaling for Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#65288;&#33258;&#25105;&#65289;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23454;&#35777;&#32553;&#25918;&#23450;&#24459;&#39044;&#27979;&#30340;&#65306;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35268;&#27169;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23547;&#25214;&#31867;&#20284;&#30340;&#32553;&#25918;&#23450;&#24459;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#22686;&#21152;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#24448;&#24448;&#20250;&#25439;&#23475;&#20854;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#19987;&#23478;&#32452;&#21512;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#29305;&#21035;&#26159;&#36719;MoE&#65288;Puigcerver&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#21152;&#20197;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32423;&#23618;&#32423;&#26631;&#31614;&#31354;&#38388;&#20013;&#35748;&#35777;&#22270;&#20687;&#20687;&#32032;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#32463;&#36807;&#35748;&#35777;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.08400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#30340;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32423;&#23618;&#32423;&#26631;&#31614;&#31354;&#38388;&#20013;&#35748;&#35777;&#22270;&#20687;&#20687;&#32032;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#32463;&#36807;&#35748;&#35777;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#30340;&#35748;&#35777;&#26041;&#27861;&#26159;&#22312;&#39044;&#23450;&#20041;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#38598;&#19978;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#26356;&#26222;&#36941;&#19988;&#23454;&#29992;&#30340;&#35774;&#32622;&#65292;&#21363;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#35748;&#35777;&#21487;&#20197;&#22312;&#30001;&#32454;&#21040;&#31895;&#30340;&#22810;&#32423;&#23618;&#32423;&#26631;&#31614;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;&#19982;&#32463;&#20856;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#32452;&#20214;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#25918;&#26494;&#20102;&#35748;&#35777;&#21040;&#23618;&#32423;&#20013;&#30340;&#26356;&#31895;&#31890;&#24230;&#30340;&#32423;&#21035;&#12290;&#36825;&#31181;&#25918;&#26494;&#38477;&#20302;&#20102;&#25918;&#24323;&#29575;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#32463;&#36807;&#35748;&#35777;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25968;&#23398;&#22320;&#24418;&#24335;&#21270;&#20102;&#38382;&#39064;&#35774;&#32622;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#31639;&#27861;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#65292;&#23427;&#21487;&#20197;&#22312;&#23618;&#32423;&#20013;&#23545;&#22270;&#20687;&#20687;&#32032;&#36827;&#34892;&#35748;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#30340;&#27491;&#30830;&#24615;&#12290;&#30001;&#20110;&#35748;&#35777;&#30340;&#20934;&#30830;&#24230;&#22312;&#36941;&#21382;&#26102;&#19981;&#32771;&#34385;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common certification methods operate on a flat pre-defined set of fine-grained classes. In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation. In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels. Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy. This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account when traversing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#30340;&#24046;&#20998;&#38544;&#31169;&#21464;&#31181;&#25512;&#23548;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#33410;&#28857;&#20043;&#38388;&#30340;&#20843;&#21350;&#31639;&#27861;&#30456;&#27604;&#65292;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#26356;&#33021;&#25552;&#20379;&#36739;&#22909;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07471</link><description>&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#28216;&#36208;&#30340;&#24046;&#20998;&#38544;&#31169;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decentralized Learning with Random Walks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07471
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#30340;&#24046;&#20998;&#38544;&#31169;&#21464;&#31181;&#25512;&#23548;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#33410;&#28857;&#20043;&#38388;&#30340;&#20843;&#21350;&#31639;&#27861;&#30456;&#27604;&#65292;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#26356;&#33021;&#25552;&#20379;&#36739;&#22909;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#22323;&#30340;&#26234;&#33021;&#25163;&#26426;&#21462;&#32780;&#20195;&#20043;&#30340;&#26159;&#20256;&#32479;&#31227;&#21160;&#30005;&#35805;&#65292;&#36825;&#26159;&#29616;&#20195;&#31185;&#25216;&#21457;&#23637;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empir
&lt;/p&gt;</description></item><item><title>PASOA&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#31243;&#24207;&#65292;&#36890;&#36807;&#25552;&#20379;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#20272;&#35745;&#65292;&#21516;&#26102;&#25191;&#34892;&#39034;&#24207;&#35774;&#35745;&#20248;&#21270;&#21644;&#21442;&#25968;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; stochastic optimization &#21644; tempered SMC &#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#30340;&#26368;&#20248;&#35774;&#35745;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.07160</link><description>&lt;p&gt;
PASOA-&#22522;&#20110;&#31890;&#23376;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#33258;&#36866;&#24212;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PASOA- PArticle baSed Bayesian Optimal Adaptive design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07160
&lt;/p&gt;
&lt;p&gt;
PASOA&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#31243;&#24207;&#65292;&#36890;&#36807;&#25552;&#20379;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#20272;&#35745;&#65292;&#21516;&#26102;&#25191;&#34892;&#39034;&#24207;&#35774;&#35745;&#20248;&#21270;&#21644;&#21442;&#25968;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; stochastic optimization &#21644; tempered SMC &#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#30340;&#26368;&#20248;&#35774;&#35745;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASOA&#30340;&#26032;&#31243;&#24207;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#65292;&#36890;&#36807;&#21516;&#26102;&#25552;&#20379;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#20272;&#35745;&#26469;&#25191;&#34892;&#39034;&#24207;&#35774;&#35745;&#20248;&#21270;&#12290;&#39034;&#24207;&#35774;&#35745;&#36807;&#31243;&#36890;&#36807;&#23545;&#27604;&#20272;&#35745;&#21407;&#21017;&#36827;&#34892;&#65292;&#20351;&#29992;&#38543;&#26426;&#20248;&#21270;&#21644;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#65288;SMC&#65289;&#37319;&#26679;&#22120;&#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65288;EIG&#65289;&#12290;&#30001;&#20110;&#36830;&#32493;&#21518;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#36234;&#22823;&#65292;&#33719;&#24471;&#30340;&#20449;&#24687;&#22686;&#30410;&#36234;&#22823;&#65292;&#22240;&#27492;&#36825;&#20010;EIG&#30446;&#26631;&#21487;&#33021;&#20250;&#24694;&#21270;&#32463;&#20856;SMC&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#28201;&#24230;&#35843;&#33410;&#65292;&#26082;&#21487;&#20197;&#33719;&#24471;&#22823;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#21448;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;SMC&#37319;&#26679;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#23545;&#24615;&#33021;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#31181;&#38543;&#26426;&#20248;&#21270;&#21644;&#28201;&#24230;&#35843;&#33410;&#30340;&#26032;&#39062;&#32452;&#21512;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#35774;&#35745;&#20248;&#21270;&#21644;&#21442;&#25968;&#25512;&#26029;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#26368;&#20248;&#35774;&#35745;&#20272;&#35745;&#37327;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#35745;&#31639;&#39044;&#31639;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#22320;&#20248;&#21270;&#20102;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#32622;&#20449;&#21306;&#38388;&#38271;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07131</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#32479;&#35745;&#25512;&#26029;&#30340;&#37325;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resampling methods for Private Statistical Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07131
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#32622;&#20449;&#21306;&#38388;&#38271;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#20998;&#21306;&#19978;&#31169;&#19979;&#35745;&#31639;&#22810;&#20010;&#8220;&#23567;&#8221;bootstrap&#30340;&#32467;&#26524;&#30340;&#20013;&#20301;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#24471;&#21040;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#28176;&#36827;&#35206;&#30422;&#35823;&#24046;&#19978;&#30028;&#12290;&#23545;&#20110;&#22266;&#23450;&#30340;&#24046;&#20998;&#38544;&#31169;&#21442;&#25968;&#949;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#22823;&#23567;n&#19978;&#30340;&#35823;&#24046;&#29575;&#19982;&#38750;&#31169;&#26377;bootstrap&#30456;&#24403;&#65292;&#21482;&#26159;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#22343;&#20540;&#20272;&#35745;&#12289;&#20013;&#20301;&#25968;&#20272;&#35745;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#32553;&#30701;&#65288;&#22823;&#32422;10&#20493;&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.07107</link><description>&lt;p&gt;
&#32034;crates&#24576;&#30097;&#30340;&#22238;&#22768;&#65306;&#22312;&#26657;&#20934;&#30340;&#35777;&#25454;&#22686;&#24378;&#23398;&#20064;&#20013;&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#28041;&#21450;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$&#26088;&#22312;&#35299;&#20915;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20998;&#21035;&#20272;&#35745;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#23427;&#23558;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#19982;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#30340;$\textit{&#20840;&#23616;}$&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#31616;&#21333;&#26041;&#24046;&#30340;$\textit{&#23616;&#37096;}$&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#20197;&#21450;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19968;&#22871;&#23567;&#22411;&#21270;&#30340;Atari&#28216;&#25103;&#65288;&#21363;MinAtar&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;CEQR-DQN&#22312;&#24471;&#20998;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#31867;&#20284;&#30340;&#29616;&#26377;&#26694;&#26550;&#12290;&#23427;&#33021;&#22815;&#20005;&#35880;&#22320;&#22788;&#29702;&#22806;&#37096;&#25968;&#25454;&#35266;&#27979;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.06737</link><description>&lt;p&gt;
ExGRG: &#29992;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;
&lt;/p&gt;
&lt;p&gt;
ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26631;&#27880;&#26631;&#31614;&#32780;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20869;&#23884;&#20449;&#21495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SSL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36890;&#36807;&#30452;&#35266;&#30340;&#25968;&#25454;&#22686;&#24378;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#22686;&#24378;&#25805;&#20316;&#25913;&#21464;&#20102;&#35821;&#20041;&#24182;&#21576;&#29616;&#20986;&#21453;&#30452;&#35266;&#30340;&#24615;&#36136;&#12290;&#38024;&#23545;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#65288;ExGRG&#65289;&#65292;&#20197;&#21462;&#20195;&#20165;&#20381;&#38752;&#20256;&#32479;&#30340;&#22522;&#20110;&#22686;&#24378;&#30340;&#38544;&#24335;&#20851;&#31995;&#22270;&#12290;ExGRG&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#30446;&#26631;&#20013;&#65292;&#20511;&#37492;&#20102;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;E&#27493;&#39588;&#28041;&#21450;&#20851;&#31995;&#22270;&#30340;&#29983;&#25104;&#65292;&#20197;&#35782;&#21035;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
&lt;/p&gt;</description></item><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperDistill&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#31616;&#24418;&#24577;&#26465;&#20214;&#36229;&#32593;&#32476;&#65292;&#21487;&#22312;&#35757;&#32451;&#21644;&#26410;&#30693;&#27979;&#35797;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#19982;&#36890;&#29992;&#30340;transformers&#31574;&#30053;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.06570</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#31616;&#24418;&#24577;&#26465;&#20214;&#36229;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#29992;&#24418;&#24577;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperDistill&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#31616;&#24418;&#24577;&#26465;&#20214;&#36229;&#32593;&#32476;&#65292;&#21487;&#22312;&#35757;&#32451;&#21644;&#26410;&#30693;&#27979;&#35797;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#19982;&#36890;&#29992;&#30340;transformers&#31574;&#30053;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#20043;&#38388;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#23545;&#26410;&#30693;&#24418;&#24577;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#36890;&#29992;&#31574;&#30053;&#38656;&#35201;&#20687;transformers&#65288;TF&#65289;&#36825;&#26679;&#20855;&#26377;&#36739;&#22823;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#22797;&#26434;&#26550;&#26500;&#65292;&#32780;&#27604;&#36739;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21017;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#22312;&#25512;&#29702;&#26102;&#26082;&#33021;&#36798;&#21040;&#20687;TF&#19968;&#26679;&#22909;&#30340;&#24615;&#33021;&#65292;&#21448;&#33021;&#20855;&#26377;&#20687;MLP&#19968;&#26679;&#30340;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDistill&#12290;&#23427;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#20010;&#24418;&#24577;&#26465;&#20214;&#30340;&#36229;&#32593;&#32476;&#65288;HN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;MLP&#31574;&#30053;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#23545;&#20110;&#25104;&#21151;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#30340;&#31574;&#30053;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;UNIMAL&#19978;&#65292;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#31181;&#19981;&#21516;&#24418;&#24577;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;HyperDistill&#22312;&#35757;&#32451;&#21644;&#26410;&#30693;&#27979;&#35797;&#26426;&#22120;&#20154;&#19978;&#37117;&#33021;&#21644;&#36890;&#29992;&#30340;TF&#25945;&#24072;&#31574;&#30053;&#19968;&#26679;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#23558;&#27169;&#22411;&#23610;&#23544;&#20943;&#23567;&#20102;6-14&#20493;&#65292;&#35745;&#31639;&#25104;&#26412;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20943;&#23567;&#20102;67-160&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#21518;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#36890;&#29992;&#21270;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#22312;&#36866;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04980</link><description>&lt;p&gt;
&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#21518;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of feature learning in two-layer networks after one gradient-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#21518;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#36890;&#29992;&#21270;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#22312;&#36866;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#22312;&#20351;&#29992;&#21333;&#19968;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#35757;&#32451;&#21518;&#22914;&#20309;&#25913;&#36827;&#26680;&#24515;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#20511;&#21161;&#20110;&#65288;Ba et al., 2022&#65289;&#19982;&#38750;&#32447;&#24615;&#23574;&#23792;&#30697;&#38453;&#27169;&#22411;&#30340;&#20851;&#32852;&#20197;&#21450;&#23545;&#39640;&#26031;&#27867;&#21270;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;Dandi et al., 2023&#65289;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#25968;$n$&#12289;&#23485;&#24230;$p$&#21644;&#36755;&#20837;&#32500;&#24230;$d$&#25104;&#27604;&#20363;&#22686;&#38271;&#30340;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#19968;&#33268;&#24615;&#35823;&#24046;&#25551;&#36848;&#12290;&#25105;&#20204;&#20934;&#30830;&#22320;&#21051;&#30011;&#20102;&#36866;&#24212;&#25968;&#25454;&#23545;&#20110;&#32593;&#32476;&#22312;&#26799;&#24230;&#26041;&#21521;&#19978;&#39640;&#25928;&#23398;&#20064;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#65292;&#32593;&#32476;&#21482;&#33021;&#34920;&#36798;&#32447;&#24615;&#20989;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#22312;&#22823;&#23398;&#20064;&#29575;$\eta=\Theta_{d}(d)$&#30340;&#24773;&#20917;&#19979;&#29305;&#24449;&#23398;&#20064;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#39318;&#20010;&#20934;&#30830;&#25551;&#36848;&#65292;&#36229;&#36234;&#20102;&#26680;&#24515;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\eta=\Theta_{d}(d)$, beyond
&lt;/p&gt;</description></item><item><title>QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04396</link><description>&lt;p&gt;
QuIP#: &#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#36827;&#34892;&#26356;&#22909;&#30340;LLM&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04396
&lt;/p&gt;
&lt;p&gt;
QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#36890;&#36807;&#23558;LLM&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26469;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuIP#&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#26435;&#37325;&#30340;PTQ&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;($\le$ 4&#27604;&#29305;&#27599;&#20010;&#26435;&#37325;)&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;QuIP#&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#24503;&#21464;&#25442;&#25913;&#36827;&#20102;QuIP&#20013;&#30340;&#38750;&#30456;&#24178;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;QuIP#&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30456;&#24178;&#26435;&#37325;&#20855;&#26377;&#30340;&#29699;&#24418;&#20122;&#39640;&#26031;&#20998;&#24067;&#29305;&#24615;&#65306;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20110;&#39640;&#24230;&#23545;&#31216;$E_8$&#26684;&#20070;&#30340;&#30828;&#20214;&#39640;&#25928;&#20195;&#30721;&#20070;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;8&#32500;&#21333;&#20301;&#29699;&#35013;&#22635;&#12290;&#31532;&#19977;&#65292;QuIP#&#20351;&#29992;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#21407;&#22987;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;QuIP#&#20248;&#20110;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#26032;&#30340;PTQ&#25193;&#23637;&#34892;&#20026;&#65292;&#24182;&#25903;&#25345;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21435;&#25481;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21527;&#65311;&#19968;&#20010;&#20108;&#38454;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03496
&lt;/p&gt;
&lt;p&gt;
&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#22914;Adam(W)&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65288;&#22914;transformers&#65289;&#30340;&#40664;&#35748;&#35757;&#32451;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#23545;&#35282;&#20808;&#39564;&#22522;&#20110;&#26799;&#24230;&#22806;&#31215;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21152;&#20837;&#21040;&#21442;&#25968;&#26356;&#26032;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#36817;&#20284;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#20294;&#24179;&#26041;&#26681;&#34920;&#31034;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21435;&#25481;&#24179;&#26041;&#26681;&#21518;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#34892;&#20026;&#22914;&#20309;&#21464;&#21270;&#65292;&#21363;&#21152;&#24378;&#23427;&#20204;&#30340;&#20108;&#38454;&#21160;&#26426;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21435;&#25481;&#24179;&#26041;&#26681;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#32553;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22312;transformers&#19978;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20108;&#38454;&#35282;&#24230;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;&#23545;&#35282;&#20808;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#22909;&#22788;&#12290;&#19982;&#20687;Shampoo&#36825;&#26679;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#23545;&#24212;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#30697;&#38453;&#24179;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02368</link><description>&lt;p&gt;
&#35745;&#26102;&#22120;: &#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Timer: Transformers for Time Series Analysis at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#27169;&#22411;&#21487;&#33021;&#36935;&#21040;&#24615;&#33021;&#29942;&#39048;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#20013;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#39281;&#21644;&#32780;&#38544;&#34109;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;&#22823;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21462;&#24471;&#20102;&#25345;&#32493;&#30340;&#36827;&#23637;&#65292;&#22312;&#23569;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#26041;&#38754;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#25913;&#21464;&#30446;&#21069;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23567;&#27169;&#22411;&#30340;&#20570;&#27861;&#65292;&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#21253;&#21547;10&#20159;&#20010;&#26102;&#38388;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#32479;&#19968;&#20026;&#21333;&#24207;&#21015;&#24207;&#21015;&#65288;S3&#65289;&#26684;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#38754;&#21521;LTSM&#30340;GPT&#39118;&#26684;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02328</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#21450;&#20854;&#22312;&#20998;&#25903;&#23450;&#30028;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design using neural networks with applications to branch-and-cut
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26159;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#19968;&#31867;&#31639;&#27861;&#20013;&#36873;&#25321;&#22312;&#26576;&#20010;&#65288;&#26410;&#30693;&#65289;&#38382;&#39064;&#23454;&#20363;&#20998;&#24067;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#30340;&#33539;&#20363;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24605;&#36335;&#65292;&#21363;&#19981;&#20165;&#20165;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#30340;&#21333;&#19968;&#31639;&#27861;&#65292;&#32780;&#26159;&#20801;&#35768;&#26681;&#25454;&#38382;&#39064;&#23454;&#20363;&#36873;&#25321;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38382;&#39064;&#23454;&#20363;&#26679;&#26412;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#38382;&#39064;&#23454;&#20363;&#26144;&#23556;&#21040;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#24605;&#36335;&#24418;&#24335;&#21270;&#65292;&#24182;&#26681;&#25454;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#24037;&#20316;&#65292;&#25512;&#23548;&#20986;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#20005;&#26684;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#21040;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#20197;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\em for that instance}. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.02322</link><description>&lt;p&gt;
&#21160;&#24577;&#22686;&#37327;&#20248;&#21270;&#29992;&#20110;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dynamic Incremental Optimization for Best Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#25915;&#20987;&#36825;&#20010;&#38750;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#22522;&#20110;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#20887;&#20313;&#35745;&#31639;&#24182;&#25913;&#36827;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;PINNs&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;NNCG&#24182;&#20248;&#21270;&#20102;PINN&#24615;&#33021;&#65292;&#20026;&#35757;&#32451;PINNs&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#21644;&#26356;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01868</link><description>&lt;p&gt;
&#35757;&#32451;PINNs&#30340;&#25361;&#25112;&#65306;&#20174;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#35282;&#24230;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Challenges in Training PINNs: A Loss Landscape Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;PINNs&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;NNCG&#24182;&#20248;&#21270;&#20102;PINN&#24615;&#33021;&#65292;&#20026;&#35757;&#32451;PINNs&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#21644;&#26356;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26368;&#23567;&#21270;PINN&#25439;&#22833;&#20989;&#25968;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#27531;&#24046;&#39033;&#20013;&#30340;&#24494;&#20998;&#31639;&#23376;&#24341;&#36215;&#30340;&#30149;&#24577;&#26465;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;Adam&#12289;L-BFGS&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;Adam+L-BFGS&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;Adam+L-BFGS&#26356;&#20248;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;NysNewton-CG&#65288;NNCG&#65289;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#30149;&#24577;&#24494;&#20998;&#31639;&#23376;&#19982;PINN&#25439;&#22833;&#20013;&#30340;&#30149;&#24577;&#26465;&#20214;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#32467;&#21512;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35757;&#32451;PINNs&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#21644;&#26356;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;PINNs&#22312;&#35299;&#20915;&#22256;&#38590;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00592</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Partial-Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36935;&#21040;&#26631;&#35760;&#27169;&#31946;&#30340;&#25968;&#25454;&#65292;&#21363;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#20026;&#30456;&#21516;&#26679;&#26412;&#20998;&#37197;&#20102;&#20914;&#31361;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20801;&#35768;&#22312;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#32463;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#21040;&#38169;&#35823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Dempster-Shafer&#29702;&#35770;&#12290;&#23545;&#20154;&#24037;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39118;&#38505;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#21644;p&#12290;</title><link>https://arxiv.org/abs/2312.07335</link><description>&lt;p&gt;
&#21160;&#37327;&#31890;&#23376;&#26368;&#22823;&#20284;&#28982;
&lt;/p&gt;
&lt;p&gt;
Momentum Particle Maximum Likelihood
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#21644;p&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#36890;&#24120;&#34987;&#37325;&#26032;&#35299;&#37322;&#20026;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#31639;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#22312;&#36825;&#20010;&#31354;&#38388;&#19978;&#36866;&#29992;&#20110;&#21512;&#36866;&#30340;&#33258;&#30001;&#33021;&#20989;&#25968;&#30340;&#22352;&#26631;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#36825;&#20010;&#35266;&#28857;&#19982;&#20174;&#26368;&#20248;&#20256;&#36755;&#21644;Wasserstein&#26799;&#24230;&#27969;&#20013;&#33719;&#24471;&#30340;&#21551;&#31034;&#30456;&#32467;&#21512;&#65292;&#21457;&#23637;&#20986;&#20102;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#27169;&#22411;&#31867;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;EM&#12290;&#21463;&#20808;&#21069;&#35770;&#25991;&#30340;&#21551;&#21457;&#65292;&#23558;&#23558;&#21160;&#37327;&#20016;&#23500;&#30340;&#20248;&#21270;&#31639;&#27861;&#35299;&#37322;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#30340;&#33258;&#30001;&#33021;&#20989;&#25968;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#21644;p&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood estimation (MLE) of latent variable models is often recast as an optimization problem over the extended space of parameters and probability distributions. For example, the Expectation Maximization (EM) algorithm can be interpreted as coordinate descent applied to a suitable free energy functional over this space. Recently, this perspective has been combined with insights from optimal transport and Wasserstein gradient flows to develop particle-based algorithms applicable to wider classes of models than standard EM.   Drawing inspiration from prior works which interpret `momentum-enriched' optimisation algorithms as discretizations of ordinary differential equations, we propose an analogous dynamical systems-inspired approach to minimizing the free energy functional over the extended space of parameters and probability distributions. The result is a dynamic system that blends elements of Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2311.12163</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Quantum Inception Score
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12163
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24320;&#22987;&#20102;&#23545;&#23427;&#20204;&#37327;&#23376;&#29256;&#26412;&#30340;&#28909;&#20999;&#25506;&#32034;&#12290;&#20026;&#20102;&#24320;&#22987;&#36825;&#19968;&#25506;&#32034;&#20043;&#26053;&#65292;&#24320;&#21457;&#19968;&#20010;&#30456;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#24456;&#37325;&#35201;&#30340;&#65307;&#22312;&#32463;&#20856;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#20415;&#26159;&#21551;&#33945;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#23427;&#23558;&#36136;&#37327;&#19982;&#29992;&#20110;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#30340;&#37327;&#23376;&#36890;&#36947;&#30340;Holevo&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#19979;&#65292;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#27604;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#30528;&#30001;&#19981;&#23545;&#31216;&#24615;&#30340;&#36164;&#28304;&#29702;&#35770;&#21644;&#32416;&#32544;&#25152;&#34920;&#24449;&#30340;&#37327;&#23376;&#30456;&#24178;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#26469;&#34920;&#24449;&#38480;&#21046;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12163v2 Announce Type: replace-cross  Abstract: Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such example is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the Holevo information of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence, characterized by the resource theory of asymmetry, and entanglement. Furthermore, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models. Finally, we apply the quantum inception score 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;(FWC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.05436</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#20844;&#24179;&#30340;&#26680;&#24515;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fair Coresets via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;(FWC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31934;&#28860;&#21644;&#26680;&#24515;&#38598;&#24050;&#25104;&#20026;&#29983;&#25104;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#36739;&#23567;&#20195;&#34920;&#24615;&#26679;&#26412;&#38598;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#31038;&#20250;&#23618;&#38754;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#24517;&#39035;&#35299;&#20915;&#23384;&#22312;&#20110;&#25968;&#25454;&#20013;&#30340;&#23376;&#32676;&#20307;&#30340;&#22266;&#26377;&#20559;&#35265;&#38382;&#39064;&#12290;&#24403;&#21069;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#30456;&#23545;&#20110;&#21407;&#22987;&#26679;&#26412;&#30340;&#23616;&#37096;&#23646;&#24615;&#26469;&#21019;&#24314;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#20294;&#20854;&#23545;&#19979;&#28216;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;&#65288;FWC&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#24515;&#38598;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#26082;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21448;&#20855;&#26377;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#32423;&#26435;&#37325;&#12290;FWC&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FWC&#30340;&#26080;&#32422;&#26463;&#29256;&#26412;&#31561;&#20215;&#20110;&#36890;&#24120;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FWC&#30340;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. Current approaches create fair synthetic representative samples by optimizing local properties relative to the original samples, but their effect on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#28508;&#22312;&#29366;&#24577;&#26469;&#23398;&#20064;&#20010;&#20307;&#29305;&#23450;&#30340;&#35745;&#25968;&#36807;&#31243;&#24378;&#24230;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#22312;&#31614;&#21517;&#31354;&#38388;&#20013;&#32447;&#24615;&#21270;&#27169;&#22411;&#65292;&#24471;&#21040;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#30340;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#23545;&#37329;&#34701;&#12289;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#39135;&#21697;&#20379;&#24212;&#38142;&#31649;&#29702;&#31561;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.17077</link><description>&lt;p&gt;
&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#19982;&#25511;&#21046;&#28508;&#22312;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamical Survival Analysis with Controlled Latent States. (arXiv:2401.17077v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#28508;&#22312;&#29366;&#24577;&#26469;&#23398;&#20064;&#20010;&#20307;&#29305;&#23450;&#30340;&#35745;&#25968;&#36807;&#31243;&#24378;&#24230;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#22312;&#31614;&#21517;&#31354;&#38388;&#20013;&#32447;&#24615;&#21270;&#27169;&#22411;&#65292;&#24471;&#21040;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#30340;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#23545;&#37329;&#34701;&#12289;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#39135;&#21697;&#20379;&#24212;&#38142;&#31649;&#29702;&#31561;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#19968;&#32452;&#38745;&#24577;&#21464;&#37327;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#20010;&#20307;&#29305;&#23450;&#30340;&#35745;&#25968;&#36807;&#31243;&#24378;&#24230;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#20013;&#24378;&#24230;&#26159;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#26469;&#35774;&#35745;&#19968;&#20010;&#31070;&#32463;&#20272;&#35745;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36275;&#22815;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#31614;&#21517;&#31354;&#38388;&#20013;&#32447;&#24615;&#21270;&#65292;&#24471;&#21040;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#30340;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CoxSig&#12290;&#25105;&#20204;&#20026;&#36825;&#20004;&#31181;&#20272;&#35745;&#22120;&#25552;&#20379;&#29702;&#35770;&#23398;&#20064;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#37329;&#34701;&#12289;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#39135;&#21697;&#20379;&#24212;&#38142;&#31649;&#29702;&#31561;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09651</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#20984;&#20108;&#32423;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;NeuPSL&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuPSL&#25512;&#29702;&#30340;&#24179;&#28369;&#21407;&#22987;&#21644;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#26174;&#31034;&#23398;&#20064;&#26799;&#24230;&#26159;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26032;&#30340;&#24418;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#20598;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#28909;&#21551;&#21160;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30456;&#27604;&#24403;&#21069;&#26368;&#22909;&#30340;NeuPSL&#25512;&#29702;&#26041;&#27861;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#20102;100&#20493;&#20197;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#30456;&#27604;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#24322;&#36136;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#26469;&#25552;&#20379;&#23545;&#19981;&#21516;&#24322;&#36136;&#24615;&#27169;&#24335;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24433;&#21709;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24322;&#36136;&#24615;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#38656;&#35201;&#19982;&#24179;&#22343;&#33410;&#28857;&#24230;&#19968;&#36215;&#35780;&#20272;&#65292;&#24182;&#19988;&#25299;&#25169;&#22122;&#22768;&#23545;&#20998;&#31867;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.09125</link><description>&lt;p&gt;
&#29702;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Heterophily for Graph Neural Networks. (arXiv:2401.09125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#24322;&#36136;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#26469;&#25552;&#20379;&#23545;&#19981;&#21516;&#24322;&#36136;&#24615;&#27169;&#24335;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24433;&#21709;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24322;&#36136;&#24615;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#38656;&#35201;&#19982;&#24179;&#22343;&#33410;&#28857;&#24230;&#19968;&#36215;&#35780;&#20272;&#65292;&#24182;&#19988;&#25299;&#25169;&#22122;&#22768;&#23545;&#20998;&#31867;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38754;&#20020;&#25361;&#25112;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#33410;&#28857;&#36890;&#36807;&#21508;&#31181;&#27169;&#24335;&#19982;&#19981;&#21516;&#30340;&#37051;&#23621;&#30456;&#36830;&#25509;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#65288;GC&#65289;&#25805;&#20316;&#21512;&#24182;&#21040;&#23436;&#20840;&#36830;&#25509;&#30340;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24322;&#36136;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#26469;&#25552;&#20379;&#23545;&#19981;&#21516;&#24322;&#36136;&#24615;&#27169;&#24335;&#23545;GNNs&#24433;&#21709;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;HSBM&#26159;&#19968;&#20010;&#21487;&#20197;&#23481;&#32435;&#22810;&#26679;&#30340;&#24322;&#36136;&#24615;&#27169;&#24335;&#30340;&#36890;&#29992;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24212;&#29992;GC&#25805;&#20316;&#65292;&#21487;&#20998;&#24615;&#22686;&#30410;&#21462;&#20915;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#21363;&#37051;&#22495;&#20998;&#24067;&#30340;&#27431;&#27663;&#36317;&#31163;&#21644;$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$&#65292;&#20854;&#20013;$\mathbb{E}\left[\operatorname{deg}\right]$&#26159;&#24179;&#22343;&#33410;&#28857;&#24230;&#12290;&#23427;&#25581;&#31034;&#20102;&#24322;&#36136;&#24615;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#38656;&#35201;&#19982;&#24179;&#22343;&#33410;&#28857;&#24230;&#19968;&#36215;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25299;&#25169;&#22122;&#22768;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where $\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03506</link><description>&lt;p&gt;
DiarizationLM: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#30446;&#26631;&#65292;&#22914;&#25913;&#21892;&#20998;&#31163;&#23545;&#35805;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#65292;&#25110;&#20943;&#23569;&#35789;&#32423;&#20998;&#31163;&#38169;&#35823;&#29575;&#65288;WDER&#65289;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#34987;&#34920;&#31034;&#20026;&#19968;&#31181;&#32039;&#20945;&#30340;&#25991;&#26412;&#26684;&#24335;&#65292;&#20854;&#21253;&#21547;&#22312;&#19968;&#20010;&#21487;&#36873;&#25321;&#35843;&#25972;&#30340;LLM&#30340;&#25552;&#31034;&#20013;&#12290;LLM&#30340;&#36755;&#20986;&#21487;&#20197;&#20316;&#20026;&#25152;&#38656;&#25913;&#36827;&#30340;&#31934;&#32454;&#21270;&#20998;&#31163;&#32467;&#26524;&#12290;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;ASR&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#22312;Fisher&#30005;&#35805;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23558;WDER&#38477;&#20302;55.5%&#65292;&#22312;Callhome&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#38477;&#20302;44.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.02429</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#35843;&#26597;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02429
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#65288;IFD&#65289;&#20316;&#20026;&#19968;&#38376;&#20851;&#27880;&#26816;&#27979;&#21644;&#25910;&#38598;&#24037;&#19994;&#35774;&#22791;&#20581;&#24247;&#29366;&#20917;&#37325;&#35201;&#20449;&#24687;&#30340;&#23398;&#31185;&#32780;&#20986;&#29616;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#25925;&#38556;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#35782;&#21035;&#12290;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23548;&#33268;&#23545;&#33258;&#21160;&#21270;&#35774;&#22791;&#30417;&#27979;&#30340;&#20851;&#27880;&#65292;&#20197;&#36991;&#20813;&#23433;&#20840;&#20107;&#25925;&#24182;&#20943;&#23569;&#23545;&#20154;&#21147;&#30340;&#20381;&#36182;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#20986;&#29616;&#22312;&#22686;&#24378;&#26234;&#33021;IFD&#31639;&#27861;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#25968;&#25454;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21270;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;ANNs&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#36164;&#28304;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#20197;&#21450;&#21463;&#38480;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#22522;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#21407;&#29702;&#30340;&#31532;&#19977;&#20195;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#20043;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2311.18426</link><description>&lt;p&gt;
&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#20043;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#23548;&#25968;&#26159;&#25972;&#25968;&#38454;&#23548;&#25968;&#30340;&#19968;&#31181;&#24191;&#20041;&#25512;&#24191;&#12290;&#23545;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#30740;&#31350;&#20351;&#29992;&#20998;&#25968;&#23548;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#26159;&#38750;&#24120;&#26377;&#24847;&#20041;&#30340;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#22312;&#30740;&#31350;&#26041;&#27861;&#21644;&#30740;&#31350;&#29615;&#22659;&#26041;&#38754;&#37117;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#20998;&#26512;&#20102;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#21464;&#31181;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24314;&#31435;&#23558;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#32852;&#31995;&#36215;&#26469;&#30340;&#26032;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#19988;&#24378;&#20984;&#20989;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23545;&#20110;&#24179;&#28369;&#19988;&#20984;&#20989;&#25968;&#30340;O(1/T)&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26356;&#36866;&#21512;&#20998;&#25968;&#23548;&#25968;&#30340;&#25193;&#23637;&#24179;&#28369;&#24230;&#27010;&#24565;-Holder&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#19988;&#38750;&#20984;&#20989;&#25968;&#30340;O(1/T)&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivative
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13544</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#23646;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#21363;&#25152;&#35859;&#30340;&#28201;&#39034;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#20986;&#29616;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#25110;&#23567;&#20998;&#23376;&#30340;&#27874;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28201;&#39034;&#20989;&#25968;&#22312;&#20219;&#20309;&#23436;&#20840;&#32500;&#24230;&#30340;&#31435;&#26041;&#20307;&#19978;&#21487;&#29992;&#20998;&#27573;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#28201;&#39034;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.08516</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#25214;&#21040;&#25512;&#29702;&#38169;&#35823;&#65292;&#20294;&#21487;&#20197;&#32416;&#27491;&#23427;&#20204;&#65281;&#65288;arXiv&#65306;2311.08516v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#25105;&#32416;&#27491;&#22312;&#25913;&#21892;LLM&#36755;&#20986;&#30340;&#39118;&#26684;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65288;&#20363;&#22914;Chen&#31561;&#65292;2023&#65307;Madaan&#31561;&#65292;2023&#65289;&#65292;&#26368;&#36817;&#23545;&#36923;&#36753;&#25110;&#25512;&#29702;&#38169;&#35823;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#30340;&#23581;&#35797;&#36890;&#24120;&#20250;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#21464;&#20026;&#38169;&#35823;&#65292;&#20174;&#32780;&#24635;&#20307;&#34920;&#29616;&#21464;&#24046;&#65288;Huang&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#12290;&#23545;&#20110;&#38169;&#35823;&#21457;&#29616;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;BIG-Bench Mistake&#65292;&#36825;&#26159;&#19968;&#20010;Chain-of-Thought&#25512;&#29702;&#36712;&#36857;&#20013;&#30340;&#36923;&#36753;&#38169;&#35823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20026;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#25552;&#20379;&#22522;&#20934;&#25968;&#65292;&#24182;&#35777;&#26126;LLM&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#23545;&#20110;&#36755;&#20986;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#26041;&#27861;&#65292;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#21487;&#20197;&#22823;&#24133;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#22238;&#28335;&#35299;&#37322;&#20026;&#23545;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#36731;&#37327;&#32423;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;60-70&#65285;&#20934;&#30830;&#29575;&#19979;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00619</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#25439;&#22833;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#27491;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#27880;&#37322;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#20010;&#21035;&#27880;&#37322;&#32773;&#32463;&#24120;&#20250;&#25552;&#20379;&#25968;&#21315;&#20010;&#35780;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30130;&#21171;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27880;&#37322;&#36807;&#31243;&#21487;&#33021;&#20250;&#25345;&#32493;&#22810;&#22825;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#38543;&#26102;&#38388;&#30340;&#19981;&#20934;&#30830;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#22810;&#26679;&#24847;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#28165;&#26970;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20462;&#25913;&#21487;&#20197;&#25913;&#21892;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#24212;&#29992;&#20110;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#30340;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#23545;&#40784;&#19978;&#30028;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#23545;&#40784;&#25439;&#22833;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#21462;&#20195;&#23545;&#25239;&#25439;&#22833;&#65292;&#25193;&#23637;&#20102;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.19690</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#30028;&#38480;&#23454;&#29616;&#23454;&#29992;&#30340;&#38750;&#23545;&#25239;&#20998;&#24067;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds. (arXiv:2310.19690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#30340;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#23545;&#40784;&#19978;&#30028;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#23545;&#40784;&#25439;&#22833;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#21462;&#20195;&#23545;&#25239;&#25439;&#22833;&#65292;&#25193;&#23637;&#20102;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#23545;&#40784;&#21487;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#24212;&#29992;&#30340;&#19981;&#21464;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#37319;&#29992;&#23545;&#25239;&#23545;&#40784;&#26041;&#27861;&#65292;&#20294;&#30001;&#27492;&#20135;&#29983;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#20248;&#21270;&#12290;&#38750;&#23545;&#25239;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#27169;&#22411;&#21487;&#36870;&#24615;&#65292;&#35201;&#20040;&#23545;&#28508;&#22312;&#20808;&#39564;&#26045;&#21152;&#32422;&#26463;&#65292;&#35201;&#20040;&#32570;&#20047;&#36890;&#29992;&#30340;&#23545;&#40784;&#26694;&#26550;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#30340;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#31649;&#36947;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#23545;&#40784;&#19978;&#30028;&#65288;&#21253;&#25324;&#19968;&#20010;&#21547;&#22122;&#38899;&#30340;&#19978;&#30028;&#65289;&#65292;&#20854;&#20855;&#26377;&#31867;&#20284;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30446;&#26631;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#20180;&#32454;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26032;&#39062;&#23545;&#40784;&#25439;&#22833;&#21487;&#20197;&#22312;&#26631;&#20934;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#31649;&#36947;&#20013;&#21462;&#20195;&#23545;&#25239;&#25439;&#22833;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#25299;&#23637;&#20102;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly bro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>OODRobustBench&#26159;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#22823;&#35268;&#27169;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31163;&#32676;&#20998;&#24067;&#27979;&#35797;&#19979;&#23384;&#22312;&#20005;&#37325;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#32780;&#20869;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#31163;&#32676;&#20998;&#24067;&#40065;&#26834;&#24615;&#21576;&#24378;&#27491;&#32447;&#24615;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.12793</link><description>&lt;p&gt;
OODRobustBench: &#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#35780;&#20272;&#21644;&#20998;&#26512;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12793
&lt;/p&gt;
&lt;p&gt;
OODRobustBench&#26159;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#22823;&#35268;&#27169;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31163;&#32676;&#20998;&#24067;&#27979;&#35797;&#19979;&#23384;&#22312;&#20005;&#37325;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#32780;&#20869;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#31163;&#32676;&#20998;&#24067;&#40065;&#26834;&#24615;&#21576;&#24378;&#27491;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#21482;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#21363;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#40065;&#26834;&#24615;&#22312;&#36755;&#20837;&#20998;&#24067;&#36801;&#31227;&#65292;&#21363;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#27979;&#35797;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#23454;&#38469;&#37096;&#32626;&#26102;&#65292;&#30001;&#20110;&#36825;&#31181;&#20998;&#24067;&#36801;&#31227;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#36825;&#19968;&#38382;&#39064;&#21313;&#20998;&#20196;&#20154;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OODRobustBench&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;23&#20010;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#65288;&#21363;&#36755;&#20837;&#20998;&#24067;&#30340;&#33258;&#28982;&#36801;&#31227;&#65289;&#21644;6&#20010;&#22522;&#20110;&#23041;&#32961;&#30340;&#36801;&#31227;&#65288;&#21363;&#26410;&#30693;&#30340;&#23545;&#25239;&#24615;&#23041;&#32961;&#27169;&#22411;&#65289;&#26469;&#20840;&#38754;&#35780;&#20272;OOD&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;OODRobustBench&#29992;&#20110;&#35780;&#20272;&#20102;706&#20010;&#40065;&#26834;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;60.7K&#27425;&#23545;&#25239;&#24615;&#35780;&#20272;&#12290;&#36825;&#20010;&#22823;&#35268;&#27169;&#20998;&#26512;&#34920;&#26126;&#65306;1&#65289;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#23384;&#22312;&#20005;&#37325;&#30340;OOD&#27867;&#21270;&#38382;&#39064;&#65307;2&#65289;ID&#40065;&#26834;&#24615;&#19982;OOD&#40065;&#26834;&#24615;&#21576;&#24378;&#27491;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear wa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35774;&#35745;&#22312;&#32447;&#31639;&#27861;&#26102;&#26368;&#20339;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#27979;&#27010;&#29575;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11558</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#30340;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Algorithms with Uncertainty-Quantified Predictions. (arXiv:2310.11558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35774;&#35745;&#22312;&#32447;&#31639;&#27861;&#26102;&#26368;&#20339;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#27979;&#27010;&#29575;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#39044;&#27979;&#30340;&#22312;&#32447;&#31639;&#27861;&#24050;&#25104;&#20026;&#31639;&#27861;&#30340;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#23545;&#26410;&#26469;&#30340;&#39044;&#27979;&#26469;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24403;&#39044;&#27979;&#33391;&#22909;&#26102;&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#20219;&#24847;&#24046;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#30028;&#38480;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#31639;&#27861;&#34987;&#35748;&#20026;&#23545;&#39044;&#27979;&#36136;&#37327;&#19981;&#30693;&#24773;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#30740;&#31350;&#20102;&#25552;&#20379;&#23545;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#36827;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25216;&#26415;&#65292;&#21363;&#25551;&#36848;&#27169;&#22411;&#23545;&#20854;&#36136;&#37327;&#30340;&#30830;&#23450;&#31243;&#24230;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#22914;&#20309;&#22312;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#20013;&#26368;&#20339;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#25551;&#36848;&#22522;&#26412;&#20107;&#23454;&#33853;&#22312;&#26576;&#20010;&#33539;&#22260;&#20869;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22686;&#24378;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#35774;&#35745;&#20102;&#20855;&#26377;&#36825;&#20123;&#27010;&#29575;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07579</link><description>&lt;p&gt;
In-Context Unlearning: &#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28040;&#38500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#26159;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#21435;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36817;&#26469;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38656;&#35201;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22312;&#29256;&#26435;&#38382;&#39064;&#19978;LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#23588;&#20854;&#30456;&#20851;&#65292;&#20294;&#22312;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#19978;&#23454;&#29616;&#31934;&#30830;&#28040;&#38500;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35775;&#38382;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#25110;&#36890;&#36807;API&#35775;&#38382;LLM&#32780;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#28040;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20998;&#21106;&#19978;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23545;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23637;&#24320;&#31995;&#25968;&#36827;&#34892;&#32534;&#30721;&#30340;&#26032;&#39062;ReLU NN&#26367;&#20195;&#27169;&#22411;&#26500;&#36896;&#65292;&#19982;&#22522;&#20110;ReLU NN&#27169;&#25311;&#22810;&#39033;&#24335;&#30340;&#26500;&#36896;&#30456;&#27604;&#65292;&#22312;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.07261</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#21644;&#39640;&#38454;&#26377;&#38480;&#20803;&#26041;&#27861;II&#65306;&#20999;&#27604;&#38634;&#22827;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. (arXiv:2310.07261v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20998;&#21106;&#19978;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23545;&#36830;&#32493;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23637;&#24320;&#31995;&#25968;&#36827;&#34892;&#32534;&#30721;&#30340;&#26032;&#39062;ReLU NN&#26367;&#20195;&#27169;&#22411;&#26500;&#36896;&#65292;&#19982;&#22522;&#20110;ReLU NN&#27169;&#25311;&#22810;&#39033;&#24335;&#30340;&#26500;&#36896;&#30456;&#27604;&#65292;&#22312;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#21306;&#38388;$(a,b)$&#19978;&#30340;&#20219;&#24847;&#26377;&#38480;&#20998;&#21106;$\mathcal{T}$&#19978;&#65292;&#36830;&#32493;&#30340;&#12289;&#20998;&#27573;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22312;&#23450;&#20041;NN&#30340;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#23545;Sobolev&#33539;&#25968;&#30340;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;ReLU NN&#26367;&#20195;&#27169;&#22411;&#26500;&#36896;&#65292;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23637;&#24320;&#31995;&#25968;&#23545;&#36817;&#20284;&#20989;&#25968;&#36827;&#34892;&#32534;&#30721;&#12290;&#21487;&#20197;&#36890;&#36807;&#23558;&#20989;&#25968;&#22312;Clenshaw-Curtis&#28857;&#19978;&#30340;&#20540;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#36870;&#21464;&#25442;&#36731;&#26494;&#22320;&#35745;&#31639;&#20986;&#20999;&#27604;&#38634;&#22827;&#31995;&#25968;&#12290;&#22312;&#34920;&#36798;&#36895;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#33719;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ReLU NN&#27169;&#25311;&#22810;&#39033;&#24335;&#30340;&#26500;&#36896;[Opschoor&#65292;Petersen&#65292;Schwab&#65292;2020]&#30340;&#30028;&#38480;&#12290;&#25152;&#26377;&#27169;&#25311;&#19978;&#30028;&#37117;&#26126;&#30830;&#22320;&#19982;&#21306;&#38388;&#30340;&#65288;&#20219;&#24847;&#65289;&#20998;&#21106;&#12289;&#30446;&#26631;&#27169;&#25311;&#31934;&#24230;&#21644;&#20998;&#21106;&#20013;&#27599;&#20010;&#20803;&#32032;&#30340;&#22810;&#39033;&#24335;&#27425;&#25968;&#26377;&#20851;&#12290;&#25552;&#20379;&#20102;ReLU NN&#27169;&#25311;&#35823;&#24046;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for variou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#65292;&#24182;&#32473;&#20986;&#20102;&#25915;&#20987;&#21487;&#33021;&#24615;&#30340;&#26465;&#20214;&#12290;&#19982;&#20197;&#24448;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#29702;&#35299;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#36824;&#21462;&#20915;&#20110;&#21457;&#21733;&#23454;&#20363;&#26159;&#21542;&#34987;&#23545;&#25163;&#30693;&#26195;&#12290;&#36825;&#34920;&#26126;&#22312;&#23454;&#36341;&#20013;&#23545;CMAB&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23545;&#25163;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#26080;&#27861;&#20102;&#35299;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.05308</link><description>&lt;p&gt;
&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Combinatorial Multi-Armed Bandits. (arXiv:2310.05308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#65292;&#24182;&#32473;&#20986;&#20102;&#25915;&#20987;&#21487;&#33021;&#24615;&#30340;&#26465;&#20214;&#12290;&#19982;&#20197;&#24448;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#29702;&#35299;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#36824;&#21462;&#20915;&#20110;&#21457;&#21733;&#23454;&#20363;&#26159;&#21542;&#34987;&#23545;&#25163;&#30693;&#26195;&#12290;&#36825;&#34920;&#26126;&#22312;&#23454;&#36341;&#20013;&#23545;CMAB&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23545;&#25163;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#26080;&#27861;&#20102;&#35299;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;CMAB&#65289;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;CMAB&#25915;&#20987;&#21487;&#33021;&#24615;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#21462;&#20915;&#20110;&#30456;&#24212;CMAB&#23454;&#20363;&#30340;&#20869;&#22312;&#29305;&#24615;&#65292;&#22914;&#36229;&#33218;&#30340;&#22870;&#21169;&#20998;&#24067;&#21644;&#22522;&#26412;&#33218;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#21487;&#25915;&#20987;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#31639;&#27861;&#12290;&#19982;&#20197;&#24448;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#29702;&#35299;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#20107;&#23454;&#65292;&#21363;&#29305;&#23450;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#36824;&#21462;&#20915;&#20110;&#21457;&#21733;&#23454;&#20363;&#26159;&#21542;&#34987;&#23545;&#25163;&#30693;&#26195;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;CMAB&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#23454;&#36341;&#20013;&#24456;&#22256;&#38590;&#65292;&#24182;&#19988;&#19981;&#23384;&#22312;&#36866;&#29992;&#20110;&#20219;&#20309;CMAB&#23454;&#20363;&#30340;&#36890;&#29992;&#25915;&#20987;&#31574;&#30053;&#65292;&#22240;&#20026;&#29615;&#22659;&#23545;&#20110;&#23545;&#25163;&#26469;&#35828;&#22823;&#37096;&#20998;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#23454;&#38469;CMAB&#24212;&#29992;&#65288;&#21253;&#25324;&#27010;&#29575;&#26368;&#22823;&#35206;&#30422;&#38382;&#39064;&#12289;&#22312;&#32447;&#26368;&#23567;&#29983;&#25104;&#26641;&#38382;&#39064;&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, which depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.00809</link><description>&lt;p&gt;
&#25351;&#21521;&#22240;&#26524;&#22522;&#30784;&#27169;&#22411;: &#22240;&#26524;&#25512;&#26029;&#19982;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#23545;&#20598;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#29702;&#35770;&#19978;&#23436;&#22791;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#22240;&#26524;&#23398;&#20064;&#65292;&#24182;&#22312;&#26032;&#25968;&#25454;&#30340;&#26410;&#35265;&#20219;&#21153;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22686;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#31283;&#20581;&#30340;&#33021;&#32791;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.12484</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#22522;&#20110;&#32570;&#22833;&#20540;&#25239;&#24178;&#25200;&#20803;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#33021;&#32791;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development. (arXiv:2309.12484v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22686;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#31283;&#20581;&#30340;&#33021;&#32791;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#32791;&#26159;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23545;&#24320;&#21457;&#20154;&#21592;&#21644;&#32456;&#31471;&#29992;&#25143;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#22312;&#28040;&#36153;&#32773;&#32771;&#34385;&#26234;&#33021;&#25163;&#26426;&#36141;&#20080;&#26102;&#65292;&#33021;&#32791;&#26159;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#20174;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#37492;&#20110;&#25968;&#21313;&#20159;&#37096;&#26234;&#33021;&#25163;&#26426;&#30340;&#24191;&#27867;&#20351;&#29992;&#25152;&#24102;&#26469;&#30340;&#37325;&#22823;&#20840;&#29699;&#24433;&#21709;&#65292;&#25506;&#32034;&#26088;&#22312;&#20943;&#23569;&#31227;&#21160;&#35774;&#22791;&#33021;&#32791;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#23545;&#29615;&#22659;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#23613;&#31649;&#23433;&#21331;&#24179;&#21488;&#65288;&#20027;&#23548;&#30340;&#31227;&#21160;&#29983;&#24577;&#31995;&#32479;&#65289;&#20013;&#23384;&#22312;&#21508;&#31181;&#33410;&#33021;&#32534;&#31243;&#23454;&#36341;&#65292;&#20294;&#20173;&#38656;&#35201;&#19987;&#38376;&#38754;&#21521;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;-based&#33021;&#32791;&#39044;&#27979;&#31639;&#27861;&#30340;&#25991;&#26723;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22686;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#33021;&#32791;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption is a fundamental concern in mobile application development, bearing substantial significance for both developers and end-users. Moreover, it is a critical determinant in the consumer's decision-making process when considering a smartphone purchase. From the sustainability perspective, it becomes imperative to explore approaches aimed at mitigating the energy consumption of mobile devices, given the significant global consequences arising from the extensive utilisation of billions of smartphones, which imparts a profound environmental impact. Despite the existence of various energy-efficient programming practices within the Android platform, the dominant mobile ecosystem, there remains a need for documented machine learning-based energy prediction algorithms tailored explicitly for mobile app development. Hence, the main objective of this research is to propose a novel neural network-based framework, enhanced by a metaheuristic approach, to achieve robust energy predi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2308.13453</link><description>&lt;p&gt;
&#23398;&#20064;&#24178;&#39044;&#27010;&#24565;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#32780;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#36890;&#36807;&#20854;&#27010;&#24565;&#34920;&#31034;&#25552;&#20379;&#22266;&#26377;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#26356;&#26032;&#27010;&#24565;&#20540;&#24182;&#32416;&#27491;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#26469;&#36827;&#34892;&#24178;&#39044;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#36825;&#20123;&#24178;&#39044;&#20165;&#24212;&#29992;&#20110;&#27169;&#22411;&#19968;&#27425;&#21518;&#21363;&#34987;&#20002;&#24323;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36825;&#26159;CBM&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CB2M&#36890;&#36807;&#21452;&#25240;&#21472;&#35760;&#24518;&#23398;&#20064;&#23558;&#24178;&#39044;&#30340;&#25512;&#24191;&#21040;&#36866;&#24403;&#30340;&#26032;&#24773;&#22659;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#23398;&#20064;&#26816;&#27979;&#38169;&#35823;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#30340;&#24178;&#39044;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CB2M&#33021;&#22815;&#20174;&#26368;&#21021;&#33719;&#24471;&#30340;&#23569;&#37327;&#24178;&#39044;&#20013;&#33258;&#21160;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#65292;CB2M&#21487;&#20197;&#26816;&#27979;&#21040;CBM&#29942;&#39048;&#30340;&#28508;&#22312;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;</title><link>http://arxiv.org/abs/2308.12000</link><description>&lt;p&gt;
&#26377;&#20851;&#22312;&#26377;&#38480;&#39044;&#31639;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#32479;&#19968;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20271;&#21162;&#21033;&#22870;&#21169;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#20351;&#29992;&#26377;&#38480;&#39044;&#31639;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65288;&#35813;&#31639;&#27861;&#34987;&#31216;&#20026;&#8220;&#22343;&#21248;&#37319;&#26679;&#8221;&#31639;&#27861;&#65289;&#65292;&#24182;&#19988;&#22312;&#33267;&#23569;&#19968;&#20010;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#35813;&#31639;&#27861;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#19981;&#23384;&#22312;&#27604;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#19968;&#33268;&#8221;&#21644;&#8220;&#31283;&#23450;&#8221;&#31639;&#27861;&#30340;&#33258;&#28982;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20219;&#20309;&#31639;&#27861;&#35201;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65292;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36890;&#36807;&#23548;&#20986;&#28385;&#36275;&#20219;&#20309;&#19968;&#33268;&#19988;&#31283;&#23450;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#19982;&#27492;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#35777;&#26126;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#20915;&#20102;\cite{qin2022open}&#20013;&#25552;&#20986;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.10167</link><description>&lt;p&gt;
VITS: &#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10167
&lt;/p&gt;
&lt;p&gt;
VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20256;&#32479;&#30340;TS&#31639;&#27861;&#22312;&#27599;&#36718;&#38656;&#35201;&#20174;&#24403;&#21069;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36817;&#20284;&#25512;&#29702;&#25216;&#26415;&#24182;&#25552;&#20379;&#25509;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36817;&#20284;&#25216;&#26415;&#35201;&#20040;&#20272;&#35745;&#19981;&#20934;&#30830;&#65288;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65288;MCMC&#26041;&#27861;&#65292;&#38598;&#25104;&#25277;&#26679;...&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#25512;&#29702;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;VITS&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#24182;&#19988;&#23481;&#26131;&#20174;&#20013;&#25277;&#26679;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#26159;TS&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#65292;VITS&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#19982;&#32500;&#24230;&#21644;&#22238;&#21512;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#24863;&#30693;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#22122;&#22768;&#29305;&#23450;&#20449;&#24687;&#65292;&#36890;&#36807;&#22122;&#22768;&#20998;&#31867;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#26469;&#22686;&#24378;&#22122;&#22768;&#35843;&#33410;&#22120;&#30340;&#22122;&#22768;&#29305;&#24322;&#24615;&#12290;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08029</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#22122;&#22768;&#24863;&#30693;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Noise-aware Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2307.08029v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#24863;&#30693;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#22122;&#22768;&#29305;&#23450;&#20449;&#24687;&#65292;&#36890;&#36807;&#22122;&#22768;&#20998;&#31867;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#26469;&#22686;&#24378;&#22122;&#22768;&#35843;&#33410;&#22120;&#30340;&#22122;&#22768;&#29305;&#24322;&#24615;&#12290;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29983;&#25104;&#24335;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#22240;&#20854;&#23545;&#26410;&#30693;&#27979;&#35797;&#22122;&#22768;&#30340;&#24040;&#22823;&#28508;&#21147;&#32780;&#21463;&#21040;&#20102;&#22823;&#37327;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28165;&#26224;&#35821;&#38899;&#30340;&#22266;&#26377;&#29305;&#24615;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#19979;&#21464;&#21270;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35821;&#38899;&#22686;&#24378;&#65288;NASE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#21462;&#22122;&#22768;&#29305;&#23450;&#20449;&#24687;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#21521;&#22788;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22122;&#22768;&#20998;&#31867;&#65288;NC&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20135;&#29983;&#22768;&#23398;&#23884;&#20837;&#20316;&#20026;&#22122;&#22768;&#35843;&#33410;&#22120;&#26469;&#25351;&#23548;&#36870;&#21521;&#38477;&#22122;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#20849;&#21516;&#20248;&#21270;SE&#21644;NC&#20219;&#21153;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#30340;&#22122;&#22768;&#35843;&#33410;&#22120;&#30340;&#22122;&#22768;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;NASE&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#25193;&#25955;SE&#27169;&#22411;&#12290;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;NASE&#36798;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech for inference, underexploiting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner for guiding the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks, in order to enhance the noise specificity of extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06887</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06887
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#26159;&#31070;&#32463;&#32593;&#32476;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#22914;&#20309;&#20197;&#21450;&#20026;&#20309;&#21457;&#29983;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#38543;&#26426;&#29305;&#24449;&#33539;&#20363;&#20013;&#24494;&#19981;&#36275;&#36947;&#30340;&#29305;&#24449;&#23398;&#20064;&#30340;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#32463;&#24120;&#22320;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#20123;&#20808;&#21069;&#30340;&#20998;&#26512;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#26368;&#24120;&#35265;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>MALIBO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36328;&#20219;&#21153;&#30340;&#26597;&#35810;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#36741;&#21161;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#31283;&#20581;&#36866;&#24212;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03565</link><description>&lt;p&gt;
MALIBO: &#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#26080;&#20284;&#28982;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03565
&lt;/p&gt;
&lt;p&gt;
MALIBO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36328;&#20219;&#21153;&#30340;&#26597;&#35810;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#36741;&#21161;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#31283;&#20581;&#36866;&#24212;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20250;&#20174;&#22836;&#24320;&#22987;&#20248;&#21270;&#27599;&#20010;&#26032;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#32780;&#20803;&#23398;&#20064;&#21017;&#26159;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#26356;&#24555;&#22320;&#20248;&#21270;&#26032;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#20934;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#35266;&#23519;&#25968;&#25454;&#30340;&#23610;&#24230;&#21644;&#22122;&#22768;&#31867;&#22411;&#38750;&#24120;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#19982;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#26377;&#26377;&#38480;&#35266;&#23519;&#25968;&#25454;&#25110;&#26032;&#20219;&#21153;&#19982;&#30456;&#20851;&#20219;&#21153;&#24046;&#24322;&#26174;&#33879;&#26102;&#65292;&#20219;&#21153;&#36866;&#24212;&#24615;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#32469;&#24320;&#26631;&#20934;&#27169;&#22411;&#65292;&#30452;&#25509;&#23398;&#20064;&#36328;&#20219;&#21153;&#30340;&#26597;&#35810;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#31283;&#20581;&#36866;&#24212;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#65288;IST&#65289;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;IST&#19982;&#20854;&#20182;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.16484</link><description>&lt;p&gt;
&#26397;&#30528;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#30340;&#26356;&#22909;&#29702;&#35770;&#29702;&#35299;&#36808;&#36827; (arXiv:2306.16484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Towards a Better Theoretical Understanding of Independent Subnetwork Training. (arXiv:2306.16484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#65288;IST&#65289;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;IST&#19982;&#20854;&#20182;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#31163;&#19981;&#24320;&#25968;&#25454;&#24182;&#34892;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#33539;&#24335;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#23545;&#36890;&#20449;&#36890;&#36947;&#26045;&#21152;&#20102;&#24040;&#22823;&#21387;&#21147;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20849;&#21516;&#35774;&#35745;&#36890;&#20449;&#21387;&#32553;&#31574;&#30053;&#21644;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#23613;&#31649;&#32431;&#25968;&#25454;&#24182;&#34892;&#24615;&#20801;&#35768;&#26356;&#22909;&#30340;&#25968;&#25454;&#25193;&#23637;&#24615;&#65292;&#20294;&#20854;&#22312;&#27169;&#22411;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#35745;&#31639;&#33410;&#28857;&#21463;&#20869;&#23384;&#38480;&#21046;&#20005;&#37325;&#38480;&#21046;&#65292;&#38459;&#27490;&#20102;&#27169;&#22411;&#23610;&#23544;&#30340;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#24040;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#26524;&#20063;&#20381;&#36182;&#20110;&#26576;&#31181;&#24418;&#24335;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#29420;&#31435;&#23376;&#32593;&#32476;&#35757;&#32451;&#65288;IST&#65289;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;IST&#21644;&#20854;&#20182;&#27169;&#22411;&#24182;&#34892;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12214</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;PAC-Bayes Bounds&#65306;&#20174;&#26377;&#30028;&#25439;&#22833;&#21040;&#20855;&#26377;&#19968;&#33324;&#24615;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#21040;&#20219;&#20309;&#26102;&#38388;&#22343;&#26377;&#25928;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#25552;&#20986;&#20102;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#26377;&#30028;&#33539;&#22260;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Catoni&#30028;&#30340;&#21152;&#24378;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#32479;&#19968;&#30028;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#24555;&#36895;&#36895;&#29575;&#21644;&#28151;&#21512;&#36895;&#29575;&#19978;&#38480;&#65292;&#36825;&#20123;&#19978;&#38480;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30028;&#38480;&#26356;&#32039;&#12290;&#20854;&#27425;&#65292;&#38024;&#23545;&#26356;&#19968;&#33324;&#30340;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#26080;&#21442;&#25968;&#19978;&#38480;&#65306;&#24403;&#25439;&#22833;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;PAC-Bayes Chernoff&#31867;&#27604;&#65292;&#21478;&#19968;&#20010;&#19978;&#38480;&#26159;&#25439;&#22833;&#30340;&#20108;&#38454;&#30697;&#26377;&#30028;&#12290;&#36825;&#20004;&#20010;&#19978;&#38480;&#26159;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#20107;&#20214;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#30340;&#26032;&#25216;&#26415;&#33719;&#24471;&#30340;&#65292;&#8220;&#22312;&#27010;&#29575;&#8221;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30028;&#38480;&#30340;&#31616;&#21333;&#25216;&#26415;&#23558;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#25193;&#23637;&#21040;&#20219;&#20309;&#26102;&#38388;&#26377;&#25928;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.11112</link><description>&lt;p&gt;
&#32416;&#27491;&#20844;&#24179;&#20998;&#31867;&#20013;&#30340;&#20302;&#20272;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#34987;&#20302;&#20272;&#20559;&#24046;&#25439;&#22351;&#30340;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#27491;&#20363;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#25935;&#24863;&#32452;&#20013;&#20197;&#19981;&#21516;&#30340;&#26410;&#30693;&#36895;&#29575;&#20174;&#25968;&#25454;&#20013;&#36807;&#28388;&#25481;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26377;&#23569;&#37327;&#26080;&#20559;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#27599;&#20010;&#32452;&#30340;&#20943;&#23569;&#21442;&#25968;&#65292;&#21363;&#20351;&#22312;&#20132;&#21449;&#32452;&#25104;&#21592;&#36164;&#26684;&#20351;&#24471;&#23398;&#20064;&#27599;&#20010;&#20132;&#21449;&#29575;&#21464;&#24471;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#32452;&#20002;&#22833;&#29575;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#25105;&#20204;&#36817;&#20284;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#65292;&#21363;&#20351;&#25105;&#20204;&#21482;&#33021;&#22312;&#19968;&#20010;&#26377;&#20559;&#26679;&#26412;&#19978;&#35266;&#23519;&#21040;&#32463;&#39564;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#35013;&#20102;&#36825;&#20010;&#23398;&#20064;&#21644;&#37325;&#26032;&#21152;&#26435;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;PAC&#39118;&#26684;&#30340;&#20445;&#35777;&#65292;&#21363;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#25105;&#20204;&#23545;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#39118;&#38505;&#30340;&#20272;&#35745;&#23558;&#19982;&#30495;&#23454;&#39118;&#38505;&#20219;&#24847;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.10649</link><description>&lt;p&gt;
CompanyKG:&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25237;&#36164;&#34892;&#19994;&#20013;&#65292;&#23545;&#20110;&#35768;&#22810;&#30446;&#30340;&#21253;&#25324;&#24066;&#22330;&#26144;&#23556;&#12289;&#31454;&#20105;&#23545;&#25163;&#20998;&#26512;&#21644;&#24182;&#36141;&#65292;&#36827;&#34892;&#32454;&#31890;&#24230;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;CompanyKG&#30340;&#30693;&#35782;&#22270;&#65292;&#29992;&#20110;&#34920;&#31034;&#21644;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1.17&#30334;&#19975;&#23478;&#20844;&#21496;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#20016;&#23500;&#20102;&#20844;&#21496;&#25551;&#36848;&#23884;&#20837;; 15&#31181;&#19981;&#21516;&#30340;&#20844;&#21496;&#38388;&#20851;&#31995;&#23548;&#33268;&#20102;5106&#30334;&#19975;&#20010;&#24102;&#26435;&#37325;&#30340;&#36793;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#32534;&#35793;&#20102;&#19977;&#20010;&#24102;&#26377;&#27880;&#37322;&#27979;&#35797;&#38598;&#30340;&#35780;&#20272;&#20219;&#21153;: &#30456;&#20284;&#24615;&#39044;&#27979;&#12289;&#31454;&#20105;&#23545;&#25163;&#26816;&#32034;&#21644;&#30456;&#20284;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;11&#31181;&#21487;&#37325;&#29616;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20998;&#20026;&#33410;&#28857;&#12289;&#36793;&#21644;&#33410;&#28857;+&#36793;&#19977;&#32452;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CompanyKG&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#22788;&#26041;&#24615;&#31995;&#25968;&#20195;&#26367;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#65292;&#21487;&#29992;&#20110;&#30830;&#23450;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#20915;&#31574;&#36136;&#37327;&#21644;&#21442;&#32771;&#20915;&#31574;&#20197;&#21450;&#20391;&#38754;&#20449;&#24687;&#22788;&#26041;&#33021;&#21147;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.05937</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Data-driven Prescriptiveness Optimization. (arXiv:2306.05937v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#22788;&#26041;&#24615;&#31995;&#25968;&#20195;&#26367;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#65292;&#21487;&#29992;&#20110;&#30830;&#23450;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#20915;&#31574;&#36136;&#37327;&#21644;&#21442;&#32771;&#20915;&#31574;&#20197;&#21450;&#20391;&#38754;&#20449;&#24687;&#22788;&#26041;&#33021;&#21147;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#37327;&#30340;&#25968;&#25454;&#20419;&#36827;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#20391;&#38754;&#20449;&#24687;&#25552;&#20379;&#26356;&#20855;&#39044;&#21028;&#24615;&#30340;&#20915;&#31574;&#12290;&#24191;&#27867;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#32972;&#26223;&#20419;&#36827;&#20102;&#35774;&#35745;&#19968;&#31181;&#31216;&#20026;&#22788;&#26041;&#24615;&#31995;&#25968;&#30340;&#36890;&#29992;&#26080;&#21333;&#20301;&#24615;&#33021;&#25351;&#26631;&#30340;&#20135;&#29983;&#12290;&#35813;&#31995;&#25968;&#26088;&#22312;&#37327;&#21270;&#19978;&#19979;&#25991;&#20915;&#31574;&#30340;&#36136;&#37327;&#19982;&#21442;&#32771;&#20915;&#31574;&#20197;&#21450;&#20391;&#38754;&#20449;&#24687;&#30340;&#22788;&#26041;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#22823;&#21270;&#21069;&#32773;&#30340;&#31574;&#30053;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#22788;&#26041;&#24615;&#31995;&#25968;&#20195;&#26367;&#20102;&#32463;&#20856;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20998;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#27169;&#22411;&#65292;&#24403;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20855;&#26377;&#36866;&#24403;&#30340;&#23884;&#22871;&#24418;&#24335;&#21644;&#22810;&#38754;&#20307;&#32467;&#26500;&#26102;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#35299;&#20915;&#19968;&#31995;&#21015;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual short
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23618;&#38388;&#21453;&#39304;&#23545;&#40784;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#21457;&#29616;FA&#19982;GD&#20043;&#38388;&#23384;&#22312;&#38544;&#24335;&#20559;&#24046;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#38416;&#26126;&#20102;ReLU&#32593;&#32476;&#20013;&#19982;&#21453;&#39304;&#30697;&#38453;&#23545;&#40784;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.01870</link><description>&lt;p&gt;
&#23618;&#38388;&#21453;&#39304;&#23545;&#40784;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20445;&#23432;&#24615;
&lt;/p&gt;
&lt;p&gt;
Layer-Wise Feedback Alignment is Conserved in Deep Neural Networks. (arXiv:2306.01870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23618;&#38388;&#21453;&#39304;&#23545;&#40784;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#21457;&#29616;FA&#19982;GD&#20043;&#38388;&#23384;&#22312;&#38544;&#24335;&#20559;&#24046;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#38416;&#26126;&#20102;ReLU&#32593;&#32476;&#20013;&#19982;&#21453;&#39304;&#30697;&#38453;&#23545;&#40784;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#29983;&#29289;&#21487;&#22609;&#24615;&#65292;&#21453;&#39304;&#23545;&#40784;&#65288;FA&#65289;&#20316;&#20026;&#20256;&#32479;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#23427;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21453;&#21521;&#20256;&#36755;&#26435;&#37325;&#26367;&#25442;&#20026;&#38543;&#26426;&#30697;&#38453;&#12290;&#34429;&#28982;FA&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#33021;&#22815;&#32469;&#36807;&#35745;&#31639;&#25361;&#25112;&#21644;&#20854;&#21487;&#20449;&#30340;&#29983;&#29289;&#23545;&#40784;&#24615;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#23398;&#20064;&#35268;&#21017;&#30340;&#29702;&#35299;&#36824;&#26159;&#26377;&#25152;&#27424;&#32570;&#30340;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#25903;&#25745;FA&#23398;&#20064;&#21160;&#24577;&#30340;&#19968;&#32452;&#23432;&#24658;&#23450;&#24459;&#65292;&#25581;&#31034;&#20102;FA&#21644;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;FA&#20855;&#26377;&#19982;GD&#34920;&#29616;&#20986;&#30340;&#38544;&#24335;&#20559;&#24046;&#30456;&#20284;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#26681;&#26412;&#19981;&#21516;&#30340;&#27969;&#34892;&#35828;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20123;&#23432;&#24658;&#23450;&#24459;&#38416;&#26126;&#20102;ReLU&#32593;&#32476;&#20013;&#19982;&#21453;&#39304;&#30697;&#38453;&#23545;&#40784;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#36825;&#24847;&#21619;&#30528;&#36807;&#21442;&#25968;&#21270;&#30340;&#21452;&#32447;&#24615;&#32593;&#32476;&#20013;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#22320;&#20195;&#26367;&#21518;&#21521;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest to enhance the efficiency and bio-plausibility of training deep neural networks, Feedback Alignment (FA), which replaces the backward pass weights with random matrices in the training process, has emerged as an alternative to traditional backpropagation. While the appeal of FA lies in its circumvention of computational challenges and its plausible biological alignment, the theoretical understanding of this learning rule remains partial. This paper uncovers a set of conservation laws underpinning the learning dynamics of FA, revealing intriguing parallels between FA and Gradient Descent (GD). Our analysis reveals that FA harbors implicit biases akin to those exhibited by GD, challenging the prevailing narrative that these learning algorithms are fundamentally different. Moreover, we demonstrate that these conservation laws elucidate sufficient conditions for layer-wise alignment with feedback matrices in ReLU networks. We further show that this implies over-parameterized tw
&lt;/p&gt;</description></item><item><title>EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01310</link><description>&lt;p&gt;
EPIC: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#30340;&#22270;&#24418;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01310
&lt;/p&gt;
&lt;p&gt;
EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#22270;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#32463;&#24120;&#38480;&#21046;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EPIC&#65288;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#32534;&#36753;&#36317;&#31163;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#32467;&#26500;&#26377;&#25152;&#21464;&#21270;&#30340;&#26032;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#24102;&#26631;&#31614;&#30340;&#22270;&#26469;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#22312;&#21407;&#22987;&#22270;&#23545;&#20043;&#38388;&#21019;&#24314;&#20102;&#22270;&#32534;&#36753;&#36335;&#24452;&#12290;&#36890;&#36807;&#20174;&#22270;&#32534;&#36753;&#36335;&#24452;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#20016;&#23500;&#20102;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#33021;&#35299;&#20915;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.16102</link><description>&lt;p&gt;
&#25581;&#31034;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#33021;&#35299;&#20915;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#25351;&#30340;&#26159;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#21464;&#24471;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#20250;&#25351;&#25968;&#32423;&#22833;&#21435;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26159;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#36824;&#23384;&#22312;&#20105;&#35758;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#38750;&#32447;&#24615;&#26102;&#21464;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#32467;&#21512;&#38750;&#40784;&#27425;&#30697;&#38453;&#20056;&#31215;&#21644;&#32852;&#21512;&#35889;&#21322;&#24452;&#29702;&#35770;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#27969;&#34892;&#35266;&#28857;&#30456;&#21453;&#65292;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#19981;&#33021;&#38450;&#27490;&#24179;&#28369;&#36807;&#24230;&#29616;&#35937;&#65292;&#24182;&#19988;&#21576;&#25351;&#25968;&#32423;&#22833;&#21435;&#34920;&#36798;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#23545;&#31216;GCN&#30340;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#31867;&#21035;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.15925</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#22240;&#20854;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20316;&#20026;&#23558;&#26368;&#36817;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#24207;&#21015;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#31532;&#19968;&#27493;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#22312;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#25552;&#20986;&#20102;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.05223</link><description>&lt;p&gt;
&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#19981;&#22343;&#21248;&#22270;&#36235;&#21183;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#20272;&#35745;&#20998;&#27573;&#24179;&#28369;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;$\ell_{2,0}$-&#33539;&#25968;&#24809;&#32602;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#21516;&#26102;&#26159;&#22522;&#20110;&#33410;&#28857;&#19978;&#30340;&#20449;&#21495;&#30340;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20854;&#20013;&#32858;&#31867;&#21644;&#21106;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#65306;&#19968;&#31181;&#26159;&#22522;&#20110;&#35889;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03807</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20445;&#25345;&#21516;&#24577;CNN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;CNN&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25104;&#21151;&#31361;&#30772;&#36825;&#20010;&#38590;&#39064;&#65292;&#20197;&#21069;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#37319;&#29992;&#20102;&#20960;&#31181;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#31616;&#21270;&#20026;&#21516;&#24577;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#29978;&#33267;&#26159;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#35757;&#32451;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;$\texttt{Quadratic Gradient}$&#65292;&#24212;&#29992;&#20110;MLR&#30340;&#22686;&#24378;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#25105;&#20204;&#37319;&#29992;&#25968;&#23398;&#20013;&#30340;&#21464;&#25442;&#24605;&#24819;&#65292;&#23558;&#21152;&#23494;&#22495;&#20013;&#30340;&#36817;&#20284;Softmax&#20989;&#25968;&#36716;&#25442;&#25104;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01470</link><description>&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01470
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25968;&#25454;&#12289;&#35745;&#31639;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#30417;&#30563;&#23398;&#20064;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#36825;&#20123;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26799;&#24230;&#22312;&#23545;&#25968;&#19978;&#21464;&#21270;&#33539;&#22260;&#24456;&#22823;&#65292;&#32780;&#22312;&#32477;&#23545;&#20540;&#19978;&#33539;&#22260;&#36739;&#23567;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#20195;&#29702;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#65292;&#23548;&#33268;&#20803;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#39640;&#24230;&#38543;&#26426;&#20132;&#20114;&#65292;&#20195;&#29702;&#26799;&#24230;&#23384;&#22312;&#36739;&#39640;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#22686;&#21152;&#20102;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00890</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#25104;&#21151;&#33021;&#30340;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;vanilla&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#21482;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30446;&#26631;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#33719;&#25104;&#23545;&#20851;&#31995;&#65292;&#19968;&#20123;&#27169;&#22411;&#23558;&#25163;&#21160;&#21151;&#33021;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#65292;&#24182;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#26469;&#29983;&#25104;&#25104;&#23545;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20154;&#30452;&#25509;&#23558;&#25163;&#21160;&#21151;&#33021;&#29992;&#20316;&#25104;&#23545;&#34920;&#31034;&#12290;&#23613;&#31649;&#27492;&#31616;&#21270;&#36991;&#20813;&#20102;&#23558;GNN&#36880;&#20010;&#38142;&#25509;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#38142;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#21644;&#19981;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#29305;&#24449;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#24456;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#65288;NCN&#65289;&#65292;&#23427;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;NCN&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#38142;&#25509;&#38382;&#39064;&#12290;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
&lt;/p&gt;</description></item><item><title>&#20449;&#24687;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#21487;&#33021;&#24615;&#26368;&#39640;&#25110;&#34920;&#31034;&#35823;&#24046;&#27604;&#29305;&#26368;&#23569;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;&#23558;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06566</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#20449;&#24687;&#35770;&#36873;&#25321;&#23458;&#35266;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
How to select an objective function using information theory. (arXiv:2212.06566v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06566
&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#21487;&#33021;&#24615;&#26368;&#39640;&#25110;&#34920;&#31034;&#35823;&#24046;&#27604;&#29305;&#26368;&#23569;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;&#23558;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25110;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#27169;&#22411;&#24615;&#33021;&#26159;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#34913;&#37327;&#30340;&#12290;&#20294;&#26159;&#20026;&#20160;&#20040;&#35201;&#36873;&#25321;&#26576;&#20010;&#23458;&#35266;&#20989;&#25968;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#65311;&#20449;&#24687;&#35770;&#32473;&#20986;&#20102;&#19968;&#20010;&#31572;&#26696;&#65306;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#25110;&#32773;&#20195;&#34920;&#35823;&#24046;&#30340;&#27604;&#29305;&#26368;&#23569;&#30340;&#20989;&#25968;&#12290;&#35201;&#35780;&#20272;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#12290;&#20316;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning or scientific computing, model performance is measured with an objective function. But why choose one objective over another? Information theory gives one answer: To maximize the information in the model, select the most likely objective function or whichever represents the error in the fewest bits. To evaluate different objectives, transform them into likelihood functions. As likelihoods, their relative magnitudes represent how much we should prefer one objective versus another, and the log of their magnitude represents the expected uncertainty of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2211.15856</link><description>&lt;p&gt;
&#36229;&#36234;&#38598;&#21512;&#24179;&#22343;&#20540;&#65306;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting. (arXiv:2211.15856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#24615;&#39044;&#27979;&#20013;&#65292;&#23545;&#20110;&#20851;&#38190;&#27668;&#20505;&#21464;&#37327;&#65288;&#22914;&#28201;&#24230;&#21644;&#38477;&#27700;&#65289;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#22320;&#34920;&#23395;&#33410;&#24615;&#26102;&#38388;&#23610;&#24230;&#39044;&#27979;&#19968;&#30452;&#23384;&#22312;&#30528;&#24046;&#36317;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#25512;&#36827;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#65288;SSF&#65289;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#29289;&#29702;&#22522;&#20110;&#27169;&#22411;&#38598;&#21512;&#30340;&#24179;&#22343;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#28982;&#32780;&#38598;&#21512;&#39044;&#27979;&#20013;&#21253;&#21547;&#20102;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#30340;&#20449;&#24687;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#21512;&#22343;&#20540;&#12290;&#20854;&#27425;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20851;&#27880;&#24179;&#22343;&#24615;&#33021;&#65292;&#28982;&#32780;&#23545;&#20110;&#35745;&#21010;&#21644;&#20943;&#28798;&#30446;&#30340;&#26469;&#35828;&#65292;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#31532;&#19977;&#65292;&#27668;&#20505;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#20010;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#32780;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32771;&#34385;&#20102;&#21709;&#24212;&#30340;&#31354;&#38388;&#21487;&#21464;&#24615;&#12290;&#27169;&#22411;&#22534;&#21472;&#21487;&#20197;&#32531;&#35299;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a va
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2210.01672</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#23558;&#26426;&#22120;&#20154;&#20998;&#31867;&#24102;&#20837;&#36830;&#32493;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20998;&#31867;&#34987;&#29992;&#20316;&#23558;&#20154;&#31867;&#30340;&#31227;&#21160;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#23618;&#27425;&#30340;&#20998;&#23618;&#25277;&#35937;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20998;&#26512;&#25235;&#21462;&#12289;&#25805;&#32437;&#25216;&#33021;&#21644;&#20840;&#36523;&#25903;&#25745;&#23039;&#21183;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#35774;&#35745;&#23618;&#27425;&#32467;&#26500;&#21644;&#22522;&#30784;&#31867;&#21035;&#26041;&#38754;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#32570;&#20047;&#22635;&#34917;&#20998;&#31867;&#23618;&#32423;&#32467;&#26500;&#21644;&#19982;&#20854;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#39640;&#32500;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#24046;&#36317;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#21452;&#26354;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#23558;&#20998;&#31867;&#27861;&#32467;&#26500;&#32435;&#20837;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
&lt;/p&gt;</description></item></channel></rss>