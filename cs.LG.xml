<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#65292;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20013;&#25366;&#25496;&#20986;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#65292;&#20197;&#24110;&#21161;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#20248;&#20808;&#22788;&#29702;&#21644;&#20027;&#21160;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.01883</link><description>&lt;p&gt;
&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#20013;&#25366;&#25496;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports. (arXiv:2401.01883v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#65292;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20013;&#25366;&#25496;&#20986;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#65292;&#20197;&#24110;&#21161;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#20248;&#20808;&#22788;&#29702;&#21644;&#20027;&#21160;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#35201;&#27714;&#20174;&#39640;&#32423;&#23545;&#25163;&#34892;&#20026;&#20837;&#25163;&#12290;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#20107;&#20214;&#30340;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#25253;&#21578;&#25551;&#36848;&#20102;&#24694;&#24847;&#34892;&#21160;&#30340;&#26102;&#38388;&#38142;&#12290;&#20026;&#20102;&#36991;&#20813;&#37325;&#22797;&#21457;&#29983;&#32593;&#32476;&#25915;&#20987;&#20107;&#20214;&#65292;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20013;&#20027;&#21160;&#35782;&#21035;&#21644;&#38450;&#24481;&#37325;&#22797;&#38142;&#24335;&#34892;&#21160; - &#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#12290;&#33258;&#21160;&#25366;&#25496;&#34892;&#21160;&#20043;&#38388;&#30340;&#27169;&#24335;&#25552;&#20379;&#20102;&#20851;&#20110;&#36807;&#21435;&#32593;&#32476;&#25915;&#20987;&#30340;&#23545;&#25163;&#34892;&#20026;&#30340;&#32467;&#26500;&#21270;&#21644;&#21487;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#20013;&#25366;&#25496;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#65292;&#24110;&#21161;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#20248;&#20808;&#22788;&#29702;&#21644;&#20027;&#21160;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChronoCTI&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#25253;&#21578;&#20013;&#25366;&#25496;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#12290;&#20026;&#26500;&#24314;ChronoCTI&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defending from cyberattacks requires practitioners to operate on high-level adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack incidents describe the chain of malicious actions with respect to time. To avoid repeating cyberattack incidents, practitioners must proactively identify and defend against recurring chain of actions - which we refer to as temporal attack patterns. Automatically mining the patterns among actions provides structured and actionable information on the adversary behavior of past cyberattacks. The goal of this paper is to aid security practitioners in prioritizing and proactive defense against cyberattacks by mining temporal attack patterns from cyberthreat intelligence reports. To this end, we propose ChronoCTI, an automated pipeline for mining temporal attack patterns from cyberthreat intelligence (CTI) reports of past cyberattacks. To construct ChronoCTI, we build the ground truth dataset of temporal attack patterns and apply state-of-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01879</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#26159;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20174;&#19968;&#20010;&#22522;&#26412;&#31574;&#30053;&#20013;&#25277;&#21462;n&#20010;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#22870;&#21169;&#20989;&#25968;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65292;&#36873;&#25321;&#25490;&#21517;&#26368;&#39640;&#30340;&#26679;&#26412;&#12290;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22768;&#31216;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#19982;&#22522;&#26412;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#31561;&#20110;$\log (n) (n-1)/n$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#35770;&#26029;&#30340;&#19981;&#27491;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21482;&#26159;&#23454;&#38469;KL&#25955;&#24230;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35813;&#19978;&#30028;&#30340;&#32039;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#23427;&#33021;&#25552;&#20379;&#19968;&#20010;&#32039;&#33268;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\log (n) (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#38754;&#27963;&#24615;&#21058;&#22810;&#24615;&#36136;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#21019;&#24314;&#26368;&#22823;&#30340;CMC&#25968;&#25454;&#24211;&#21644;&#39318;&#20010;&#22823;&#22411;&#34920;&#38754;&#36801;&#31227;&#27987;&#24230;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#34920;&#38754;&#27963;&#24615;&#21058;&#30340;&#20851;&#38190;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2401.01874</link><description>&lt;p&gt;
&#34920;&#38754;&#27963;&#24615;&#21058;&#22810;&#24615;&#36136;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Surfactant Multi-Property Prediction. (arXiv:2401.01874v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#38754;&#27963;&#24615;&#21058;&#22810;&#24615;&#36136;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#21019;&#24314;&#26368;&#22823;&#30340;CMC&#25968;&#25454;&#24211;&#21644;&#39318;&#20010;&#22823;&#22411;&#34920;&#38754;&#36801;&#31227;&#27987;&#24230;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#34920;&#38754;&#27963;&#24615;&#21058;&#30340;&#20851;&#38190;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#27963;&#24615;&#21058;&#22312;&#21270;&#22918;&#21697;&#12289;&#27927;&#28068;&#21058;&#12289;&#27833;&#30000;&#22238;&#25910;&#21644;&#33647;&#29289;&#20256;&#36882;&#31995;&#32479;&#31561;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#20013;&#20855;&#26377;&#39640;&#24230;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#34920;&#38754;&#27963;&#24615;&#21058;&#30340;&#23450;&#37327;&#32467;&#26500;-&#24615;&#33021;&#20851;&#31995;&#65288;QSPR&#65289;&#27169;&#22411;&#12290;&#27599;&#20010;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20391;&#37325;&#20110;&#19968;&#31867;&#34920;&#38754;&#27963;&#24615;&#21058;&#65292;&#20027;&#35201;&#26159;&#38750;&#31163;&#23376;&#34920;&#38754;&#27963;&#24615;&#21058;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#31163;&#23376;&#28082;&#20307;&#12289;&#32858;&#21512;&#29289;&#21644;&#33647;&#29289;&#31561;&#39046;&#22495;&#30340;&#24615;&#33021;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#23545;&#20110;&#34920;&#38754;&#27963;&#24615;&#21058;&#65292;GNNs&#21487;&#20197;&#25104;&#21151;&#39044;&#27979;&#20851;&#38190;&#33014;&#26463;&#27987;&#24230;&#65288;CMC&#65289;&#65292;&#36825;&#26159;&#19982;&#33014;&#26463;&#21270;&#30456;&#20851;&#30340;&#20851;&#38190;&#34920;&#38754;&#27963;&#24615;&#21058;&#24615;&#36136;&#12290;&#23545;&#20110;QSPR&#21644;GNN&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22522;&#20110;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#30740;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#21487;&#29992;CMC&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;429&#20010;&#20998;&#23376;&#65292;&#24182;&#19988;&#39318;&#27425;&#36827;&#34892;&#20102;&#19982;&#27873;&#27819;&#30456;&#20851;&#30340;&#21478;&#19968;&#34920;&#38754;&#27963;&#24615;&#21058;&#24615;&#36136;&#8212;&#8212;&#34920;&#38754;&#36801;&#31227;&#27987;&#24230;&#65288;$\Gamma$$_{m}$&#65289;&#30340;&#22823;&#22411;&#25968;&#25454;&#25910;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;164&#20010;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surfactants are of high importance in different industrial sectors such as cosmetics, detergents, oil recovery and drug delivery systems. Therefore, many quantitative structure-property relationship (QSPR) models have been developed for surfactants. Each predictive model typically focuses on one surfactant class, mostly nonionics. Graph Neural Networks (GNNs) have exhibited a great predictive performance for property prediction of ionic liquids, polymers and drugs in general. Specifically for surfactants, GNNs can successfully predict critical micelle concentration (CMC), a key surfactant property associated with micellization. A key factor in the predictive ability of QSPR and GNN models is the data available for training. Based on extensive literature search, we create the largest available CMC database with 429 molecules and the first large data collection for surface excess concentration ($\Gamma$$_{m}$), another surfactant property associated with foaming, with 164 molecules. Then
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#24050;&#30693;&#30340;&#38382;&#39064;&#23545;&#31216;&#24615;&#26080;&#27861;&#32531;&#35299;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#65292;&#24182;&#32473;&#20986;&#20102;&#22810;&#31181;&#32593;&#32476;&#24418;&#24335;&#30340;&#19979;&#30028;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01869</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#23545;&#31216;&#24615;&#22256;&#38590;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the hardness of learning under symmetries. (arXiv:2401.01869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#24050;&#30693;&#30340;&#38382;&#39064;&#23545;&#31216;&#24615;&#26080;&#27861;&#32531;&#35299;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#65292;&#24182;&#32473;&#20986;&#20102;&#22810;&#31181;&#32593;&#32476;&#24418;&#24335;&#30340;&#19979;&#30028;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;&#23558;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65288;"&#31561;&#21464;&#24615;"&#65289;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24050;&#32463;&#22312;&#20174;&#29983;&#29289;&#23398;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#23398;&#20064;&#27969;&#31243;&#20013;&#32463;&#39564;&#19978;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#32780;&#29420;&#31435;&#30340;&#23398;&#20064;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#30456;&#20851;&#32479;&#35745;&#26597;&#35810;&#65288;CSQ&#65289;&#27169;&#22411;&#20013;&#65292;&#23454;&#38469;&#23398;&#20064;&#27973;&#23618;&#20840;&#36830;&#25509;&#65288;&#21363;&#38750;&#23545;&#31216;&#65289;&#32593;&#32476;&#22312;&#22797;&#26434;&#24615;&#19978;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#22797;&#26434;&#24230;&#65292;&#35813;&#27169;&#22411;&#21253;&#25324;&#26799;&#24230;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24050;&#30693;&#30340;&#38382;&#39064;&#23545;&#31216;&#24615;&#26159;&#21542;&#36275;&#20197;&#32531;&#35299;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#22256;&#38590;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#21542;&#23450;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#27973;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#32593;&#32476;&#12289;&#19981;&#21464;&#22810;&#39033;&#24335;&#21644;&#32622;&#25442;&#23376;&#32676;&#30340;&#24103;&#24179;&#22343;&#32593;&#32476;&#30340;&#19979;&#30028;&#65292;&#23427;&#20204;&#22312;&#30456;&#20851;&#36755;&#20837;&#32500;&#24230;&#19978;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#35201;&#20040;&#36229;&#22810;&#39033;&#24335;&#65292;&#35201;&#20040;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries ("equivariance") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimens
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#20010;&#21035;&#31034;&#20363;&#30340;&#38590;&#24230;&#35780;&#20998;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35757;&#32451;&#36816;&#34892;&#12289;&#35780;&#20998;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#20043;&#38388;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#19968;&#20123;&#25935;&#24863;&#24615;&#25351;&#32441;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.01867</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#38590;&#24230;&#21644;&#24402;&#32435;&#20559;&#24046;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dataset Difficulty and the Role of Inductive Bias. (arXiv:2401.01867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#20010;&#21035;&#31034;&#20363;&#30340;&#38590;&#24230;&#35780;&#20998;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35757;&#32451;&#36816;&#34892;&#12289;&#35780;&#20998;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#20043;&#38388;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#19968;&#20123;&#25935;&#24863;&#24615;&#25351;&#32441;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#32570;&#38519;&#35782;&#21035;&#30446;&#26631;&#30340;&#21551;&#21457;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#35780;&#20998;&#25968;&#25454;&#38598;&#20013;&#30340;&#20010;&#21035;&#31034;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31034;&#20363;&#38590;&#24230;&#35780;&#20998;&#8221;&#65292;&#36890;&#24120;&#29992;&#20110;&#23545;&#31034;&#20363;&#36827;&#34892;&#25490;&#21517;&#25110;&#20998;&#31867;&#65292;&#20294;&#26159;&#19981;&#21516;&#35757;&#32451;&#36816;&#34892;&#12289;&#35780;&#20998;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#20043;&#38388;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#30830;&#23450;&#30001;&#20110;&#36825;&#20123;&#38543;&#26426;&#21644;&#21463;&#25511;&#25928;&#24212;&#32780;&#23548;&#33268;&#30340;&#31034;&#20363;&#25490;&#21517;&#30340;&#21464;&#21270;&#24773;&#20917;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#19981;&#21516;&#24471;&#20998;&#20844;&#24335;&#22312;&#22810;&#20010;&#36816;&#34892;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#21457;&#29616;&#24471;&#20998;&#20027;&#35201;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#23427;&#20204;&#22312;&#27169;&#22411;&#30340;&#21333;&#27425;&#36816;&#34892;&#20013;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#19982;&#19968;&#31181;&#38590;&#24230;&#27010;&#24565;&#24378;&#30456;&#20851;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#31034;&#20363;&#22312;&#26576;&#20123;&#27169;&#22411;&#26550;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#20013;&#20174;&#39640;&#24230;&#25935;&#24863;&#21040;&#19981;&#25935;&#24863;&#30340;&#21464;&#21270;&#12290;&#20511;&#37492;&#32479;&#35745;&#36951;&#20256;&#23398;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#19968;&#20123;&#25935;&#24863;&#24615;&#25351;&#32441;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call "example difficulty scores", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21644;&#35782;&#21035;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#31995;&#32479;&#23637;&#31034;&#20102;LLMs&#23545;&#35270;&#35273;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#30830;&#24314;&#27169;&#23383;&#31526;&#20018;&#21487;&#20197;&#25945;&#20250;LLMs&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;LLMs&#21487;&#20197;&#35757;&#32451;&#33021;&#22815;&#23545;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#35780;&#20272;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01862</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21644;&#35782;&#21035;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#31995;&#32479;&#23637;&#31034;&#20102;LLMs&#23545;&#35270;&#35273;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#30830;&#24314;&#27169;&#23383;&#31526;&#20018;&#21487;&#20197;&#25945;&#20250;LLMs&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;LLMs&#21487;&#20197;&#35757;&#32451;&#33021;&#22815;&#23545;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#35780;&#20272;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24314;&#27169;&#23383;&#31526;&#20018;&#20043;&#38388;&#20851;&#31995;&#26159;&#21542;&#33021;&#25945;&#20250;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#30693;&#35782;&#65311;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#21644;&#35782;&#21035;&#21508;&#31181;&#36880;&#28176;&#22797;&#26434;&#30340;&#35270;&#35273;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#27169;&#22411;&#26469;&#35757;&#32451;&#21021;&#27493;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#31995;&#32479;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#20197;&#20687;&#32032;&#24418;&#24335;&#33719;&#21462;&#25110;&#36755;&#20986;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#20195;&#30721;&#26469;&#34920;&#31034;&#22270;&#20687;&#12290;&#34429;&#28982;LLMs&#29983;&#25104;&#30340;&#22270;&#20687;&#30475;&#36215;&#26469;&#19981;&#20687;&#33258;&#28982;&#22270;&#20687;&#65292;&#20294;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#27169;&#22411;&#32416;&#27491;&#36825;&#20123;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#26041;&#38754;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#30830;&#24314;&#27169;&#23383;&#31526;&#20018;&#21487;&#20197;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#19982;&#25991;&#26412;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#23454;&#39564;&#35777;&#26126;&#20102;&#20165;&#20351;&#29992;LLMs&#21487;&#20197;&#35757;&#32451;&#33021;&#22815;&#23545;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#35780;&#20272;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#20132;&#21449;&#23398;&#20064;&#8221;&#35774;&#32622;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#65292;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21327;&#35843;&#22810;&#20010;&#26102;&#26399;&#20869;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#25191;&#34892;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#12290;&#27492;&#31639;&#27861;&#35299;&#20915;&#20102;&#23398;&#20064;&#22312;&#19968;&#20215;&#25293;&#21334;&#21644;&#30561;&#35273;&#36172;&#24466;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01857</link><description>&lt;p&gt;
&#26410;&#30693;&#19978;&#19979;&#25991;&#20998;&#24067;&#19979;&#19978;&#19979;&#25991;&#36172;&#21338;&#26368;&#20248;&#20132;&#21449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal cross-learning for contextual bandits with unknown context distributions. (arXiv:2401.01857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#20132;&#21449;&#23398;&#20064;&#8221;&#35774;&#32622;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#65292;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21327;&#35843;&#22810;&#20010;&#26102;&#26399;&#20869;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#25191;&#34892;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#12290;&#27492;&#31639;&#27861;&#35299;&#20915;&#20102;&#23398;&#20064;&#22312;&#19968;&#20215;&#25293;&#21334;&#21644;&#30561;&#35273;&#36172;&#24466;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;Balseiro&#31561;&#20154;&#30340;&#8220;&#20132;&#21449;&#23398;&#20064;&#8221;&#35774;&#32622;&#20013;&#35774;&#35745;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#35266;&#23519;&#21040;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#19978;&#19979;&#25991;&#20013;&#20182;&#20204;&#25152;&#36873;&#25321;&#30340;&#21160;&#20316;&#30340;&#25439;&#22833;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#22238;&#21512;&#30340;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#22312;&#25439;&#22833;&#34987;&#23545;&#25239;&#24615;&#22320;&#36873;&#25321;&#21644;&#19978;&#19979;&#25991;&#20174;&#26410;&#30693;&#20998;&#24067;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#25928;&#29575;&#31639;&#27861;&#35299;&#20915;&#20102;Balseiro&#31561;&#20154;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#20854;&#36951;&#25022;&#30028;&#20960;&#20046;&#36798;&#21040;&#32039;&#33268;&#27700;&#24179;&#65288;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#20026;$\widetilde{O}(\sqrt{TK})$&#65292;&#19981;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#22312;&#26410;&#30693;&#20540;&#20998;&#24067;&#30340;&#19968;&#20215;&#25293;&#21334;&#20013;&#23398;&#20064;&#20986;&#20960;&#20046;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#21644;&#20855;&#26377;&#38543;&#26426;&#21160;&#20316;&#38598;&#30340;&#30561;&#35273;&#36172;&#24466;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of designing contextual bandit algorithms in the ``cross-learning'' setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. We specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution. In this setting, we resolve an open problem of Balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\widetilde{O}(\sqrt{TK})$, independent of the number of contexts. As a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.  At the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21464;&#21387;&#22120;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#65288;T-NAFs&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.01855</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;
&lt;/p&gt;
&lt;p&gt;
Transformer Neural Autoregressive Flows. (arXiv:2401.01855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21464;&#21387;&#22120;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#65288;T-NAFs&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#20272;&#35745;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#65288;NFs&#65289;&#26469;&#25191;&#34892;&#12290;NFs&#30001;&#19968;&#31995;&#21015;&#21487;&#36870;&#21464;&#25442;&#32452;&#25104;&#65292;&#21033;&#29992;&#21464;&#37327;&#21464;&#25442;&#23450;&#29702;&#23558;&#22797;&#26434;&#30340;&#30446;&#26631;&#20998;&#24067;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#20998;&#24067;&#12290;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#65288;NAFs&#65289;&#21644;&#22359;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#65288;B-NAFs&#65289;&#21487;&#20197;&#35828;&#26159;NF&#23478;&#26063;&#20013;&#24615;&#33021;&#26368;&#22909;&#30340;&#25104;&#21592;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#32593;&#32476;&#32467;&#26500;&#26045;&#21152;&#30340;&#32422;&#26463;&#65292;&#23427;&#20204;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#23450;&#20041;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#27969;&#65292;&#31216;&#20026;&#21464;&#21387;&#22120;&#31070;&#32463;&#33258;&#22238;&#24402;&#27969;&#65288;T-NAFs&#65289;&#12290;T-NAFs&#23558;&#38543;&#26426;&#21464;&#37327;&#30340;&#27599;&#20010;&#32500;&#24230;&#35270;&#20026;&#21333;&#29420;&#30340;&#36755;&#20837;&#20196;&#29260;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#25513;&#30721;&#26469;&#24378;&#21046;&#25191;&#34892;&#33258;&#22238;&#24402;&#32422;&#26463;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#31867;&#20284;&#20110;&#20998;&#26399;&#20607;&#36824;&#30340;&#26041;&#27861;&#65292;&#21464;&#21387;&#22120;&#36755;&#20986;&#21487;&#36870;&#21464;&#25442;&#30340;&#21442;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;.
&lt;/p&gt;
&lt;p&gt;
Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs). NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant members of the NF family. However, they suffer scalability issues and training instability due to the constraints imposed on the network structure. In this paper, we propose a novel solution to these challenges by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint. We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation. The experimental results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>DGDNN&#26159;&#19968;&#31181;&#35299;&#32806;&#30340;&#22270;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;&#65292;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#21160;&#24577;&#32929;&#31080;&#22270;&#24182;&#23398;&#20064;&#32929;&#31080;&#20043;&#38388;&#30340;&#20219;&#21153;&#26368;&#20248;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.01846</link><description>&lt;p&gt;
DGDNN&#65306;&#35299;&#32806;&#22270;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction. (arXiv:2401.01846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01846
&lt;/p&gt;
&lt;p&gt;
DGDNN&#26159;&#19968;&#31181;&#35299;&#32806;&#30340;&#22270;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;&#65292;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#21160;&#24577;&#32929;&#31080;&#22270;&#24182;&#23398;&#20064;&#32929;&#31080;&#20043;&#38388;&#30340;&#20219;&#21153;&#26368;&#20248;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26410;&#26469;&#32929;&#31080;&#36235;&#21183;&#23545;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38543;&#26426;&#30340;&#32929;&#31080;&#38388;&#21160;&#24577;&#21644;&#23618;&#27425;&#24615;&#32929;&#31080;&#20869;&#37096;&#21160;&#24577;&#24433;&#21709;&#32929;&#31080;&#20215;&#26684;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#22810;&#20010;&#32929;&#31080;&#26500;&#24314;&#20026;&#22270;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#22312;&#27492;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#20154;&#24037;&#23450;&#20041;&#30340;&#22240;&#32032;&#26469;&#26500;&#24314;&#38745;&#24577;&#32929;&#31080;&#22270;&#65292;&#26080;&#27861;&#25429;&#25417;&#36805;&#36895;&#28436;&#21464;&#30340;&#32929;&#31080;&#38388;&#20869;&#22312;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#32929;&#31080;&#30340;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#20002;&#22833;&#20102;&#20854;&#20013;&#30340;&#29305;&#27530;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#19987;&#23478;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20449;&#21495;&#22788;&#29702;&#30340;&#35282;&#24230;&#33258;&#21160;&#26500;&#24314;&#21160;&#24577;&#32929;&#31080;&#22270;&#65292;&#36890;&#36807;&#22522;&#20110;&#29109;&#30340;&#36793;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#20041;&#22270;&#20256;&#25773;&#36827;&#19968;&#27493;&#23398;&#20064;&#32929;&#31080;&#20043;&#38388;&#30340;&#20219;&#21153;&#26368;&#20248;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices. In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data. However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve. In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within. In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues. First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective. Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#27969;&#24418;&#27491;&#21017;&#21270;&#30340;Wasserstein&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#24352;&#37327;&#25968;&#25454;&#21644;&#37325;&#26500;&#25968;&#25454;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21033;&#29992;&#29305;&#24449;&#30340;&#30456;&#20851;&#20449;&#24687;&#21644;&#26679;&#26412;&#30340;&#27969;&#24418;&#20449;&#24687;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#37096;&#20998;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.01842</link><description>&lt;p&gt;
&#24102;&#26377;&#27969;&#24418;&#27491;&#21017;&#21270;&#30340;Wasserstein&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Nonnegative Tensor Factorization with Manifold Regularization. (arXiv:2401.01842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01842
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#27969;&#24418;&#27491;&#21017;&#21270;&#30340;Wasserstein&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#24352;&#37327;&#25968;&#25454;&#21644;&#37325;&#26500;&#25968;&#25454;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21033;&#29992;&#29305;&#24449;&#30340;&#30456;&#20851;&#20449;&#24687;&#21644;&#26679;&#26412;&#30340;&#27969;&#24418;&#20449;&#24687;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#37096;&#20998;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;(NTF)&#24050;&#25104;&#20026;&#20174;&#38750;&#36127;&#39640;&#38454;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#21644;&#22522;&#20110;&#37096;&#20998;&#34920;&#31034;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20869;&#22312;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;NTF&#26041;&#27861;&#37319;&#29992;&#27431;&#20960;&#37324;&#24471;&#25110;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#27599;&#20010;&#29305;&#24449;&#37117;&#35270;&#20026;&#30456;&#31561;&#65292;&#23548;&#33268;&#24573;&#35270;&#20102;&#29305;&#24449;&#30340;&#26049;&#36793;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#29305;&#24449;&#30340;&#30456;&#20851;&#20449;&#24687;&#21644;&#26679;&#26412;&#30340;&#27969;&#24418;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#27969;&#24418;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;(WMNTF)&#65292;&#20854;&#23558;&#36755;&#20837;&#24352;&#37327;&#25968;&#25454;&#30340;&#20998;&#24067;&#21644;&#37325;&#26500;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#23613;&#31649;&#22312;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#20013;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#39640;&#38454;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;Wasserstein&#36317;&#31163;(&#20063;&#31216;&#20026;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#25110;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;)&#20316;&#20026;&#24230;&#37327;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#22270;&#27491;&#21017;&#21270;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data. However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features. To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction. Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data. We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01841</link><description>&lt;p&gt;
&#25353;&#29031;&#20320;&#30340;&#23398;&#20064;&#34892;&#21160;&#65306;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#22788;&#29702;&#38750;&#31283;&#24577;&#29615;&#22659;&#26159;&#19968;&#20010;&#22522;&#26412;&#65288;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#35299;&#20915;&#30340;&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#22806;&#37096;&#29615;&#22659;&#26465;&#20214;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#31867;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NSMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NSMDP&#20915;&#31574;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#35774;&#24403;&#21069;&#26102;&#21051;&#26356;&#26032;&#30340;&#29615;&#22659;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65288;&#23613;&#31649;&#26410;&#26469;&#21160;&#24577;&#21487;&#33021;&#20250;&#25913;&#21464;&#65289;&#65307;&#20854;&#27425;&#65292;&#35268;&#21010;&#36807;&#31243;&#20027;&#35201;&#26159;&#24754;&#35266;&#30340;&#65292;&#21363;&#20195;&#29702;&#20154;&#20250;&#8220;&#23433;&#20840;&#34892;&#21160;&#8221;&#20197;&#32771;&#34385;&#29615;&#22659;&#30340;&#38750;&#31283;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#25928;&#30340;-&#26356;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#24456;&#23569;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24403;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#26102;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#26356;&#26032;&#30340;&#21160;&#24577;&#24182;&#36991;&#20813;&#24754;&#35266;&#65292;&#33267;&#23569;&#22312;&#20854;&#23545;&#21160;&#24577;&#26377;&#20449;&#24515;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#65292;&#36890;&#36807;&#36845;&#20195;&#25513;&#30721;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01830</link><description>&lt;p&gt;
&#36845;&#20195;&#25513;&#30721;&#22635;&#20805;&#65306;&#19968;&#31181;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30340;&#26377;&#25928;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#65292;&#36890;&#36807;&#36845;&#20195;&#25513;&#30721;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23427;&#30340;&#24212;&#29992;&#36828;&#19981;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#36845;&#20195;&#25513;&#30721;&#65292;&#24182;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#37117;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20027;&#39064;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#33410;&#32454;&#32990;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#25918;&#30005;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27491;&#21322;&#23450;&#24230;&#37327;&#30697;&#38453;&#26469;&#23450;&#20041;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32454;&#32990;&#28508;&#22312;&#25805;&#20316;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.01813</link><description>&lt;p&gt;
&#35270;&#32593;&#33180;&#20013;&#30340;&#20449;&#21495;&#22788;&#29702;&#65306;&#21487;&#35299;&#37322;&#30340;&#22270;&#20998;&#31867;&#22120;&#39044;&#27979;&#31070;&#32463;&#33410;&#32454;&#32990;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses. (arXiv:2401.01813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#33410;&#32454;&#32990;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#25918;&#30005;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27491;&#21322;&#23450;&#24230;&#37327;&#30697;&#38453;&#26469;&#23450;&#20041;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32454;&#32990;&#28508;&#22312;&#25805;&#20316;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#26377;&#19968;&#20010;&#26222;&#36941;&#30340;&#20551;&#35774;&#35748;&#20026;&#35270;&#32593;&#33180;&#20013;&#30340;&#31070;&#32463;&#33410;&#32454;&#32990;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#26816;&#27979;&#35266;&#23519;&#22330;&#26223;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#34987;&#28608;&#27963;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31070;&#32463;&#33410;&#32454;&#32990;&#30340;&#25918;&#30005;&#65292;&#20294;&#32593;&#32476;&#20173;&#28982;&#26159;&#26080;&#27861;&#35299;&#37322;&#30340;&#65292;&#22240;&#27492;&#23545;&#32454;&#32990;&#28508;&#22312;&#25805;&#20316;&#30340;&#29702;&#35299;&#21313;&#20998;&#26377;&#38480;&#12290;&#20026;&#20102;&#20174;&#32454;&#32990;&#25918;&#30005;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#33410;&#32454;&#32990;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#25918;&#30005;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#27491;&#21322;&#23450;&#65288;PSD&#65289;&#24230;&#37327;&#30697;&#38453;$\mathbf{M} \succeq 0$&#65292;&#35813;&#30697;&#38453;&#23450;&#20041;&#20102;&#20855;&#26377;&#39044;&#35745;&#31639;&#29305;&#24449;&#21521;&#37327;&#30340;&#22270;&#33410;&#28857;&#65288;&#35270;&#35273;&#20107;&#20214;&#65289;&#20043;&#38388;&#30340;&#39532;&#27663;&#36317;&#31163;&#65307;&#35745;&#31639;&#30340;&#33410;&#28857;&#38388;&#36317;&#31163;&#23548;&#33268;&#36793;&#26435;&#37325;&#21644;&#21487;&#36827;&#34892;&#20108;&#36827;&#21046;&#20998;&#31867;&#30340;&#32452;&#21512;&#22270;&#12290;&#25968;&#23398;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#36793;&#30028;&#26368;&#36817;&#37051;&#30340;&#22270;&#36866;&#24212;&#24418;&#24335;&#26469;&#23450;&#20041;&#24230;&#37327;&#30697;&#38453;$\mathbf{M}$&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene. While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations. To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli. Specifically, we learn a positive semi-definite (PSD) metric matrix $\mathbf{M} \succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification. Mathematically, we define the objective of metric matrix $\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbo
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.01801</link><description>&lt;p&gt;
&#19968;&#20010;&#37327;&#23376;&#21551;&#21457;&#30340;&#29992;&#20110;&#20960;&#20309;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#29289;&#29702;&#31995;&#32479;&#26500;&#24819;&#20026;3D&#22810;&#20307;&#28857;&#20113;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#22914;SE(3)/E(3)&#31561;&#25928;GNN&#65292;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#39640;&#25928;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20351;&#23427;&#20204;&#33021;&#22815;&#29087;&#32451;&#22320;&#23545;&#20998;&#23376;&#21644;&#26230;&#20307;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20960;&#20309;GNN&#21482;&#25552;&#20379;&#20102;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#23553;&#35013;&#22312;&#20004;&#20307;&#28040;&#24687;&#20256;&#36882;&#20013;&#65292;&#22240;&#27492;&#22312;&#25429;&#25417;&#36825;&#20123;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#26377;&#25152;&#27424;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#35745;&#31639;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#39640;&#38454;&#24352;&#37327;&#26469;&#22788;&#29702;&#22810;&#20307;&#31995;&#32479;&#30340;&#24352;&#37327;&#32593;&#32476;&#34987;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#24352;&#37327;&#21270;&#32593;&#32476;&#25972;&#21512;&#21040;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#20013;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#21644;&#23545;&#31216;&#24615;&#20445;&#25345;&#65288;&#22914;&#32622;&#25442;&#21644;&#26059;&#36716;&#65289;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31561;&#21464;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#25193;&#25955;&#25945;&#24072;&#27169;&#22411;&#21644;&#25552;&#21462;&#33258;&#19968;&#33268;&#24615;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#22768;&#38899;&#29983;&#25104;&#21644;&#39640;&#36895;&#30340;&#37319;&#26679;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#22768;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01792</link><description>&lt;p&gt;
CoMoSVC: &#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
CoMoSVC: Consistency Model-based Singing Voice Conversion. (arXiv:2401.01792v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#25193;&#25955;&#25945;&#24072;&#27169;&#22411;&#21644;&#25552;&#21462;&#33258;&#19968;&#33268;&#24615;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#22768;&#38899;&#29983;&#25104;&#21644;&#39640;&#36895;&#30340;&#37319;&#26679;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#22768;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;&#65288;SVC&#65289;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20135;&#29983;&#20102;&#19982;&#30446;&#26631;&#38899;&#33394;&#30456;&#20284;&#24230;&#39640;&#30340;&#33258;&#28982;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#37319;&#26679;&#36807;&#31243;&#23548;&#33268;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#22240;&#27492;&#21152;&#36895;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;CoMoSVC&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#39640;&#36895;&#37319;&#26679;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#38024;&#23545;SVC&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#29305;&#24615;&#36827;&#19968;&#27493;&#25552;&#21462;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#19968;&#27493;&#37319;&#26679;&#12290;&#22312;&#21333;&#20010;NVIDIA GTX4090 GPU&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;CoMoSVC&#30340;&#25512;&#29702;&#36895;&#24230;&#27604;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#22522;&#20110;&#25193;&#25955;&#30340;SVC&#31995;&#32479;&#35201;&#24555;&#24471;&#22810;&#65292;&#20294;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#19978;&#20173;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#36716;&#25442;&#24615;&#33021;&#12290;&#38899;&#39057;&#26679;&#26412;&#21644;&#20195;&#30721;&#21487;&#22312;&#32593;&#22336;https://comosvc.github.io/&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diffusion-based Singing Voice Conversion (SVC) methods have achieved remarkable performances, producing natural audios with high similarity to the target timbre. However, the iterative sampling process results in slow inference speed, and acceleration thus becomes crucial. In this paper, we propose CoMoSVC, a consistency model-based SVC method, which aims to achieve both high-quality generation and high-speed sampling. A diffusion-based teacher model is first specially designed for SVC, and a student model is further distilled under self-consistency properties to achieve one-step sampling. Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a significantly faster inference speed than the state-of-the-art (SOTA) diffusion-based SVC system, it still achieves comparable or superior conversion performance based on both subjective and objective metrics. Audio samples and codes are available at https://comosvc.github.io/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;LSTM&#32593;&#32476;&#65289;&#35780;&#20272;&#20102;&#20998;&#25968;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;Hurst&#21442;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#21644;&#20998;&#25968;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20013;&#65292;LSTM&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#65292;&#20294;&#22312;&#32447;&#24615;&#20998;&#25968;&#31283;&#23450;&#36816;&#21160;&#36807;&#31243;&#20013;&#20934;&#30830;&#24615;&#21463;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01789</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32447;&#24615;&#20998;&#25968;&#36807;&#31243;&#30340;Hurst&#21442;&#25968;&#21450;&#20854;&#21487;&#38752;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep learning the Hurst parameter of linear fractional processes and assessing its reliability. (arXiv:2401.01789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;LSTM&#32593;&#32476;&#65289;&#35780;&#20272;&#20102;&#20998;&#25968;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;Hurst&#21442;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#21644;&#20998;&#25968;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20013;&#65292;LSTM&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#65292;&#20294;&#22312;&#32447;&#24615;&#20998;&#25968;&#31283;&#23450;&#36816;&#21160;&#36807;&#31243;&#20013;&#20934;&#30830;&#24615;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65289;&#22312;&#20272;&#35745;&#20998;&#25968;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;Hurst&#21442;&#25968;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#38598;&#20013;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#36807;&#31243;&#19978;&#65306;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;fBm&#65289;&#65292;&#20998;&#25968;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#65288;fOU&#65289;&#36807;&#31243;&#21644;&#32447;&#24615;&#20998;&#25968;&#31283;&#23450;&#36816;&#21160;&#65288;lfsm&#65289;&#12290;&#30740;&#31350;&#21253;&#25324;&#23545;fBm&#21644;fOU&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#29983;&#25104;&#65292;&#20197;&#20415;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#35757;&#32451;LSTM&#32593;&#32476;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;LSTM&#32593;&#32476;&#22312;Hurst&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#65292;&#30456;&#23545;&#35823;&#24046;&#30340;&#20998;&#20301;&#25968;&#31561;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;fBm&#21644;fOU&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;LSTM&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;lfsm&#36807;&#31243;&#19978;&#30340;&#20934;&#30830;&#24615;&#26377;&#38480;&#12290;&#30740;&#31350;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#35757;&#32451;&#38271;&#24230;&#21644;&#35780;&#20272;&#24207;&#21015;&#38271;&#24230;&#23545;LSTM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This research explores the reliability of deep learning, specifically Long Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in fractional stochastic processes. The study focuses on three types of processes: fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process, and linear fractional stable motions (lfsm). The work involves a fast generation of extensive datasets for fBm and fOU to train the LSTM network on a large volume of data in a feasible time. The study analyses the accuracy of the LSTM network's Hurst parameter estimation regarding various performance measures like RMSE, MAE, MRE, and quantiles of the absolute and relative errors. It finds that LSTM outperforms the traditional statistical methods in the case of fBm and fOU processes; however, it has limited accuracy on lfsm processes. The research also delves into the implications of training length and valuation sequence length on the LSTM's performance. The methodology is applied by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#21644;&#36755;&#20837;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#39640;&#25104;&#26412;&#30417;&#27979;&#12289;&#20302;&#25104;&#26412;&#29289;&#32852;&#32593;&#21644;&#28151;&#21512;&#39044;&#27979;&#36825;&#19977;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review. (arXiv:2401.01788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#21644;&#36755;&#20837;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#39640;&#25104;&#26412;&#30417;&#27979;&#12289;&#20302;&#25104;&#26412;&#29289;&#32852;&#32593;&#21644;&#28151;&#21512;&#39044;&#27979;&#36825;&#19977;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#31354;&#27668;&#27745;&#26579;&#27599;&#24180;&#33268;&#20351;&#19971;&#30334;&#19975;&#20154;&#27515;&#20129;&#12290;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#26159;&#24433;&#21709;&#20302;&#25910;&#20837;&#12289;&#20013;&#25910;&#20837;&#21644;&#39640;&#25910;&#20837;&#22269;&#23478;&#30340;&#20027;&#35201;&#29615;&#22659;&#20581;&#24247;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#30740;&#31350;&#30028;&#24050;&#32463;&#25506;&#32034;&#20102;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#31995;&#32479;&#22320;&#32508;&#36848;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#20351;&#29992;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#21644;&#36755;&#20837;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#26412;&#25991;&#20026;&#27492;&#32508;&#36848;&#21046;&#23450;&#20102;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#21021;&#22987;PRISMA&#38454;&#27573;&#20849;&#25910;&#38598;&#20102;1086&#31687;&#25991;&#29486;&#12290;&#36890;&#36807;&#31579;&#36873;&#21644;&#30830;&#23450;&#36164;&#26684;&#30340;&#38454;&#27573;&#65292;&#26368;&#32456;&#36873;&#25321;&#20102;37&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#22522;&#20110;&#25104;&#26412;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#39640;&#25104;&#26412;&#30417;&#27979;&#12289;&#20302;&#25104;&#26412;&#29289;&#32852;&#32593;&#21644;&#28151;&#21512;&#39044;&#27979;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#19977;&#31181;&#39044;&#27979;&#26041;&#27861;&#65306;&#26102;&#38388;&#24207;&#21015;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the World Health Organization (WHO), air pollution kills seven million people every year. Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries. In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction. The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used. Two research questions were formulated for this review. 1086 publications were collected in the initial PRISMA stage. After the screening and eligibility phases, 37 papers were selected for inclusion. A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction. Three methods of prediction were identified: time series, feature-based and spatio-t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01783</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#30340;&#36229;&#27874;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#29992;&#20110;&#25968;&#20540;&#35299;PDE&#65292;&#26368;&#36817;&#21457;&#23637;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22914;PINN&#21644;&#31070;&#32463;&#31639;&#23376;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#32570;&#28857;&#65292;&#26377;&#24456;&#22810;&#31181;&#31867;&#22411;&#30340;&#30740;&#31350;&#23558;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#65292;&#23558;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#25968;&#20540;&#36890;&#37327;&#26367;&#25442;&#20026;&#31070;&#32463;&#31639;&#23376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#21463;&#25968;&#20540;&#26041;&#26696;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;FNO&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19982;&#21407;&#22987;&#26041;&#27861;&#30340;&#27604;&#36739;&#20855;&#26377;&#25968;&#20540;&#26041;&#26696;&#21644;FNO&#30340;&#20248;&#21183;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
&lt;/p&gt;</description></item><item><title>&#29702;&#35299;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#31867;&#21035;&#32423;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#22312;&#25552;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20250;&#26174;&#33879;&#38477;&#20302;&#20010;&#21035;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#24433;&#21709;&#20027;&#35201;&#20986;&#29616;&#22312;&#27169;&#31946;&#12289;&#20849;&#29616;&#25110;&#32454;&#31890;&#24230;&#21306;&#21035;&#30340;&#31867;&#21035;&#19978;&#65292;&#25968;&#25454;&#22686;&#24378;&#21463;&#25511;&#20110;&#27169;&#22411;&#23545;&#20854;&#20013;&#19968;&#20010;&#30456;&#20851;&#31867;&#21035;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.01764</link><description>&lt;p&gt;
&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21033;&#31867;&#32423;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Detrimental Class-level Effects of Data Augmentation. (arXiv:2401.01764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01764
&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#31867;&#21035;&#32423;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#22312;&#25552;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20250;&#26174;&#33879;&#38477;&#20302;&#20010;&#21035;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#24433;&#21709;&#20027;&#35201;&#20986;&#29616;&#22312;&#27169;&#31946;&#12289;&#20849;&#29616;&#25110;&#32454;&#31890;&#24230;&#21306;&#21035;&#30340;&#31867;&#21035;&#19978;&#65292;&#25968;&#25454;&#22686;&#24378;&#21463;&#25511;&#20110;&#27169;&#22411;&#23545;&#20854;&#20013;&#19968;&#20010;&#30456;&#20851;&#31867;&#21035;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#23545;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#32534;&#30721;&#20102;&#19981;&#21464;&#24615;&#24182;&#25552;&#20379;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;DA&#25552;&#39640;&#20102;&#24179;&#22343;&#20934;&#30830;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#20854;&#24433;&#21709;&#21487;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#31867;&#21035;&#65306;&#36798;&#21040;&#26368;&#20339;&#24179;&#22343;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#20250;&#23548;&#33268;ImageNet&#19978;&#21333;&#20010;&#31867;&#21035;&#20934;&#30830;&#24230;&#26174;&#33879;&#19979;&#38477;&#39640;&#36798;20&#65285;&#12290;&#30001;&#20110;&#23545;&#36825;&#20123;&#24433;&#21709;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#23545;&#35299;&#20915;&#31867;&#21035;&#32423;&#20934;&#30830;&#24230;&#19979;&#38477;&#30340;&#36827;&#23637;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;DA&#19982;&#31867;&#21035;&#32423;&#23398;&#20064;&#21160;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#21033;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;ImageNet&#22810;&#26631;&#31614;&#27880;&#37322;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#21463;&#24433;&#21709;&#30340;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#31867;&#21035;&#26412;&#36136;&#19978;&#26159;&#27169;&#31946;&#30340;&#12289;&#20849;&#29616;&#30340;&#25110;&#28041;&#21450;&#32454;&#31890;&#24230;&#30340;&#21306;&#21035;&#65292;&#32780;DA&#25511;&#21046;&#20102;&#27169;&#22411;&#23545;&#20854;&#20013;&#19968;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#30340;&#20559;&#22909;&#12290;&#35768;&#22810;&#20808;&#21069;&#25253;&#21578;&#30340;&#24615;&#33021;&#19979;&#38477;&#36890;&#36807;&#22810;&#26631;&#31614;&#27880;&#37322;&#21487;&#20197;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) encodes invariance and provides implicit regularization critical to a model's performance in image classification tasks. However, while DA improves average accuracy, recent studies have shown that its impact can be highly class dependent: achieving optimal average accuracy comes at the cost of significantly hurting individual class accuracy by as much as 20% on ImageNet. There has been little progress in resolving class-level accuracy drops due to a limited understanding of these effects. In this work, we present a framework for understanding how DA interacts with class-level learning dynamics. Using higher-quality multi-label annotations on ImageNet, we systematically categorize the affected classes and find that the majority are inherently ambiguous, co-occur, or involve fine-grained distinctions, while DA controls the model's bias towards one of the closely related classes. While many of the previously reported performance drops are explained by multi-label an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#22312;&#27700;&#37197;&#27700;&#32593;&#32476;&#27844;&#28431;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28418;&#31227;&#26816;&#27979;&#30340;&#25216;&#26415;&#26469;&#23450;&#20301;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2401.01733</link><description>&lt;p&gt;
&#30740;&#31350;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#22312;&#27700;&#37197;&#27700;&#32593;&#32476;&#27844;&#28431;&#26816;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks. (arXiv:2401.01733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#22312;&#27700;&#37197;&#27700;&#32593;&#32476;&#27844;&#28431;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28418;&#31227;&#26816;&#27979;&#30340;&#25216;&#26415;&#26469;&#23450;&#20301;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27844;&#28431;&#26159;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20204;&#23548;&#33268;&#27700;&#36164;&#28304;&#30340;&#25439;&#22833;&#24182;&#22686;&#21152;&#20102;&#27745;&#26579;&#39118;&#38505;&#12290;&#27844;&#28431;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#27700;&#37197;&#27700;&#32593;&#32476;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#29305;&#21035;&#26159;&#65292;&#24456;&#38590;&#26816;&#27979;&#21040;&#23567;&#27844;&#28431;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#27844;&#28431;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#27010;&#24565;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#24191;&#27867;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#26696;&#20284;&#20046;&#26159;&#26816;&#27979;&#27844;&#28431;&#30340;&#21512;&#36866;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#27169;&#22411;&#25439;&#22833;&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#22312;&#22788;&#29702;&#27844;&#28431;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#30340;&#26102;&#24207;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24403;&#24212;&#29992;&#22522;&#20110;&#20998;&#24067;&#30340;&#26816;&#27979;&#26102;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#26041;&#27861;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#26816;&#27979;&#26102;&#38388;&#30340;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28418;&#31227;&#26816;&#27979;&#30340;&#25216;&#26415;&#26469;&#23450;&#20301;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks. Leakage detection is a difficult task due to the complex dynamics of water distribution networks. In particular, small leakages are hard to detect. From a machine-learning perspective, leakages can be modeled as concept drift. Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages. In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection. We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection. We evaluate different methods systematically for leakages of different sizes and detection times. Additionally, we propose a first drift-detection-based technique for localizing leakages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#26412;&#26694;&#26550;&#8212;&#8212;&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;&#65288;TENet&#65289;&#65292;&#23427;&#19981;&#20165;&#33021;&#23436;&#25104;&#20219;&#21153;&#65292;&#36824;&#33021;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#26679;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#24378;&#35843;&#25972;&#20010;AI&#39046;&#22495;&#37117;&#24212;&#35813;&#37325;&#35270;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01732</link><description>&lt;p&gt;
&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Task and Explanation Network. (arXiv:2401.01732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#26412;&#26694;&#26550;&#8212;&#8212;&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;&#65288;TENet&#65289;&#65292;&#23427;&#19981;&#20165;&#33021;&#23436;&#25104;&#20219;&#21153;&#65292;&#36824;&#33021;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#26679;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#24378;&#35843;&#25972;&#20010;AI&#39046;&#22495;&#37117;&#24212;&#35813;&#37325;&#35270;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#27492;&#35770;&#25991;&#20013;&#25552;&#20986;&#65292;AI&#19981;&#20165;&#35201;&#23436;&#25104;&#20219;&#21153;&#65292;&#36824;&#35201;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#26679;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#26694;&#26550;&#8212;&#8212;&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;&#65288;TENet&#65289;&#65292;&#23427;&#23436;&#20840;&#25972;&#21512;&#20102;&#20219;&#21153;&#23436;&#25104;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#20010;AI&#39046;&#22495;&#37117;&#24212;&#35813;&#24378;&#35843;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability in deep networks has gained increased importance in recent years. We argue herein that an AI must be tasked not just with a task but also with an explanation of why said task was accomplished as such. We present a basic framework -- Task and Explanation Network (TENet) -- which fully integrates task completion and its explanation. We believe that the field of AI as a whole should insist -- quite emphatically -- on explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01728</link><description>&lt;p&gt;
Ravnest: &#24322;&#26500;&#35774;&#22791;&#19978;&#30340;&#20998;&#25955;&#24335;&#24322;&#27493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24322;&#24120;&#30340;&#27867;&#21270;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#36235;&#21183;&#39044;&#35745;&#23558;&#32487;&#32493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22686;&#22823;&#20351;&#24471;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#22312;&#36825;&#31181;&#35268;&#27169;&#19978;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#36830;&#25509;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#26469;&#23454;&#29616;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;&#36825;&#20123;&#38598;&#32676;&#21442;&#19982;$\textit{&#38646;&#27668;&#27873;&#24322;&#27493;&#27169;&#22411;&#24182;&#34892;}$&#35757;&#32451;&#65292;&#21033;&#29992;$\textit{&#24182;&#34892;&#22810;&#29615;&#20840;&#23616;&#27719;&#32858;}$&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#36890;&#20449;&#21644;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPA&#30340;&#20986;&#22495;&#26816;&#27979;&#22120;&#65292;&#20854;&#21463;&#21040;&#31070;&#32463;&#23849;&#28291;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#35266;&#23519;&#29305;&#24449;&#19982;&#20869;&#22495;&#29305;&#24449;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#20027;&#35282;&#24230;&#65292;EPA&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;&#20986;&#22495;&#26679;&#26412;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01710</link><description>&lt;p&gt;
EPA&#65306;&#21463;&#31070;&#32463;&#23849;&#28291;&#21551;&#21457;&#30340;&#31283;&#20581;&#30340;&#20986;&#22495;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector. (arXiv:2401.01710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPA&#30340;&#20986;&#22495;&#26816;&#27979;&#22120;&#65292;&#20854;&#21463;&#21040;&#31070;&#32463;&#23849;&#28291;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#35266;&#23519;&#29305;&#24449;&#19982;&#20869;&#22495;&#29305;&#24449;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#20027;&#35282;&#24230;&#65292;EPA&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;&#20986;&#22495;&#26679;&#26412;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#22495;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#20869;&#22495;&#65288;ID&#65289;&#26679;&#26412;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24418;&#25104;&#19968;&#20010;&#23376;&#31354;&#38388;&#30340;&#20107;&#23454;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20869;&#22495;&#23376;&#31354;&#38388;&#30340;&#32508;&#21512;&#29305;&#24449;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#20869;&#22495;&#23376;&#31354;&#38388;&#30340;&#26032;&#29305;&#24615;&#12290;&#21033;&#29992;NC&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29305;&#24449;&#19982;&#20869;&#22495;&#29305;&#24449;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#20027;&#35282;&#24230;&#22312;&#34913;&#37327;&#20986;&#22495;&#21487;&#33021;&#24615;&#26041;&#38754;&#24418;&#25104;&#20102;&#19968;&#31181;&#20248;&#36234;&#30340;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NC&#21463;&#21551;&#21457;&#30340;&#20986;&#22495;&#35780;&#20998;&#20989;&#25968;&#65292;&#21517;&#20026;&#22686;&#24378;&#29109;&#20027;&#35282;&#24230;&#65288;EPA&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#20869;&#22495;&#23376;&#31354;&#38388;&#30340;&#20840;&#23616;&#29305;&#24449;&#21644;&#20854;&#20869;&#37096;&#23646;&#24615;&#12290;&#25105;&#20204;&#23545;EPA&#21644;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection plays a crucial role in ensuring the security of neural networks. Existing works have leveraged the fact that In-distribution (ID) samples form a subspace in the feature space, achieving state-of-the-art (SOTA) performance. However, the comprehensive characteristics of the ID subspace still leave under-explored. Recently, the discovery of Neural Collapse ($\mathcal{NC}$) sheds light on novel properties of the ID subspace. Leveraging insight from $\mathcal{NC}$, we observe that the Principal Angle between the features and the ID feature subspace forms a superior representation for measuring the likelihood of OOD. Building upon this observation, we propose a novel $\mathcal{NC}$-inspired OOD scoring function, named Entropy-enhanced Principal Angle (EPA), which integrates both the global characteristic of the ID subspace and its inner property. We experimentally compare EPA with various SOTA approaches, validating its superior performance and robustness
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#19979;&#30340;&#38646;&#26679;&#26412;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#26469;&#36873;&#25321;&#26368;&#20339;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#27880;&#37322;&#39044;&#31639;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01690</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#19979;&#30340;&#38646;&#26679;&#26412;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Active Learning Using Self Supervised Learning. (arXiv:2401.01690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01690
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#19979;&#30340;&#38646;&#26679;&#26412;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#26469;&#36873;&#25321;&#26368;&#20339;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#27880;&#37322;&#39044;&#31639;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#35828;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#24456;&#22823;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#38543;&#30528;&#27880;&#37322;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25913;&#21892;&#12290;&#34429;&#28982;&#25910;&#38598;&#26410;&#26631;&#35760;&#25968;&#25454;&#26356;&#23481;&#26131;&#65288;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#20174;&#20114;&#32852;&#32593;&#19978;&#33719;&#21462;&#65289;&#65292;&#20294;&#27880;&#37322;&#23427;&#20204;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#20219;&#21153;&#12290;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#27880;&#37322;&#39044;&#31639;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#26377;&#21161;&#20110;&#36873;&#25321;&#36866;&#21512;&#27880;&#37322;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#20197;&#20351;&#22312;&#27492;&#39044;&#31639;&#19979;&#23545;&#35813;&#23376;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26368;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#26082;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20063;&#19981;&#38656;&#35201;&#36845;&#20195;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#33719;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.
&lt;/p&gt;</description></item><item><title>LESEN&#26159;&#19968;&#31181;&#26631;&#31614;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#38598;&#25104;&#21644;&#21487;&#38752;&#30340;&#26410;&#26631;&#27880;&#26679;&#26412;&#36873;&#25321;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;&#22810;&#21442;&#25968;MRI&#30340;&#35270;&#35273;&#36884;&#24452;&#20998;&#21106;&#20013;&#26377;&#38480;&#26631;&#27880;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01654</link><description>&lt;p&gt;
LESEN:&#22522;&#20110;&#22810;&#21442;&#25968;MRI&#30340;&#35270;&#35273;&#36884;&#24452;&#20998;&#21106;&#30340;&#26631;&#31614;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LESEN: Label-Efficient deep learning for Multi-parametric MRI-based Visual Pathway Segmentation. (arXiv:2401.01654v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01654
&lt;/p&gt;
&lt;p&gt;
LESEN&#26159;&#19968;&#31181;&#26631;&#31614;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#38598;&#25104;&#21644;&#21487;&#38752;&#30340;&#26410;&#26631;&#27880;&#26679;&#26412;&#36873;&#25321;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;&#22810;&#21442;&#25968;MRI&#30340;&#35270;&#35273;&#36884;&#24452;&#20998;&#21106;&#20013;&#26377;&#38480;&#26631;&#27880;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#20110;&#22810;&#21442;&#25968;MRI&#30340;&#35270;&#35273;&#36884;&#24452;&#20998;&#21106;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#25968;&#25454;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26377;&#38480;&#26631;&#27880;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#38598;&#25104;&#30340;&#26631;&#31614;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;LESEN&#65289;&#12290;LESEN&#32467;&#21512;&#20102;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#25439;&#22833;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#20114;&#30456;&#23398;&#20064;&#65292;&#24418;&#25104;&#33258;&#25105;&#38598;&#25104;&#30340;&#22343;&#20540;&#25945;&#24072;&#26694;&#26550;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#38752;&#30340;&#26410;&#26631;&#27880;&#26679;&#26412;&#36873;&#25321;&#65288;RUSS&#65289;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;LESEN&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#65288;HCP&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#20020;&#24202;&#21644;&#30740;&#31350;&#29615;&#22659;&#20013;&#30340;&#22810;&#27169;&#24577;VP&#20998;&#21106;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#23454;&#29616;&#20195;&#30721;&#23558;&#22312;&#20197;&#19979;&#38142;&#25509;&#22788;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown the potential of deep learning in multi-parametric MRI-based visual pathway (VP) segmentation. However, obtaining labeled data for training is laborious and time-consuming. Therefore, it is crucial to develop effective algorithms in situations with limited labeled samples. In this work, we propose a label-efficient deep learning method with self-ensembling (LESEN). LESEN incorporates supervised and unsupervised losses, enabling the student and teacher models to mutually learn from each other, forming a self-ensembling mean teacher framework. Additionally, we introduce a reliable unlabeled sample selection (RUSS) mechanism to further enhance LESEN's effectiveness. Our experiments on the human connectome project (HCP) dataset demonstrate the superior performance of our method when compared to state-of-the-art techniques, advancing multimodal VP segmentation for comprehensive analysis in clinical and research settings. The implementation code will be available at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#37329;&#34701;&#20132;&#26131;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#21345;&#29255;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.01641</link><description>&lt;p&gt;
&#26397;&#30528;&#22522;&#20110;&#20132;&#26131;&#24207;&#21015;&#30340;&#22522;&#30784;&#37319;&#36141;&#27169;&#22411;: &#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#33258;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#37329;&#34701;&#20132;&#26131;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#21345;&#29255;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#20013;&#65292;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;&#21644;&#30041;&#23384;&#39044;&#27979;&#31561;&#29992;&#20363;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#22522;&#20110;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;&#25163;&#21160;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#23558;&#20854;&#24212;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#33719;&#21462;&#37329;&#34701;&#20132;&#26131;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21253;&#21547;51&#20159;&#31508;&#20132;&#26131;&#30340;180&#20010;&#21457;&#21345;&#38134;&#34892;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20445;&#30041;&#25968;&#25454;&#38598;&#19978;&#30340;&#21345;&#29255;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#12290;&#23884;&#20837;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly impro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#20854;&#22312;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01640</link><description>&lt;p&gt;
&#23545;&#20110;&#39034;&#24207;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data. (arXiv:2401.01640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#20854;&#22312;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#27169;&#22411;&#30340;&#23454;&#38469;&#35757;&#32451;&#33539;&#24335;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#21518;&#36319;&#38543;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#30417;&#30563;&#24494;&#35843;&#12290;&#26412;&#30740;&#31350;&#20551;&#35774;SSL&#27169;&#22411;&#20250;&#23398;&#20064;&#21040;&#26356;&#36890;&#29992;&#12289;&#22240;&#27492;&#26356;&#23569;&#26377;&#20559;&#35265;&#30340;&#34920;&#31034;&#65292;&#25506;&#32034;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#23545;&#20844;&#24179;&#24615;&#65288;&#21363;&#22312;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#26512;&#20013;&#34920;&#29616;&#24179;&#31561;&#65289;&#30340;&#24433;&#21709;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#65292;&#21463;&#21040;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#27604;&#36739;SSL&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#30417;&#30563;&#27169;&#22411;&#65292;&#20174;&#27169;&#22411;&#12289;&#23618;&#27425;&#21644;&#24230;&#37327;&#27700;&#24179;&#19978;&#35299;&#37322;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SSL&#33021;&#22815;&#23454;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#22312;&#24615;&#33021;&#25439;&#22833;1%&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#20844;&#24179;&#24615;&#39640;&#36798;27%&#12290;&#26368;&#32456;&#65292;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;SSL&#22312;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#30340;&#25968;&#25454;&#24773;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has become the de facto training paradigm of large models where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Hypothesizing that SSL models would learn more generic, hence less biased, representations, this study explores the impact of pre-training and fine-tuning strategies on fairness (i.e., performing equally on different demographic breakdowns). Motivated by human-centric applications on real-world timeseries data, we interpret inductive biases on the model, layer, and metric levels by systematically comparing SSL models to their supervised counterparts. Our findings demonstrate that SSL has the capacity to achieve performance on par with supervised methods while significantly enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1% loss in performance through self-supervision. Ultimately, this work underscores SSL's potential in human-centric computing, particularly high-stakes, data-s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#23427;&#20171;&#32461;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#26041;&#27861;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#20844;&#24179;&#24615;&#12289;&#20943;&#23569;&#20559;&#35265;&#21644;&#32500;&#25252;&#20262;&#29702;&#26631;&#20934;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01629</link><description>&lt;p&gt;
AI&#20013;&#30340;&#21512;&#25104;&#25968;&#25454;:&#25361;&#25112;&#65292;&#24212;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data in AI: Challenges, Applications, and Ethical Implications. (arXiv:2401.01629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#23427;&#20171;&#32461;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#26041;&#27861;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#20844;&#24179;&#24615;&#12289;&#20943;&#23569;&#20559;&#35265;&#21644;&#32500;&#25252;&#20262;&#29702;&#26631;&#20934;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25253;&#21578;&#28145;&#20837;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#22810;&#26041;&#38754;&#38382;&#39064;&#65292;&#29305;&#21035;&#24378;&#35843;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#20559;&#35265;&#12290;&#23427;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26412;&#25253;&#21578;&#36824;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;&#19982;&#21512;&#25104;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#20262;&#29702;&#32771;&#34385;&#21644;&#27861;&#24459;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#30830;&#20445;&#20844;&#24179;&#24615;&#65292;&#20943;&#23569;&#20559;&#35265;&#21644;&#32500;&#25252;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#20262;&#29702;&#26631;&#20934;&#30340;&#26426;&#21046;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant. This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor. It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains. The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01626</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01626
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;GNN&#21487;&#20197;&#35299;&#20915;&#31038;&#20250;&#31185;&#23398;&#12289;&#21270;&#23398;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;GNN&#26550;&#26500;&#30340;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#33410;&#28857;&#25110;&#22270;&#20998;&#31867;&#31561;&#20219;&#21153;&#30340;&#23454;&#35777;&#24615;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#21017;&#23547;&#27714;&#25214;&#21040;&#20855;&#26377;&#29702;&#35770;&#29305;&#24615;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#30740;&#31350;&#20854;&#34920;&#36798;&#33021;&#21147;&#24182;&#35774;&#35745;&#26368;&#22823;&#21270;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#20851;&#20110;&#22914;&#20309;&#23450;&#20041;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#36824;&#27809;&#26377;&#20849;&#35782;&#65292;&#20294;&#21487;&#20197;&#20174;&#20960;&#20010;&#26377;&#24456;&#22909;&#21160;&#26426;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#12290;&#20063;&#35768;&#26368;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65292;&#23601;&#20687;MLP&#30340;&#36825;&#31181;&#24615;&#36136;&#19968;&#26679;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#20851;&#27880;&#30340;&#26159;GNN&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph
&lt;/p&gt;</description></item><item><title>SCALA&#26159;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#21892;&#32593;&#32476;&#30340;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#21270;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01625</link><description>&lt;p&gt;
SCALA: &#22522;&#20110;&#31232;&#30095;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2401.01625v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01625
&lt;/p&gt;
&lt;p&gt;
SCALA&#26159;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#21892;&#32593;&#32476;&#30340;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#21270;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#25214;&#21040;&#19982;&#20854;&#20182;&#22823;&#22810;&#25968;&#33410;&#28857;&#34892;&#20026;&#26126;&#26174;&#19981;&#21516;&#30340;&#33410;&#28857;&#12290;&#36890;&#24120;&#65292;&#32593;&#32476;&#25968;&#25454;&#21253;&#21547;&#23454;&#20307;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#24322;&#24120;&#36890;&#24120;&#20307;&#29616;&#22312;&#36825;&#20123;&#20851;&#31995;&#20013;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20840;&#38754;&#22320;&#24314;&#27169;&#32593;&#32476;&#20013;&#22797;&#26434;&#30340;&#20132;&#20114;&#27169;&#24335;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#21487;&#20197;&#35266;&#23519;&#21040;&#65292;&#22312;&#32593;&#32476;&#20013;&#65292;&#24322;&#24120;&#36829;&#21453;&#20102;&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#21482;&#26159;&#38388;&#25509;&#22320;&#32771;&#34385;&#20102;&#36825;&#31181;&#29616;&#35937;&#65292;&#32780;&#19981;&#26159;&#26174;&#24335;&#22320;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#27491;&#24120;&#23454;&#20307;&#30340;&#33410;&#28857;&#34920;&#31034;&#24456;&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#33410;&#28857;&#24341;&#20837;&#30340;&#22122;&#22768;&#20851;&#31995;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;SCALA&#65292;&#26088;&#22312;&#25913;&#21892;&#32593;&#32476;&#30340;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#21270;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes. Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships. Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus. It can be observed that anomalies in networks violate the homophily assumption. However, most existing studies only considered this phenomenon obliquely rather than explicitly. Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes. To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into
&lt;/p&gt;</description></item><item><title>PLLaMa&#26159;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#25968;&#25454;&#24211;&#22686;&#24378;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#20854;&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#65292;&#24182;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#30340;&#21512;&#20316;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01600</link><description>&lt;p&gt;
PLLaMa&#65306;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01600
&lt;/p&gt;
&lt;p&gt;
PLLaMa&#26159;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#25968;&#25454;&#24211;&#22686;&#24378;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#20854;&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#65292;&#24182;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#30340;&#21512;&#20316;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#19982;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#20132;&#20114;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26893;&#29289;&#31185;&#23398;&#31561;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PLLaMa&#65292;&#19968;&#31181;&#20174;LLaMa-2&#36827;&#21270;&#32780;&#26469;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21253;&#25324;&#36229;&#36807;150&#19975;&#31687;&#26893;&#29289;&#31185;&#23398;&#23398;&#26415;&#25991;&#31456;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#36827;&#34892;&#22686;&#24378;&#12290;&#36825;&#19968;&#21457;&#23637;&#26174;&#33879;&#20016;&#23500;&#20102;PLLaMa&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#27979;&#35797;&#20013;&#65292;&#28041;&#21450;&#19982;&#26893;&#29289;&#21644;&#20892;&#19994;&#30456;&#20851;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#26174;&#31034;PLLaMa&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26893;&#29289;&#31185;&#23398;&#30456;&#20851;&#20027;&#39064;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32452;&#24314;&#20102;&#19968;&#20010;&#22269;&#38469;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#65292;&#21253;&#25324;&#26893;&#29289;&#31185;&#23398;&#23478;&#12289;&#20892;&#19994;&#24037;&#31243;&#24072;&#21644;&#26893;&#29289;&#32946;&#31181;&#21592;&#12290;&#35813;&#22242;&#38431;&#22312;&#39564;&#35777;PLLaMa&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable capabilities in understanding and interacting with natural language across various sectors. However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields. This paper introduces PLLaMa, an open-source language model that evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising more than 1.5 million scholarly articles in plant science. This development significantly enriches PLLaMa with extensive knowledge and proficiency in plant and agricultural sciences. Our initial tests, involving specific datasets related to plants and agriculture, show that PLLaMa substantially improves its understanding of plant science-related topics. Moreover, we have formed an international panel of professionals, including plant scientists, agricultural engineers, and plant breeders. This team plays a crucial role in verifying the accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.01599</link><description>&lt;p&gt;
&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#24130;&#24459;&#34928;&#20943;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay. (arXiv:2401.01599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26576;&#20123;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#26088;&#22312;&#30830;&#23450;&#22312;&#19981;&#21516;&#28304;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#30830;&#20999;&#39034;&#24207;&#65292;&#32780;&#19981;&#26159;&#26368;&#23567;&#21270;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20005;&#26684;&#32473;&#20986;&#20102;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;&#20197;&#21450;&#22823;&#31867;&#20998;&#26512;&#35889;&#31639;&#27861;&#65289;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#26680;&#25554;&#20540;&#30340;&#36817;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#28548;&#28165;&#20855;&#26377;&#26356;&#39640;&#36164;&#26684;&#30340;&#26680;&#22238;&#24402;&#31639;&#27861;&#30340;&#39281;&#21644;&#25928;&#24212;&#65292;&#31561;&#31561;&#12290;&#30001;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#29702;&#35770;&#30340;&#24110;&#21161;&#65292;&#36825;&#20123;&#32467;&#26524;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65292;&#21363;&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#65292;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization error curve of certain kernel regression method aims at determining the exact order of generalization error with various source condition, noise level and choice of the regularization parameter rather than the minimax rate. In this work, under mild assumptions, we rigorously provide a full characterization of the generalization error curves of the kernel gradient descent method (and a large class of analytic spectral algorithms) in kernel regression. Consequently, we could sharpen the near inconsistency of kernel interpolation and clarify the saturation effects of kernel regression algorithms with higher qualification, etc. Thanks to the neural tangent kernel theory, these results greatly improve our understanding of the generalization behavior of training the wide neural networks. A novel technical contribution, the analytic functional argument, might be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#28436;&#21270;&#31574;&#30053;&#31639;&#27861;&#65292;&#22312;&#36125;&#21494;&#26031;&#33021;&#21147;&#19978;&#38480;&#30340;&#20219;&#21153;&#20013;&#19982;&#39046;&#20808;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#31454;&#20105;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#21253;&#21547;&#21382;&#21490;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#32500;&#39640;&#26031;&#20998;&#24067;&#19978;&#31034;&#20363;&#21270;&#20102;&#19968;&#31181;&#19981;&#21464;&#19988;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#22120;&#12290;&#35813;&#31639;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#22522;&#20110;&#39640;&#26031;&#30340;&#28436;&#21270;&#31574;&#30053;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01579</link><description>&lt;p&gt;
&#39640;&#32500;&#22312;&#32447;&#20248;&#21270;&#30340;&#19981;&#21464;&#20449;&#24687;&#20960;&#20309;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Invariant Information Geometric Method for High-Dimensional Online Optimization. (arXiv:2401.01579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#28436;&#21270;&#31574;&#30053;&#31639;&#27861;&#65292;&#22312;&#36125;&#21494;&#26031;&#33021;&#21147;&#19978;&#38480;&#30340;&#20219;&#21153;&#20013;&#19982;&#39046;&#20808;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#31454;&#20105;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#21253;&#21547;&#21382;&#21490;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#32500;&#39640;&#26031;&#20998;&#24067;&#19978;&#31034;&#20363;&#21270;&#20102;&#19968;&#31181;&#19981;&#21464;&#19988;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#22120;&#12290;&#35813;&#31639;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#22522;&#20110;&#39640;&#26031;&#30340;&#28436;&#21270;&#31574;&#30053;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20248;&#21270;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#23545;&#20110;&#26114;&#36149;&#35780;&#20272;&#21644;&#38646;&#38454;&#21453;&#39304;&#30340;&#40657;&#30418;&#22330;&#26223;&#23588;&#20026;&#37325;&#35201;&#12290;&#24403;&#35745;&#31639;&#36164;&#28304;&#20805;&#36275;&#26102;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#24120;&#20248;&#20110;&#28436;&#21270;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#28436;&#21270;&#31574;&#30053;&#31639;&#27861;&#65292;&#20174;&#20854;&#30456;&#24212;&#30340;&#26694;&#26550;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#22312;&#36125;&#21494;&#26031;&#33021;&#21147;&#19978;&#38480;&#30340;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#19982;&#39046;&#20808;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#31454;&#20105;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#21253;&#21547;&#21382;&#21490;&#20449;&#24687;&#30340;&#26694;&#26550;InvIGO&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23436;&#20840;&#19981;&#21464;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#32500;&#39640;&#26031;&#20998;&#24067;&#19978;&#31034;&#20363;&#21270;&#20102;InvIGO&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#19981;&#21464;&#19988;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#22120;SynCMA&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#22522;&#20110;&#39640;&#26031;&#30340;&#28436;&#21270;&#31574;&#30053;&#30340;&#29702;&#35770;&#34892;&#20026;&#21644;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;SynCMA&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20027;&#35201;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is crucial in optimization, particularly in black-box scenarios characterized by expensive evaluations and zeroth-order feedback. When computing resources are plentiful, Bayesian optimization is often favored over evolution strategies. In this paper, we introduce a full invariance oriented evolution strategies algorithm, derived from its corresponding framework, that effectively rivals the leading Bayesian optimization method in tasks with dimensions at the upper limit of Bayesian capability. Specifically, we first build the framework InvIGO that fully incorporates historical information while retaining the full invariant and computational complexity. We then exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant and scalable optimizer SynCMA . The theoretical behavior and advantages of our algorithm over other Gaussian-based evolution strategies are further analyzed. Finally, We benchmark SynCMA against leading algorithms in Bayesian optimization an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31526;&#21512;&#24615;&#39044;&#27979;&#26469;&#27169;&#25311;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#33258;&#35299;&#37322;&#32593;&#32476;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#21516;&#26102;&#29983;&#25104;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#30456;&#24212;&#35299;&#37322;&#30340;&#20998;&#24067;&#26080;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#31435;&#20854;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2401.01549</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#24615;&#39044;&#27979;&#26469;&#27169;&#25311;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction. (arXiv:2401.01549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01549
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31526;&#21512;&#24615;&#39044;&#27979;&#26469;&#27169;&#25311;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#33258;&#35299;&#37322;&#32593;&#32476;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#21516;&#26102;&#29983;&#25104;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#30456;&#24212;&#35299;&#37322;&#30340;&#20998;&#24067;&#26080;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#31435;&#20854;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21462;&#24471;&#20102;&#36817;&#24180;&#26469;&#30340;&#36827;&#23637;&#65292;&#20294;&#35299;&#37322;DNN&#30340;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;DNN&#35299;&#37322;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21363;&#20351;&#29992;&#21478;&#19968;&#20010;&#35299;&#37322;&#27169;&#22411;&#26469;&#25552;&#20379;&#35299;&#37322;&#12290;&#20107;&#21518;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25581;&#31034;DNN&#30340;&#23454;&#38469;&#21407;&#22987;&#25512;&#29702;&#36807;&#31243;&#65292;&#36825;&#24341;&#21457;&#20102;&#26500;&#24314;&#20855;&#26377;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#30340;DNN&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20204;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36824;&#33021;&#22815;&#28165;&#26224;&#30452;&#35266;&#22320;&#35299;&#37322;&#20026;&#20160;&#20040;&#20570;&#20986;&#29305;&#23450;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#35299;&#37322;&#32593;&#32476;&#22312;&#25552;&#20379;&#20998;&#24067;&#26080;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#29983;&#25104;&#30340;&#20004;&#20010;&#39044;&#27979;&#32467;&#26524;&#65288;&#21363;&#26679;&#26412;&#30340;&#26368;&#32456;&#39044;&#27979;&#21644;&#35299;&#37322;&#35813;&#39044;&#27979;&#30340;&#30456;&#24212;&#35299;&#37322;&#65289;&#20043;&#38388;&#20063;&#26080;&#27861;&#24314;&#31435;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in deep neural networks (DNNs), it remains challenging to explain the predictions made by DNNs. Existing explanation methods for DNNs mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations. The fact that post-hoc methods can fail to reveal the actual original reasoning process of DNNs raises the need to build DNNs with built-in interpretability. Motivated by this, many self-explaining neural networks have been proposed to generate not only accurate predictions but also clear and intuitive insights into why a particular decision was made. However, existing self-explaining networks are limited in providing distribution-free uncertainty quantification for the two simultaneously generated prediction outcomes (i.e., a sample's final prediction and its corresponding explanations for interpreting that prediction). Importantly, they also fail to establish a connection between the confidence values assigned to the ge
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#20219;&#21153;&#21644;&#35821;&#20041;&#36890;&#20449;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#38598;&#25104;&#65292;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21457;&#36865;&#31471;&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#22810;&#20010;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#25509;&#25910;&#22120;&#29615;&#22659;&#19979;&#21033;&#29992;&#20998;&#25955;&#24335;&#23398;&#20064;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#21644;&#38544;&#31169;&#20851;&#27880;&#12290;&#38656;&#35201;&#27880;&#24847;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01531</link><description>&lt;p&gt;
6G&#20250;&#25104;&#20026;&#35821;&#20041;&#36890;&#20449;&#21527;&#65311;&#20174;&#38754;&#21521;&#20219;&#21153;&#21644;&#23433;&#20840;&#36890;&#20449;&#21040;&#38598;&#25104;&#24863;&#30693;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Will 6G be Semantic Communications? Opportunities and Challenges from Task Oriented and Secure Communications to Integrated Sensing. (arXiv:2401.01531v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#20219;&#21153;&#21644;&#35821;&#20041;&#36890;&#20449;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#38598;&#25104;&#65292;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21457;&#36865;&#31471;&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#22810;&#20010;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#25509;&#25910;&#22120;&#29615;&#22659;&#19979;&#21033;&#29992;&#20998;&#25955;&#24335;&#23398;&#20064;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#21644;&#38544;&#31169;&#20851;&#27880;&#12290;&#38656;&#35201;&#27880;&#24847;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#38598;&#25104;&#65292;&#25506;&#35752;&#20102;&#19979;&#19968;&#20195;&#36890;&#20449;&#32593;&#32476;&#20013;&#38754;&#21521;&#20219;&#21153;&#21644;&#35821;&#20041;&#36890;&#20449;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21457;&#36865;&#31471;&#20351;&#29992;&#19987;&#29992;&#32534;&#30721;&#22120;&#21644;&#25509;&#25910;&#31471;&#30340;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#35299;&#30721;&#22120;&#65292;&#20849;&#21516;&#35757;&#32451;&#20197;&#22788;&#29702;&#21253;&#25324;&#35821;&#20041;&#20449;&#24687;&#20445;&#30041;&#12289;&#28304;&#36755;&#20837;&#37325;&#24314;&#20197;&#21450;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;&#20026;&#20102;&#23558;&#20854;&#24212;&#29992;&#20174;&#28857;&#23545;&#28857;&#38142;&#25509;&#25193;&#23637;&#21040;&#22810;&#25509;&#25910;&#22120;&#29615;&#22659;&#65292;&#25105;&#20204;&#35774;&#24819;&#22312;&#21508;&#20010;&#25509;&#25910;&#22120;&#22788;&#37096;&#32626;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20998;&#25955;&#24335;&#23398;&#20064;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#21644;&#38544;&#31169;&#20851;&#27880;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#25955;&#24335;&#33410;&#28857;&#19978;&#20998;&#21457;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#35201;&#21462;&#20915;&#20110;&#25152;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#30340;&#28431;&#27934;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores opportunities and challenges of task (goal)-oriented and semantic communications for next-generation (NextG) communication networks through the integration of multi-task learning. This approach employs deep neural networks representing a dedicated encoder at the transmitter and multiple task-specific decoders at the receiver, collectively trained to handle diverse tasks including semantic information preservation, source input reconstruction, and integrated sensing and communications. To extend the applicability from point-to-point links to multi-receiver settings, we envision the deployment of decoders at various receivers, where decentralized learning addresses the challenges of communication load and privacy concerns, leveraging federated learning techniques that distribute model updates across decentralized nodes. However, the efficacy of this approach is contingent on the robustness of the employed deep learning models. We scrutinize potential vulnerabilities s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#22810;&#23545;&#19968;&#21305;&#37197;&#24066;&#22330;&#20013;&#25913;&#36827;&#36172;&#21338;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;-&#24310;&#36831;&#25509;&#21463;&#65288;AETDA&#65289;&#31639;&#27861;&#26469;&#25552;&#39640;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01528</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#22810;&#23545;&#19968;&#21305;&#37197;&#24066;&#22330;&#20013;&#25913;&#36827;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility. (arXiv:2401.01528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#22810;&#23545;&#19968;&#21305;&#37197;&#24066;&#22330;&#20013;&#25913;&#36827;&#36172;&#21338;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;-&#24310;&#36831;&#25509;&#21463;&#65288;AETDA&#65289;&#31639;&#27861;&#26469;&#25552;&#39640;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20016;&#23500;&#30340;&#24212;&#29992;&#65292;&#21452;&#36793;&#21305;&#37197;&#24066;&#22330;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#30001;&#20110;&#21442;&#19982;&#32773;&#36890;&#24120;&#23545;&#33258;&#24049;&#30340;&#20559;&#22909;&#19981;&#30830;&#23450;&#65292;&#26368;&#36817;&#37319;&#29992;&#22312;&#32447;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#26469;&#23398;&#20064;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#22810;&#23545;&#19968;&#35774;&#32622;&#20013;&#30340;&#32467;&#26524;&#36828;&#38750;&#26368;&#20248;&#65292;&#24182;&#32570;&#20047;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#38024;&#23545;&#22810;&#23545;&#19968;&#24066;&#22330;&#22312;&#25552;&#39640;&#36951;&#25022;&#19978;&#38480;&#30340;&#21516;&#26102;&#30830;&#20445;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;-&#24310;&#36831;&#25509;&#21463;&#65288;AETDA&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21709;&#24212;&#24615;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-sided matching markets have been widely studied in the literature due to their rich applications. Since participants are usually uncertain about their preferences, online algorithms have recently been adopted to learn them through iterative interactions. \citet{wang2022bandit} initiate the study of this problem in a many-to-one setting with \textit{responsiveness}. However, their results are far from optimal and lack guarantees of incentive compatibility. An extension of \citet{kong2023player} to this more general setting achieves a near-optimal bound for player-optimal regret. Nevertheless, due to the substantial requirement for collaboration, a single player's deviation could lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for others. In this paper, we aim to enhance the regret bound in many-to-one markets while ensuring incentive compatibility. We first propose the adaptively explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;S$^{2}$-DMs&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;$L_{skip}$&#37325;&#26032;&#25972;&#21512;&#36873;&#25321;&#24615;&#37319;&#26679;&#38454;&#27573;&#20013;&#30465;&#30053;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#65292;&#23545;&#20195;&#30721;&#20462;&#25913;&#35201;&#27714;&#23569;&#65292;&#19982;&#21508;&#31181;&#37319;&#26679;&#31639;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2401.01520</link><description>&lt;p&gt;
S$^{2}$-DMs&#65306;&#36339;&#36807;&#27493;&#39588;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;S$^{2}$-DMs&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;$L_{skip}$&#37325;&#26032;&#25972;&#21512;&#36873;&#25321;&#24615;&#37319;&#26679;&#38454;&#27573;&#20013;&#30465;&#30053;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#65292;&#23545;&#20195;&#30721;&#20462;&#25913;&#35201;&#27714;&#23569;&#65292;&#19982;&#21508;&#31181;&#37319;&#26679;&#31639;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#29983;&#25104;&#24037;&#20855;&#65292;&#26679;&#26412;&#36136;&#37327;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#21453;&#26144;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20284;&#28982;&#20998;&#25968;&#12290;&#20854;&#20013;&#19968;&#37096;&#20998;&#27169;&#22411;&#65292;&#22914;DDIMs&#65292;&#23637;&#31034;&#20102;&#22266;&#26377;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;$T$&#20010;&#27493;&#39588;&#65292;&#20294;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21482;&#20174;&#20854;&#20013;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#36825;&#31181;&#36873;&#25321;&#24615;&#37319;&#26679;&#26041;&#27861;&#34429;&#28982;&#20248;&#21270;&#20102;&#36895;&#24230;&#65292;&#20294;&#26080;&#24847;&#20013;&#38169;&#36807;&#20102;&#26410;&#37319;&#26679;&#27493;&#39588;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#21487;&#33021;&#20986;&#29616;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S$^{2}$-DMs&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#21019;&#26032;&#30340;$L_{skip}$&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#37325;&#26032;&#25972;&#21512;&#22312;&#36873;&#25321;&#24615;&#37319;&#26679;&#38454;&#27573;&#20013;&#30465;&#30053;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22909;&#22788;&#24456;&#22810;&#65306;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#36136;&#37327;&#65292;&#23454;&#29616;&#36215;&#26469;&#38750;&#24120;&#31616;&#21333;&#65292;&#38656;&#35201;&#26368;&#23569;&#30340;&#20195;&#30721;&#20462;&#25913;&#65292;&#24182;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#21508;&#31181;&#37319;&#26679;&#31639;&#27861;&#20860;&#23481;&#12290;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as powerful generative tools, rivaling GANs in sample quality and mirroring the likelihood scores of autoregressive models. A subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry: they are trained over $T$ steps but only sample from a subset of $T$ during generation. This selective sampling approach, though optimized for speed, inadvertently misses out on vital information from the unsampled steps, leading to potential compromises in sample quality. To address this issue, we present the S$^{2}$-DMs, which is a new training method by using an innovative $L_{skip}$, meticulously designed to reintegrate the information omitted during the selective sampling phase. The benefits of this approach are manifold: it notably enhances sample quality, is exceptionally simple to implement, requires minimal code modifications, and is flexible enough to be compatible with various sampling algorithms. On the CIFAR10 dataset, models trained using our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21270;&#21512;&#29289;&#20445;&#30041;&#25351;&#25968;&#65292;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#21892;&#21270;&#23398;&#37492;&#23450;&#26041;&#27861;&#21644;&#24211;&#30340;&#36136;&#37327;&#12290; (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.)</title><link>http://arxiv.org/abs/2401.01506</link><description>&lt;p&gt;
AIRI: &#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#20445;&#30041;&#25351;&#25968;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence. (arXiv:2401.01506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01506
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21270;&#21512;&#29289;&#20445;&#30041;&#25351;&#25968;&#65292;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#21892;&#21270;&#23398;&#37492;&#23450;&#26041;&#27861;&#21644;&#24211;&#30340;&#36136;&#37327;&#12290; (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kov&#225;t's&#20445;&#30041;&#25351;&#25968;&#65288;RI&#65289;&#26159;&#20351;&#29992;&#27668;&#30456;&#33394;&#35889;&#27979;&#37327;&#30340;&#21270;&#23398;&#32467;&#26500;&#37492;&#23450;&#20013;&#24120;&#29992;&#30340;&#25351;&#26631;&#12290;&#30001;&#20110;&#21019;&#24314;&#35266;&#23519;&#21040;&#30340;RI&#20540;&#30340;&#24211;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#32467;&#26500;&#39044;&#27979;&#26631;&#20934;&#21322;&#26497;&#24615;&#26609;&#30340;RI&#20540;&#12290;&#35813;&#32593;&#32476;&#29983;&#25104;&#30340;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;15.1&#65292;&#22312;&#35823;&#24046;&#20998;&#24067;&#23614;&#37096;&#30340;&#37327;&#21270;&#20013;&#65292;95%&#30340;&#32477;&#23545;&#35823;&#24046;&#20026;46.5&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#20445;&#30041;&#25351;&#25968;&#65288;AIRI&#65289;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#23427;&#34987;&#29992;&#20110;&#39044;&#27979;NIST EI-MS&#20809;&#35889;&#24211;&#30340;RI&#20540;&#12290;&#36825;&#20123;RI&#20540;&#29992;&#20110;&#25913;&#36827;&#21270;&#23398;&#37492;&#23450;&#26041;&#27861;&#21644;&#25552;&#39640;&#24211;&#30340;&#36136;&#37327;&#12290;&#22312;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;&#20026;&#20102;&#37327;&#21270;&#25105;&#20204;&#32593;&#32476;&#23545;&#27599;&#20010;&#21333;&#29420;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;8&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#26469;&#35745;&#31639;&#39044;&#27979;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
The Kov\'ats Retention index (RI) is a quantity measured using gas chromatography and commonly used in the identification of chemical structures. Creating libraries of observed RI values is a laborious task, so we explore the use of a deep neural network for predicting RI values from structure for standard semipolar columns. This network generated predictions with a mean absolute error of 15.1 and, in a quantification of the tail of the error distribution, a 95th percentile absolute error of 46.5. Because of the Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was used to predict RI values for the NIST EI-MS spectral libraries. These RI values are used to improve chemical identification methods and the quality of the library. Estimating uncertainty is an important practical need when using prediction models. To quantify the uncertainty of our network for each individual prediction, we used the outputs of an ensemble of 8 networks to calculate a predicted standard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Pontryagin&#27169;&#24335;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#20849;&#36717;&#29366;&#24577;&#22238;&#28378;&#20043;&#38388;&#30340;&#24046;&#24322;&#19978;&#23450;&#20041;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#21442;&#25968;&#21270;&#29366;&#24577;&#32422;&#26463;&#30340;&#21338;&#24328;&#20013;&#20540;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01502</link><description>&lt;p&gt;
Pontryagin&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;&#21270;&#26222;&#36890;&#21644;&#24046;&#20998;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games. (arXiv:2401.01502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Pontryagin&#27169;&#24335;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#20849;&#36717;&#29366;&#24577;&#22238;&#28378;&#20043;&#38388;&#30340;&#24046;&#24322;&#19978;&#23450;&#20041;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#21442;&#25968;&#21270;&#29366;&#24577;&#32422;&#26463;&#30340;&#21338;&#24328;&#20013;&#20540;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#29609;&#23478;&#30340;&#26222;&#36890;&#21644;&#24046;&#20998;&#21338;&#24328;&#30340;&#20540;&#26159;Hamilton-Jacobi-Isaacs&#65288;HJI&#65289;&#26041;&#31243;&#30340;&#31896;&#24615;&#35299;&#12290;&#36825;&#31181;&#21338;&#24328;&#30340;&#20540;&#21644;&#31574;&#30053;&#36924;&#36817;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#65288;CoD&#65289;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20943;&#36731;CoD&#26102;&#65292;&#30001;&#20110;&#29366;&#24577;&#32422;&#26463;&#24341;&#36215;&#30340;&#20540;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20250;&#36935;&#21040;&#25910;&#25947;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25361;&#25112;&#20043;&#22806;&#65292;&#22312;&#23545;&#21338;&#24328;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#23398;&#20064;&#26102;&#65288;&#20363;&#22914;&#65292;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#26102;&#36827;&#34892;&#21338;&#24328;&#21442;&#25968;&#25512;&#26029;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#20540;&#21644;&#31574;&#30053;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;Pontryagin&#27169;&#24335;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#22312;&#20855;&#26377;&#21442;&#25968;&#21270;&#29366;&#24577;&#32422;&#26463;&#30340;&#21338;&#24328;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65288;SOTA&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#20849;&#36717;&#29366;&#24577;&#22238;&#28378;&#20043;&#38388;&#30340;&#24046;&#24322;&#19978;&#23450;&#20041;&#30340;&#20849;&#36717;&#29366;&#24577;&#25439;&#22833;&#65292;&#36825;&#31181;&#25439;&#22833;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20849;&#36717;&#29366;&#24577;&#21160;&#21147;&#23398;&#30340;&#19981;&#36830;&#32493;&#24615;&#22312;&#35813;&#31639;&#23376;&#30340;&#24615;&#33021;&#25913;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy approximations for such games suffer from the curse of dimensionality (CoD). Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints. On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete. To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints. Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap. We show that the discontinuity of costate dynamics (in th
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31070;&#32463;&#36716;&#23548;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#20581;&#39640;&#25928;&#30340;&#23545;&#40784;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;&#29983;&#25104;&#22120;&#21512;&#25104;&#35821;&#38899;&#27874;&#24418;&#12290;&#35813;&#26694;&#26550;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01498</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#36716;&#23548;&#22120;&#36827;&#34892;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#30340;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction. (arXiv:2401.01498v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01498
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#36716;&#23548;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#20581;&#39640;&#25928;&#30340;&#23545;&#40784;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;&#29983;&#25104;&#22120;&#21512;&#25104;&#35821;&#38899;&#27874;&#24418;&#12290;&#35813;&#26694;&#26550;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26694;&#26550;&#65292;&#23427;&#20197;&#31070;&#32463;&#36716;&#23548;&#22120;&#20026;&#26680;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25972;&#20010;TTS&#27969;&#31243;&#21010;&#20998;&#20026;&#35821;&#20041;&#32423;&#21035;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;(seq2seq)&#24314;&#27169;&#21644;&#32454;&#31890;&#24230;&#30340;&#22768;&#23398;&#24314;&#27169;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;wav2vec2.0&#23884;&#20837;&#20013;&#33719;&#21462;&#30340;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#39640;&#25928;&#30340;&#23545;&#40784;&#24314;&#27169;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#26631;&#35760;&#36716;&#23548;&#22120;&#30340;&#31070;&#32463;&#36716;&#23548;&#22120;&#26469;&#36827;&#34892;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;&#65292;&#20174;&#20013;&#33719;&#24471;&#20102;&#30828;&#21333;&#35843;&#23545;&#40784;&#32422;&#26463;&#30340;&#30410;&#22788;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;(NAR)&#35821;&#38899;&#29983;&#25104;&#22120;&#20174;&#36825;&#20123;&#35821;&#20041;&#26631;&#35760;&#26377;&#25928;&#22320;&#21512;&#25104;&#27874;&#24418;&#12290;&#21478;&#22806;&#65292;&#21442;&#32771;&#35821;&#38899;&#22312;&#27599;&#20010;&#38454;&#27573;&#25511;&#21046;&#30528;&#26102;&#38388;&#21160;&#24577;&#21644;&#22768;&#23398;&#26465;&#20214;&#12290;&#36825;&#31181;&#35299;&#32806;&#30340;&#26694;&#26550;&#20943;&#23569;&#20102;TTS&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#27599;&#20010;&#38454;&#27573;&#19987;&#27880;&#20110;&#35821;&#20041;&#21644;&#22768;&#23398;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#38646;-shot&#33258;&#36866;&#24212;TTS&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24230;&#26041;&#38754;&#36229;&#36234;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similari
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PRFL&#30340;&#38544;&#31169;&#20445;&#25252;TFGC&#26694;&#26550;&#65292;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#36965;&#24863;&#30446;&#26631;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01493</link><description>&lt;p&gt;
&#33258;&#30001;&#21320;&#39184;&#20026;&#32852;&#37030;&#36965;&#24863;&#30446;&#26631;&#32454;&#31890;&#24230;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework. (arXiv:2401.01493v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PRFL&#30340;&#38544;&#31169;&#20445;&#25252;TFGC&#26694;&#26550;&#65292;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#36965;&#24863;&#30446;&#26631;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#30446;&#26631;&#30340;&#32454;&#31890;&#24230;&#20998;&#31867;&#23545;&#20891;&#20107;&#21644;&#27665;&#29992;&#39046;&#22495;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#30001;&#20110;&#20301;&#32622;&#24046;&#24322;&#12289;&#25968;&#25454;&#35268;&#27169;&#22686;&#38271;&#21644;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#20648;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#19981;&#21516;&#22320;&#21306;/&#22269;&#23478;&#30340;&#19981;&#21516;&#25968;&#25454;&#24211;&#20013;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#27861;&#24459;&#21644;&#22269;&#23478;&#23433;&#20840;&#38382;&#39064;&#38480;&#21046;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#20998;&#26512;&#36825;&#20123;&#25935;&#24863;&#30340;&#36965;&#24863;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#20302;&#36164;&#28304;&#30340;&#36965;&#24863;&#35774;&#22791;&#22312;&#22788;&#29702;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#26102;&#65292;&#20063;&#38754;&#20020;&#36890;&#20449;&#24320;&#38144;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;&#38544;&#31169;&#20445;&#25252;TFGC&#26694;&#26550;&#65292;&#31216;&#20026;PRFL&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20801;&#35768;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#32479;&#35745;&#24322;&#26500;&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65292;IID&#65289;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#31169;&#26377;&#25968;&#25454;&#30340;&#26412;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote Sensing Target Fine-grained Classification (TFGC) is of great significance in both military and civilian fields. Due to location differences, growth in data size, and centralized server storage constraints, these data are usually stored under different databases across regions/countries. However, privacy laws and national security concerns constrain researchers from accessing these sensitive remote sensing images for further analysis. Additionally, low-resource remote sensing devices encounter challenges in terms of communication overhead and efficiency when dealing with the ever-increasing data and model scales. To solve the above challenges, this paper proposes a novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed PRFL. The proposed framework allows each client to learn global and local knowledge to enhance the local representation of private data in environments with extreme statistical heterogeneity (non. Independent and Identically Distributed, IID). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#21033;&#29992;&#32929;&#31080;&#30334;&#20998;&#27604;&#21464;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20998;&#26512;&#20844;&#24320;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24635;&#20307;&#32929;&#31080;&#36235;&#21183;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#29305;&#23450;&#25968;&#25454;&#29305;&#24449;&#21644;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01487</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#22810;&#27169;&#24335;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing and Multimodal Stock Price Prediction. (arXiv:2401.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#21033;&#29992;&#32929;&#31080;&#30334;&#20998;&#27604;&#21464;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20998;&#26512;&#20844;&#24320;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24635;&#20307;&#32929;&#31080;&#36235;&#21183;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#29305;&#23450;&#25968;&#25454;&#29305;&#24449;&#21644;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#20915;&#31574;&#39046;&#22495;&#65292;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24120;&#34987;&#29992;&#20110;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#12290;&#26412;&#25991;&#21033;&#29992;&#32929;&#31080;&#30334;&#20998;&#27604;&#21464;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#19982;&#20256;&#32479;&#20351;&#29992;&#21407;&#22987;&#36135;&#24065;&#20540;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;&#37325;&#28857;&#20998;&#26512;&#20844;&#24320;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#30334;&#20998;&#27604;&#21464;&#21270;&#30340;&#36873;&#25321;&#26088;&#22312;&#20026;&#27169;&#22411;&#25552;&#20379;&#20851;&#20110;&#20215;&#26684;&#27874;&#21160;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#32929;&#31080;&#25972;&#20307;&#20215;&#26684;&#21464;&#21270;&#30340;&#24433;&#21709;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;BERT&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#65292;&#29305;&#21035;&#24378;&#35843;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24635;&#20307;&#32929;&#31080;&#36235;&#21183;&#30340;&#33021;&#21147;&#65292;&#24182;&#20984;&#26174;&#20102;&#29305;&#23450;&#25968;&#25454;&#29305;&#24449;&#21644;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of financial decision-making, predicting stock prices is pivotal. Artificial intelligence techniques such as long short-term memory networks (LSTMs), support-vector machines (SVMs), and natural language processing (NLP) models are commonly employed to predict said prices. This paper utilizes stock percentage change as training data, in contrast to the traditional use of raw currency values, with a focus on analyzing publicly released news articles. The choice of percentage change aims to provide models with context regarding the significance of price fluctuations and overall price change impact on a given stock. The study employs specialized BERT natural language processing models to predict stock price trends, with a particular emphasis on various data modalities. The results showcase the capabilities of such strategies with a small natural language processing model to accurately predict overall stock trends, and highlight the effectiveness of certain data features and se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#22238;&#24402;&#32593;&#32476;&#65288;ERN&#65289;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20351;ERN&#33021;&#22815;&#20174;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.01484</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#35268;&#33539;&#21270;&#30340;&#35777;&#25454;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Regularized Evidential Regression. (arXiv:2401.01484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#22238;&#24402;&#32593;&#32476;&#65288;ERN&#65289;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20351;ERN&#33021;&#22815;&#20174;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#22238;&#24402;&#32593;&#32476;&#65288;ERN&#65289;&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;Dempster-Shafer&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#30446;&#26631;&#24182;&#37327;&#21270;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#24213;&#23618;&#29702;&#35770;&#30340;&#25351;&#23548;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#29305;&#23450;&#30340;&#28608;&#27963;&#20989;&#25968;&#26469;&#24378;&#21046;&#38750;&#36127;&#20540;&#65292;&#36825;&#31181;&#32422;&#26463;&#38480;&#21046;&#20102;&#27169;&#22411;&#20174;&#25152;&#26377;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#21361;&#23475;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#38480;&#21046;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#20811;&#26381;&#23427;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#23398;&#20064;&#26679;&#26412;&#30340;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;ERN&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20010;&#32422;&#26463;&#12290;&#22522;&#20110;&#25105;&#20204;&#20998;&#26512;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#20351;ERN&#33021;&#22815;&#20174;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Evidential Regression Network (ERN) represents a novel approach that integrates deep learning with Dempster-Shafer's theory to predict a target and quantify the associated uncertainty. Guided by the underlying theory, specific activation functions must be employed to enforce non-negative values, which is a constraint that compromises model performance by limiting its ability to learn from all samples. This paper provides a theoretical analysis of this limitation and introduces an improvement to overcome it. Initially, we define the region where the models can't effectively learn from the samples. Following this, we thoroughly analyze the ERN and investigate this constraint. Leveraging the insights from our analysis, we address the limitation by introducing a novel regularization term that empowers the ERN to learn from the whole training set. Our extensive experiments substantiate our theoretical findings and demonstrate the effectiveness of the proposed solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.01482</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#24773;&#26223;&#19979;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#35774;&#35745;&#21644;&#29615;&#22659;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;&#36825;&#20123;&#36716;&#31227;&#19979;&#30340;&#23545;&#35937;&#27010;&#24565;&#65292;&#38656;&#35201;&#35843;&#25972;&#31867;&#21035;&#34920;&#31034;&#12290;&#22312;&#32570;&#20047;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#21033;&#29992;&#22320;&#29702;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#25551;&#36848;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#25506;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38598;&#25104;&#30693;&#35782;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#22312;&#19968;&#32452;&#28304;&#22320;&#29702;&#20301;&#32622;&#19978;&#35757;&#32451;&#30340;&#36719;&#25552;&#31034;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#38598;&#21512;&#12290;&#24403;&#20165;&#20381;&#36182;&#26469;&#33258;&#27431;&#27954;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#22312;DollarStreet&#19978;&#30340;&#22686;&#30410;&#36798;&#21040;&#20102;+2.8&#20010;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
&lt;/p&gt;</description></item><item><title>Kernel-U-Net&#26159;&#19968;&#31181;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01479</link><description>&lt;p&gt;
Kernel-U-Net: &#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting. (arXiv:2401.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01479
&lt;/p&gt;
&lt;p&gt;
Kernel-U-Net&#26159;&#19968;&#31181;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#26159;&#22522;&#20110;&#21382;&#21490;&#20449;&#24687;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#22522;&#20110;U-Net&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#30495;&#23454;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#22522;&#20110;&#34917;&#19969;&#27169;&#22411;&#25110;&#32447;&#24615;&#27169;&#22411;&#30340;&#27169;&#22411;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#23618;&#27425;&#21270;&#30340;&#26694;&#26550;&#65292;Kernel-U-Net&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#23558;&#36755;&#20837;&#24207;&#21015;&#20999;&#21106;&#25104;&#29255;&#27573;&#65292;&#28982;&#21518;&#20351;&#29992;&#21367;&#31215;&#26680;&#36827;&#34892;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#23427;&#25193;&#23637;&#20102;&#32463;&#20856;U-Net&#20013;&#30340;&#21367;&#31215;&#26680;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#25509;&#21463;&#31526;&#21512;&#30456;&#21516;&#35774;&#35745;&#27169;&#24335;&#30340;&#33258;&#23450;&#20041;&#21367;&#31215;&#26680;&#12290;&#19982;&#29616;&#26377;&#30340;&#32447;&#24615;&#25110;&#22522;&#20110;transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65306;1&#65289;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65306;&#21442;&#25968;&#22823;&#23567;&#20026;$O(log(L)^2)$&#65292;&#20854;&#20013;$L$&#20026;&#22238;&#28335;&#31383;&#21475;&#22823;&#23567;&#65307;2&#65289;&#28789;&#27963;&#24615;&#65306;&#20854;&#21367;&#31215;&#26680;&#21487;&#20197;&#23450;&#21046;&#21644;&#36866;&#24212;&#25968;&#25454;&#38598;&#65307;3&#65289;&#35745;&#31639;&#25928;&#29575;&#65306;&#22914;&#26524;&#20351;&#29992;&#27492;&#27169;&#22411;&#65292;transformer&#27169;&#22359;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23567;&#20026;$O(log(L)^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting task predicts future trends based on historical information. Recent U-Net-based methods have demonstrated superior performance in predicting real-world datasets. However, the performance of these models is lower than patch-based models or linear models. In this work, we propose a symmetric and hierarchical framework, Kernel-U-Net, which cuts the input sequence into slices at each layer of the network and then computes them using kernels. Furthermore, it generalizes the concept of convolutional kernels in classic U-Net to accept custom kernels that follow the same design pattern. Compared to the existing linear or transformer-based solution, our model contains 3 advantages: 1) A small number of parameters: the parameters size is $O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its kernels can be customized and fitted to the datasets, 3) Computation efficiency: the computation complexity of transformer modules is reduced to $O(log(L)^2)$ if th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01470</link><description>&lt;p&gt;
&#39640;&#25928;&#35270;&#35273;Transformer&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#19978;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#36880;&#28176;&#20943;&#23569;&#20196;&#29260;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20551;&#35774;&#19968;&#20010;&#23618;&#20013;&#30340;&#20196;&#29260;&#20887;&#20313;&#24847;&#21619;&#30528;&#25152;&#26377;&#21518;&#32493;&#23618;&#20013;&#20063;&#26377;&#20887;&#20313;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#27491;&#30830;&#30340;&#65292;&#21363;&#19968;&#20010;&#23618;&#20013;&#22810;&#20313;&#30340;&#20196;&#29260;&#22312;&#21518;&#38754;&#30340;&#23618;&#20013;&#21487;&#20197;&#26159;&#26377;&#29992;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20196;&#29260;&#20998;&#24067;&#65292;&#21363;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#29992;&#26469;&#25511;&#21046;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#20196;&#29260;&#21033;&#29992;&#12290;&#20026;&#20102;&#25913;&#21892;&#20196;&#29260;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#26426;&#21046;&#65292;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#26377;&#21161;&#20110;&#21435;&#38500;&#22122;&#22768;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#38598;&#21512;&#32447;&#24615;&#21270;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#28857;&#20113;&#23884;&#20837;&#21040;L2&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#28857;&#20113;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#36817;&#20284;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#32593;&#32476;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01460</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#38598;&#21512;&#32447;&#24615;&#21270;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#28857;&#20113;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Classification via Deep Set Linearized Optimal Transport. (arXiv:2401.01460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#38598;&#21512;&#32447;&#24615;&#21270;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#28857;&#20113;&#23884;&#20837;&#21040;L2&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#28857;&#20113;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#36817;&#20284;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#32593;&#32476;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#28145;&#24230;&#38598;&#21512;&#32447;&#24615;&#21270;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#23558;&#28857;&#20113;&#39640;&#25928;&#22320;&#23884;&#20837;&#21040;L2&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#23884;&#20837;&#22312;&#20445;&#30041;Wasserstein&#31354;&#38388;&#20013;&#29305;&#23450;&#20302;&#32500;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#28857;&#20113;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#65292;&#21363;&#19981;&#21516;&#28857;&#20113;&#30340;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#20043;&#38388;&#30340;L2&#36317;&#31163;&#65288;&#28304;&#33258;&#20849;&#20139;&#30340;&#22266;&#23450;&#21442;&#32771;&#20998;&#24067;&#65289;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#36817;&#20284;&#34920;&#31034;&#36825;&#20123;&#28857;&#20113;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#20123;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#37319;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;ICNNs&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;ICNNs&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#38750;&#24120;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#37492;&#21035;&#22120;&#32593;&#32476;&#65292;&#23545;&#36825;&#20123;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#32622;&#25442;&#19981;&#21464;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Deep Set Linearized Optimal Transport, an algorithm designed for the efficient simultaneous embedding of point clouds into an $L^2-$space. This embedding preserves specific low-dimensional structures within the Wasserstein space while constructing a classifier to distinguish between various classes of point clouds. Our approach is motivated by the observation that $L^2-$distances between optimal transport maps for distinct point clouds, originating from a shared fixed reference distribution, provide an approximation of the Wasserstein-2 distance between these point clouds, under certain assumptions. To learn approximations of these transport maps, we employ input convex neural networks (ICNNs) and establish that, under specific conditions, Euclidean distances between samples from these ICNNs closely mirror Wasserstein-2 distances between the true distributions. Additionally, we train a discriminator network that attaches weights these samples and creates a permutation inva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#21457;&#33258;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#25805;&#20316;&#20013;&#26816;&#27979;&#24182;&#32416;&#27491;&#21333;&#20010;&#21644;&#22810;&#20010;&#27704;&#20037;&#21644;&#36719;&#38169;&#35823;&#65292;&#36866;&#29992;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.01458</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#21457;&#33258;&#27979;
&lt;/p&gt;
&lt;p&gt;
Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint. (arXiv:2401.01458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#21457;&#33258;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#25805;&#20316;&#20013;&#26816;&#27979;&#24182;&#32416;&#27491;&#21333;&#20010;&#21644;&#22810;&#20010;&#27704;&#20037;&#21644;&#36719;&#38169;&#35823;&#65292;&#36866;&#29992;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;NN-HA&#65289;&#19978;&#30340;&#22987;&#32456;&#24320;&#21551;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#24212;&#29992;&#37319;&#29992;&#21508;&#31181;&#23384;&#20648;&#25216;&#26415;&#12290;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26469;&#35828;&#65292;&#21487;&#38752;&#30340;&#25345;&#32493;&#25805;&#20316;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#22312;&#32447;&#25805;&#20316;&#20013;&#65292;NN&#22240;&#36752;&#23556;&#12289;&#32769;&#21270;&#21644;&#28909;&#25928;&#24212;&#31561;&#22240;&#32032;&#32780;&#23481;&#26131;&#20986;&#29616;&#21333;&#20010;&#21644;&#22810;&#20010;&#27704;&#20037;&#21644;&#36719;&#38169;&#35823;&#12290;&#26174;&#24335;&#30340;NN-HA&#27979;&#35797;&#26041;&#27861;&#19981;&#33021;&#26816;&#27979;&#21040;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#30636;&#24577;&#25925;&#38556;&#65292;&#23545;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#24212;&#29992;&#26469;&#35828;&#20063;&#19981;&#36866;&#29992;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;&#21644;&#23384;&#20648;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;NN&#22312;&#32447;&#25925;&#38556;&#29366;&#24577;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#8221;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#21452;&#22836;NN&#25299;&#25169;&#32467;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#21644;NN&#30340;&#20027;&#35201;&#39044;&#27979;&#12290;&#22312;&#22312;&#32447;&#25805;&#20316;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#21305;&#37197;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#65292;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#33258;&#27979;NN&#65292;&#21487;&#36798;&#21040;100
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) are increasingly used in always-on safety-critical applications deployed on hardware accelerators (NN-HAs) employing various memory technologies. Reliable continuous operation of NN is essential for safety-critical applications. During online operation, NNs are susceptible to single and multiple permanent and soft errors due to factors such as radiation, aging, and thermal effects. Explicit NN-HA testing methods cannot detect transient faults during inference, are unsuitable for always-on applications, and require extensive test vector generation and storage. Therefore, in this paper, we propose the \emph{uncertainty fingerprint} approach representing the online fault status of NN. Furthermore, we propose a dual head NN topology specifically designed to produce uncertainty fingerprints and the primary prediction of the NN in \emph{a single shot}. During the online operation, by matching the uncertainty fingerprint, we can concurrently self-test NNs with up to $100
&lt;/p&gt;</description></item><item><title>ProbMCL&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#22312;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#22797;&#26434;&#27169;&#22359;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2401.01448</link><description>&lt;p&gt;
ProbMCL: &#31616;&#21333;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#26631;&#31614;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification. (arXiv:2401.01448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01448
&lt;/p&gt;
&lt;p&gt;
ProbMCL&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#22312;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#22797;&#26434;&#27169;&#22359;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21307;&#23398;&#24433;&#20687;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#24341;&#20837;&#20102;&#22522;&#20110;&#22270;&#21644;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#24182;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21253;&#21547;&#22797;&#26434;&#30340;&#27169;&#22359;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#22810;&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#65288;ProbMCL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#26681;&#25454;&#20915;&#31574;&#38408;&#20540;&#23558;&#19982;&#38170;&#22270;&#20687;&#20855;&#26377;&#36275;&#22815;&#26631;&#31614;&#30340;&#26679;&#26412;&#24341;&#20837;&#27491;&#26679;&#26412;&#38598;&#12290;&#36825;&#31181;&#32467;&#26500;&#36890;&#36807;&#23558;&#27491;&#26679;&#26412;&#23545;&#30340;&#23884;&#20837;&#25289;&#36817;&#65292;&#24182;&#25512;&#31163;&#20302;&#20110;&#38408;&#20540;&#30340;&#36127;&#26679;&#26412;&#26469;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#34701;&#20837;&#23545;&#27604;&#23398;&#20064;&#20013;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label image classification presents a challenging task in many domains, including computer vision and medical imaging. Recent advancements have introduced graph-based and transformer-based methods to improve performance and capture label dependencies. However, these methods often include complex modules that entail heavy computation and lack interpretability. In this paper, we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel framework to address these challenges in multi-label image classification tasks. Our simple yet effective approach employs supervised contrastive learning, in which samples that share enough labels with an anchor image based on a decision threshold are introduced as a positive set. This structure captures label dependencies by pulling positive pair embeddings together and pushing away negative samples that fall below the threshold. We enhance representation learning by incorporating a mixture density network into contrastive learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#24847;&#35782;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25509;&#25910;&#26426;&#30340;&#24402;&#19968;&#21270;&#22240;&#23376;&#26469;&#26368;&#23567;&#21270;&#24178;&#25200;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#32858;&#21512;&#23545;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01442</link><description>&lt;p&gt;
&#20855;&#26377;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#24847;&#35782;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity. (arXiv:2401.01442v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#24847;&#35782;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25509;&#25910;&#26426;&#30340;&#24402;&#19968;&#21270;&#22240;&#23376;&#26469;&#26368;&#23567;&#21270;&#24178;&#25200;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#32858;&#21512;&#23545;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#23454;&#26045;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26102;&#65292;&#21487;&#25193;&#23637;&#24615;&#20445;&#35777;&#21644;&#22788;&#29702;&#24178;&#25200;&#21644;&#35774;&#22791;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#36890;&#36807;&#31354;&#20013;&#35745;&#31639;&#39640;&#25928;&#21033;&#29992;&#21333;&#19968;&#26080;&#32447;&#36164;&#28304;&#30340;&#21487;&#25193;&#23637;&#20256;&#36755;&#26041;&#26696;&#12290;&#20026;&#20102;&#25552;&#20379;&#23545;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26799;&#24230;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20248;&#21270;&#25509;&#25910;&#26426;&#24402;&#19968;&#21270;&#22240;&#23376;&#65292;&#26368;&#23567;&#21270;&#20102;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#20960;&#20309;&#27169;&#22411;&#23545;&#22810;&#31751;&#26080;&#32447;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#32858;&#21512;&#20272;&#35745;&#30340;&#22343;&#26041;&#35823;&#24046;&#34920;&#24449;&#20026;&#32593;&#32476;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial. This work introduces a learning method designed to address these challenges, along with a scalable transmission scheme that efficiently uses a single wireless resource through over-the-air computation. To provide resistance against data heterogeneity, we employ gradient aggregations. Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors. For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters. We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2401.01426</link><description>&lt;p&gt;
&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#22312;&#35266;&#27979;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35745;&#31639;&#21487;&#36776;&#35782;&#22240;&#26524;&#26597;&#35810;&#30340;&#22768;&#38899;&#21644;&#23436;&#25972;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#23618;&#27425;&#30340;&#22240;&#26524;&#32467;&#26500;&#21644;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36739;&#20302;&#23618;&#27425;&#30340;&#23618;&#27425;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#22914;&#22270;&#20687;&#36825;&#26679;&#30340;&#39640;&#32500;&#21464;&#37327;&#26159;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;&#22914;&#20309;&#20934;&#30830;&#22320;&#20174;&#36825;&#26679;&#30340;&#39640;&#32500;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#29305;&#21035;&#26159;&#38543;&#30528;&#22270;&#20687;&#22522;&#27169;&#22411;&#30340;&#26368;&#36817;&#20852;&#36215;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22238;&#31572;&#24102;&#26377;&#36825;&#26679;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#35757;&#32451;&#31639;&#27861;&#65292;&#32473;&#23450;&#22240;&#26524;&#32467;&#26500;&#21644;&#39044;&#35757;&#32451;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#35745;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#35774;&#35745;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#33258;&#21160;&#36229;&#36710;&#21644;&#25442;&#36947;&#31574;&#30053;&#35268;&#21010;&#27169;&#22411;SwapTransformer&#12290;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#27604;&#36739;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01425</link><description>&lt;p&gt;
SwapTransformer&#65306;&#36890;&#36807;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#36895;&#20844;&#36335;&#36229;&#36710;&#31574;&#30053;&#35268;&#21010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset. (arXiv:2401.01425v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#35774;&#35745;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#33258;&#21160;&#36229;&#36710;&#21644;&#25442;&#36947;&#31574;&#30053;&#35268;&#21010;&#27169;&#22411;SwapTransformer&#12290;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#27604;&#36739;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#30340;&#39640;&#32423;&#20915;&#31574;&#38382;&#39064;&#65292;&#20363;&#22914;&#25442;&#36947;&#21644;&#36229;&#36710;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#29992;&#20110;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#33258;&#21160;&#36229;&#36710;&#21644;&#25442;&#36947;&#30340;&#20986;&#34892;&#36741;&#21161;&#21151;&#33021;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#25910;&#38598;&#20102;&#32422;900&#19975;&#20010;&#26679;&#26412;&#65292;&#21253;&#25324;&#36710;&#36947;&#22270;&#20687;&#21644;&#20854;&#20182;&#21160;&#24577;&#29289;&#20307;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;OSHA&#65288;&#27169;&#25311;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36229;&#36710;&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;SwapTransformer&#30340;&#26550;&#26500;&#65292;&#20316;&#20026;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#36741;&#21161;&#20219;&#21153;&#65292;&#22914;&#26410;&#26469;&#28857;&#21644;&#36710;&#36742;&#36317;&#31163;&#32593;&#32476;&#39044;&#27979;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#19982;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles. In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways. About 9 million samples including lane images and other dynamic objects are collected in simulation. This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge. To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset. Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment. The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment. We also demonstrate the performance of the model 
&lt;/p&gt;</description></item><item><title>VALD-MD&#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#35270;&#35273;&#24402;&#22240;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#27491;&#24120;&#23545;&#24212;&#30340;&#24322;&#24120;&#22270;&#20687;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#19982;&#35786;&#26029;&#30456;&#20851;&#30340;&#22270;&#20687;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.01414</link><description>&lt;p&gt;
VALD-MD: &#38754;&#21521;&#21307;&#30103;&#35786;&#26029;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#35270;&#35273;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics. (arXiv:2401.01414v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01414
&lt;/p&gt;
&lt;p&gt;
VALD-MD&#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#35270;&#35273;&#24402;&#22240;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#27491;&#24120;&#23545;&#24212;&#30340;&#24322;&#24120;&#22270;&#20687;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#19982;&#35786;&#26029;&#30456;&#20851;&#30340;&#22270;&#20687;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#35270;&#35273;&#24402;&#22240;&#26088;&#22312;&#28165;&#26224;&#23637;&#31034;&#21307;&#23398;&#22270;&#20687;&#20013;&#19982;&#35786;&#26029;&#30456;&#20851;&#30340;&#37096;&#20998;&#65292;&#19982;&#26631;&#20934;&#26426;&#22120;&#35270;&#35273;&#27969;&#31243;&#20013;&#26816;&#27979;&#30149;&#21464;&#32452;&#32455;&#30340;&#26041;&#27861;&#30456;&#27604;&#65288;&#23545;&#20020;&#24202;&#21307;&#29983;&#26469;&#35828;&#19981;&#22826;&#30452;&#35266;&#21487;&#35299;&#37322;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#35270;&#35273;&#24402;&#22240;&#25216;&#26415;&#65292;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#24322;&#24120;&#22270;&#20687;&#30340;&#27491;&#24120;&#23545;&#24212;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#20135;&#29983;&#20102;&#25351;&#31034;&#35786;&#26029;&#30456;&#20851;&#22270;&#20687;&#37096;&#20998;&#30340;&#26144;&#23556;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#36866;&#24403;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#21253;&#25324;&#20174;&#21307;&#23398;&#31185;&#23398;&#21644;&#24212;&#29992;&#25918;&#23556;&#23398;&#20013;&#33719;&#21462;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;COVID-19&#25918;&#23556;&#23398;&#25968;&#25454;&#24211;&#30340;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual attribution in medical imaging seeks to make evident the diagnostically-relevant components of a medical image, in contrast to the more common detection of diseased tissue deployed in standard machine vision pipelines (which are less straightforwardly interpretable/explainable to clinicians). We here present a novel generative visual attribution technique, one that leverages latent diffusion models in combination with domain-specific large language models, in order to generate normal counterparts of abnormal images. The discrepancy between the two hence gives rise to a mapping indicating the diagnostically-relevant image components. To achieve this, we deploy image priors in conjunction with appropriate conditioning mechanisms in order to control the image generative process, including natural language text prompts acquired from medical science and applied radiology. We perform experiments and quantitatively evaluate our results on the COVID-19 Radiography Database containing la
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;</title><link>http://arxiv.org/abs/2401.01404</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23376;&#20108;&#27425;&#26102;&#38388;&#32593;&#32476;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#37325;&#24314;&#26159;&#25351;&#22312;&#21482;&#26377;&#20851;&#20110;&#26465;&#20214;&#20598;&#32852;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#25110;&#22270;&#27169;&#22411;&#30340;&#29420;&#31435;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;N&#20010;&#33410;&#28857;&#20043;&#38388;&#26410;&#35266;&#27979;&#21040;&#30340;&#25104;&#23545;&#32806;&#21512;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20284;&#20046;&#26080;&#27861;&#36991;&#20813;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;O(N^2)&#65292;&#21363;&#35201;&#32771;&#34385;&#27599;&#31181;&#21487;&#33021;&#30340;&#25104;&#23545;&#32806;&#21512;&#33267;&#23569;&#19968;&#27425;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#24863;&#20852;&#36259;&#30340;&#32593;&#32476;&#37117;&#26159;&#31232;&#30095;&#30340;&#65292;&#38750;&#38646;&#32806;&#21512;&#30340;&#25968;&#37327;&#21482;&#26377;O(N)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#37325;&#24314;&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#20854;&#22312;&#23376;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#20854;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24230;&#23485;&#26494;&#19978;&#30028;&#20026;O(N^(3/2)logN)&#65292;&#20294;&#20855;&#26377;&#26356;&#20856;&#22411;&#30340;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;O(Nlog^2 N)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#65292;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31232;&#30095;&#23376;&#31354;&#38388;&#21152;&#36895;&#40657;&#31665;&#20998;&#23376;&#24615;&#36136;&#20248;&#21270;&#30340;&#25361;&#25112;&#24050;&#32463;&#36890;&#36807;&#23398;&#20064;&#26356;&#20302;&#32500;&#30340;&#32534;&#30721;&#24471;&#21040;&#35299;&#20915;</title><link>http://arxiv.org/abs/2401.01398</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31232;&#30095;&#23376;&#31354;&#38388;&#21152;&#36895;&#40657;&#31665;&#20998;&#23376;&#24615;&#36136;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces. (arXiv:2401.01398v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31232;&#30095;&#23376;&#31354;&#38388;&#21152;&#36895;&#40657;&#31665;&#20998;&#23376;&#24615;&#36136;&#20248;&#21270;&#30340;&#25361;&#25112;&#24050;&#32463;&#36890;&#36807;&#23398;&#20064;&#26356;&#20302;&#32500;&#30340;&#32534;&#30721;&#24471;&#21040;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#20248;&#21270;&#38382;&#39064;&#26412;&#36136;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#31163;&#25955;&#12289;&#38750;&#32467;&#26500;&#21270;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#26631;&#35760;&#36807;&#31243;&#28041;&#21450;&#26114;&#36149;&#30340;&#27169;&#25311;&#25110;&#23454;&#39564;&#65292;&#36825;&#20174;&#26681;&#26412;&#19978;&#38480;&#21046;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#20248;&#21270;&#22024;&#26434;&#30340;&#12289;&#40657;&#31665;&#30446;&#26631;&#20989;&#25968;&#65288;&#20363;&#22914;&#27979;&#37327;&#30340;&#24615;&#36136;&#20540;&#65289;&#30340;&#24378;&#22823;&#19988;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#22240;&#27492;&#23545;&#20110;&#20998;&#23376;&#24615;&#36136;&#20248;&#21270;&#32780;&#35328;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#12290;&#35201;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#20248;&#21270;&#38382;&#39064;&#65292;&#24517;&#39035;&#36873;&#25321;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26500;&#24314;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#12290;&#35768;&#22810;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#28982;&#32780;&#23427;&#20204;&#37117;&#26159;&#39640;&#32500;&#30340;&#65292;&#36825;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#32500;&#24230;&#30340;&#35781;&#21650;&#20351;&#24471;&#38590;&#20197;&#23450;&#20041;&#21644;&#25191;&#34892;&#36866;&#21512;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#26368;&#36817;&#36890;&#36807;&#23398;&#20064;&#26356;&#20302;&#32500;&#30340;&#32534;&#30721;&#26469;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Molecular property optimization (MPO) problems are inherently challenging since they are formulated over discrete, unstructured spaces and the labeling process involves expensive simulations or experiments, which fundamentally limits the amount of available data. Bayesian optimization (BO) is a powerful and popular framework for efficient optimization of noisy, black-box objective functions (e.g., measured property values), thus is a potentially attractive framework for MPO. To apply BO to MPO problems, one must select a structured molecular representation that enables construction of a probabilistic surrogate model. Many molecular representations have been developed, however, they are all high-dimensional, which introduces important challenges in the BO process -- mainly because the curse of dimensionality makes it difficult to define and perform inference over a suitable class of surrogate models. This challenge has been recently addressed by learning a lower-dimensional encoding of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#25429;&#25417;&#20016;&#23500;&#30340;&#31354;&#38388;&#30456;&#20851;&#27169;&#24335;&#26041;&#38754;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#25972;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2401.01395</link><description>&lt;p&gt;
&#28145;&#24230;&#33258;&#22238;&#24402;&#24314;&#27169;&#29992;&#20110;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Deep autoregressive modeling for land use land cover. (arXiv:2401.01395v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#25429;&#25417;&#20016;&#23500;&#30340;&#31354;&#38388;&#30456;&#20851;&#27169;&#24335;&#26041;&#38754;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#25972;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22320;&#29702;&#35201;&#32032;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20197;&#21450;&#19982;&#22320;&#24418;&#12289;&#29983;&#24577;&#21644;&#20154;&#31867;&#21457;&#23637;&#30456;&#20851;&#30340;&#26126;&#26174;&#31354;&#38388;&#27169;&#24335;&#65292;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#24314;&#27169;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#22303;&#22320;&#21033;&#29992;&#31354;&#38388;&#27169;&#24335;&#24314;&#27169;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22270;&#20687;&#20462;&#22797;&#20219;&#21153;&#23384;&#22312;&#23494;&#20999;&#32852;&#31995;&#65292;&#24182;&#23545;&#20855;&#26377;&#32422;1900&#19975;&#21442;&#25968;&#30340;&#20462;&#25913;PixelCNN&#26550;&#26500;&#36827;&#34892;&#20102;LULC&#24314;&#27169;&#30340;&#30740;&#31350;&#12290;&#19982;&#22522;&#20934;&#31354;&#38388;&#32479;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#21069;&#32773;&#33021;&#22815;&#25429;&#25417;&#21040;&#26356;&#20016;&#23500;&#30340;&#31354;&#38388;&#30456;&#20851;&#27169;&#24335;&#65292;&#22914;&#36947;&#36335;&#21644;&#27700;&#20307;&#65292;&#20294;&#19981;&#33021;&#20135;&#29983;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19982;&#37325;&#35201;&#30340;&#29983;&#24577;&#30456;&#20851;&#22303;&#22320;&#21033;&#29992;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;&#26001;&#22359;&#25968;&#37327;&#21644;&#30456;&#37051;&#24615;&#65289;&#30456;&#27604;&#20013;&#23384;&#22312;&#39044;&#27979;&#24615;&#20302;&#20998;&#25955;&#30340;&#35777;&#25454;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#25805;&#20316;&#25277;&#26679;&#21487;&#21464;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Land use / land cover (LULC) modeling is a challenging task due to long-range dependencies between geographic features and distinct spatial patterns related to topography, ecology, and human development. We identify a close connection between modeling of spatial patterns of land use and the task of image inpainting from computer vision and conduct a study of a modified PixelCNN architecture with approximately 19 million parameters for modeling LULC. In comparison with a benchmark spatial statistical model, we find that the former is capable of capturing much richer spatial correlation patterns such as roads and water bodies but does not produce a calibrated predictive distribution, suggesting the need for additional tuning. We find evidence of predictive underdispersion with regard to important ecologically-relevant land use statistics such as patch count and adjacency which can be ameliorated to some extent by manipulating sampling variability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22238;&#28335;&#26032;&#30340;Q-Newton&#26041;&#27861;&#65288;BNQN&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;Newton&#26041;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#21457;&#29616;BNQN&#22312;&#22810;&#39033;&#24335;&#21644;&#20122;&#32431;&#20989;&#25968;&#30340;&#26681;&#25628;&#32034;&#20013;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#21560;&#24341;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;Newton&#27969;&#21644;Voronoi&#22270;&#30340;&#32852;&#31995;&#25552;&#20986;&#20102;&#19968;&#20123;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#38754;&#23545;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;BNQN&#27604;Newton&#26041;&#27861;&#21644;&#38543;&#26426;&#26494;&#24347;Newton&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01393</link><description>&lt;p&gt;
&#22238;&#28335;&#26032;Q-Newton&#26041;&#27861;&#65292;Newton&#27969;&#65292;Voronoi&#22270;&#21644;&#38543;&#26426;&#27714;&#26681;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and Stochastic root finding. (arXiv:2401.01393v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22238;&#28335;&#26032;&#30340;Q-Newton&#26041;&#27861;&#65288;BNQN&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;Newton&#26041;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#21457;&#29616;BNQN&#22312;&#22810;&#39033;&#24335;&#21644;&#20122;&#32431;&#20989;&#25968;&#30340;&#26681;&#25628;&#32034;&#20013;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#21560;&#24341;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;Newton&#27969;&#21644;Voronoi&#22270;&#30340;&#32852;&#31995;&#25552;&#20986;&#20102;&#19968;&#20123;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#38754;&#23545;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;BNQN&#27604;Newton&#26041;&#27861;&#21644;&#38543;&#26426;&#26494;&#24347;Newton&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31532;&#19977;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22238;&#28335;&#26032;Q-Newton&#26041;&#27861;(BNQN)&#30340;Newton&#26041;&#27861;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20808;&#21069;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20351;&#29992;BNQN&#21487;&#20197;&#25214;&#21040;&#22810;&#39033;&#24335;&#21644;&#20122;&#32431;&#20989;&#25968;&#30340;&#26681;&#26102;&#65292;&#21560;&#24341;&#22495;&#20855;&#26377;&#19968;&#20123;&#26174;&#33879;&#30340;&#29305;&#24615;&#12290;&#24635;&#20307;&#19978;&#30475;&#65292;&#23427;&#20204;&#27604;Newton&#26041;&#27861;&#30340;&#21560;&#24341;&#22495;&#26356;&#21152;&#24179;&#28369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32487;&#32493;&#28145;&#20837;&#23454;&#39564;&#30740;&#31350;&#36825;&#19968;&#26174;&#33879;&#29616;&#35937;&#65292;&#24182;&#23558;BNQN&#19982;Newton&#27969;&#21644;Voronoi&#22270;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#31181;&#32852;&#31995;&#32473;&#20986;&#20102;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38590;&#39064;&#38656;&#35201;&#35299;&#37322;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#19982;Newton&#26041;&#27861;&#21644;&#38543;&#26426;&#26494;&#24347;Newton&#26041;&#27861;&#30456;&#27604;&#65292;BNQN&#23545;&#38543;&#26426;&#25200;&#21160;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new variant of Newton's method - named Backtracking New Q-Newton's method (BNQN) - which has strong theoretical guarantee, is easy to implement, and has good experimental performance, was recently introduced by the third author.  Experiments performed previously showed some remarkable properties of the basins of attractions for finding roots of polynomials and meromorphic functions, with BNQN. In general, they look more smooth than that of Newton's method.  In this paper, we continue to experimentally explore in depth this remarkable phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link poses a couple of challenging puzzles to be explained. Experiments also indicate that BNQN is more robust against random perturbations than Newton's method and Random Relaxed Newton's method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01391</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#30340;MLP&#23398;&#20064;SDF&#30340;&#26368;&#20248;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#22330;&#65292;&#22914;&#24418;&#29366;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#34920;&#31034;&#26041;&#27861;&#65292;&#20363;&#22914;&#32534;&#30721;3D&#24418;&#29366;&#21644;&#25191;&#34892;&#30896;&#25758;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#38544;&#24335;&#22330;&#30001;&#24102;&#26377;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#36827;&#34892;&#32534;&#30721;&#20197;&#25429;&#25417;&#39640;&#39057;&#20960;&#20309;&#32454;&#33410;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24102;&#26377;PE&#30340;MLP&#30340;&#19968;&#20010;&#26174;&#33879;&#21103;&#20316;&#29992;&#26159;&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#22330;&#20013;&#23384;&#22312;&#22122;&#22768;&#20266;&#24433;&#12290;&#23613;&#31649;&#22686;&#21152;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#20266;&#24433;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#30340;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#33391;&#29616;&#35937;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#30830;&#23450;&#23398;&#20064;&#31934;&#30830;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#36866;&#24403;&#37319;&#26679;&#29575;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#21103;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#32593;&#32476;&#21709;&#24212;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#29992;&#20110;&#20272;&#35745;&#24102;&#26377;&#38543;&#26426;&#26435;&#37325;&#30340;&#32473;&#23450;&#32593;&#32476;&#30340;&#20869;&#22312;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit fields, such as the neural signed distance field (SDF) of a shape, have emerged as a powerful representation for many applications, e.g., encoding a 3D shape and performing collision detection. Typically, implicit fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding (PE) to capture high-frequency geometric details. However, a notable side effect of such PE-equipped MLPs is the noisy artifacts present in the learned implicit fields. While increasing the sampling rate could in general mitigate these artifacts, in this paper we aim to explain this adverse phenomenon through the lens of Fourier analysis. We devise a tool to determine the appropriate sampling rate for learning an accurate neural implicit field without undesirable side effects. Specifically, we propose a simple yet effective method to estimate the intrinsic frequency of a given network with randomized weights based on the Fourier analysis of the network's responses. It is observed that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#38271;&#36317;&#31163;&#31359;&#22681;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;ESP32-S3&#19982;&#23450;&#21521;&#21452;&#26497;&#22825;&#32447;&#21644;ESP32-S3&#19982;&#21360;&#21047;&#20498;F&#22825;&#32447;&#32467;&#21512;&#30340;&#20004;&#20010;&#26377;&#21069;&#26223;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#22312;WiFi-based HAR&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01388</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#38271;&#36317;&#31163;&#31359;&#22681;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition. (arXiv:2401.01388v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#38271;&#36317;&#31163;&#31359;&#22681;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;ESP32-S3&#19982;&#23450;&#21521;&#21452;&#26497;&#22825;&#32447;&#21644;ESP32-S3&#19982;&#21360;&#21047;&#20498;F&#22825;&#32447;&#32467;&#21512;&#30340;&#20004;&#20010;&#26377;&#21069;&#26223;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#22312;WiFi-based HAR&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20351;&#24471;&#22312;&#31354;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#20445;&#25345;&#35270;&#35273;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38750;&#25509;&#35302;&#24335;&#12289;&#38271;&#36317;&#31163;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21608;&#22260;&#23384;&#22312;&#35768;&#22810;&#25903;&#25345;WiFi&#30340;&#35774;&#22791;&#65292;&#20294;&#24456;&#23569;&#26377;&#35774;&#22791;&#21521;&#29992;&#25143;&#20844;&#24320;CSI&#65292;&#23548;&#33268;&#24863;&#30693;&#30828;&#20214;&#36873;&#25321;&#26377;&#38480;&#12290;Espressif ESP32&#30340;&#21464;&#20307;&#24050;&#32463;&#25104;&#20026;&#28508;&#22312;&#30340;&#20302;&#25104;&#26412;&#12289;&#26131;&#20110;&#37096;&#32626;&#30340;WiFi CSI-based HAR&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23545;&#22522;&#20110;&#22235;&#20010;ESP32-S3&#30340;2.4GHz&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#25903;&#25345;&#38271;&#36317;&#31163;&#31359;&#22681;HAR&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#26377;&#21069;&#26223;&#30340;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#23558;ESP32-S3&#19982;&#23450;&#21521;&#21452;&#26497;&#22825;&#32447;&#30456;&#32467;&#21512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#32452;&#21512;&#26159;WiFi&#22522;&#30784;HAR&#20013;&#30340;&#39318;&#27425;&#28436;&#31034;&#12290;&#31532;&#20108;&#20010;&#31995;&#32479;&#20381;&#38752;ESP32-S3&#30340;&#20869;&#32622;&#21360;&#21047;&#20498;F&#22825;&#32447;&#65288;PIFA&#65289;&#24182;&#36890;&#36807;&#24179;&#38754;&#21453;&#23556;&#22120;&#23454;&#29616;&#26041;&#21521;&#24615;&#12290;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#65292;&#36825;&#20004;&#20010;&#31995;&#32479;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi Channel State Information (CSI)-based human activity recognition (HAR) enables contactless, long-range sensing in spatially constrained environments while preserving visual privacy. However, despite the presence of numerous WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of sensing hardware options. Variants of the Espressif ESP32 have emerged as potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for their ability to facilitate long-range through-wall HAR. Two promising systems are proposed, one of which combines the ESP32-S3 with a directional biquad antenna. This combination represents, to the best of our knowledge, the first demonstration of such a system in WiFi-based HAR. The second system relies on the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves directionality through a plane reflector. In a comprehensive evaluation 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#65292;&#20294;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#38656;&#35201;&#34987;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.01386</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#30340;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#65292;&#20294;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#38656;&#35201;&#34987;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#30149;&#29702;&#23398;&#20998;&#26512;&#21644;&#35786;&#26029;&#26159;&#30001;&#19987;&#23478;&#22312;&#26174;&#24494;&#38236;&#19979;&#36890;&#36807;&#35266;&#23519;&#29627;&#29827;&#20999;&#29255;&#26631;&#26412;&#36827;&#34892;&#25163;&#21160;&#30524;&#29699;&#21028;&#26029;&#26469;&#23436;&#25104;&#30340;&#12290;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#26159;&#20174;&#29627;&#29827;&#20999;&#29255;&#21046;&#20316;&#30340;&#25968;&#23383;&#26631;&#26412;&#12290;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#20351;&#24471;&#26631;&#26412;&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#23631;&#24149;&#19978;&#35266;&#23519;&#65292;&#24182;&#24341;&#21457;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#65292;&#20854;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#33258;&#21160;&#20998;&#26512;&#21644;&#35786;&#26029;&#12290;&#20511;&#21161;&#24403;&#21069;&#30340;&#35745;&#31639;&#36827;&#23637;&#65292;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#65288;&#22914;&#32452;&#32455;&#35126;&#30385;&#25110;&#27668;&#27873;&#65289;&#30340;&#24433;&#21709;&#65292;&#21017;&#20998;&#26512;&#21487;&#33021;&#20250;&#22833;&#36133;&#25110;&#23548;&#33268;&#38169;&#35823;&#30340;&#35786;&#26029;&#65292;&#36825;&#21462;&#20915;&#20110;&#20266;&#24433;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#29616;&#26377;&#30340;&#20266;&#24433;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#19987;&#23478;&#23545;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#65292;&#20197;&#28040;&#38500;&#21463;&#21040;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#12289;&#32321;&#29712;&#65292;&#24182;&#19988;&#26377;&#25439;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#25110;&#20266;&#24433;&#21435;&#38500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, pathological analysis and diagnosis are performed by manually eyeballing glass slide specimens under a microscope by an expert. The whole slide image is the digital specimen produced from the glass slide. Whole slide image enabled specimens to be observed on a computer screen and led to computational pathology where computer vision and artificial intelligence are utilized for automated analysis and diagnosis. With the current computational advancement, the entire whole slide image can be analyzed autonomously without human supervision. However, the analysis could fail or lead to wrong diagnosis if the whole slide image is affected by tissue artifacts such as tissue fold or air bubbles depending on the severity. Existing artifact detection methods rely on experts for severity assessment to eliminate artifact affected regions from the analysis. This process is time consuming, exhausting and undermines the goal of automated analysis or removal of artifacts without evaluatin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36882;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TransGNN&#65289;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#35282;&#24230;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20960;&#20010;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01384</link><description>&lt;p&gt;
&#24378;&#20256;&#36882;&#20851;&#31995;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strong Transitivity Relations and Graph Neural Networks. (arXiv:2401.01384v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01384
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36882;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TransGNN&#65289;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#35282;&#24230;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20960;&#20010;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#20013;&#65292;&#23616;&#37096;&#37051;&#22495;&#22312;&#23884;&#20837;&#29983;&#25104;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33410;&#28857;&#24212;&#35813;&#20855;&#26377;&#31867;&#20284;&#20110;&#20854;&#37051;&#23621;&#30340;&#23884;&#20837;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23558;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#20174;&#38468;&#36817;&#37051;&#22495;&#25193;&#23637;&#21040;&#25972;&#20010;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36882;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#25193;&#23637;&#65292;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#20840;&#23616;&#30456;&#20284;&#24615;&#21644;&#23616;&#37096;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TransGNN&#65289;&#65292;&#23427;&#19981;&#20165;&#32771;&#34385;&#20102;&#23616;&#37096;&#33410;&#28857;&#30456;&#20284;&#24615;&#65292;&#36824;&#36890;&#36807;&#21306;&#20998;&#24378;&#20256;&#36882;&#20851;&#31995;&#21644;&#24369;&#20256;&#36882;&#20851;&#31995;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#32771;&#34385;&#20840;&#23616;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#23427;&#26174;&#33879;&#25913;&#21892;&#20102;&#20960;&#20010;&#30693;&#21517;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#33410;&#28857;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local neighborhoods play a crucial role in embedding generation in graph-based learning. It is commonly believed that nodes ought to have embeddings that resemble those of their neighbors. In this research, we try to carefully expand the concept of similarity from nearby neighborhoods to the entire graph. We provide an extension of similarity that is based on transitivity relations, which enables Graph Neural Networks (GNNs) to capture both global similarities and local similarities over the whole graph. We introduce Transitivity Graph Neural Network (TransGNN), which more than local node similarities, takes into account global similarities by distinguishing strong transitivity relations from weak ones and exploiting them. We evaluate our model over several real-world datasets and showed that it considerably improves the performance of several well-known GNN models, for tasks such as node classification.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.01383</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#32570;&#25968;&#25454;&#21644;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01383
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35782;&#21035;&#26089;&#26399;&#33041;&#36830;&#25509;&#24615;&#21457;&#23637;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#20102;&#35299;&#23156;&#20799;&#33041;&#32593;&#32476;&#22312;&#20986;&#29983;&#21518;&#30340;&#31532;&#19968;&#24180;&#20013;&#30340;&#22797;&#26434;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#19981;&#33021;&#27867;&#21270;&#21040;&#22810;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#36712;&#36857;&#23545;&#24212;&#20110;&#29305;&#23450;&#30340;&#25104;&#20687;&#27169;&#24577;&#25110;&#36830;&#25509;&#31867;&#22411;&#65288;&#20363;&#22914;T1-w MRI&#65289;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#21462;&#12290;&#31532;&#19977;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#21033;&#29992;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedGmTE-Net++&#65292;&#19968;&#31181;&#32852;&#37030;&#22270;&#24418;&#22810;&#36712;&#36857;&#28436;&#21270;&#32593;&#32476;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#25105;&#20204;&#22312;&#26377;&#38480;&#30340;&#21307;&#38498;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#20102;&#19981;&#21516;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#12290;&#32467;&#26524;&#21363;&#21487;&#25552;&#39640;&#27599;&#20010;&#21307;&#38498;&#26412;&#22320;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26680;&#26691;&#27700;&#20998;&#32961;&#36843;&#27979;&#32472;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#65292;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26377;&#25928;&#20272;&#35745;&#20102;&#26680;&#26691;&#26641;&#30340;&#33550;&#27700;&#21183;&#65292;&#20026;&#26680;&#26691;&#31934;&#20934;&#28748;&#28297;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01375</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#36827;&#34892;&#26680;&#26691;&#27700;&#20998;&#32961;&#36843;&#30340;&#27979;&#32472;
&lt;/p&gt;
&lt;p&gt;
Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26680;&#26691;&#27700;&#20998;&#32961;&#36843;&#27979;&#32472;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#65292;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26377;&#25928;&#20272;&#35745;&#20102;&#26680;&#26691;&#26641;&#30340;&#33550;&#27700;&#21183;&#65292;&#20026;&#26680;&#26691;&#31934;&#20934;&#28748;&#28297;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30417;&#27979;&#26680;&#26691;&#30340;&#27700;&#20998;&#29366;&#24577;&#21644;&#32961;&#36843;&#27700;&#24179;&#23545;&#20110;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#37325;&#35201;&#20892;&#20316;&#29289;&#26680;&#26691;&#30340;&#31934;&#20934;&#28748;&#28297;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#32467;&#21512;&#26080;&#20154;&#26426;&#33322;&#25293;&#30340;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#36965;&#24863;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#26469;&#27979;&#32472;&#33550;&#27700;&#21183;&#65288;SWP&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20174;2017&#24180;&#21040;2018&#24180;&#65292;&#20351;&#29992;&#19968;&#26550;&#35013;&#22791;&#26377;&#19971;&#27874;&#27573;&#22810;&#20809;&#35889;&#30456;&#26426;&#30340;&#26080;&#20154;&#26426;&#65292;&#22312;&#19968;&#20010;&#21830;&#19994;&#26680;&#26691;&#22253;&#36827;&#34892;&#20102;&#20116;&#27425;&#39134;&#34892;&#65292;&#21516;&#26102;&#20276;&#38543;&#23545;&#25277;&#26679;&#26680;&#26691;&#26893;&#26666;&#30340;&#22320;&#38754;&#27979;&#37327;&#12290;RF&#22238;&#24402;&#27169;&#22411;&#21033;&#29992;&#26469;&#33258;&#27491;&#23556;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#26893;&#34987;&#25351;&#25968;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#22320;&#38754;&#27979;&#37327;&#30340;SWPs&#65292;&#36798;&#21040;&#20102;0.63&#30340;R^2&#20540;&#21644;0.80&#24052;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#12290;&#22825;&#27668;&#25968;&#25454;&#30340;&#25972;&#21512;&#23588;&#20026;&#37325;&#35201;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#39134;&#34892;&#26085;&#26399;&#30340;&#25968;&#25454;&#12290;&#26174;&#33879;&#30340;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Effective monitoring of walnut water status and stress level across the whole orchard is an essential step towards precision irrigation management of walnuts, a significant crop in California. This study presents a machine learning approach using Random Forest (RF) models to map stem water potential (SWP) by integrating high-resolution multispectral remote sensing imagery from Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018, five flights of an UAV equipped with a seven-band multispectral camera were conducted over a commercial walnut orchard, paired with concurrent ground measurements of sampled walnut plants. The RF regression model, utilizing vegetation indices derived from orthomosaiced UAV imagery and weather data, effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a mean absolute error (MAE) of 0.80 bars. The integration of weather data was particularly crucial for consolidating data across various flight dates. Significant variab
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01373</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01373
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#26816;&#27979;&#26159;&#21046;&#36896;&#19994;&#36136;&#37327;&#25511;&#21046;&#38454;&#27573;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#65292;&#24182;&#22312;&#32599;&#20271;&#29305;&#183;&#21338;&#19990;&#21046;&#36896;&#21378;&#29983;&#20135;&#30340;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#32452;&#20214;&#30340;&#30495;&#23454;&#32570;&#38519;&#26816;&#27979;&#24212;&#29992;&#20013;&#32771;&#23519;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;T-CNN&#22312;&#20943;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#19978;&#36816;&#34892;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31561;&#25928;CNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;T-CNN&#21487;&#20197;&#36890;&#36807;&#36136;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#19982;&#20256;&#32479;CNN&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#21482;&#26377;&#20854;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;T-CNN&#22823;&#22823;&#36229;&#36234;&#20102;&#20256;&#32479;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30340;&#32467;&#26524;&#65292;&#22312;&#24403;&#21069;&#21046;&#36896;&#24212;&#29992;&#20013;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4% to 19% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.
&lt;/p&gt;</description></item><item><title>RL-MPCA&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01369</link><description>&lt;p&gt;
RL-MPCA: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01369
&lt;/p&gt;
&lt;p&gt;
RL-MPCA&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#20174;&#22823;&#37327;&#20505;&#36873;&#39033;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#38543;&#30528;&#29992;&#25143;&#35831;&#27714;&#30340;&#22686;&#21152;&#21644;&#26381;&#21153;&#65288;&#25110;&#27169;&#22411;&#65289;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22312;&#38431;&#21015;&#25130;&#26029;&#22330;&#26223;&#20013;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#65288;&#21363;&#20998;&#37197;&#20505;&#36873;&#39033;&#30340;&#22823;&#23567;&#65289;&#65292;&#24182;&#23558;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#32422;&#26463;&#26465;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20123;&#30740;&#31350;&#38598;&#20013;&#20110;&#21333;&#38454;&#27573;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#38598;&#20013;&#20110;&#22810;&#38454;&#27573;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#20123;&#20851;&#20110;&#38431;&#21015;&#25130;&#26029;&#22330;&#26223;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#20854;&#20182;&#24773;&#26223;&#19979;&#65288;&#22914;&#26816;&#32034;&#36890;&#36947;&#36873;&#25321;&#21644;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#65289;&#26159;&#19981;&#25104;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#35831;&#27714;&#22312;&#19981;&#21516;&#38454;&#27573;&#20043;&#38388;&#30340;&#29366;&#24577;&#36716;&#31227;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiven
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#21518;&#32487;&#34920;&#31034;&#35757;&#32451;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#27169;&#25311;&#20301;&#32622;&#32454;&#32990;&#21160;&#24577;&#21644;&#35748;&#30693;&#22320;&#22270;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#20174;&#19968;&#31181;&#24418;&#24335;&#25512;&#26029;&#21040;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#23545;&#20110;&#25913;&#21892;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#29702;&#35299;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.01364</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#21518;&#32487;&#34920;&#31034;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#35748;&#30693;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations. (arXiv:2401.01364v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#21518;&#32487;&#34920;&#31034;&#35757;&#32451;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#27169;&#25311;&#20301;&#32622;&#32454;&#32990;&#21160;&#24577;&#21644;&#35748;&#30693;&#22320;&#22270;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#20174;&#19968;&#31181;&#24418;&#24335;&#25512;&#26029;&#21040;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#23545;&#20110;&#25913;&#21892;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#29702;&#35299;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#22320;&#22270;&#26159;&#20851;&#20110;&#22823;&#33041;&#22914;&#20309;&#26377;&#25928;&#22320;&#32452;&#32455;&#35760;&#24518;&#21644;&#25552;&#21462;&#19978;&#19979;&#25991;&#30340;&#19968;&#20010;&#25552;&#35758;&#27010;&#24565;&#12290;&#20869;&#38544;-&#28023;&#39532;&#22797;&#21512;&#20307;&#22312;&#24773;&#33410;&#21644;&#20851;&#31995;&#35760;&#24518;&#22788;&#29702;&#20197;&#21450;&#31354;&#38388;&#23548;&#33322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#34987;&#35748;&#20026;&#36890;&#36807;&#20301;&#32622;&#32454;&#32990;&#21644;&#32593;&#26684;&#32454;&#32990;&#24314;&#31435;&#35748;&#30693;&#22320;&#22270;&#12290;&#20026;&#20102;&#21033;&#29992;&#35748;&#30693;&#22320;&#22270;&#30340;&#26377;&#24076;&#26395;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#21518;&#32487;&#34920;&#31034;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#27169;&#25311;&#20301;&#32622;&#32454;&#32990;&#21160;&#24577;&#21644;&#35748;&#30693;&#22320;&#22270;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;&#22270;&#20687;&#21644;&#35789;&#23884;&#20837;&#32452;&#25104;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#12290;&#32593;&#32476;&#23398;&#20064;&#20102;&#26032;&#36755;&#20837;&#19982;&#35757;&#32451;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25104;&#21151;&#24314;&#31435;&#20102;&#35748;&#30693;&#22320;&#22270;&#30340;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#32593;&#32476;&#30340;&#39044;&#27979;&#21487;&#20197;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#20174;&#19968;&#31181;&#24418;&#24335;&#25512;&#26029;&#21040;&#21478;&#19968;&#31181;&#24418;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#33021;&#25104;&#20026;&#25913;&#36827;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20197;&#26356;&#22909;&#29702;&#35299;&#29615;&#22659;&#30340;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive maps are a proposed concept on how the brain efficiently organizes memories and retrieves context out of them. The entorhinal-hippocampal complex is heavily involved in episodic and relational memory processing, as well as spatial navigation and is thought to built cognitive maps via place and grid cells. To make use of the promising properties of cognitive maps, we set up a multi-modal neural network using successor representations which is able to model place cell dynamics and cognitive map representations. Here, we use multi-modal inputs consisting of images and word embeddings. The network learns the similarities between novel inputs and the training database and therefore the representation of the cognitive map successfully. Subsequently, the prediction of the network can be used to infer from one modality to another with over $90\%$ accuracy. The proposed method could therefore be a building block to improve current AI systems for better understanding of the environment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#24314;&#31435;&#21367;&#31215;&#23618;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;CNN&#26550;&#26500;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01361</link><description>&lt;p&gt;
&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimizing Convolutional Neural Network Architecture. (arXiv:2401.01361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#24314;&#31435;&#21367;&#31215;&#23618;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;CNN&#26550;&#26500;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#35832;&#22914;&#35821;&#38899;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25110;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;CNN&#26550;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#21644;&#22797;&#26434;&#65292;&#20854;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#33021;&#28304;&#25104;&#26412;&#65292;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21098;&#26525;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;OCNNA&#65289;&#65292;&#26088;&#22312;&#24314;&#31435;&#21367;&#31215;&#23618;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#35780;&#20272;&#20102;&#35813;&#25552;&#35758;&#65292;&#21253;&#25324;&#26368;&#22909;&#30340;&#24050;&#30693;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#65292;CIFAR-100&#21644;Imagenet&#65289;&#21644;CNN&#26550;&#26500;&#65288;VGG-16&#65292;ResNet-50&#65292;DenseNet-40&#21644;MobileNet&#65289;&#65292;&#36890;&#36807;&#31934;&#24230;&#19979;&#38477;&#21644;&#21097;&#20313;&#21442;&#25968;&#27604;&#29575;&#20316;&#20026;&#23458;&#35266;&#25351;&#26631;&#26469;&#27604;&#36739;OCNNA&#19982;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;20&#22810;&#20010;&#21367;&#31215;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#30456;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNN) are widely used to face challenging tasks like speech recognition, natural language processing or computer vision. As CNN architectures get larger and more complex, their computational requirements increase, incurring significant energetic costs and challenging their deployment on resource-restricted devices. In this paper, we propose Optimizing Convolutional Neural Network Architecture (OCNNA), a novel CNN optimization and construction method based on pruning and knowledge distillation designed to establish the importance of convolutional layers. The proposal has been evaluated though a thorough empirical study including the best known datasets (CIFAR-10, CIFAR-100 and Imagenet) and CNN architectures (VGG-16, ResNet-50, DenseNet-40 and MobileNet), setting Accuracy Drop and Remaining Parameters Ratio as objective metrics to compare the performance of OCNNA against the other state-of-art approaches. Our method has been compared with more than 20 convo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.01343</link><description>&lt;p&gt;
IoTGeM: &#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#32593;&#32476;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#25915;&#20987;&#26816;&#27979;&#30740;&#31350;&#65292;&#25152;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24314;&#27169;&#29289;&#32852;&#32593;&#32593;&#32476;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#27169;&#22411;&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24120;&#35265;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#20986;&#25903;&#25745;&#25915;&#20987;&#20934;&#30830;&#26816;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research on behaviour-based attack detection on networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited, and often not demonstrated. In this paper we present an approach for modelling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. First, we present an improved rolling window approach for feature extraction, and introduce a multi-step feature selection process that reduces overfitting. Second, we build and test models using isolated train and test datasets, thereby avoiding common data leaks that have limited the generalizability of previous models. Third, we rigorously evaluate our methodology using a diverse portfolio of machine learning models, evaluation metrics and datasets. Finally, we build confidence in the models by using explainable AI techniques, allowing us to identify the features that underlie accurate detection of attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#65292;&#37325;&#28857;&#26159;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#38598;&#25104;&#65292;&#29992;&#20110;&#24322;&#24120;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27169;&#22411;&#25972;&#21512;&#21040;&#32593;&#32476;&#23433;&#20840;&#21644;&#31227;&#21160;&#35774;&#22791;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.01342</link><description>&lt;p&gt;
&#20445;&#25252;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#21644;&#25968;&#23383;&#20135;&#19994;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21152;&#24378;&#24694;&#24847;&#36719;&#20214;&#21644;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection. (arXiv:2401.01342v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#65292;&#37325;&#28857;&#26159;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#38598;&#25104;&#65292;&#29992;&#20110;&#24322;&#24120;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27169;&#22411;&#25972;&#21512;&#21040;&#32593;&#32476;&#23433;&#20840;&#21644;&#31227;&#21160;&#35774;&#22791;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#21313;&#24180;&#30340;&#25216;&#26415;&#36827;&#27493;&#20197;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31561;&#29616;&#20195;&#25216;&#26415;&#20026;&#21160;&#21147;&#12290;&#19990;&#30028;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#25968;&#23383;&#21270;&#36830;&#25509;&#65292;&#20294;&#25105;&#20204;&#38754;&#20020;&#30528;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#32593;&#32476;&#29359;&#32618;&#65292;&#23427;&#24050;&#25104;&#20026;&#23545;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#31038;&#20250;&#30340;&#20840;&#29699;&#23041;&#32961;&#12290;&#25968;&#23383;&#25216;&#26415;&#30340;&#26222;&#21450;&#24615;&#32467;&#21512;&#19981;&#26029;&#21464;&#21270;&#30340;&#25216;&#26415;&#22522;&#30784;&#20026;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#21019;&#36896;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#24378;&#22823;&#30340;&#28216;&#20048;&#22330;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#23041;&#32961;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;AI&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#65292;&#20197;&#20445;&#25252;&#25105;&#20204;&#29616;&#20195;&#25968;&#23383;&#29983;&#24577;&#31995;&#32479;&#12290;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#22522;&#20110;ML&#30340;&#20998;&#31867;&#22120;&#21644;&#38598;&#25104;&#65292;&#29992;&#20110;&#24322;&#24120;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65292;&#24182;&#25506;&#35752;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#31227;&#21160;&#35774;&#22791;&#29615;&#22659;&#19979;&#22914;&#20309;&#38598;&#25104;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decades have been characterized by unprecedented technological advances, many of them powered by modern technologies such as Artificial Intelligence (AI) and Machine Learning (ML). The world has become more digitally connected than ever, but we face major challenges. One of the most significant is cybercrime, which has emerged as a global threat to governments, businesses, and civil societies. The pervasiveness of digital technologies combined with a constantly shifting technological foundation has created a complex and powerful playground for cybercriminals, which triggered a surge in demand for intelligent threat detection systems based on machine and deep learning. This paper investigates AI-based cyber threat detection to protect our modern digital ecosystems. The primary focus is on evaluating ML-based classifiers and ensembles for anomaly-based malware detection and network intrusion detection and how to integrate those models in the context of network security, mobile s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.01262</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20844;&#24179;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#25307;&#32856;&#31561;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#20316;&#20026;&#19987;&#23478;&#31995;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#23548;&#24072;&#12290;&#30001;&#20110;NLP&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#28183;&#20837;NLP&#31995;&#32479;&#65292;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#27495;&#35270;&#23569;&#25968;&#32676;&#20307;&#25110;&#24341;&#21457;&#27861;&#24459;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;NLP&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#37319;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37327;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#22810;&#20301;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#35775;&#35848;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;NLP&#30340;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#21270;&#20026;18&#20010;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#20026;&#23454;&#26045;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.00744</link><description>&lt;p&gt;
&#22312;&#26230;&#20307;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#23558;&#21327;&#26041;&#24046;&#21644;&#34920;&#36798;&#33021;&#21147;&#34701;&#21512;&#20026;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#65306;&#19968;&#31181;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00744
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#21704;&#23494;&#39039;&#22238;&#24402;&#37327;&#23376;&#31995;&#32479;&#38656;&#35201;&#28385;&#36275;&#21327;&#26041;&#24046;&#23450;&#24459;&#65292;&#20854;&#20013;&#23454;&#29616;SO(3)&#31561;&#21464;&#24615;&#32780;&#19981;&#25439;&#22833;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#29702;&#35770;&#31561;&#21464;&#24615;&#20445;&#35777;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#21327;&#26041;&#24046;-&#34920;&#36798;&#33021;&#21147;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#20998;&#20026;&#20004;&#20010;&#32423;&#32852;&#22238;&#24402;&#38454;&#27573;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#23545;&#31216;&#24615;&#65292;&#20135;&#29983;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#21704;&#23494;&#39039;&#39044;&#27979;&#65292;&#24110;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#21464;&#24615;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#38750;&#32447;&#24615;&#19977;&#32500;&#22270;&#24418;Transformer&#32593;&#32476;&#26469;&#36827;&#34892;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#32467;&#26500;&#24314;&#27169;&#65292;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#31934;&#32454;&#21270;&#20026;&#20855;&#26377;&#26356;&#22909;&#34920;&#36798;&#33021;&#21147;&#30340;&#21704;&#23494;&#39039;&#39044;&#27979;&#12290;&#36890;&#36807;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#24615;&#21644;&#26356;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#21704;&#23494;&#39039;&#22238;&#24402;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#31867;&#21035;&#37327;&#21270;&#30340;&#26680;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#30446;&#21069;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#26356;&#22909;&#22320;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#31867;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.00490</link><description>&lt;p&gt;
&#22810;&#31867;&#21035;&#37327;&#21270;&#30340;&#26680;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Kernel Density Estimation for Multiclass Quantification. (arXiv:2401.00490v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#31867;&#21035;&#37327;&#21270;&#30340;&#26680;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#30446;&#21069;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#26356;&#22909;&#22320;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#31867;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#23398;&#31185;&#65292;&#22914;&#31038;&#20250;&#31185;&#23398;&#12289;&#27969;&#34892;&#30149;&#23398;&#12289;&#24773;&#24863;&#20998;&#26512;&#25110;&#24066;&#22330;&#30740;&#31350;&#65292;&#20851;&#27880;&#30340;&#26159;&#20102;&#35299;&#20154;&#32676;&#20013;&#21508;&#31867;&#21035;&#30340;&#20998;&#24067;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20010;&#20307;&#30340;&#26631;&#31614;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#33719;&#24471;&#20934;&#30830;&#30340;&#31867;&#21035;&#39057;&#29575;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#26631;&#31614;&#20559;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#20998;&#24067;&#21305;&#37197;&#65288;DM&#65289;&#26041;&#27861;&#26159;&#36804;&#20170;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#37327;&#21270;&#26041;&#27861;&#20013;&#26368;&#37325;&#35201;&#30340;&#23478;&#26063;&#20043;&#19968;&#12290;&#30446;&#21069;&#30340;DM&#26041;&#27861;&#36890;&#36807;&#21518;&#39564;&#27010;&#29575;&#30340;&#30452;&#26041;&#22270;&#23545;&#28041;&#21450;&#30340;&#20154;&#32676;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#31867;&#21035;&#35774;&#32622;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#30452;&#26041;&#22270;&#21464;&#25104;&#20102;&#31867;&#21035;&#29305;&#23450;&#30340;&#65292;&#26080;&#27861;&#27169;&#25311;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#31867;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#23494;&#24230;&#30340;&#26032;&#30340;&#34920;&#31034;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. Quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. The distribution-matching (DM) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. Current DM approaches model the involved populations by means of histograms of posterior probabilities. In this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. We propose a new representation mechanism based on multivariate densities that we model via k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#27867;&#21270;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#26032;&#23450;&#20041;OoD&#25968;&#25454;&#20197;&#21450;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#20445;&#35777;&#20102;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.16243</link><description>&lt;p&gt;
&#26159;&#21542;&#25152;&#26377;&#26410;&#35265;&#25968;&#25454;&#37117;&#26159;OoD&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#27867;&#21270;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#26032;&#23450;&#20041;OoD&#25968;&#25454;&#20197;&#21450;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#20445;&#35777;&#20102;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#30452;&#34987;&#35270;&#20026;&#26159;OoD&#65288;out-of-distribution&#65289;&#65292;&#20351;&#20854;&#27867;&#21270;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24456;&#22810;&#35777;&#25454;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#22686;&#21152;&#21487;&#20197;&#21333;&#35843;&#22320;&#38477;&#20302;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#20174;&#20854;&#20182;&#35266;&#23519;&#21644;&#20998;&#26512;&#26469;&#30475;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#65292;&#24182;&#19988;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#35823;&#24046;&#37117;&#20250;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#20943;&#23567;&#12290;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;&#36825;&#31181;&#38750;&#21333;&#35843;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#22312;&#19981;&#21516;&#35270;&#35273;&#22522;&#20934;&#19978;&#36827;&#34892;&#32463;&#39564;&#35777;&#23454;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;OoD&#25968;&#25454;&#65292;&#23558;&#20854;&#35270;&#20026;&#35757;&#32451;&#22495;&#30340;&#20984;&#21253;&#20043;&#22806;&#30340;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#26032;&#23450;&#20041;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#23427;&#24847;&#21619;&#30528;&#23545;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#35265;&#36807;&#30340;&#25968;&#25454;&#65292;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M3D&#30340;&#26032;&#22411;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#20248;&#21270;&#36807;&#31243;&#22312;&#23454;&#38469;&#21644;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15927</link><description>&lt;p&gt;
M3D&#65306;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#36827;&#34892;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. (arXiv:2312.15927v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M3D&#30340;&#26032;&#22411;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#20248;&#21270;&#36807;&#31243;&#22312;&#23454;&#38469;&#21644;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#23384;&#20648;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#38598;&#21512;&#26469;&#20445;&#30041;&#21407;&#22987;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#30446;&#21069;&#65292;&#20197;&#20248;&#21270;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#26159;&#25968;&#25454;&#38598;&#21387;&#32553;&#39046;&#22495;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21452;&#23618;&#20248;&#21270;&#36807;&#31243;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#21644;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;&#21387;&#32553;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20998;&#24067;&#21305;&#37197;&#65288;DM&#65289;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21387;&#32553;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19987;&#27880;&#20110;&#23545;&#40784;&#20998;&#24067;&#30340;&#19968;&#38454;&#30697;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;DM&#30340;&#26041;&#27861;&#19982;&#20197;&#20248;&#21270;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#32467;&#26524;&#19981;&#22826;&#21487;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;M3D&#30340;&#26032;&#22411;&#22522;&#20110;DM&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training state-of-the-art (SOTA) deep models often requires extensive data, resulting in substantial training and storage costs. To address these challenges, dataset condensation has been developed to learn a small synthetic set that preserves essential information from the original large-scale dataset. Nowadays, optimization-oriented methods have been the primary method in the field of dataset condensation for achieving SOTA results. However, the bi-level optimization process hinders the practical application of such methods to realistic and larger datasets. To enhance condensation efficiency, previous works proposed Distribution-Matching (DM) as an alternative, which significantly reduces the condensation cost. Nonetheless, current DM-based methods have yielded less comparable results to optimization-oriented methods due to their focus on aligning only the first moment of the distributions. In this paper, we present a novel DM-based method named M3D for dataset condensation by Minimi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#32034;&#24863;&#30693;&#30340;&#38598;&#25104;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#35770;&#35777;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#22411;&#22810;&#26679;&#24615;&#19979;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#24773;&#20917;&#19979;CEs&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2312.15097</link><description>&lt;p&gt;
&#36890;&#36807;&#35770;&#25454;&#38598;&#25104;&#23454;&#29616;&#27169;&#22411;&#22810;&#26679;&#24615;&#19979;&#30340;&#36861;&#32034;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#32034;&#24863;&#30693;&#30340;&#38598;&#25104;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#35770;&#35777;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#22411;&#22810;&#26679;&#24615;&#19979;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#24773;&#20917;&#19979;CEs&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22810;&#26679;&#24615;&#65288;MM&#65289;&#26159;&#25351;&#22312;&#35299;&#20915;&#21516;&#19968;&#39044;&#27979;&#20219;&#21153;&#26102;&#21487;&#20197;&#35757;&#32451;&#20986;&#22810;&#20010;&#24615;&#33021;&#30456;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;MM&#19979;&#33719;&#24471;&#30340;&#27169;&#22411;&#23545;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#21487;&#33021;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#24403;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#20026;&#21463;&#21040;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;&#36861;&#32034;&#24863;&#30693;&#30340;&#38598;&#25104;&#65292;&#24182;&#30830;&#23450;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#24212;&#35813;&#28385;&#36275;&#30340;&#20960;&#20010;&#29702;&#24819;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#38598;&#25104;&#26041;&#27861;&#22312;&#19981;&#21516;&#26041;&#24335;&#19979;&#33258;&#28982;&#25193;&#23637;&#20197;&#25552;&#20379;CEs&#26102;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35770;&#35777;&#38598;&#25104;&#65292;&#37319;&#29992;&#35745;&#31639;&#35770;&#35777;&#26041;&#27861;&#26469;&#30830;&#20445;CEs&#23545;MM&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36866;&#24212;&#21487;&#23450;&#21046;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#35770;&#35777;&#38598;&#25104;&#28385;&#36275;&#20102;&#36825;&#20123;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Multiplicity (MM) arises when multiple, equally performing machine learning models can be trained to solve the same prediction task. Recent studies show that models obtained under MM may produce inconsistent predictions for the same input. When this occurs, it becomes challenging to provide counterfactual explanations (CEs), a common means for offering recourse recommendations to individuals negatively affected by models' predictions. In this paper, we formalise this problem, which we name recourse-aware ensembling, and identify several desirable properties which methods for solving it should satisfy. We show that existing ensembling methods, naturally extended in different ways to provide CEs, fail to satisfy these properties. We then introduce argumentative ensembling, deploying computational argumentation to guarantee robustness of CEs to MM, while also accommodating customisable user preferences. We show theoretically and experimentally that argumentative ensembling satisfies
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCUNet++&#30340;&#33258;&#21160;&#32954;&#26643;&#22622;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;Swin Transformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#22810;&#37325;&#34701;&#21512;&#31264;&#23494;&#36339;&#36291;&#36830;&#25509;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#23454;&#29616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#29305;&#24449;&#20998;&#23618;&#32467;&#26500;&#21644;&#23616;&#37096;&#12289;&#20840;&#23616;&#31354;&#38388;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.14705</link><description>&lt;p&gt;
SCUNet++: Swin-UNet&#21644;CNN&#29942;&#39048;&#28151;&#21512;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#37325;&#34701;&#21512;&#31264;&#23494;&#36339;&#36291;&#36830;&#25509;&#29992;&#20110;&#32954;&#26643;&#22622;CT&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation. (arXiv:2312.14705v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14705
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCUNet++&#30340;&#33258;&#21160;&#32954;&#26643;&#22622;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;Swin Transformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#22810;&#37325;&#34701;&#21512;&#31264;&#23494;&#36339;&#36291;&#36830;&#25509;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#23454;&#29616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#29305;&#24449;&#20998;&#23618;&#32467;&#26500;&#21644;&#23616;&#37096;&#12289;&#20840;&#23616;&#31354;&#38388;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#26643;&#22622;&#65288;PE&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#32954;&#37096;&#30142;&#30149;&#65292;&#20005;&#37325;&#26102;&#21487;&#23548;&#33268;&#21491;&#24515;&#23460;&#32933;&#21402;&#21644;&#34928;&#31469;&#65292;&#22312;&#20005;&#37325;&#31243;&#24230;&#19978;&#20165;&#27425;&#20110;&#24515;&#32908;&#26775;&#27515;&#21644;&#29469;&#27515;&#12290;&#32954;&#21160;&#33033;CT&#34880;&#31649;&#36896;&#24433;&#65288;CTPA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;PE&#35786;&#26029;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24433;&#20687;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;PE&#30340;&#26816;&#27979;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;CTPA&#21487;&#33021;&#20135;&#29983;&#19982;PE&#31867;&#20284;&#30340;&#22122;&#22768;&#65292;&#20351;&#24471;&#30830;&#35748;&#20854;&#23384;&#22312;&#21464;&#24471;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PE&#20998;&#21106;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#29305;&#24449;&#30340;&#20998;&#23618;&#32467;&#26500;&#12289;PE CT&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#31354;&#38388;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#30340;PE&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;SCUNet++&#65288;Swin Conv UNet++&#65289;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#24341;&#20837;&#20102;&#22810;&#37325;&#34701;&#21512;&#31264;&#23494;&#36339;&#36291;&#36830;&#25509;&#65292;&#21033;&#29992;Swin Transformer&#20316;&#20026;&#32534;&#30721;&#22120;&#12290;&#24182;&#22312;&#35299;&#30721;&#22120;&#23376;&#32593;&#32476;&#20013;&#34701;&#21512;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#65292;&#20197;&#24357;&#34917;&#29305;&#24449;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary embolism (PE) is a prevalent lung disease that can lead to right ventricular hypertrophy and failure in severe cases, ranking second in severity only to myocardial infarction and sudden death. Pulmonary artery CT angiography (CTPA) is a widely used diagnostic method for PE. However, PE detection presents challenges in clinical practice due to limitations in imaging technology. CTPA can produce noises similar to PE, making confirmation of its presence time-consuming and prone to overdiagnosis. Nevertheless, the traditional segmentation method of PE can not fully consider the hierarchical structure of features, local and global spatial features of PE CT images. In this paper, we propose an automatic PE segmentation method called SCUNet++ (Swin Conv UNet++). This method incorporates multiple fusion dense skip connections between the encoder and decoder, utilizing the Swin Transformer as the encoder. And fuses features of different scales in the decoder subnetwork to compensate f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36712;&#36857;&#39044;&#27979;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#23433;&#20840;&#23041;&#32961;&#8212;&#8212;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#19968;&#20123;&#35302;&#21457;&#22120;&#30340;&#25805;&#32437;&#65292;&#21487;&#20197;&#20351;&#29616;&#26377;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#22312;&#21463;&#21040;&#29305;&#23450;&#26465;&#20214;&#21050;&#28608;&#26102;&#39044;&#27979;&#38169;&#35823;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.13863</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#38376;&#25805;&#32437;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Manipulating Trajectory Prediction with Backdoors. (arXiv:2312.13863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36712;&#36857;&#39044;&#27979;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#23433;&#20840;&#23041;&#32961;&#8212;&#8212;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#19968;&#20123;&#35302;&#21457;&#22120;&#30340;&#25805;&#32437;&#65292;&#21487;&#20197;&#20351;&#29616;&#26377;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#22312;&#21463;&#21040;&#29305;&#23450;&#26465;&#20214;&#21050;&#28608;&#26102;&#39044;&#27979;&#38169;&#35823;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#36712;&#36857;&#65292;&#20197;&#30830;&#20445;&#22312;&#19981;&#30830;&#23450;&#21644;&#22797;&#26434;&#30340;&#20132;&#36890;&#24773;&#20917;&#19979;&#36827;&#34892;&#23433;&#20840;&#25805;&#20316;&#12290;&#38543;&#30528;&#20844;&#21496;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#36712;&#36857;&#39044;&#27979;&#65292;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#21518;&#38376;&#8212;&#8212;&#19968;&#31181;&#22312;&#20854;&#20182;&#39046;&#22495;&#34987;&#35748;&#21487;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#20294;&#22312;&#36712;&#36857;&#39044;&#27979;&#20013;&#36804;&#20170;&#34987;&#24573;&#35270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#24182;&#30740;&#31350;&#20102;&#22235;&#20010;&#21487;&#33021;&#24433;&#21709;&#36712;&#36857;&#39044;&#27979;&#30340;&#35302;&#21457;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#21049;&#36710;&#36710;&#36742;&#65289;&#65292;&#24403;&#19982;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26399;&#26395;&#36755;&#20986;&#65288;&#20363;&#22914;&#65292;&#36716;&#24367;&#65289;&#30456;&#20851;&#26102;&#65292;&#20250;&#23548;&#33268;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#26399;&#26395;&#36755;&#20986;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#35813;&#27169;&#22411;&#22312;&#27491;&#24120;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#21363;&#20351;&#35302;&#21457;&#25805;&#32437;&#26159;&#30001;&#30446;&#26631;&#36710;&#36742;&#21518;&#38754;&#30340;&#38750;&#27491;&#24120;&#20195;&#29702;&#25191;&#34892;&#30340;&#65292;&#36825;&#31181;&#24773;&#20917;&#20063;&#36866;&#29992;&#12290;&#20316;&#20026;&#21103;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26377;&#36259;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles ought to predict the surrounding agents' trajectories to allow safe maneuvers in uncertain and complex traffic situations. As companies increasingly apply trajectory prediction in the real world, security becomes a relevant concern. In this paper, we focus on backdoors - a security threat acknowledged in other fields but so far overlooked for trajectory prediction. To this end, we describe and investigate four triggers that could affect trajectory prediction. We then show that these triggers (for example, a braking vehicle), when correlated with a desired output (for example, a curve) during training, cause the desired output of a state-of-the-art trajectory prediction model. In other words, the model has good benign performance but is vulnerable to backdoors. This is the case even if the trigger maneuver is performed by a non-casual agent behind the target vehicle. As a side-effect, our analysis reveals interesting limitations within trajectory prediction models. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#25991;&#26412;&#21040;4D&#23545;&#35937;&#21512;&#25104;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#19977;&#32500;&#39640;&#26031;&#31751;&#21644;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#21516;&#26102;&#23454;&#29616;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#21644;&#36924;&#30495;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#26041;&#27861;&#26469;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#24182;&#22686;&#24378;&#36816;&#21160;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.13763</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#19977;&#32500;&#39640;&#26031;&#21644;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#21040;4D&#23545;&#35937;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. (arXiv:2312.13763v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#25991;&#26412;&#21040;4D&#23545;&#35937;&#21512;&#25104;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#19977;&#32500;&#39640;&#26031;&#31751;&#21644;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#21516;&#26102;&#23454;&#29616;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#21644;&#36924;&#30495;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#26041;&#27861;&#26469;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#24182;&#22686;&#24378;&#36816;&#21160;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#20248;&#21270;&#30340;3D&#23545;&#35937;&#21512;&#25104;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#25991;&#26412;&#21040;4D&#23545;&#35937;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#20351;&#29992;&#20998;&#25968;&#33976;&#39311;&#26041;&#27861;&#21512;&#25104;&#20855;&#26377;&#39069;&#22806;&#26102;&#38388;&#32500;&#24230;&#30340;&#21160;&#24577;&#30340;&#12289;&#21160;&#30011;&#30340;3D&#23545;&#35937;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#21644;3D&#24863;&#30693;&#22810;&#35270;&#22270;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;4D&#23545;&#35937;&#20248;&#21270;&#36807;&#31243;&#20013;&#25552;&#20379;&#21453;&#39304;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#21644;&#36924;&#30495;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;Align Your Gaussians&#65288;AYG&#65289;&#65292;&#21033;&#29992;&#20102;&#24102;&#26377;&#24418;&#21464;&#22330;&#30340;&#21160;&#24577;&#19977;&#32500;&#39640;&#26031;&#31751;&#36827;&#34892;4D&#34920;&#31034;&#12290;AYG&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31181;&#29992;&#26469;&#35268;&#33539;&#31227;&#21160;&#30340;&#19977;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#24182;&#24341;&#23548;&#36816;&#21160;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#25918;&#22823;&#26426;&#21046;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#35843;&#25972;&#19982;&#22122;&#22768;&#39044;&#27979;&#38590;&#24230;&#30456;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.13307</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#27493;&#39588;&#37117;&#30456;&#31561;&#65306;&#36827;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#35843;&#25972;&#19982;&#22122;&#22768;&#39044;&#27979;&#38590;&#24230;&#30456;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#33021;&#65292;&#20855;&#26377;&#21435;&#22122;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#19978;&#37117;&#37319;&#29992;&#32479;&#19968;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#22122;&#22768;&#28508;&#22312;&#21464;&#21270;&#23548;&#33268;&#20102;&#35757;&#32451;&#20013;&#30340;&#20914;&#31361;&#65292;&#38480;&#21046;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;&#35757;&#32451;&#19968;&#20010;&#22522;&#30784;&#30340;&#21435;&#22122;&#27169;&#22411;&#26469;&#21253;&#25324;&#25152;&#26377;&#30340;&#26102;&#38388;&#27493;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#27493;&#20998;&#20026;&#19981;&#21516;&#30340;&#32452;&#65292;&#23545;&#27599;&#20010;&#32452;&#20869;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#36798;&#21040;&#19987;&#38376;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#65292;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#22122;&#22768;&#39044;&#27979;&#22256;&#38590;&#31243;&#24230;&#26159;&#19981;&#21516;&#30340;&#65292;&#25152;&#20197;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26679;&#30340;&#27169;&#22411;&#22823;&#23567;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#20449;&#22122;&#27604;&#26469;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#65292;&#20197;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#12290;&#27492;&#35843;&#25972;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#27969;&#31243;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable efficacy in various generative tasks with the predictive prowess of denoising model. Currently, these models employ a uniform denoising approach across all timesteps. However, the inherent variations in noisy latents at each timestep lead to conflicts during training, constraining the potential of diffusion models. To address this challenge, we propose a novel two-stage training strategy termed Step-Adaptive Training. In the initial stage, a base denoising model is trained to encompass all timesteps. Subsequently, we partition the timesteps into distinct groups, fine-tuning the model within each group to achieve specialized denoising capabilities. Recognizing that the difficulties of predicting noise at different timesteps vary, we introduce a diverse model size requirement. We dynamically adjust the model size for each timestep by estimating task difficulty based on its signal-to-noise ratio before fine-tuning. This adjustment is facilitat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20840;&#38754;&#25506;&#32034;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#35752;&#35770;&#20102;&#36890;&#36807;&#25299;&#25169;&#20449;&#24687;&#26469;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#23558;&#24037;&#20316;&#20998;&#20026;&#22235;&#20010;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12289;&#20915;&#31574;&#21306;&#22495;&#12289;&#20869;&#37096;&#34920;&#31034;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2312.05840</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis for Neural Network Analysis: A Comprehensive Survey. (arXiv:2312.05840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20840;&#38754;&#25506;&#32034;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#35752;&#35770;&#20102;&#36890;&#36807;&#25299;&#25169;&#20449;&#24687;&#26469;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#23558;&#24037;&#20316;&#20998;&#20026;&#22235;&#20010;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12289;&#20915;&#31574;&#21306;&#22495;&#12289;&#20869;&#37096;&#34920;&#31034;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#25506;&#32034;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#22312;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#25345;&#20037;&#21516;&#35843;&#21644;Mapper&#31561;TDA&#24037;&#20855;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#34892;&#20026;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;TDA&#20174;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#21462;&#25299;&#25169;&#20449;&#24687;&#30340;&#19981;&#21516;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#22914;&#20309;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#23545;&#25239;&#24615;&#26816;&#27979;&#21644;&#27169;&#22411;&#36873;&#25321;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#24037;&#20316;&#20998;&#20026;&#22235;&#20010;&#24191;&#27867;&#30340;&#39046;&#22495;: 1. &#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25551;&#36848;; 2. &#20915;&#31574;&#21306;&#22495;&#21644;&#36793;&#30028;&#30340;&#20998;&#26512;; 3. &#20869;&#37096;&#34920;&#31034;&#12289;&#28608;&#27963;&#21644;&#21442;&#25968;&#30340;&#30740;&#31350;; 4. &#35757;&#32451;&#21160;&#21147;&#23398;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides a comprehensive exploration of applications of Topological Data Analysis (TDA) within neural network analysis. Using TDA tools such as persistent homology and Mapper, we delve into the intricate structures and behaviors of neural networks and their datasets. We discuss different strategies to obtain topological information from data and neural networks by means of TDA. Additionally, we review how topological information can be leveraged to analyze properties of neural networks, such as their generalization capacity or expressivity. We explore practical implications of deep learning, specifically focusing on areas like adversarial detection and model selection. Our survey organizes the examined works into four broad domains: 1. Characterization of neural network architectures; 2. Analysis of decision regions and boundaries; 3. Study of internal representations, activations, and parameters; 4. Exploration of training dynamics and loss functions. Within each category,
&lt;/p&gt;</description></item><item><title>DISPLACE Challenge 2023&#26159;&#19968;&#20010;&#20851;&#20110;&#22312;&#22810;&#35821;&#35328;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#21457;&#35328;&#20154;&#21644;&#35821;&#35328;&#36776;&#35782;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#25216;&#26415;&#30340;&#34920;&#29616;&#26469;&#25552;&#39640;&#36825;&#31181;&#36776;&#35782;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.12564</link><description>&lt;p&gt;
DISPLACE Challenge 2023 -- &#20250;&#35805;&#29615;&#22659;&#20013;&#30340;&#21457;&#35328;&#20154;&#21644;&#35821;&#35328;&#36776;&#35782;&#38382;&#39064;&#30340;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and LAnguage in Conversational Environments. (arXiv:2311.12564v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12564
&lt;/p&gt;
&lt;p&gt;
DISPLACE Challenge 2023&#26159;&#19968;&#20010;&#20851;&#20110;&#22312;&#22810;&#35821;&#35328;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#21457;&#35328;&#20154;&#21644;&#35821;&#35328;&#36776;&#35782;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#25216;&#26415;&#30340;&#34920;&#29616;&#26469;&#25552;&#39640;&#36825;&#31181;&#36776;&#35782;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#65292;&#24403;&#19968;&#20010;&#23567;&#22320;&#29702;&#21306;&#22495;&#20013;&#21516;&#26102;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#38750;&#27491;&#24335;&#23545;&#35805;&#26102;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#39640;&#25928;&#22320;&#20174;&#36825;&#20123;&#23545;&#35805;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#22240;&#20026;&#35821;&#38899;&#25968;&#25454;&#21253;&#21547;&#22810;&#31181;&#35821;&#35328;&#21644;&#21457;&#35328;&#20154;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;DISPLACE&#65288;&#20250;&#35805;&#29615;&#22659;&#20013;&#30340;&#21457;&#35328;&#20154;&#21644;&#35821;&#35328;&#36776;&#35782;&#38382;&#39064;&#65289;&#25361;&#25112;&#36890;&#36807;&#19968;&#20010;&#24320;&#25918;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22810;&#21457;&#35328;&#20154;&#21644;&#22810;&#35821;&#35328;&#30340;&#36776;&#35782;&#25216;&#26415;&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#20219;&#21153;1&#20851;&#27880;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#21457;&#35328;&#20154;&#36776;&#35782;&#65292;&#20219;&#21153;2&#21017;&#20851;&#27880;&#22810;&#21457;&#35328;&#20154;&#24773;&#22659;&#20013;&#30340;&#35821;&#35328;&#36776;&#35782;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20351;&#29992;&#30456;&#21516;&#30340;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#20419;&#36827;&#35780;&#20272;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#35821;&#35328;&#12289;&#22810;&#21457;&#35328;&#20154;&#30340;&#36828;&#22330;&#23545;&#35805;&#35821;&#38899;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#22522;&#20934;&#31995;&#32479;&#29992;&#20110;&#23545;&#25361;&#25112;&#20219;&#21153;&#36827;&#34892;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-lingual societies, where multiple languages are spoken in a small geographic vicinity, informal conversations often involve mix of languages. Existing speech technologies may be inefficient in extracting information from such conversations, where the speech data is rich in diversity with multiple languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in Conversational Environments) challenge constitutes an open-call for evaluating and bench-marking the speaker and language diarization technologies on this challenging condition. The challenge entailed two tracks: Track-1 focused on speaker diarization (SD) in multilingual situations while, Track-2 addressed the language diarization (LD) in a multi-speaker scenario. Both the tracks were evaluated using the same underlying audio data. To facilitate this evaluation, a real-world dataset featuring multilingual, multi-speaker conversational far-field speech was recorded and distributed. Furthermore, a baseline sys
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#23383;CT&#25195;&#25551;&#22270;&#20687;&#39044;&#27979;&#23721;&#30707;&#30340;&#26377;&#25928;&#24377;&#24615;&#27169;&#37327;&#12290;&#36890;&#36807;&#23558;&#23721;&#30707;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#25968;&#25454;&#38598;&#24182;&#32463;&#36807;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#24377;&#24615;&#27169;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.19274</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23721;&#30707;&#30340;&#26377;&#25928;&#24377;&#24615;&#27169;&#37327;
&lt;/p&gt;
&lt;p&gt;
Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks. (arXiv:2310.19274v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#23383;CT&#25195;&#25551;&#22270;&#20687;&#39044;&#27979;&#23721;&#30707;&#30340;&#26377;&#25928;&#24377;&#24615;&#27169;&#37327;&#12290;&#36890;&#36807;&#23558;&#23721;&#30707;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#25968;&#25454;&#38598;&#24182;&#32463;&#36807;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#24377;&#24615;&#27169;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23383;CT&#25195;&#25551;&#22270;&#20687;&#26469;&#39044;&#27979;&#23721;&#30707;&#30340;&#26377;&#25928;&#24377;&#24615;&#27169;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;Mapper&#31639;&#27861;&#23558;&#19977;&#32500;&#25968;&#23383;&#23721;&#30707;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#24517;&#35201;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#36825;&#20123;&#22270;&#22312;&#39044;&#27979;&#24377;&#24615;&#27169;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;GNN&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#23376;&#31435;&#26041;&#20307;&#23610;&#23544;&#25152;&#24471;&#21040;&#30340;&#19981;&#21516;&#22270;&#23610;&#23544;&#19978;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23427;&#19981;&#20165;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#22312;&#26410;&#35265;&#36807;&#30340;&#23721;&#30707;&#21644;&#26410;&#25506;&#32034;&#30340;&#23376;&#31435;&#26041;&#20307;&#23610;&#23544;&#19978;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;GNN&#22312;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#23721;&#30707;&#23646;&#24615;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24494;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#26174;&#33879;&#38477;&#20302;&#20102;GPU&#20869;&#23384;&#38656;&#27714;&#65288;&#19982;CNN&#30340;&#32593;&#26684;&#34920;&#31034;&#30456;&#27604;&#65289;&#65292;&#20351;&#24471;&#25209;&#27425;&#22823;&#23567;&#36873;&#25321;&#26356;&#21152;&#28789;&#27963;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
This study presents a Graph Neural Networks (GNNs)-based approach for predicting the effective elastic moduli of rocks from their digital CT-scan images. We use the Mapper algorithm to transform 3D digital rock images into graph datasets, encapsulating essential geometrical information. These graphs, after training, prove effective in predicting elastic moduli. Our GNN model shows robust predictive capabilities across various graph sizes derived from various subcube dimensions. Not only does it perform well on the test dataset, but it also maintains high prediction accuracy for unseen rocks and unexplored subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs) reveals the superior performance of GNNs in predicting unseen rock properties. Moreover, the graph representation of microstructures significantly reduces GPU memory requirements (compared to the grid representation for CNNs), enabling greater flexibility in the batch size selection. This work demonstrates t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.06452</link><description>&lt;p&gt;
&#29702;&#35299;RLHF&#23545;LLM&#27867;&#21270;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#22914;OpenAI&#30340;ChatGPT&#25110;Anthropic&#30340;Claude&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#24320;&#21457;&#26041;&#38754;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#23545;RLHF&#36807;&#31243;&#20013;&#27599;&#20010;&#38454;&#27573;&#30340;&#21033;&#19982;&#24330;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#38454;&#27573;&#65288;&#21363;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22870;&#21169;&#24314;&#27169;&#21644;RLHF&#65289;&#22914;&#20309;&#24433;&#21709;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65306;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#24773;&#26223;&#30340;&#32972;&#26223;&#19979;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#36755;&#20986;&#22810;&#26679;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#21508;&#31181;&#29992;&#20363;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25688;&#35201;&#21644;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#20013;&#23545;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21518;&#32773;&#38750;&#24120;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04171</link><description>&lt;p&gt;
&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26088;&#22312;&#21457;&#29616;&#27450;&#35784;&#32773;&#36890;&#36807;&#30041;&#19979;&#20551;&#35780;&#35770;&#25110;&#36827;&#34892;&#24322;&#24120;&#20132;&#26131;&#27450;&#39575;&#20854;&#20182;&#29992;&#25143;&#12290;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#31867;&#21035;&#65288;&#27450;&#35784;&#25110;&#27491;&#24120;&#65289;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#23454;&#38469;&#19990;&#30028;&#22270;&#34920;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#32858;&#21512;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#20989;&#25968;&#20026;&#27599;&#20010;&#20851;&#31995;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#32858;&#21512;&#36807;&#31243;&#20013;&#37319;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#30340;&#21078;&#26512;&#65292;&#25581;&#31034;&#20102;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#20063;&#21457;&#29616;&#20102;&#19982;&#27492;&#31639;&#27861;&#30456;&#20851;&#30340;&#26356;&#39640;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#36798;&#21040;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.00488</link><description>&lt;p&gt;
&#20851;&#20110;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#30340;&#35760;&#24518;&#21644;&#38544;&#31169;&#39118;&#38505;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Memorization and Privacy Risks of Sharpness Aware Minimization. (arXiv:2310.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#30340;&#21078;&#26512;&#65292;&#25581;&#31034;&#20102;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#20063;&#21457;&#29616;&#20102;&#19982;&#27492;&#31639;&#27861;&#30456;&#20851;&#30340;&#26356;&#39640;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#36798;&#21040;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#35774;&#35745;&#23547;&#27714;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20248;&#21270;&#20013;&#26356;&#24179;&#22374;&#30340;&#26497;&#20540;&#30340;&#31639;&#27861;&#25104;&#20026;&#28966;&#28857;&#65292;&#22240;&#20026;&#26377;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#36825;&#20250;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#35270;&#35282;&#26469;&#21078;&#26512;&#36825;&#20123;&#24615;&#33021;&#25910;&#30410;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#30456;&#23545;&#20110;&#26222;&#36890;SGD&#65292;&#23547;&#27714;&#26356;&#24179;&#22374;&#26497;&#20540;&#30340;&#31639;&#27861;&#22312;&#21738;&#20123;&#25968;&#25454;&#28857;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#25152;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#29305;&#21035;&#26174;&#33879;&#65292;&#36825;&#38656;&#35201;&#35760;&#24518;&#12290;&#36825;&#19968;&#35748;&#35782;&#24110;&#21161;&#25105;&#20204;&#25581;&#31034;&#19982;SAM&#30456;&#20851;&#30340;&#26356;&#39640;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#36890;&#36807;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many recent works, there is an increased focus on designing algorithms that seek flatter optima for neural network loss optimization as there is empirical evidence that it leads to better generalization performance in many datasets. In this work, we dissect these performance gains through the lens of data memorization in overparameterized models. We define a new metric that helps us identify which data points specifically do algorithms seeking flatter optima do better when compared to vanilla SGD. We find that the generalization gains achieved by Sharpness Aware Minimization (SAM) are particularly pronounced for atypical data points, which necessitate memorization. This insight helps us unearth higher privacy risks associated with SAM, which we verify through exhaustive empirical evaluations. Finally, we propose mitigation strategies to achieve a more desirable accuracy vs privacy tradeoff.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#20043;&#38388;&#30340;&#20132;&#21449;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#35780;&#20272;&#20102;&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00077</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;BBOB&#21644;OpenAI Gym&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym. (arXiv:2310.00077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#20043;&#38388;&#30340;&#20132;&#21449;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#35780;&#20272;&#20102;&#20302;&#25104;&#26412;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#22522;&#20110;BO&#30340;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#31639;&#27861;&#37197;&#32622;&#31561;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38382;&#39064;&#32500;&#24230;&#21644;&#35780;&#20272;&#39044;&#31639;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#25928;&#29575;&#20250;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#22312;&#20248;&#21270;&#31038;&#21306;&#20013;&#29420;&#31435;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#26159;&#21542;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;BBO&#20043;&#38388;&#36827;&#34892;&#20132;&#21449;&#21463;&#31934;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#22312;BBO&#20013;&#26159;&#21542;&#21516;&#26679;&#26377;&#25928;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27604;&#36739;&#23454;&#39564;&#36890;&#24120;&#28041;&#21450;&#30456;&#23545;&#36739;&#23567;&#30340;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#20013;&#30340;&#21487;&#35265;&#38382;&#39064;&#65292;&#22914;&#22522;&#32447;&#21021;&#22987;&#21270;&#19981;&#33391;&#12289;&#36807;&#24230;&#25311;&#21512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing ubiquity of machine learning (ML) has led it to enter various areas of computer science, including black-box optimization (BBO). Recent research is particularly concerned with Bayesian optimization (BO). BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase. Meanwhile, derivative-free optimization methods have evolved independently in the optimization community. Therefore, we urge to understand whether cross-fertilization is possible between the two communities, ML and BBO, i.e., whether algorithms that are heavily used in ML also work well in BBO and vice versa. Comparative experiments often involve rather small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17207</link><description>&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#65306;&#23545;&#20869;&#23384;&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#22312;&#26080;&#23613;&#20219;&#21153;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#29305;&#21035;&#26159;&#23558;&#38376;&#24490;&#29615;&#21333;&#20803;(GRU)&#19982;Transformer-XL(TrXL)&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#20110;&#35760;&#24518;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#12289;&#25239;&#22122;&#22768;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#37319;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#21363;Mortar Mayhem&#12289;Mystery Path&#21644;Searing Spotlights&#12290;&#36825;&#20123;&#26368;&#21021;&#26159;&#26377;&#38480;&#30340;&#29615;&#22659;&#34987;&#25512;&#24191;&#20026;&#26032;&#39062;&#30340;&#26080;&#23613;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#65292;&#20174;&#36710;&#28216;&#25103;"I packed my bag"&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#36825;&#20123;&#26080;&#23613;&#20219;&#21153;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#25928;&#29575;&#65292;&#32780;&#19988;&#26377;&#36259;&#22320;&#35780;&#20272;&#20102;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#35760;&#24518;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;TrXL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;&#26412;&#23454;&#29616;&#21033;&#29992;TrXL&#20316;&#20026;&#20197;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20351;&#29992;&#30340;&#24773;&#33410;&#24615;&#35760;&#24518;&#12290;&#22312;&#26377;&#38480;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2309.15216</link><description>&lt;p&gt;
&#20351;&#29992;CodeBERT&#21644;Random Forest Regressor&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;C&#32534;&#31243;&#20316;&#19994;
&lt;/p&gt;
&lt;p&gt;
Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35780;&#20998;&#32534;&#31243;&#20316;&#19994;&#22240;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#31616;&#21270;&#20102;&#20219;&#21153;&#12290;&#23427;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#30721;&#36136;&#37327;&#65292;&#26816;&#27979;&#38169;&#35823;&#65292;&#24182;&#20934;&#30830;&#22320;&#20998;&#37197;&#20998;&#25968;&#65292;&#20943;&#36731;&#20102;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#39640;&#25928;&#21644;&#20844;&#24179;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22238;&#24402;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#12290;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#65292;&#23558;&#25991;&#26412;&#20195;&#30721;&#36755;&#20837;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#21521;&#37327;&#36755;&#20837;&#21040;&#20960;&#20010;&#27169;&#22411;&#20013;&#12290;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20102;&#24314;&#35758;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;1.89&#12290;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35823;&#24046;&#20943;&#23569;(ER)&#31163;&#32676;&#24471;&#20998;&#26469;&#25913;&#36827;&#26222;&#36890;VAE&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02084</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders. (arXiv:2309.02084v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35823;&#24046;&#20943;&#23569;(ER)&#31163;&#32676;&#24471;&#20998;&#26469;&#25913;&#36827;&#26222;&#36890;VAE&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#30340;&#26222;&#36890;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;DGMs&#30456;&#27604;&#65292;&#20351;&#23427;&#20204;&#38750;&#24120;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32676;&#24471;&#20998;&#31216;&#20026;&#35823;&#24046;&#20943;&#23569;(ER)&#65292;&#19987;&#38376;&#20026;&#26222;&#36890;VAE&#35774;&#35745;&#12290;ER&#34701;&#21512;&#20102;&#20174;&#26377;&#25439;&#22270;&#20687;&#36755;&#20837;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#24605;&#24819;&#65292;&#24182;&#32771;&#34385;&#20102;&#22270;&#20687;&#30340;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24615;&#12290;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#33719;&#21462;&#65306;https://github.com/ZJLAB-AMMI/VAE4OOD&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08469</link><description>&lt;p&gt;
LLM4TS:&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#20004;&#38454;&#27573;&#24494;&#35843;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08469
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20511;&#37492;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#32479;&#19968;&#27169;&#22411;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#31867;&#20284;&#30340;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#26500;&#24314;&#31283;&#20581;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LLM4TS&#19987;&#27880;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20462;&#34917;&#19982;&#26102;&#38388;&#32534;&#30721;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20248;&#20808;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#36807;&#31243;&#65306;&#39318;&#20808;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#20351;LLMs&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#19981;&#36827;&#34892;&#22823;&#37327;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#39044;&#35757;&#32451;LLMs&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#38024;&#23545;NISQ&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12289;&#28789;&#27963;&#24615;&#21644;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00583</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#36827;&#34892;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel. (arXiv:2308.00583v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00583
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#38024;&#23545;NISQ&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12289;&#28789;&#27963;&#24615;&#21644;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#28041;&#21450;&#35782;&#21035;&#19982;&#20854;&#20182;&#25968;&#25454;&#26377;&#25152;&#19981;&#21516;&#30340;&#35266;&#27979;&#25110;&#20107;&#20214;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#25104;&#21151;&#65292;&#36890;&#36807;&#26816;&#27979;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#27169;&#24335;&#21644;&#20559;&#24046;&#12290;&#37327;&#23376;&#35745;&#31639;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#24050;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#20197;&#24320;&#21457;&#36866;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#23545;&#36817;&#26399;NISQ&#35774;&#22791;&#30340;QML&#31639;&#27861;&#30340;&#25628;&#32034;&#27491;&#22312;&#20840;&#21147;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;NISQ&#35774;&#22791;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#37327;&#23376;&#27604;&#29305;&#30456;&#24178;&#26102;&#38388;&#12289;&#36739;&#23569;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#21644;&#39640;&#35823;&#24046;&#29575;&#65292;&#23384;&#22312;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#26680;&#26041;&#27861;&#24050;&#25104;&#20026;NISQ&#35774;&#22791;&#19978;&#36827;&#34892;QML&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12289;&#28789;&#27963;&#24615;&#21644;&#19982;NISQ&#32422;&#26463;&#30340;&#20860;&#23481;&#24615;&#12290;&#29305;&#21035;&#26159;&#21033;&#29992;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) involves identifying observations or events that deviate in some way from the rest of the data. Machine learning techniques have shown success in automating this process by detecting hidden patterns and deviations in large-scale data. The potential of quantum computing for machine learning has been widely recognized, leading to extensive research efforts to develop suitable quantum machine learning (QML) algorithms. In particular, the search for QML algorithms for near-term NISQ devices is in full swing. However, NISQ devices pose additional challenges due to their limited qubit coherence times, low number of qubits, and high error rates. Kernel methods based on quantum kernel estimation have emerged as a promising approach to QML on NISQ devices, offering theoretical guarantees, versatility, and compatibility with NISQ constraints. Especially support vector machines (SVM) utilizing quantum kernel estimation have shown success in various supervised learning tasks
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#32806;&#21512;&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#32593;&#32476;&#24615;&#33021;&#12289;&#21160;&#21147;&#23398;&#21644;&#35760;&#24518;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#21487;&#20197;&#22686;&#21152;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.14823</link><description>&lt;p&gt;
&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;&#20013;&#30340;&#35114;&#36864;&#35760;&#24518;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14823
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#32806;&#21512;&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#32593;&#32476;&#24615;&#33021;&#12289;&#21160;&#21147;&#23398;&#21644;&#35760;&#24518;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#21487;&#20197;&#22686;&#21152;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#36830;&#25509;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26550;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20197;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#35757;&#32451;&#30340;&#21069;&#39304;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#29190;&#28856;&#21644;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#27531;&#24046;&#36830;&#25509;&#22914;&#20309;&#24433;&#21709;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#21644;&#35114;&#36864;&#35760;&#24518;&#23646;&#24615;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24369;&#32806;&#21512;&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;(WCRNNs)&#65292;&#20854;&#20013;&#27531;&#24046;&#36830;&#25509;&#23548;&#33268;&#20102;&#26126;&#30830;&#23450;&#20041;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#35114;&#36864;&#35760;&#24518;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;WCRNNs&#30340;&#27531;&#24046;&#36830;&#25509;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#32593;&#32476;&#21160;&#21147;&#23398;&#21644;&#35760;&#24518;&#23646;&#24615;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#30340;&#27531;&#24046;&#36830;&#25509;&#65306;(i) &#23548;&#33268;&#32593;&#32476;&#21160;&#24577;&#25509;&#36817;&#28151;&#27788;&#30340;&#36793;&#32536;&#65292;(ii) &#20801;&#35768;&#32593;&#32476;&#22312;&#38271;&#26102;&#38388;&#20869;&#20445;&#25345;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#20117;&#26354;&#32447;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10253</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#29992;&#20110;&#20117;&#26354;&#32447;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#20117;&#26354;&#32447;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#26680;&#24515;&#38075;&#20117;&#36880;&#28176;&#25104;&#20026;&#22320;&#36136;&#24037;&#31243;&#20013;&#30340;&#20027;&#35201;&#21208;&#25506;&#26041;&#27861;&#65292;&#20117;&#26354;&#32447;&#20316;&#20026;&#22320;&#36136;&#20449;&#24687;&#30340;&#20027;&#35201;&#36733;&#20307;&#26085;&#30410;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22320;&#36136;&#29615;&#22659;&#12289;&#27979;&#20117;&#35774;&#22791;&#12289;&#38075;&#23380;&#36136;&#37327;&#21644;&#31361;&#21457;&#20107;&#20214;&#31561;&#22240;&#32032;&#37117;&#20250;&#24433;&#21709;&#20117;&#26354;&#32447;&#30340;&#36136;&#37327;&#12290;&#20197;&#24448;&#30340;&#37325;&#26032;&#27979;&#20117;&#25110;&#25163;&#24037;&#20462;&#27491;&#26041;&#27861;&#25104;&#26412;&#39640;&#25928;&#29575;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#39044;&#27979;&#32570;&#22833;&#20117;&#26354;&#32447;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#25152;&#25552;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#19978;&#21152;&#20837;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#20998;&#26512;&#25968;&#25454;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#26377;&#36873;&#25321;&#22320;&#23558;LSTM&#20013;&#30340;&#20027;&#23548;&#35745;&#31639;&#32467;&#26524;&#21253;&#25324;&#22312;&#20869;&#65292;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#33267;O(nlogn)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#26088;&#22312;&#22312;&#20154;&#32676;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#20197;&#25913;&#21892;&#20581;&#24247;&#29366;&#24577;&#30340;&#25512;&#26029;&#12290;&#30001;&#20110;&#24046;&#24322;&#21644;&#25968;&#25454;&#20002;&#22833;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2307.06263</link><description>&lt;p&gt;
&#20851;&#20110;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
On the hierarchical Bayesian modelling of frequency response functions. (arXiv:2307.06263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06263
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#26088;&#22312;&#22312;&#20154;&#32676;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#20197;&#25913;&#21892;&#20581;&#24247;&#29366;&#24577;&#30340;&#25512;&#26029;&#12290;&#30001;&#20110;&#24046;&#24322;&#21644;&#25968;&#25454;&#20002;&#22833;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#32676;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#26088;&#22312;&#22312;&#20154;&#32676;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#27491;&#24120;&#21644;&#25439;&#20260;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#23545;&#25104;&#21592;&#20581;&#24247;&#29366;&#24577;&#30340;&#25512;&#26029;&#12290;&#21363;&#20351;&#20154;&#32676;&#30001;&#21517;&#20041;&#19978;&#30456;&#21516;&#30340;&#32467;&#26500;&#32452;&#25104;&#65292;&#30001;&#20110;&#26448;&#26009;&#24615;&#36136;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#36793;&#30028;&#26465;&#20214;&#25110;&#29615;&#22659;&#24433;&#21709;(&#20363;&#22914;&#28201;&#24230;&#21464;&#21270;)&#30340;&#32454;&#24494;&#24046;&#24322;&#20135;&#29983;&#20102; benign variations&#12290;&#36825;&#20123;&#24046;&#24322;&#21487;&#20197;&#24433;&#21709;&#27169;&#24577;&#24615;&#36136;&#65292;&#24182;&#34920;&#29616;&#20026;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;(FRF)&#30340;&#20849;&#25391;&#23792;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#35768;&#22810;SHM&#31574;&#30053;&#20381;&#36182;&#20110;&#23545;&#32467;&#26500;&#30340;&#21160;&#24577;&#29305;&#24615;&#36827;&#34892;&#30417;&#27979;&#65292;&#22240;&#27492; benign variations &#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#23454;&#26045;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25391;&#21160;&#24335;SHM&#30340;&#21478;&#19968;&#20010;&#24120;&#35265;&#25361;&#25112;&#26159;&#25968;&#25454;&#20002;&#22833;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20256;&#36755;&#38382;&#39064;&#12289;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#37319;&#26679;&#29575;&#19981;&#21305;&#37197;&#31561;&#21407;&#22240;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Population-based structural health monitoring (PBSHM) aims to share valuable information among members of a population, such as normal- and damage-condition data, to improve inferences regarding the health states of the members. Even when the population is comprised of nominally-identical structures, benign variations among the members will exist as a result of slight differences in material properties, geometry, boundary conditions, or environmental effects (e.g., temperature changes). These discrepancies can affect modal properties and present as changes in the characteristics of the resonance peaks of the frequency response function (FRF). Many SHM strategies depend on monitoring the dynamic properties of structures, so benign variations can be challenging for the practical implementation of these systems. Another common challenge with vibration-based SHM is data loss, which may result from transmission issues, sensor failure, a sample-rate mismatch between sensors, and other causes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05520</link><description>&lt;p&gt;
DL&#27169;&#22411;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#33021;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#24615;&#33021;&#19978;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20851;&#20110;&#35757;&#32451;DL&#27169;&#22411;&#24102;&#26469;&#24040;&#22823;&#30899;&#36275;&#36857;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#23545;&#23454;&#29616;&#26356;&#29615;&#20445;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#27491;&#30830;&#24615;&#22312;&#26368;&#20339;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#29615;&#22659;&#23545;&#29983;&#25104;&#26356;&#29615;&#20445;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#20851;&#31995;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#38598;&#20102;&#19982;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#22312;&#27979;&#37327;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#35757;&#32451;&#29615;&#22659;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#39564;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05134</link><description>&lt;p&gt;
TIAM -- &#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#35780;&#20272;&#20854;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#28210;&#26579;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#37325;&#35201;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#31561;&#39069;&#22806;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#20174;&#38543;&#26426;&#36215;&#22987;&#28857;&#24320;&#22987;&#30340;&#65292;&#20294;&#36890;&#24120;&#19981;&#32771;&#34385;&#36825;&#19968;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#25552;&#31034;&#20013;&#25351;&#23450;&#30340;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#26356;&#22909;&#22320;&#25551;&#36848;&#23545;&#40784;&#24615;&#65292;&#21253;&#25324;&#25351;&#23450;&#23545;&#35937;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;T2I&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39069;&#22806;&#32467;&#26524;&#65292;&#21363;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#39034;&#24207;&#31639;&#27861;&#30456;&#27604;&#65292;&#24182;&#34892;&#31639;&#27861;&#33021;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04049</link><description>&lt;p&gt;
&#24182;&#34892;&#31639;&#27861;&#19982;&#31070;&#32463;&#25191;&#34892;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Parallel Algorithms Align with Neural Execution. (arXiv:2307.04049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#39034;&#24207;&#31639;&#27861;&#30456;&#27604;&#65292;&#24182;&#34892;&#31639;&#27861;&#33021;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#26159;&#24182;&#34892;&#22788;&#29702;&#22120;&#12290;&#25945;&#25480;&#23427;&#20204;&#39034;&#24207;&#31639;&#27861;&#19982;&#20854;&#24615;&#36136;&#30456;&#30683;&#30462;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#35745;&#31639;&#20013;&#21253;&#21547;&#24456;&#22810;&#20887;&#20313;&#12290;&#28982;&#32780;&#24182;&#34892;&#31639;&#27861;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#21482;&#38656;&#25191;&#34892;&#36739;&#23569;&#30340;&#23618;&#27425;&#12290;&#36825;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;CLRS&#26694;&#26550;&#19978;&#39034;&#24207;&#23454;&#29616;&#30340;&#25628;&#32034;&#12289;&#25490;&#24207;&#21644;&#23547;&#25214;&#24378;&#36830;&#25509;&#32452;&#20214;&#30456;&#27604;&#65292;&#24182;&#34892;&#23454;&#29616;&#30340;&#35757;&#32451;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#24182;&#34892;&#29256;&#26412;&#30340;&#39044;&#27979;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural algorithmic reasoners are parallel processors. Teaching them sequential algorithms contradicts this nature, rendering a significant share of their computations redundant. Parallel algorithms however may exploit their full computational power, therefore requiring fewer layers to be executed. This drastically reduces training times, as we observe when comparing parallel implementations of searching, sorting and finding strongly connected components to their sequential counterparts on the CLRS framework. Additionally, parallel versions achieve strongly superior predictive performance in most cases.
&lt;/p&gt;</description></item><item><title>CardiGraphormer&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#39072;&#35206;&#20102;&#33647;&#29289;&#21457;&#29616;&#30340;&#26041;&#24335;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#25191;&#34892;&#21508;&#31181;&#19982;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.00859</link><description>&lt;p&gt;
CardiGraphormer: &#25581;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#39072;&#35206;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00859
&lt;/p&gt;
&lt;p&gt;
CardiGraphormer&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#39072;&#35206;&#20102;&#33647;&#29289;&#21457;&#29616;&#30340;&#26041;&#24335;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#25191;&#34892;&#21508;&#31181;&#19982;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#38420;&#30340;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#65292;&#24050;&#30693;&#33647;&#29289;&#32422;&#26377;15,000&#31181;&#65292;&#20294;&#21482;&#26377;&#22823;&#32422;4,200&#31181;&#24471;&#21040;&#20102;&#25209;&#20934;&#65292;&#21270;&#23398;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#25552;&#20379;&#20102;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#25104;&#20026;&#20102;&#26377;&#21147;&#30340;&#20249;&#20276;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#20173;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CardiGraphormer&#65292;&#36825;&#26159;&#19968;&#31181;&#21010;&#26102;&#20195;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#39072;&#35206;&#33647;&#29289;&#21457;&#29616;&#12290;CardiGraphormer&#26159;Graphormer&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#21033;&#29992;SSL&#23398;&#20064;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;GNN&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#23427;&#22312;&#22788;&#29702;&#20998;&#23376;&#32467;&#26500;&#31561;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#25191;&#34892;&#19982;&#33410;&#28857;&#12289;&#33410;&#28857;&#23545;&#12289;&#23376;&#22270;&#25110;&#25972;&#20010;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#65292;&#36890;&#36807;&#23558;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#65292;&#20351;&#29992;&#25968;&#20540;&#31283;&#23450;&#30340;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.07774</link><description>&lt;p&gt;
&#38477;&#31209;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65306;&#22312;&#39640;&#32500;&#20013;&#36827;&#34892;&#36817;&#20284;&#20302;&#31209;&#21160;&#24577;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#65292;&#36890;&#36807;&#23558;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#65292;&#20351;&#29992;&#25968;&#20540;&#31283;&#23450;&#30340;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#30340;&#25512;&#26029;&#21644;&#27169;&#25311;&#20013;&#65292;&#38656;&#35201;&#36827;&#34892;&#26576;&#31181;&#24418;&#24335;&#30340;&#38477;&#32500;&#25165;&#33021;&#20351;&#38382;&#39064;&#20855;&#26377;&#21487;&#22788;&#29702;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#39044;&#27979;&#27493;&#39588;&#30456;&#20851;&#30340;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#26469;&#23454;&#29616;&#30340;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#36817;&#24320;&#21457;&#30340;&#25968;&#20540;&#31283;&#23450;&#12289;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#36825;&#20123;&#26041;&#31243;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36890;&#36807;&#27880;&#24847;&#21327;&#26041;&#24046;&#26356;&#26032;&#20165;&#36716;&#25442;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#65292;&#32780;&#35813;&#31354;&#38388;&#30001;&#26500;&#36896;&#24471;&#21040;&#65292;&#20174;&#32780;&#20351;&#26356;&#26032;&#27493;&#39588;&#20855;&#26377;&#21487;&#22788;&#29702;&#24615;&#12290;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#65292;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20363;&#22914;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21270;&#23398;&#20998;&#23376;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#32467;&#26500;&#65292;&#20854;&#34892;&#20026;&#21160;&#24577;&#21464;&#21270;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#35745;&#31639;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#39640;&#26031;&#20998;&#24067;&#65292;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#20998;&#23376;&#25152;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#34920;&#38754;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35266;&#23519;&#21040;&#65292;&#21452;&#26354;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#23618;&#32467;&#26500;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#19988;&#26356;&#23481;&#26131;&#34987;&#25429;&#25417;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#21452;&#26354;&#23884;&#20837;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#21452;&#26354;&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#65292;&#21363;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;$\ell_p$&#23376;&#31354;&#38388;&#23884;&#20837;&#30340;&#28789;&#25935;&#24230;&#37319;&#26679;&#30028;&#38480;&#65292;&#21462;&#24471;&#20102;&#27604;&#36890;&#29992;&#30028;&#38480;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;$1\leq p&lt;2$&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#36798;&#21040;&#20102;$\mathfrak{S}^{2/p}$&#65292;&#23545;&#20110;$2&lt;p&lt;\infty$&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#36798;&#21040;&#20102;$\mathfrak{S}^{2-2/p}$&#12290;</title><link>http://arxiv.org/abs/2306.00732</link><description>&lt;p&gt;
$\ell_p$&#28789;&#25935;&#24230;&#37319;&#26679;&#30340;&#26356;&#20005;&#26684;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Sharper Bounds for $\ell_p$ Sensitivity Sampling. (arXiv:2306.00732v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;$\ell_p$&#23376;&#31354;&#38388;&#23884;&#20837;&#30340;&#28789;&#25935;&#24230;&#37319;&#26679;&#30028;&#38480;&#65292;&#21462;&#24471;&#20102;&#27604;&#36890;&#29992;&#30028;&#38480;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;$1\leq p&lt;2$&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#36798;&#21040;&#20102;$\mathfrak{S}^{2/p}$&#65292;&#23545;&#20110;$2&lt;p&lt;\infty$&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#36798;&#21040;&#20102;$\mathfrak{S}^{2-2/p}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38543;&#26426;&#37319;&#26679;&#26159;&#19968;&#31181;&#36817;&#20284;&#25968;&#25454;&#38598;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#36890;&#36807;&#19968;&#23567;&#37096;&#20998;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#31034;&#20363;&#26469;&#36827;&#34892;&#12290;&#29305;&#21035;&#22320;&#65292;&#28789;&#25935;&#24230;&#37319;&#26679;&#26159;&#19968;&#31181;&#24378;&#28872;&#30740;&#31350;&#30340;&#25216;&#26415;&#65292;&#23427;&#22312;&#26497;&#20854;&#26222;&#36941;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#36817;&#20284;&#36136;&#37327;&#20445;&#35777;&#65292;&#21516;&#26102;&#23558;&#31034;&#20363;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;VC&#32500;$d$&#21644;&#24635;&#28789;&#25935;&#24230;$\mathfrak{S}$&#30340;&#20056;&#31215;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;$\ell_2$&#23376;&#31354;&#38388;&#23884;&#20837;&#20197;&#22806;&#65292;&#24456;&#23569;&#26377;&#20445;&#35777;&#36229;&#36807;&#36825;&#20010;$\mathfrak{S}d$&#36890;&#29992;&#30028;&#38480;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#20197;&#21069;&#30340;&#24037;&#20316;&#38750;&#24120;&#24378;&#35843;&#28789;&#25935;&#24230;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#23545;&#20110;$ p\neq2$&#30340;$\ell_p$&#23376;&#31354;&#38388;&#23884;&#20837;&#30340;&#28789;&#25935;&#24230;&#37319;&#26679;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#36229;&#36807;&#20102;&#19968;&#33324;&#30340;$\mathfrak{S}d$&#30028;&#38480;&#65292;&#23545;&#20110;$1\leq p&lt;2$&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#22823;&#32422;$\mathfrak{S}^{2/p}$&#30340;&#30028;&#38480;&#65292;&#24182;&#19988;&#23545;&#20110;$2&lt;p&lt;\infty$&#65292;&#21462;&#24471;&#20102;$\mathfrak{S}^{2-2/p}$&#30340;&#30028;&#38480;&#12290;&#22312;$1\leq p&lt;2$&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#36793;&#30028;&#26159;&#23494;&#20999;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the VC dimension $d$ and the total sensitivity $\mathfrak S$ in remarkably general settings. However, guarantees going beyond this general bound of $\mathfrak S d$ are known in perhaps only one setting, for $\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\ell_p$ subspace embeddings for $p\neq 2$ that improve over the general $\mathfrak S d$ bound, achieving a bound of roughly $\mathfrak S^{2/p}$ for $1\leq p&lt;2$ and $\mathfrak S^{2-2/p}$ for $2&lt;p&lt;\infty$. For $1\leq p&lt;2$, we show that this bound is tight, in the sense that there exist matrices for which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19555</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#20316;&#20026;&#25277;&#35937;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#21151;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;LLMs&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#25110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36824;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#20063;&#19981;&#30830;&#23450;&#12290;&#25277;&#35937;&#25512;&#29702;&#26159;&#35748;&#30693;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25214;&#21040;&#21644;&#24212;&#29992;&#19968;&#33324;&#27169;&#24335;&#12290;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#22312;&#23616;&#38480;&#24615;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30446;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#25506;&#31350;&#20102;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17033</link><description>&lt;p&gt;
&#12298;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;2023&#65306;&#20851;&#27880;&#20799;&#31185;&#65288;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs&#65289;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#12290;&#20799;&#31461;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#30340;&#20116;&#24180;&#29983;&#23384;&#29575;&#19981;&#21040;20&#65285;&#12290;&#30001;&#20110;&#32597;&#35265;&#65292;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#36890;&#24120;&#20250;&#24310;&#36831;&#65292;&#20854;&#27835;&#30103;&#20027;&#35201;&#22522;&#20110;&#21382;&#21490;&#27835;&#30103;&#29702;&#24565;&#65292;&#24182;&#19988;&#20020;&#24202;&#35797;&#39564;&#38656;&#35201;&#22810;&#26426;&#26500;&#21512;&#20316;&#12290;MICCAI&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#26159;&#19968;&#20010;&#37324;&#31243;&#30865;&#24335;&#30340;&#31038;&#21306;&#22522;&#20934;&#20107;&#20214;&#65292;&#24050;&#32463;&#25104;&#21151;&#21019;&#24314;&#36164;&#28304;12&#24180;&#65292;&#29992;&#20110;&#25104;&#20154;&#33014;&#36136;&#30244;&#30340;&#20998;&#21106;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#22269;&#38469;&#21512;&#20316;&#32452;&#32455;&#19987;&#27880;&#20110;&#20799;&#31185;&#31070;&#32463;&#32959;&#30244;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#25968;&#25454;&#12290;BraTS-PEDs 2023&#25361;&#25112;&#20391;&#37325;&#20110;&#35780;&#20272;&#29992;&#20110;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.11348</link><description>&lt;p&gt;
&#20197;&#20844;&#24179;&#21517;&#20041;&#65306;&#35780;&#20272;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#23545;&#20110;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21512;&#27861;&#20849;&#20139;&#20020;&#24202;&#25968;&#25454;&#38656;&#35201;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#21024;&#38500;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#36825;&#20010;&#36807;&#31243;&#65292;&#31216;&#20026;&#21435;&#35782;&#21035;&#65292;&#36890;&#24120;&#36890;&#36807;&#35768;&#22810;&#21830;&#19994;&#21644;&#24320;&#28304;&#31995;&#32479;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30340;&#26816;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#21517;&#31216;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;16&#20010;&#21517;&#31216;&#38598;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#32500;&#24230;&#65306;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#21517;&#31216;&#27969;&#34892;&#24230;&#21644;&#27969;&#34892;&#30340;&#21313;&#24180;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21517;&#31216;&#25554;&#20837;&#21040;100&#20010;&#25163;&#21160;&#31579;&#36873;&#30340;&#20020;&#24202;&#27169;&#26495;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#20061;&#31181;&#20844;&#20849;&#21644;&#31169;&#20154;&#21435;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#21517;&#31216;&#26041;&#38754;&#23384;&#22312;&#32479;&#35745;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#38899;&#39057;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#38450;&#27490;&#20174;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#29305;&#24449;&#20013;&#26816;&#27979;&#21040;&#35821;&#38899;&#27963;&#21160;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#24471;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#19982;&#19981;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#26080;&#27861;&#34987;&#35821;&#38899;&#20998;&#31867;&#22120;&#21306;&#20998;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2305.00011</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#34920;&#24449;&#23398;&#20064;&#22312;&#38899;&#39057;&#38544;&#31169;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Representation Learning for Robust Privacy Preservation in Audio. (arXiv:2305.00011v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#38899;&#39057;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#38450;&#27490;&#20174;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#29305;&#24449;&#20013;&#26816;&#27979;&#21040;&#35821;&#38899;&#27963;&#21160;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#24471;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#19982;&#19981;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#26080;&#27861;&#34987;&#35821;&#38899;&#20998;&#31867;&#22120;&#21306;&#20998;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#30417;&#35270;&#21644;&#29615;&#22659;&#30417;&#27979;&#65292;&#20854;&#20013;&#25968;&#25454;&#34987;&#33258;&#21160;&#25910;&#38598;&#12289;&#22788;&#29702;&#24182;&#21457;&#36865;&#21040;&#20113;&#20013;&#36827;&#34892;&#22768;&#38899;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#26080;&#24847;&#20013;&#27844;&#38706;&#29992;&#25143;&#25110;&#20854;&#21608;&#22260;&#29615;&#22659;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#24341;&#36215;&#20102;&#38544;&#31169;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#38899;&#39057;&#35760;&#24405;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#38450;&#27490;&#20174;&#35760;&#24405;&#30340;&#28508;&#22312;&#29305;&#24449;&#20013;&#26816;&#27979;&#21040;&#35821;&#38899;&#27963;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#19981;&#21464;&#28508;&#22312;&#34920;&#24449;&#65292;&#36825;&#20123;&#34920;&#24449;&#19981;&#33021;&#30001;&#35821;&#38899;&#20998;&#31867;&#22120;&#20174;&#38750;&#35821;&#38899;&#35760;&#24405;&#20013;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#20248;&#21270;&#31639;&#27861;&#20013;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#23450;&#26399;&#34987;&#35757;&#32451;&#22312;&#26377;&#30417;&#30563;&#26041;&#24335;&#19979;&#30340;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#25152;&#26367;&#25442;&#12290;&#36825;&#22686;&#21152;&#20102;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound event detection systems are widely used in various applications such as surveillance and environmental monitoring where data is automatically collected, processed, and sent to a cloud for sound recognition. However, this process may inadvertently reveal sensitive information about users or their surroundings, hence raising privacy concerns. In this study, we propose a novel adversarial training method for learning representations of audio recordings that effectively prevents the detection of speech activity from the latent features of the recordings. The proposed method trains a model to generate invariant latent representations of speech-containing audio recordings that cannot be distinguished from non-speech recordings by a speech classifier. The novelty of our work is in the optimization algorithm, where the speech classifier's weights are regularly replaced with the weights of classifiers trained in a supervised manner. This increases the discrimination power of the speech cl
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;RKHSs&#19978;&#65292;&#35889;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.14942</link><description>&lt;p&gt;
&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Optimality of Misspecified Spectral Algorithms. (arXiv:2303.14942v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;RKHSs&#19978;&#65292;&#35889;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#38382;&#39064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20551;&#35774;&#22320;&#19979;&#30495;&#23454;&#20989;&#25968;$f_{\rho}^{*} \in [\mathcal{H}]^{s}$&#65292;&#20854;&#20013;$\mathcal{H}$&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#36739;&#24179;&#28369;&#25554;&#20540;&#31354;&#38388;&#65292;$s\in (0,1)$&#12290;&#29616;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#32467;&#26524;&#35201;&#27714;$\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$&#65292;&#36825;&#38544;&#21547;&#22320;&#35201;&#27714;$s &gt; \alpha_{0}$&#65292;&#20854;&#20013;$\alpha_{0}\in (0,1)$&#26159;&#23884;&#20837;&#25351;&#25968;&#65292;&#19968;&#20010;&#20381;&#36182;&#20110;$\mathcal{H}$&#30340;&#24120;&#25968;&#12290;&#20851;&#20110;&#35889;&#31639;&#27861;&#26159;&#21542;&#23545;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26368;&#20248;&#30340;&#38382;&#39064;&#24050;&#32463;&#23384;&#22312;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35889;&#31639;&#27861;&#26159;&#23545;&#20110;&#20219;&#24847;&#30340;$\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#20854;&#20013;$\beta$&#26159;$\mathcal{H}$&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#20960;&#31867;&#28385;&#36275;$ \alpha_0 = \frac{1}{\beta} $&#30340;RKHSs&#65292;&#22240;&#27492;&#65292;&#35889;&#31639;&#27861;&#22312;&#36825;&#20123;RKHSs&#19978;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the misspecified spectral algorithms problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$ which implicitly requires $s &gt; \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the spectral algorithms are optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that spectral algorithms are minimax optimal for any $\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$, where $\beta$ is the eigenvalue decay rate of $\mathcal{H}$. We also give several classes of RKHSs whose embedding index satisfies $ \alpha_0 = \frac{1}{\beta} $. Thus, the spectral algorithms are minimax optimal for all $s\in (0,1)$ on these RKHSs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.03421</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#25512;&#23548;&#65288;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#12290;&#19982;&#22823;&#22810;&#25968;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#20219;&#20309;&#26102;&#38388;&#37117;&#26377;&#25928;&#30340;&#65288;&#21363;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25353;&#29031;&#20197;&#19979;&#39034;&#24207;&#32467;&#21512;&#20102;&#22235;&#31181;&#24037;&#20855;&#65306;&#65288;a&#65289;&#38750;&#36127;&#36229;&#39532;&#19969;&#26684;&#23572;&#25110;&#21453;&#21521;&#20122;&#39532;&#36874;&#65292;&#65288;b&#65289;&#28151;&#21512;&#27861;&#65292;&#65288;c&#65289;Donsker-Varadhan&#20844;&#24335;&#65288;&#25110;&#20854;&#23427;&#20984;&#24615;&#23545;&#20598;&#21407;&#29702;&#65289;&#21644;&#65288;d&#65289;Ville&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#19968;&#20010;PAC-Bayes&#23450;&#29702;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31163;&#25955;&#38543;&#26426;&#36807;&#31243;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32467;&#26524;&#22914;&#20309;&#25512;&#20986;&#30693;&#21517;&#30340;&#32463;&#20856;PAC-Bayes&#30028;&#38480;&#65292;&#20363;&#22914;Seeger&#12289;McAllester&#12289;Maurer&#21644;Catoni&#30340;&#30028;&#38480;&#65292;&#20197;&#21450;&#35768;&#22810;&#26368;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25918;&#26494;&#20256;&#32479;&#30340;&#20551;&#35774;&#65307;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22522;&#32447;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2212.08123</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#38598;&#21512;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Bayesian posterior approximation with stochastic ensembles. (arXiv:2212.08123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22522;&#32447;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#38598;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#12290;&#23427;&#23558;&#38543;&#26426;&#26041;&#27861;&#65288;&#22914;dropout&#65289;&#19982;&#28145;&#24230;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#38543;&#26426;&#38598;&#21512;&#20844;&#24335;&#21270;&#20026;&#20998;&#24067;&#26063;&#65292;&#24182;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35757;&#32451;&#20197;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29609;&#20855;&#38382;&#39064;&#21644;CIFAR&#22270;&#20687;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;&#22522;&#20110;Monte Carlo Dropout&#65292;DropConnect&#21644;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#29256;&#26412;&#30340;&#38543;&#26426;&#38598;&#21512;&#65292;&#24182;&#30452;&#25509;&#19982;&#21704;&#23494;&#39039;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#27604;&#36739;&#36136;&#37327;&#26469;&#27979;&#35797;&#21518;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#38598;&#21512;&#25552;&#20379;&#20102;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22522;&#32447;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochastic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with variational inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamiltonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#40065;&#26834;&#24615;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#12290;&#36890;&#36807;&#19968;&#20010;&#36741;&#21161;&#32593;&#32476;&#25429;&#25417;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20013;&#24212;&#29992;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#26368;&#23567;&#21270;&#20102;&#36749;&#23398;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05987</link><description>&lt;p&gt;
&#20351;&#29992;&#40065;&#26834;&#24615;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Selective classification using a robust meta-learning approach. (arXiv:2212.05987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#40065;&#26834;&#24615;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#12290;&#36890;&#36807;&#19968;&#20010;&#36741;&#21161;&#32593;&#32476;&#25429;&#25417;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20013;&#24212;&#29992;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#26368;&#23567;&#21270;&#20102;&#36749;&#23398;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;-&#27169;&#22411;&#23545;&#20854;&#22312;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;-&#23545;&#20110;&#36890;&#36807;&#35757;&#32451;&#24178;&#39044;&#26500;&#24314;&#40065;&#26834;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#20998;&#31867;&#31561;&#27979;&#35797;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20363;&#26465;&#20214;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#21033;&#29992;&#36741;&#21161;&#32593;&#32476;&#25429;&#25417;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#32479;&#19968;&#20102;&#36825;&#20123;&#35757;&#32451;&#26102;&#38388;&#21644;&#27979;&#35797;&#26102;&#38388;&#24212;&#29992;&#12290;&#36741;&#21161;&#32593;&#32476;&#26159;&#22312;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#20013;&#20351;&#29992;&#20803;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#25552;&#35758;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#26368;&#23567;&#21270;&#36749;&#23398;&#26041;&#24046;&#30340;&#20803;&#30446;&#26631;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#36825;&#20010;&#20803;&#30446;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#26679;&#21270;&#29305;&#23450;&#27010;&#24565;&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#25429;&#25417;&#21040;&#26576;&#20123;&#26041;&#38754;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35774;&#32622;-&#36873;&#25321;&#24615;&#20998;&#31867;&#12289;&#26631;&#31614;&#22122;&#22768;&#12289;&#39046;&#22495;&#36866;&#24212;&#12289;&#26657;&#20934;&#21644;&#25968;&#25454;&#38598;-Imagenet&#12289;Cifar100&#20013;&#36716;&#21270;&#20026;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive uncertainty-a model's self awareness regarding its accuracy on an input-is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditioned reweighting approach that captures predictive uncertainty using an auxiliary network and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing the dropout variance, an approximation of Bayesian Predictive uncertainty. We show in controlled experiments that we effectively capture the diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings-selective classification, label noise, domain adaptation, calibration-and across datasets-Imagenet, Cifar100, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2212.03932</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#34892;&#20026;&#31574;&#30053;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#65292;&#38656;&#35201;&#35780;&#20272;&#30446;&#26631;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#38656;&#35201;&#20351;&#29992;&#30001;&#34892;&#20026;&#31574;&#30053;&#37319;&#38598;&#30340;&#26679;&#26412;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#27010;&#29575;&#27604;&#20540;&#30340;&#20056;&#31215;&#32780;&#23548;&#33268;&#26041;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#28041;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#20986;&#29616;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28508;&#22312;&#29305;&#24449;&#20998;&#35299;&#20026;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#35299;&#34920;&#31034;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26131;&#20110;&#35299;&#37322;&#21644;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00086</link><description>&lt;p&gt;
&#35299;&#24320; (&#19981;)&#21487;&#25511;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangled (Un)Controllable Features. (arXiv:2211.00086v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28508;&#22312;&#29305;&#24449;&#20998;&#35299;&#20026;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#35299;&#34920;&#31034;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26131;&#20110;&#35299;&#37322;&#21644;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#39640;&#32500;&#29366;&#24577;&#30340;MDP&#32972;&#26223;&#19979;&#65292;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#22312;&#21407;&#22987;&#36755;&#20837;&#31354;&#38388;&#30340;&#21387;&#32553;&#12289;&#20302;&#32500;&#34920;&#31034;&#19978;&#36816;&#34892;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33719;&#24471;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#23398;&#20064;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#36890;&#24120;&#32570;&#20047;&#23545;&#19981;&#21516;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#28508;&#22312;&#29305;&#24449;&#20998;&#35299;&#20026;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20998;&#21306;&#34920;&#31034;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#29615;&#22659;&#20013;&#26159;&#26131;&#20110;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#22312;&#20998;&#31163;&#30340;&#21487;&#25511;&#28508;&#22312;&#20998;&#21306;&#20013;&#65292;&#33021;&#22815;&#22312;&#19968;&#32452;&#31243;&#24207;&#29983;&#25104;&#30340;&#36855;&#23467;&#29615;&#22659;&#20013;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of MDPs with high-dimensional states, downstream tasks are predominantly applied on a compressed, low-dimensional representation of the original input space. A variety of learning objectives have therefore been used to attain useful representations. However, these representations usually lack interpretability of the different features. We present a novel approach that is able to disentangle latent features into a controllable and an uncontrollable partition. We illustrate that the resulting partitioned representations are easily interpretable on three types of environments and show that, in a distribution of procedurally generated maze environments, it is feasible to interpretably employ a planning algorithm in the isolated controllable latent partition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#25935;&#25463;&#25163;&#37096;&#25805;&#20316;&#30340;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#21508;&#31181;&#26465;&#20214;&#30340;&#31574;&#30053;&#21644;&#40065;&#26834;&#20301;&#23039;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#25163;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#28789;&#24039;&#25805;&#20316;&#65292;&#24182;&#35777;&#23454;&#20102;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#31227;&#23545;&#25935;&#25463;&#25805;&#20316;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13702</link><description>&lt;p&gt;
DeXtreme: &#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#25935;&#25463;&#25163;&#37096;&#25805;&#20316;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality. (arXiv:2210.13702v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#25935;&#25463;&#25163;&#37096;&#25805;&#20316;&#30340;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#21508;&#31181;&#26465;&#20214;&#30340;&#31574;&#30053;&#21644;&#40065;&#26834;&#20301;&#23039;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#25163;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#28789;&#24039;&#25805;&#20316;&#65292;&#24182;&#35777;&#23454;&#20102;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#31227;&#23545;&#25935;&#25463;&#25805;&#20316;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#21253;&#25324;&#22810;&#25351;&#25805;&#20316;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36716;&#31227;&#21487;&#33021;&#20250;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;a&#65289;&#19968;&#31181;&#31574;&#30053;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#25163;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#28789;&#24039;&#25805;&#20316;&#21644;b&#65289;&#19968;&#31181;&#36866;&#21512;&#25552;&#20379;&#21487;&#38752;&#23454;&#26102;&#20449;&#24687;&#30340;&#40065;&#26834;&#20301;&#23039;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#34987;&#25805;&#20316;&#29289;&#20307;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#32463;&#36807;&#22312;&#27169;&#25311;&#20013;&#36866;&#24212;&#21508;&#31181;&#26465;&#20214;&#30340;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#22312;&#37325;&#26032;&#23450;&#21521;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#25991;&#29486;&#20013;&#26368;&#22909;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#36890;&#36807;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#25552;&#20379;&#29305;&#26435;&#29366;&#24577;&#20449;&#24687;&#30340;&#31574;&#30053;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#23454;&#20102;&#25935;&#25463;&#25805;&#20316;&#22312;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#31227;&#20013;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in 
&lt;/p&gt;</description></item><item><title>&#30446;&#26631;&#32593;&#32476;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#20248;&#21270;&#30340;&#24433;&#21709;&#36824;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#30446;&#26631;&#32593;&#32476;&#26469;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12282</link><description>&lt;p&gt;
&#32553;&#23567;&#30446;&#26631;&#32593;&#32476;&#19982;&#21151;&#33021;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2210.12282v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12282
&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#32593;&#32476;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#20248;&#21270;&#30340;&#24433;&#21709;&#36824;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#30446;&#26631;&#32593;&#32476;&#26469;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#27861;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#24341;&#23548;&#27861;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#24120;&#24120;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#22240;&#20026;&#30446;&#26631;&#20540;&#21464;&#21270;&#24555;&#36895;&#12290;&#30446;&#26631;&#32593;&#32476;&#34987;&#29992;&#20110;&#36890;&#36807;&#20351;&#29992;&#28382;&#21518;&#21442;&#25968;&#38598;&#26469;&#20272;&#35745;&#30446;&#26631;&#20540;&#26469;&#31283;&#23450;&#35757;&#32451;&#12290;&#23613;&#31649;&#30446;&#26631;&#32593;&#32476;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#23545;&#20248;&#21270;&#30340;&#24433;&#21709;&#20173;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#22120;&#30340;&#20316;&#29992;&#12290;&#36825;&#31181;&#27491;&#21017;&#21270;&#22120;&#26377;&#19981;&#28789;&#27963;&#21644;&#38750;&#20984;&#20248;&#28857;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23427;&#26159;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#20984;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#36731;&#26494;&#35843;&#33410;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#29992;&#26356;&#20855;&#29702;&#35770;&#22522;&#30784;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#21462;&#20195;&#30446;&#26631;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping is behind much of the successes of Deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer. This regularizer has disadvantages such as being inflexible and non convex. To overcome these issues, we propose an explicit Functional Regularization that is a convex regularizer in function space and can easily be tuned. We analyze the convergence of our method theoretically and empirically demonstrate that replacing Target Networks with the more theoretically grounded Functional Regularization approach leads to better sample efficiency and performance improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24046;&#24322;&#20256;&#25773;&#39564;&#35777;&#22797;&#21512;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#28508;&#22312;&#19981;&#20934;&#30830;&#30340;&#27169;&#25311;&#20013;&#25512;&#23548;&#20986;&#30495;&#23454;&#31995;&#32479;&#30340;&#25925;&#38556;&#27010;&#29575;&#19978;&#30028;&#12290;&#23545;&#20110;&#24230;&#37327;&#22914;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322; (MMD) &#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#39564;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12061</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#24322;&#20256;&#25773;&#39564;&#35777;&#22797;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Validation of Composite Systems by Discrepancy Propagation. (arXiv:2210.12061v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24046;&#24322;&#20256;&#25773;&#39564;&#35777;&#22797;&#21512;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#28508;&#22312;&#19981;&#20934;&#30830;&#30340;&#27169;&#25311;&#20013;&#25512;&#23548;&#20986;&#30495;&#23454;&#31995;&#32479;&#30340;&#25925;&#38556;&#27010;&#29575;&#19978;&#30028;&#12290;&#23545;&#20110;&#24230;&#37327;&#22914;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322; (MMD) &#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#22522;&#20110;&#32473;&#23450;&#36136;&#37327;&#26631;&#20934;&#35780;&#20272;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#26159;&#19968;&#39033;&#24120;&#35265;&#20294;&#26114;&#36149;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#12290;&#36890;&#36807;&#27169;&#25311;&#26469;&#39564;&#35777;&#36825;&#31867;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26356;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#38656;&#35201;&#35780;&#20272;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#31471;&#21040;&#31471;&#27979;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#21644;&#23454;&#38469;&#20351;&#29992;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#21487;&#33021;&#23548;&#33268;&#20272;&#35745;&#36825;&#31867;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#21512;&#31995;&#32479;&#20256;&#25773;&#20998;&#24067;&#24046;&#24322;&#24230;&#37327;&#30340;&#30028;&#38480;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#20174;&#28508;&#22312;&#19981;&#20934;&#30830;&#30340;&#27169;&#25311;&#20013;&#25512;&#23548;&#20986;&#30495;&#23454;&#31995;&#32479;&#30340;&#25925;&#38556;&#27010;&#29575;&#19978;&#30028;&#12290;&#27599;&#20010;&#20256;&#25773;&#27493;&#39588;&#37117;&#28041;&#21450;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#20687;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322; (MMD) &#36825;&#26679;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#22522;&#20110;&#21322;&#23450;&#31243;&#24207;&#24320;&#21457;&#20102;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#20063;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#22797;&#21512;&#31995;&#32479;&#39564;&#35777;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the validity of a real-world system with respect to given quality criteria is a common yet costly task in industrial applications due to the vast number of required real-world tests. Validating such systems by means of simulation offers a promising and less expensive alternative, but requires an assessment of the simulation accuracy and therefore end-to-end measurements. Additionally, covariate shifts between simulations and actual usage can cause difficulties for estimating the reliability of such systems. In this work, we present a validation method that propagates bounds on distributional discrepancy measures through a composite system, thereby allowing us to derive an upper bound on the failure probability of the real system from potentially inaccurate simulations. Each propagation step entails an optimization problem, where -- for measures such as maximum mean discrepancy (MMD) -- we develop tight convex relaxations based on semidefinite programs. We demonstrate that our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26469;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#26032;&#39062;&#30340;Bregman&#25955;&#24230;&#35270;&#35282;&#65292;&#20998;&#26512;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#25214;&#21040;&#20102;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12511</link><description>&lt;p&gt;
&#38477;&#20302;&#38590;&#24230;&#21644;&#25552;&#39640;&#40065;&#26834;&#24615;&#65306;&#22522;&#20110; Bregman &#25955;&#24230;&#35270;&#35282;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26469;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#26032;&#39062;&#30340;Bregman&#25955;&#24230;&#35270;&#35282;&#65292;&#20998;&#26512;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#25214;&#21040;&#20102;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26469;&#25913;&#21892;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#20013;&#33719;&#24471;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110; Bregman &#25955;&#24230;&#30340;&#23545;&#25239;&#35757;&#32451;&#35270;&#35282;&#65292;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;AT&#21487;&#20197;&#30475;&#20316;&#26159;&#35757;&#32451;&#25968;&#25454;&#28857;&#22312;&#36127;&#29109;&#26354;&#32447;&#19978;&#30340;&#28369;&#21160;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20010;&#35270;&#35282;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;AT&#26041;&#27861;&#65292;&#21363;PGD-AT&#21644;TRADES&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#21457;&#29616;TRADES&#30340;&#20248;&#21270;&#36807;&#31243;&#27604;PGD-AT&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;TRADES&#21487;&#20197;&#20998;&#31163;PGD-AT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;TRADES&#20013;&#29109;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#39640;&#29109;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;&#21463;&#21040;&#20197;&#19978;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;FAIT&#21644;MER&#65292;&#23427;&#20204;&#19981;&#20165;&#21487;&#20197;&#22312;10&#27493;PGD&#23545;&#25163;&#19979;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#65292;&#36824;&#33021;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;10&#27493;PGD&#23545;&#25163;&#19979;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#33021;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate on improving the adversarial robustness obtained in adversarial training (AT) via reducing the difficulty of optimization. To better study this problem, we build a novel Bregman divergence perspective for AT, in which AT can be viewed as the sliding process of the training data points on the negative entropy curve. Based on this perspective, we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and TRADES, and we find that the optimization process of TRADES is easier than PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function of entropy in TRADES, and we find that models with high entropy can be better robustness learners. Inspired by the above findings, we propose two methods, i.e., FAIT and MER, which can both not only reduce the difficulty of optimization under the 10-step PGD adversaries, but also provide better robustness. Our work suggests that reducing the difficulty of optimization under the 10-step PGD a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#39044;&#27979;&#33391;&#22909;&#30340;&#21453;&#24212;&#22352;&#26631;&#20197;&#21450;MD&#36712;&#36857;&#30340;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#27491;&#21017;&#21270;&#32422;&#26463;&#23545;&#20110;&#36873;&#25321;&#37325;&#35201;&#21453;&#24212;&#22352;&#26631;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2208.10962</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#39044;&#27979;&#33391;&#22909;&#21453;&#24212;&#22352;&#26631;&#21644;MD&#36712;&#36857;&#30340;&#26410;&#26469;&#28436;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach. (arXiv:2208.10962v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#39044;&#27979;&#33391;&#22909;&#30340;&#21453;&#24212;&#22352;&#26631;&#20197;&#21450;MD&#36712;&#36857;&#30340;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#27491;&#21017;&#21270;&#32422;&#26463;&#23545;&#20110;&#36873;&#25321;&#37325;&#35201;&#21453;&#24212;&#22352;&#26631;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#23450;&#21453;&#24212;&#22352;&#26631;(RCs)&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;RCs&#22312;&#30830;&#23450;&#21270;&#23398;&#21453;&#24212;&#30340;&#36827;&#23637;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36873;&#25321;&#21453;&#24212;&#22352;&#26631;&#36890;&#24120;&#22522;&#20110;&#21551;&#21457;&#24335;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#30340;&#26631;&#20934;&#20043;&#19968;&#26159;&#35813;&#22352;&#26631;&#24212;&#28165;&#26224;&#22320;&#25429;&#33719;&#21453;&#24212;&#29289;&#21644;&#29983;&#25104;&#29289;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#22352;&#26631;&#24212;&#35813;&#26159;&#26368;&#24930;&#30340;&#65292;&#20351;&#24471;&#25152;&#26377;&#20854;&#20182;&#33258;&#30001;&#24230;&#21487;&#20197;&#27839;&#30528;&#21453;&#24212;&#22352;&#26631;&#36731;&#26494;&#36798;&#21040;&#24179;&#34913;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#21363;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#26469;&#21457;&#29616;&#19968;&#32452;&#20851;&#38190;&#30340;&#21453;&#24212;&#22352;&#26631;&#12290;&#38500;&#20102;&#21457;&#29616;&#21453;&#24212;&#22352;&#26631;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39044;&#27979;&#20998;&#23376;&#21160;&#21147;&#23398;(MD)&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21253;&#25324;&#31232;&#30095;&#32422;&#26463;&#27491;&#21017;&#21270;&#26377;&#21161;&#20110;&#36873;&#25321;&#19968;&#20010;&#23567;&#20294;&#37325;&#35201;&#30340;&#19968;&#32452;&#21453;&#24212;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying reaction coordinates(RCs) is an active area of research, given the crucial role RCs play in determining the progress of a chemical reaction. The choice of the reaction coordinate is often based on heuristic knowledge. However, an essential criterion for the choice is that the coordinate should capture both the reactant and product states unequivocally. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. We used a regularised sparse autoencoder, an energy-based model, to discover a crucial set of reaction coordinates. Along with discovering reaction coordinates, our model also predicts the evolution of a molecular dynamics(MD) trajectory. We showcased that including sparsity enforcing regularisation helps in choosing a small but important set of r
&lt;/p&gt;</description></item><item><title>SYNTA&#26159;&#19968;&#31181;&#20351;&#29992;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32908;&#32905;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#36827;&#34892;&#24378;&#22823;&#19988;&#19987;&#19994;&#27700;&#24179;&#30340;&#20998;&#21106;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#21160;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2207.14650</link><description>&lt;p&gt;
&#20351;&#29992;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#26512;&#30340;SYNTA&#26032;&#26041;&#27861; &#22312;&#32908;&#32905;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14650
&lt;/p&gt;
&lt;p&gt;
SYNTA&#26159;&#19968;&#31181;&#20351;&#29992;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32908;&#32905;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#36827;&#34892;&#24378;&#22823;&#19988;&#19987;&#19994;&#27700;&#24179;&#30340;&#20998;&#21106;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;(DL)&#26041;&#27861;&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35201;&#21457;&#25381;&#36825;&#20123;&#26041;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#22823;&#37327;&#20195;&#34920;&#24615;&#30340;&#23454;&#39564;&#33719;&#21462;&#22270;&#20687;&#65292;&#21253;&#21547;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#23545;&#35937;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SYNTA&#65288;&#21512;&#25104;&#25968;&#25454;&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#12289;&#36924;&#30495;&#30340;&#12289;&#39640;&#24230;&#22797;&#26434;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;DL&#31995;&#32479;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32452;&#32455;&#23398;&#25130;&#38754;&#20013;&#32908;&#32420;&#32500;&#21644;&#32467;&#32532;&#32452;&#32455;&#20998;&#26512;&#30340;&#32972;&#26223;&#19979;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21487;&#20197;&#36827;&#34892;&#31283;&#20581;&#21644;&#19987;&#23478;&#32423;&#30340;&#20998;&#21106;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#27880;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#23436;&#20840;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25511;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI), machine learning, and deep learning (DL) methods are becoming increasingly important in the field of biomedical image analysis. However, to exploit the full potential of such methods, a representative number of experimentally acquired images containing a significant number of manually annotated objects is needed as training data. Here we introduce SYNTA (synthetic data) as a novel approach for the generation of synthetic, photo-realistic, and highly complex biomedical images as training data for DL systems. We show the versatility of our approach in the context of muscle fiber and connective tissue analysis in histological sections. We demonstrate that it is possible to perform robust and expert-level segmentation tasks on previously unseen real-world data, without the need for manual annotations using synthetic training data alone. Being a fully parametric technique, our approach poses an interpretable and controllable alternative to Generative Adversaria
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#26381;&#21153;&#29616;&#24050;&#24320;&#22987;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#20197;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#24182;&#38477;&#20302;&#20113;&#24320;&#38144;&#65292;&#32780;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35774;&#22791;&#19978;&#35757;&#32451;&#26694;&#26550;NNTrainer&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#24182;&#23454;&#29616;&#26234;&#33021;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.04688</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#39046;&#22495;&#65306;&#35774;&#22791;&#19978;&#30340;AI&#35757;&#32451;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
A New Frontier of AI: On-Device AI Training and Personalization. (arXiv:2206.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04688
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#26381;&#21153;&#29616;&#24050;&#24320;&#22987;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#20197;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#24182;&#38477;&#20302;&#20113;&#24320;&#38144;&#65292;&#32780;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35774;&#22791;&#19978;&#35757;&#32451;&#26694;&#26550;NNTrainer&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#24182;&#23454;&#29616;&#26234;&#33021;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28040;&#36153;&#30005;&#23376;&#35774;&#22791;&#24320;&#22987;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#26381;&#21153;&#65292;&#32780;&#19981;&#26159;&#22312;&#20113;&#26381;&#21153;&#22120;&#19978;&#65292;&#20197;&#20445;&#25345;&#20010;&#20154;&#25968;&#25454;&#22312;&#35774;&#22791;&#19978;&#24182;&#38477;&#20302;&#32593;&#32476;&#21644;&#20113;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#36235;&#21183;&#35270;&#20026;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#25968;&#25454;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#32780;&#26080;&#38656;&#23558;&#25968;&#25454;&#26292;&#38706;&#22312;&#35774;&#22791;&#20043;&#22806;&#26469;&#20010;&#24615;&#21270;&#26234;&#33021;&#26381;&#21153;&#30340;&#26426;&#20250;&#65306;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#36164;&#28304;&#26377;&#38480;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35774;&#22791;&#19978;&#35757;&#32451;&#26694;&#26550;NNTrainer&#65292;&#23427;&#25552;&#20379;&#39640;&#24230;&#20869;&#23384;&#25928;&#29575;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#32454;&#31890;&#24230;&#25191;&#34892;&#39034;&#24207;&#20998;&#26512;&#36827;&#34892;&#20027;&#21160;&#20132;&#25442;&#12290;&#27492;&#22806;&#65292;&#20854;&#20248;&#21270;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#31639;&#27861;&#26159;&#36879;&#26126;&#30340;&#65307;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#30740;&#31350;&#21487;&#20197;&#22312;NNTrainer&#20043;&#19978;&#23454;&#29616;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;NNTrainer&#21487;&#20197;&#23558;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#21040;1/20&#65288;&#33410;&#30465;95%&#65281;&#65289;&#65292;&#24182;&#26377;&#25928;&#22320;&#20010;&#24615;&#21270;&#26234;&#33021;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern consumer electronic devices have started executing deep learning-based intelligence services on devices, not cloud servers, to keep personal data on devices and to reduce network and cloud costs. We find such a trend as the opportunity to personalize intelligence services by updating neural networks with user data without exposing the data out of devices: on-device training. However, the limited resources of devices incurs significant difficulties. We propose a light-weight on-device training framework, NNTrainer, which provides highly memory-efficient neural network training techniques and proactive swapping based on fine-grained execution order analysis for neural networks. Moreover, its optimizations do not sacrifice accuracy and are transparent to training algorithms; thus, prior algorithmic studies may be implemented on top of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption down to 1/20 (saving 95%!) and effectively personalizes intelligence ser
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#38750;&#20984;&#38382;&#39064;&#20013;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#12289;&#37096;&#20998;&#21442;&#19982;&#21644;&#21387;&#32553;&#36890;&#20449;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#26368;&#20339;&#39044;&#35328;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#26080;&#38656;&#25152;&#26377;&#33410;&#28857;&#21442;&#19982;&#21644;&#26377;&#30028;&#26799;&#24230;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2205.15580</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21442;&#19982;&#29615;&#22659;&#20013;&#35299;&#20915;&#20998;&#24067;&#24335;&#38750;&#20984;&#38382;&#39064;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting. (arXiv:2205.15580v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#38750;&#20984;&#38382;&#39064;&#20013;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#12289;&#37096;&#20998;&#21442;&#19982;&#21644;&#21387;&#32553;&#36890;&#20449;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#26368;&#20339;&#39044;&#35328;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#26080;&#38656;&#25152;&#26377;&#33410;&#28857;&#21442;&#19982;&#21644;&#26377;&#30028;&#26799;&#24230;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#32553;&#20943;&#12289;&#37096;&#20998;&#21442;&#19982;&#21644;&#21387;&#32553;&#36890;&#20449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#22312;&#37096;&#20998;&#21442;&#19982;&#29615;&#22659;&#20013;&#20855;&#26377;&#26368;&#20339;&#30340;&#39044;&#35328;&#22797;&#26434;&#24230;&#21644;&#26368;&#20808;&#36827;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#26080;&#35770;&#36890;&#20449;&#21387;&#32553;&#29305;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#32467;&#21512;&#20102;&#26041;&#24046;&#32553;&#20943;&#21644;&#37096;&#20998;&#21442;&#19982;&#65306;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#39044;&#35328;&#22797;&#26434;&#24230;&#65292;&#26080;&#38656;&#25152;&#26377;&#33410;&#28857;&#30340;&#21442;&#19982;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26377;&#30028;&#26799;&#24230;&#65288;&#24046;&#24322;&#24615;&#65289;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method that includes three key components of distributed optimization and federated learning: variance reduction of stochastic gradients, partial participation, and compressed communication. We prove that the new method has optimal oracle complexity and state-of-the-art communication complexity in the partial participation setting. Regardless of the communication compression feature, our method successfully combines variance reduction and partial participation: we get the optimal oracle complexity, never need the participation of all nodes, and do not require the bounded gradients (dissimilarity) assumption.
&lt;/p&gt;</description></item><item><title>DIRA&#26159;&#19968;&#20010;&#29992;&#20110;DNN&#20998;&#31867;&#22120;&#30340;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23454;&#29616;&#37325;&#26032;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.00147</link><description>&lt;p&gt;
DIRA: &#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#39046;&#22495;&#22686;&#37327;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00147
&lt;/p&gt;
&lt;p&gt;
DIRA&#26159;&#19968;&#20010;&#29992;&#20110;DNN&#20998;&#31867;&#22120;&#30340;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23454;&#29616;&#37325;&#26032;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#32463;&#24120;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20998;&#31867;&#22120;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#22797;&#26434;&#12289;&#39640;&#32500;&#12289;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#30001;&#20110;&#36825;&#20123;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#24403;DNN&#20998;&#31867;&#22120;&#38754;&#23545;&#24320;&#21457;&#36807;&#31243;&#20013;&#26410;&#35782;&#21035;&#30340;&#39046;&#22495;&#26102;&#65292;&#21487;&#33021;&#20250;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#36755;&#20986;&#38169;&#35823;&#20998;&#31867;&#12290;&#38543;&#30528;AS&#30340;&#25968;&#37327;&#22686;&#21152;&#65292;&#23558;&#31995;&#32479;&#20174;&#36816;&#34892;&#20013;&#31227;&#38500;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#22686;&#21152;AS&#30340;&#21487;&#38752;&#24615;&#24182;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;DNN&#20998;&#31867;&#22120;&#38656;&#35201;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#36866;&#24212;&#19981;&#21516;&#30340;&#25805;&#20316;&#39046;&#22495;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#65288;&#20363;&#22914;100&#20010;&#26679;&#26412;&#65289;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22312;&#23569;&#37327;&#26679;&#26412;&#19978;&#37325;&#26032;&#35757;&#32451;DNN&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22686;&#37327;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#65288;DIRA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#23454;&#29616;DNN&#20998;&#31867;&#22120;&#30340;&#25805;&#20316;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to allow them to operate in complex, high dimensional, non-linear, and dynamically changing environments. Due to the complexity of these environments, DNN classifiers may output misclassifications during operation when they face domains not identified during development. Removing a system from operation for retraining becomes impractical as the number of such AS increase. To increase AS reliability and overcome this limitation, DNN classifiers need to have the ability to adapt during operation when faced with different operational domains using a few samples (e.g. 100 samples). However, retraining DNNs on a few samples is known to cause catastrophic forgetting. In this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), a framework for operational domain adaption of DNN classifiers using regularisation techniques to overcome catastrophic forgetting and achieve adaptation when retraining using few
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#36861;&#36394;&#25968;&#25454;&#20013;&#25552;&#21462;"&#26368;&#20339;&#23454;&#36341;"&#24182;&#21521;&#20154;&#31867;&#20256;&#36798;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20154;&#31867;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#34394;&#25311;&#21416;&#25151;&#31649;&#29702;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.08454</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#20154;&#31867;&#30340;&#39034;&#24207;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Improving Human Sequential Decision-Making with Reinforcement Learning. (arXiv:2108.08454v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#36861;&#36394;&#25968;&#25454;&#20013;&#25552;&#21462;"&#26368;&#20339;&#23454;&#36341;"&#24182;&#21521;&#20154;&#31867;&#20256;&#36798;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20154;&#31867;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#34394;&#25311;&#21416;&#25151;&#31649;&#29702;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#32773;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#23398;&#20064;&#22914;&#20309;&#20570;&#20986;&#27491;&#30830;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#19968;&#20010;&#32473;&#23450;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#20363;&#22914;&#65292;&#20915;&#31574;&#32467;&#26524;&#36890;&#24120;&#26159;&#38271;&#26399;&#30340;&#65292;&#24182;&#19988;&#19982;&#21407;&#22987;&#20915;&#31574;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#23398;&#20064;&#33391;&#22909;&#30340;&#20915;&#31574;&#31574;&#30053;&#24456;&#22256;&#38590;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21487;&#20197;&#31616;&#27905;&#26126;&#20102;&#22320;&#34920;&#36798;&#12290;&#38024;&#23545;&#39034;&#24207;&#20915;&#31574;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#36861;&#36394;&#25968;&#25454;&#20013;&#25552;&#21462;&#8220;&#26368;&#20339;&#23454;&#36341;&#8221;&#24182;&#20197;&#21487;&#35299;&#37322;&#30340;&#8220;&#25552;&#31034;&#8221;&#30340;&#24418;&#24335;&#20256;&#36798;&#20854;&#35265;&#35299;&#32473;&#20154;&#31867;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#30340;&#25552;&#31034;&#65292;&#20197;&#22635;&#34917;&#20154;&#31867;&#24037;&#20316;&#32773;&#37319;&#21462;&#30340;&#34892;&#21160;&#19982;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#32771;&#34385;&#21738;&#20123;&#34892;&#21160;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21442;&#19982;&#32773;&#31649;&#29702;&#19968;&#20010;&#34394;&#25311;&#21416;&#25151;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Workers spend a significant amount of time learning how to make good decisions. Evaluating the efficacy of a given decision, however, can be complicated -- e.g., decision outcomes are often long-term and relate to the original decision in complex ways. Surprisingly, even though learning good decision-making strategies is difficult, they can often be expressed in simple and concise forms. Focusing on sequential decision-making, we design a novel machine learning algorithm that is capable of extracting "best practices" from trace data and conveying its insights to humans in the form of interpretable "tips". Our algorithm selects the tip that best bridges the gap between the actions taken by human workers and those taken by the optimal policy in a way that accounts for which actions are consequential for achieving higher performance. We evaluate our approach through a series of randomized controlled experiments where participants manage a virtual kitchen. Our experiments show that the tip
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#21644;&#25351;&#21335;&#12290;&#20854;&#20013;&#21253;&#25324;&#22312;&#27169;&#22411;&#26500;&#24314;&#20043;&#21069;&#30340;&#20934;&#22791;&#24037;&#20316;&#12289;&#21487;&#38752;&#22320;&#26500;&#24314;&#27169;&#22411;&#12289;&#31283;&#20581;&#22320;&#35780;&#20272;&#27169;&#22411;&#12289;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#20197;&#21450;&#25253;&#21578;&#32467;&#26524;&#31561;&#20116;&#20010;&#20851;&#38190;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2108.02497</link><description>&lt;p&gt;
&#22914;&#20309;&#36991;&#20813;&#26426;&#22120;&#23398;&#20064;&#38519;&#38449;&#65306;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.02497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#21644;&#25351;&#21335;&#12290;&#20854;&#20013;&#21253;&#25324;&#22312;&#27169;&#22411;&#26500;&#24314;&#20043;&#21069;&#30340;&#20934;&#22791;&#24037;&#20316;&#12289;&#21487;&#38752;&#22320;&#26500;&#24314;&#27169;&#22411;&#12289;&#31283;&#20581;&#22320;&#35780;&#20272;&#27169;&#22411;&#12289;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#20197;&#21450;&#25253;&#21578;&#32467;&#26524;&#31561;&#20116;&#20010;&#20851;&#38190;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#24120;&#35265;&#30340;&#19968;&#20123;&#38169;&#35823;&#65292;&#20197;&#21450;&#22914;&#20309;&#36991;&#20813;&#23427;&#20204;&#12290;&#34429;&#28982;&#23545;&#20110;&#20855;&#26377;&#22522;&#26412;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29702;&#35299;&#30340;&#20219;&#20309;&#20154;&#37117;&#24212;&#35813;&#26131;&#20110;&#29702;&#35299;&#65292;&#20294;&#23427;&#26368;&#21021;&#26159;&#20026;&#30740;&#31350;&#29983;&#25776;&#20889;&#30340;&#65292;&#24182;&#20851;&#27880;&#23398;&#26415;&#30740;&#31350;&#20013;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36827;&#34892;&#20005;&#26684;&#27604;&#36739;&#21644;&#24471;&#20986;&#26377;&#25928;&#32467;&#35770;&#30340;&#38656;&#27714;&#12290;&#23427;&#28085;&#30422;&#20102;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30340;&#20116;&#20010;&#38454;&#27573;&#65306;&#22914;&#20309;&#22312;&#27169;&#22411;&#26500;&#24314;&#20043;&#21069;&#36827;&#34892;&#20934;&#22791;&#65292;&#22914;&#20309;&#21487;&#38752;&#22320;&#26500;&#24314;&#27169;&#22411;&#65292;&#22914;&#20309;&#31283;&#20581;&#22320;&#35780;&#20272;&#27169;&#22411;&#65292;&#22914;&#20309;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#65292;&#20197;&#21450;&#22914;&#20309;&#25253;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document outlines some of the common mistakes that occur when using machine learning, and what can be done to avoid them. Whilst it should be accessible to anyone with a basic understanding of machine learning techniques, it was originally written for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.
&lt;/p&gt;</description></item></channel></rss>