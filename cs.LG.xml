<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01261</link><description>&lt;p&gt;
TEDDY: &#22522;&#20110;&#24230;&#37327;&#21028;&#21035;&#31574;&#30053;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEDDY: Trimming Edges with Degree-based Discrimination strategY
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01261
&lt;/p&gt;
&lt;p&gt;
TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Chen&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#23547;&#25214;&#22270;&#25277;&#22870;&#31080;&#65288;GLT&#65289;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;GNN&#31038;&#21306;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#26356;&#31232;&#30095;&#30340;GLT&#12290;&#21516;&#26102;&#65292;&#22270;&#32467;&#26500;&#20316;&#20026;GNN&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#30340;&#38416;&#26126;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20851;&#20110;GLT&#30340;&#30740;&#31350;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#32467;&#26500;&#20013;&#30340;&#20869;&#22312;&#36335;&#24452;&#65292;&#24182;&#20197;&#36845;&#20195;&#26041;&#24335;&#35782;&#21035;&#31080;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;TEDDY&#65292;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#24182;&#25972;&#21512;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#19968;&#27425;&#24615;&#36793;&#32536;&#31232;&#30095;&#21270;&#26694;&#26550;&#12290;&#22312;&#36827;&#34892;&#36793;&#32536;&#31232;&#30095;&#21270;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#25511;&#24615;&#30340;&#30450;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#36873;&#25321;&#31227;&#38500;&#25110;&#20445;&#30041;&#29305;&#23450;&#31867;&#22411;&#30340;&#38477;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312; minimal computational cost&#30340;&#24773;&#20917;&#19979;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#37096;&#20998;&#25110;&#23436;&#20840;&#24674;&#22797;</title><link>https://arxiv.org/abs/2403.10520</link><description>&lt;p&gt;
&#24378;&#22823;&#19988;&#21487;&#25511;&#30340;&#30450;&#22270;&#20687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Strong and Controllable Blind Image Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#25511;&#24615;&#30340;&#30450;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#36873;&#25321;&#31227;&#38500;&#25110;&#20445;&#30041;&#29305;&#23450;&#31867;&#22411;&#30340;&#38477;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312; minimal computational cost&#30340;&#24773;&#20917;&#19979;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#37096;&#20998;&#25110;&#23436;&#20840;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#22270;&#20687;&#20998;&#35299;&#26088;&#22312;&#20998;&#35299;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#24120;&#29992;&#20110;&#24674;&#22797;&#19968;&#20010;&#22810;&#37325;&#38477;&#20302;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;&#34429;&#28982;&#23436;&#20840;&#24674;&#22797;&#24178;&#20928;&#30340;&#22270;&#20687;&#24456;&#21560;&#24341;&#20154;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#20445;&#30041;&#26576;&#20123;&#38477;&#35299;&#65292;&#20363;&#22914;&#27700;&#21360;&#65292;&#20197;&#36827;&#34892;&#29256;&#26435;&#20445;&#25252;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#35201;&#65292;&#25105;&#20204;&#22312;&#30450;&#22270;&#20687;&#20998;&#35299;&#36807;&#31243;&#20013;&#28155;&#21152;&#20102;&#21487;&#25511;&#24615;&#65292;&#20801;&#35768;&#29992;&#25143;&#36755;&#20837;&#35201;&#31227;&#38500;&#25110;&#20445;&#30041;&#30340;&#38477;&#35299;&#31867;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#21487;&#25511;&#30450;&#22270;&#20687;&#20998;&#35299;&#32593;&#32476;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#36755;&#20837;&#30340;&#29305;&#24449;&#22270;&#20998;&#35299;&#65292;&#28982;&#21518;&#26681;&#25454;&#29992;&#25143;&#30340;&#25351;&#20196;&#37325;&#26032;&#32452;&#21512;&#23427;&#20204;&#12290;&#26377;&#21033;&#30340;&#26159;&#65292;&#36825;&#31181;&#21151;&#33021;&#20197;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#65306;&#20998;&#35299;&#21644;&#37325;&#32452;&#37117;&#26159;&#26080;&#21442;&#25968;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#30450;&#22270;&#20687;&#20998;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#36755;&#20986;&#37096;&#20998;&#25110;&#23436;&#20840;&#24674;&#22797;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10520v1 Announce Type: cross  Abstract: Blind image decomposition aims to decompose all components present in an image, typically used to restore a multi-degraded input image. While fully recovering the clean image is appealing, in some scenarios, users might want to retain certain degradations, such as watermarks, for copyright protection. To address this need, we add controllability to the blind image decomposition process, allowing users to enter which types of degradation to remove or retain. We design an architecture named controllable blind image decomposition network. Inserted in the middle of U-Net structure, our method first decomposes the input feature maps and then recombines them according to user instructions. Advantageously, this functionality is implemented at minimal computational cost: decomposition and recombination are all parameter-free. Experimentally, our system excels in blind image decomposition tasks and can outputs partially or fully restored images
&lt;/p&gt;</description></item><item><title>FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10516</link><description>&lt;p&gt;
FeatUp: &#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29305;&#24449;&#20219;&#24847;&#20998;&#36776;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FeatUp: A Model-Agnostic Framework for Features at Any Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10516
&lt;/p&gt;
&lt;p&gt;
FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#22522;&#30707;&#65292;&#25429;&#25417;&#22270;&#20687;&#35821;&#20041;&#24182;&#20351;&#31038;&#21306;&#33021;&#22815;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#38646;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#32570;&#20047;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#20687;&#20998;&#21106;&#21644;&#28145;&#24230;&#39044;&#27979;&#36825;&#26679;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#36807;&#20110;&#32858;&#21512;&#22823;&#33539;&#22260;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#65292;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#28145;&#24230;&#29305;&#24449;&#20013;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#24341;&#23548;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#36866;&#24212;&#21333;&#20010;&#22270;&#20687;&#24182;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#26500;&#29305;&#24449;&#30340;&#38544;&#24335;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#19982; NeRF &#31867;&#20284;&#30340;&#28145;&#24230;&#31867;&#27604;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#20445;&#30041;&#20854;&#21407;&#22987;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#26367;&#25442;&#29616;&#26377;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.10506</link><description>&lt;p&gt;
HumanoidBench&#65306;&#29992;&#20110;&#20840;&#36523;&#36816;&#21160;&#21644;&#25805;&#20316;&#30340;&#20223;&#30495;&#20154;&#22411;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#21327;&#21161;&#20154;&#31867;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#31867;&#20154;&#24418;&#24577;&#12290;&#28982;&#32780;&#65292;&#20154;&#22411;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#19988;&#26131;&#25439;&#30340;&#30828;&#20214;&#35774;&#32622;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#21152;&#36895;&#20154;&#22411;&#26426;&#22120;&#20154;&#31639;&#27861;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;HumanoidBench&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#37197;&#22791;&#28789;&#24039;&#25163;&#37096;&#21644;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20840;&#36523;&#25805;&#20316;&#21644;&#36816;&#21160;&#20219;&#21153;&#30340;&#20154;&#22411;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#30340;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#22312;&#34892;&#36208;&#25110;&#21040;&#36798;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#20511;&#21161;HumanoidBench&#65292;&#25105;&#20204;&#20026;&#26426;&#22120;&#20154;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#35782;&#21035;&#35299;&#20915;&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20419;&#36827;&#31639;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10506v1 Announce Type: cross  Abstract: Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitatin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;CLIP&#36827;&#34892;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#28085;&#30422;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#12289;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#22810;&#20010;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10499</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;CLIP&#36827;&#34892;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#28085;&#30422;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#12289;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#22810;&#20010;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20851;&#20110;&#22270;&#20687;&#30340;&#21407;&#22987;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#22270;&#20687;&#34920;&#31034;&#65292;&#20351;&#24471;&#38646;&#26679;&#26412;&#35270;&#35273;&#20256;&#36755;&#33267;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#22312;&#20114;&#32852;&#32593;&#19978;&#37319;&#38598;&#30340;&#25968;&#30334;&#19975;&#26679;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#65292;&#36890;&#24120;&#22312;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;&#38500;&#20102;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#40723;&#33310;&#20154;&#24515;&#20043;&#22806;&#65292;&#25253;&#36947;&#31216;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#19979;&#19982;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#26469;&#32553;&#23567;&#40065;&#26834;&#24615;&#24046;&#36317;&#12290;&#30001;&#20110;&#40065;&#26834;&#24615;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28085;&#30422;7&#31181;&#33258;&#28982;&#12289;3&#31181;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;11&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;CLIP&#20316;&#20026;&#35797;&#28857;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CLIP&#23548;&#33268;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10499v1 Announce Type: cross  Abstract: Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a signif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#23631;&#38556;&#35777;&#26126;&#27010;&#24565;&#65292;&#24182;&#30452;&#25509;&#20174;&#31995;&#32479;&#36712;&#36857;&#38598;&#20013;&#23398;&#20064;&#35777;&#26126;&#65292;&#24320;&#21457;&#20102;&#21487;&#25193;&#23637;&#30340;&#24418;&#24335;&#39564;&#35777;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10497</link><description>&lt;p&gt;
&#20351;&#29992;&#23631;&#38556;&#35777;&#26126;&#21644;&#26465;&#20214;&#22343;&#20540;&#23884;&#20837;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#23631;&#38556;&#35777;&#26126;&#27010;&#24565;&#65292;&#24182;&#30452;&#25509;&#20174;&#31995;&#32479;&#36712;&#36857;&#38598;&#20013;&#23398;&#20064;&#35777;&#26126;&#65292;&#24320;&#21457;&#20102;&#21487;&#25193;&#23637;&#30340;&#24418;&#24335;&#39564;&#35777;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#31995;&#32479;&#30340;&#31639;&#27861;&#39564;&#35777;&#38656;&#35201;&#28385;&#36275;&#23433;&#20840;&#21644;&#20854;&#20182;&#26102;&#38388;&#35201;&#27714;&#65292;&#20294;&#25152;&#37319;&#29992;&#30340;&#24418;&#24335;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#20026;&#35774;&#35745;&#20855;&#26377;&#20005;&#26684;&#20445;&#35777;&#30340;&#31995;&#32479;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#20381;&#36182;&#20110;&#24213;&#23618;&#31995;&#32479;&#30340;&#31934;&#30830;&#27169;&#22411;&#12290;&#30001;&#20110;&#36825;&#19968;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#33021;&#24471;&#21040;&#28385;&#36275;&#65292;&#22240;&#27492;&#24517;&#39035;&#20174;&#27979;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#27169;&#22411;&#25110;&#23436;&#20840;&#32469;&#36807;&#27169;&#22411;&#12290;&#20026;&#20102;&#24320;&#21457;&#21487;&#25193;&#23637;&#30340;&#24418;&#24335;&#39564;&#35777;&#31639;&#27861;&#32780;&#19981;&#23558;&#38382;&#39064;&#36716;&#31227;&#21040;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#19978;&#65292;&#25105;&#20204;&#36816;&#29992;&#20102;&#23631;&#38556;&#35777;&#26126;&#30340;&#27010;&#24565;&#65292;&#35813;&#35777;&#26126;&#21487;&#20197;&#20445;&#35777;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#30452;&#25509;&#20174;&#32039;&#20945;&#31995;&#32479;&#36712;&#36857;&#38598;&#20013;&#23398;&#20064;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10497v1 Announce Type: cross  Abstract: Algorithmic verification of realistic systems to satisfy safety and other temporal requirements has suffered from poor scalability of the employed formal approaches. To design systems with rigorous guarantees, many approaches still rely on exact models of the underlying systems. Since this assumption can rarely be met in practice, models have to be inferred from measurement data or are bypassed completely. Whilst former usually requires the model structure to be known a-priori and immense amounts of data to be available, latter gives rise to a plethora of restrictive mathematical assumptions about the unknown dynamics. In a pursuit of developing scalable formal verification algorithms without shifting the problem to unrealistic assumptions, we employ the concept of barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a compact set of system trajectories. We use conditional mean embeddi
&lt;/p&gt;</description></item><item><title>&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#21516;&#26102;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10488</link><description>&lt;p&gt;
&#37326;&#22806;&#24773;&#24863;&#32500;&#24230;&#35782;&#21035;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer
&lt;/p&gt;
&lt;p&gt;
Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#21516;&#26102;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#21333;&#27169;&#24615;&#33021;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#35270;&#35273;&#21644;&#21548;&#35273;&#27169;&#24577;&#20043;&#38388;&#20197;&#21450;&#27169;&#24577;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#38190;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#21033;&#29992;&#35270;&#39057;&#20013;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65288;&#38754;&#37096;&#34920;&#24773;&#21644;&#35821;&#38899;&#27169;&#24335;&#65289;&#30340;&#20114;&#34917;&#24615;&#65292;&#30456;&#36739;&#20110;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#21333;&#29420;&#30340;&#20027;&#24178;&#32593;&#32476;&#26469;&#25429;&#33719;&#27599;&#31181;&#27169;&#24577;&#65288;&#38899;&#39057;&#21644;&#35270;&#35273;&#65289;&#20869;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#38598;&#25104;&#20102;&#21508;&#33258;&#27169;&#24577;&#30340;&#23884;&#20837;&#65292;&#20351;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#33719;&#27169;&#24577;&#38388;&#65288;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#65289;&#21644;&#27169;&#24577;&#20869;&#37096;&#65288;&#27599;&#31181;&#27169;&#24577;&#20869;&#37096;&#65289;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10488v1 Announce Type: cross  Abstract: Audiovisual emotion recognition (ER) in videos has immense potential over unimodal performance. It effectively leverages the inter- and intra-modal dependencies between visual and auditory modalities. This work proposes a novel audio-visual emotion recognition system utilizing a joint multimodal transformer architecture with key-based cross-attention. This framework aims to exploit the complementary nature of audio and visual cues (facial expressions and vocal patterns) in videos, leading to superior performance compared to solely relying on a single modality. The proposed model leverages separate backbones for capturing intra-modal temporal dependencies within each modality (audio and visual). Subsequently, a joint multimodal transformer architecture integrates the individual modality embeddings, enabling the model to effectively capture inter-modal (between audio and visual) and intra-modal (within each modality) relationships. Exten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10476</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#36817;&#20284;&#38646;&#31354;&#38388;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Approximate Nullspace Augmented Finetuning for Robust Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#39046;&#22495;&#20013;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#20013;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#20010;&#38382;&#39064;&#19978;&#65292;&#21363;&#35270;&#35273;&#21464;&#25442;&#22120;&#26159;&#21542;&#21487;&#20197;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#38646;&#31354;&#38388;&#23646;&#24615;&#30340;&#36755;&#20837;&#21464;&#21270;&#38887;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20174;&#35813;&#38646;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#25200;&#21160;&#28155;&#21152;&#21040;&#36755;&#20837;&#26102;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35768;&#22810;&#39044;&#35757;&#32451;&#30340;ViTs&#65292;&#23384;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38646;&#31354;&#38388;&#65292;&#36825;&#26159;&#30001;&#20110;&#23384;&#22312;&#20462;&#34917;&#23884;&#20837;&#23618;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#38646;&#31354;&#38388;&#26159;&#19982;&#32447;&#24615;&#20195;&#25968;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#20248;&#21270;&#31574;&#30053;&#20026;ViTs&#30340;&#38750;&#32447;&#24615;&#22359;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10476v1 Announce Type: cross  Abstract: Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment. In this work, we provide a finetuning approach to enhance the robustness of vision transformers inspired by the concept of nullspace from linear algebra. Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model's output when added to the input. Firstly, we show that for many pretrained ViTs, a non-trivial nullspace exists due to the presence of the patch embedding layer. Secondly, as nullspace is a concept associated with linear algebra, we demonstrate that it is possible to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing an optimisation strategy. Finally, we propose a fine-t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#26469;&#25345;&#32493;&#38598;&#25104;&#23545;&#25239;&#35757;&#32451;&#26679;&#26412;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#23454;&#38469;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#25968;&#25454;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#23545;&#25239;&#23041;&#32961;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10461</link><description>&lt;p&gt;
&#24341;&#20837;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#20197;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10461
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#26469;&#25345;&#32493;&#38598;&#25104;&#23545;&#25239;&#35757;&#32451;&#26679;&#26412;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#23454;&#38469;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#25968;&#25454;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#23545;&#25239;&#23041;&#32961;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26131;&#21463;&#38024;&#23545;ML&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#27450;&#39575;ML&#27169;&#22411;&#65292;&#20351;&#20854;&#20135;&#29983;&#38169;&#35823;&#39044;&#27979;&#12290; &#23545;&#25239;&#35757;&#32451;&#34987;&#21457;&#29616;&#33021;&#25552;&#39640;ML&#27169;&#22411;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#21644;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#27010;&#24565;&#28418;&#31227;&#21152;&#28145;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#32593;&#32476;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21160;&#24577;&#39046;&#22495;&#20013;&#65292;&#38656;&#35201;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#23450;&#26399;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#36830;&#32493;&#23545;&#25239;&#35757;&#32451;&#65288;ACAT&#65289;&#65292;&#20197;&#22312;&#25345;&#32493;&#30340;&#23398;&#20064;&#20250;&#35805;&#26399;&#38388;&#25345;&#32493;&#23558;&#23545;&#25239;&#35757;&#32451;&#26679;&#26412;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#23454;&#38469;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#23545;&#25239;&#23041;&#32961;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290; ACAT&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#21033;&#29992;&#23450;&#26399;&#37325;&#26032;&#35757;&#32451;&#26469;&#26377;&#25928;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#21516;&#26102;&#20943;&#36731;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10461v1 Announce Type: new  Abstract: Machine Learning (ML) is susceptible to adversarial attacks that aim to trick ML models, making them produce faulty predictions. Adversarial training was found to increase the robustness of ML models against these attacks. However, in network and cybersecurity, obtaining labeled training and adversarial training data is challenging and costly. Furthermore, concept drift deepens the challenge, particularly in dynamic domains like network and cybersecurity, and requires various models to conduct periodic retraining. This letter introduces Adaptive Continuous Adversarial Training (ACAT) to continuously integrate adversarial training samples into the model during ongoing learning sessions, using real-world detected adversarial data, to enhance model resilience against evolving adversarial threats. ACAT is an adaptive defense mechanism that utilizes periodic retraining to effectively counter adversarial attacks while mitigating catastrophic f
&lt;/p&gt;</description></item><item><title>&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24222;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#36825;&#23601;&#26159;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.10459</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Double Descent Phenomenon in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10459
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24222;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#36825;&#23601;&#26159;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;&#23481;&#37327;&#25511;&#21046;&#30456;&#32467;&#21512;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#32463;&#20856;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#25511;&#21046;&#27867;&#21270;&#24046;&#36317;&#24182;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#22240;&#20026;&#27169;&#22411;&#31867;&#23481;&#37327;&#21464;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#20013;&#65292;&#38750;&#24120;&#24222;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#34987;&#20248;&#21270;&#20197;&#23436;&#32654;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36229;&#36234;&#25554;&#20540;&#28857;&#21518;&#65292;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#20284;&#20046;&#23454;&#38469;&#19978;&#20250;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#21452;&#19979;&#38477;&#30340;&#27010;&#24565;&#21450;&#20854;&#26426;&#21046;&#12290;&#31532;&#19968;&#37096;&#20998;&#24314;&#31435;&#20102;&#32463;&#20856;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#24182;&#20171;&#32461;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#35266;&#23519;&#22810;&#20010;&#31034;&#20363;&#65292;&#31532;&#20108;&#37096;&#20998;&#20171;&#32461;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#22312;&#21452;&#19979;&#38477;&#20013;&#36873;&#25321;&#24179;&#28369;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#31532;&#19977;&#37096;&#20998;&#25506;&#35752;&#20102;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10459v1 Announce Type: new  Abstract: Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.   In this tutorial, we explain the concept of double descent and its mechanisms. The first section sets the classical statistical learning framework and introduces the double descent phenomenon. By looking at a number of examples, section 2 introduces inductive biases that appear to have a key role in double descent by selecting, among the multiple interpolating solutions, a smooth empirical risk minimizer. Finally, section 3 explores t
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;RAG&#31995;&#32479;&#22686;&#24378;LLMs&#65292;&#25552;&#39640;&#20854;&#23545;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10446</link><description>&lt;p&gt;
&#21033;&#29992;RAG&#22686;&#24378;LLM&#20107;&#23454;&#20934;&#30830;&#24615;&#20197;&#28040;&#38500;&#24187;&#35273;&#65306;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#29305;&#23450;&#39046;&#22495;&#26597;&#35810;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10446
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;RAG&#31995;&#32479;&#22686;&#24378;LLMs&#65292;&#25552;&#39640;&#20854;&#23545;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#35774;&#35745;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#19982;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#30456;&#20851;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;RAG&#31649;&#36947;&#19982;&#19978;&#28216;&#25968;&#25454;&#38598;&#22788;&#29702;&#21644;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#35299;&#20915;LLM&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#29992;&#28304;&#33258;CMU&#24191;&#27867;&#36164;&#28304;&#24182;&#29992;&#25945;&#24072;&#27169;&#22411;&#27880;&#37322;&#30340;&#31579;&#36873;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#31572;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#20351;&#29992;&#23567;&#35268;&#27169;&#21644;&#20542;&#26012;&#25968;&#25454;&#38598;&#24494;&#35843;LLMs&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;RAG&#31995;&#32479;&#22312;&#22686;&#24378;LLMs&#19982;&#22806;&#37096;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10446v1 Announce Type: new  Abstract: We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;</title><link>https://arxiv.org/abs/2403.10444</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#26368;&#20339;&#22359;&#32423;&#33609;&#31295;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Block-Level Draft Verification for Accelerating Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26080;&#25439;&#21152;&#36895;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290; &#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#36215;&#33609;&#19968;&#22359;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#28982;&#21518;&#30001;&#22823;&#22411;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#26631;&#35760;&#23558;&#34987;&#20445;&#30041;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#22411;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290; &#22312;&#20197;&#24448;&#30340;&#25152;&#26377;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#20013;&#65292;&#36215;&#33609;&#39564;&#35777;&#26159;&#29420;&#31435;&#22320;&#36880;&#20010;&#26631;&#35760;&#25191;&#34892;&#30340;&#12290; &#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#22681;&#38047;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36215;&#33609;&#26631;&#35760;&#12290; &#25105;&#20204;&#39318;&#20808;&#23558;&#36215;&#33609;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290; &#22359;&#32423;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#36215;&#33609;&#20013;&#39044;&#26399;&#33719;&#24471;&#26356;&#22810;&#25509;&#21463;&#30340;&#26631;&#35760;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#19968;&#25968;&#23398;&#30446;&#26631;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#21512;&#25104;&#25968;&#25454;&#24212;&#35813;&#20174;&#19982;&#35266;&#27979;&#25968;&#25454;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25552;&#21462;&#65292;&#24182;&#19988;&#25512;&#29702;&#20102;&#20219;&#20309;&#19968;&#32452;&#25351;&#26631;&#30340;&#23436;&#25972;&#24615;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#40723;&#21169;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#22522;&#32447;&#21644;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.10424</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#35780;&#20272;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Structured Evaluation of Synthetic Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#19968;&#25968;&#23398;&#30446;&#26631;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#21512;&#25104;&#25968;&#25454;&#24212;&#35813;&#20174;&#19982;&#35266;&#27979;&#25968;&#25454;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25552;&#21462;&#65292;&#24182;&#19988;&#25512;&#29702;&#20102;&#20219;&#20309;&#19968;&#32452;&#25351;&#26631;&#30340;&#23436;&#25972;&#24615;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#40723;&#21169;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#22522;&#32447;&#21644;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#20294;&#24448;&#24448;&#19981;&#23436;&#25972;&#65292;&#25968;&#25454;&#37327;&#36739;&#23567;&#65292;&#24182;&#19988;&#30001;&#20110;&#38544;&#31169;&#21407;&#22240;&#21463;&#38480;&#20110;&#35775;&#38382;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23384;&#22312;&#35768;&#22810;&#29992;&#20110;&#35780;&#20272;&#21512;&#25104;&#34920;&#26684;&#24335;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#36825;&#20123;&#25351;&#26631;&#30340;&#23458;&#35266;&#12289;&#36830;&#36143;&#30340;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#19968;&#25968;&#23398;&#30446;&#26631;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35748;&#20026;&#21512;&#25104;&#25968;&#25454;&#24212;&#35813;&#20174;&#19982;&#35266;&#27979;&#25968;&#25454;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25552;&#21462;&#12290;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#21508;&#31181;&#32467;&#26500;&#20998;&#35299;&#65292;&#35813;&#26694;&#26550;&#39318;&#27425;&#20801;&#35768;&#25105;&#20204;&#25512;&#29702;&#20219;&#20309;&#19968;&#32452;&#25351;&#26631;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#32479;&#19968;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#21253;&#25324;&#28304;&#33258;&#24544;&#23454;&#24615;&#32771;&#34385;&#12289;&#19979;&#28216;&#24212;&#29992;&#21644;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30340;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#28608;&#21169;&#20102;&#26080;&#27169;&#22411;&#22522;&#32447;&#21644;&#19968;&#31995;&#21015;&#26032;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#32467;&#26500;&#21270;&#20449;&#24687;&#21512;&#25104;&#22120;&#21644;&#21512;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10424v1 Announce Type: new  Abstract: Tabular data is common yet typically incomplete, small in volume, and access-restricted due to privacy concerns. Synthetic data generation offers potential solutions. Many metrics exist for evaluating the quality of synthetic tabular data; however, we lack an objective, coherent interpretation of the many metrics. To address this issue, we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data. Through various structural decomposition of the objective, this framework allows us to reason for the first time the completeness of any set of metrics, as well as unifies existing metrics, including those that stem from fidelity considerations, downstream application, and model-based approaches. Moreover, the framework motivates model-free baselines and a new spectrum of metrics. We evaluate structurally informed synthesizers and syn
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#38750;&#20984;&#20248;&#21270;&#20013;&#33021;&#22815;&#36991;&#20813;&#25910;&#25947;&#21040;&#38797;&#28857;&#65292;&#30830;&#20445;&#25910;&#25947;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.10423</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36991;&#20813;&#38797;&#28857;&#30340;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantization Avoids Saddle Points in Distributed Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10423
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#38750;&#20984;&#20248;&#21270;&#20013;&#33021;&#22815;&#36991;&#20813;&#25910;&#25947;&#21040;&#38797;&#28857;&#65292;&#30830;&#20445;&#25910;&#25947;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38750;&#20984;&#20248;&#21270;&#25903;&#25745;&#30528;&#20247;&#22810;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#20851;&#38190;&#21151;&#33021;&#65292;&#20174;&#30005;&#21147;&#31995;&#32479;&#12289;&#26234;&#33021;&#24314;&#31569;&#12289;&#21327;&#20316;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#32593;&#32476;&#21040;&#20256;&#24863;&#22120;&#32593;&#32476;&#31561;&#31561;&#12290;&#26368;&#36817;&#65292;&#23427;&#20063;&#20316;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#26469;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#24040;&#22823;&#22686;&#38271;&#12290;&#20998;&#24067;&#24335;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36991;&#20813;&#25910;&#25947;&#21040;&#38797;&#28857;&#65292;&#38797;&#28857;&#20250;&#26174;&#33879;&#38477;&#20302;&#20248;&#21270;&#31934;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#37327;&#21270;&#36807;&#31243;&#65292;&#23545;&#20110;&#25152;&#26377;&#25968;&#23383;&#36890;&#20449;&#37117;&#26159;&#24517;&#38656;&#30340;&#65292;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#23454;&#29616;&#36991;&#20813;&#38797;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#24320;&#38797;&#28857;&#65292;&#30830;&#20445;&#22312;&#20998;&#24067;&#24335;&#38750;&#20984;&#20248;&#21270;&#20013;&#25910;&#25947;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#36731;&#26494;&#35843;&#25972;&#37327;&#21270;&#31890;&#24230;&#26469;&#25511;&#21046;&#20301;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10423v1 Announce Type: cross  Abstract: Distributed nonconvex optimization underpins key functionalities of numerous distributed systems, ranging from power systems, smart buildings, cooperative robots, vehicle networks to sensor networks. Recently, it has also merged as a promising solution to handle the enormous growth in data and model sizes in deep learning. A fundamental problem in distributed nonconvex optimization is avoiding convergence to saddle points, which significantly degrade optimization accuracy. We discover that the process of quantization, which is necessary for all digital communications, can be exploited to enable saddle-point avoidance. More specifically, we propose a stochastic quantization scheme and prove that it can effectively escape saddle points and ensure convergence to a second-order stationary point in distributed nonconvex optimization. With an easily adjustable quantization granularity, the approach allows a user to control the number of bits
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;Huber&#27745;&#26579;&#27169;&#22411;&#19979;&#36827;&#34892;&#39640;&#26031;&#31232;&#30095;&#20272;&#35745;&#20219;&#21153;&#30340;&#40065;&#26834;&#20272;&#35745;&#22120;&#65292;&#20026;&#22343;&#20540;&#20272;&#35745;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#22238;&#24402;&#25552;&#20379;&#20102;&#20855;&#26377;&#26368;&#20248;&#35823;&#24046;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32500;&#28388;&#27874;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10416</link><description>&lt;p&gt;
&#22312;Huber&#27745;&#26579;&#27169;&#22411;&#19979;&#20855;&#26377;&#26368;&#20248;&#35823;&#24046;&#30340;&#39640;&#26031;&#31232;&#30095;&#20272;&#35745;&#30340;&#40065;&#26834;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10416
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;Huber&#27745;&#26579;&#27169;&#22411;&#19979;&#36827;&#34892;&#39640;&#26031;&#31232;&#30095;&#20272;&#35745;&#20219;&#21153;&#30340;&#40065;&#26834;&#20272;&#35745;&#22120;&#65292;&#20026;&#22343;&#20540;&#20272;&#35745;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#22238;&#24402;&#25552;&#20379;&#20102;&#20855;&#26377;&#26368;&#20248;&#35823;&#24046;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32500;&#28388;&#27874;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;Huber&#30340;&#27745;&#26579;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#39640;&#26031;&#31232;&#30095;&#20272;&#35745;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#22343;&#20540;&#20272;&#35745;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#22238;&#24402;&#12290;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#40065;&#26834;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#19988;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#12290;&#25152;&#26377;&#20808;&#21069;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#30340;&#39640;&#25928;&#31639;&#27861;&#37117;&#23548;&#33268;&#37327;&#21270;&#30340;&#27425;&#20248;&#35823;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#22312;$\mathbb{R}^d$&#19978;&#24102;&#26377;&#27745;&#26579;&#29575;$\epsilon&gt;0$&#30340;&#39640;&#26031;&#31283;&#20581;$k$-&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;$(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$&#65292;&#22312;&#26679;&#26412;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#22312;$\ell_2$-&#35823;&#24046;&#20026;$O(\epsilon)$&#30340;&#24773;&#20917;&#19979;&#36924;&#36817;&#30446;&#26631;&#22343;&#20540;&#12290;&#20808;&#21069;&#30340;&#39640;&#25928;&#31639;&#27861;&#22266;&#26377;&#22320;&#20135;&#29983;&#35823;&#24046;&#20026;$\Omega(\epsilon \sqrt{\log(1/\epsilon)})$&#12290;&#22312;&#25216;&#26415;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#39046;&#22495;&#20013;&#21487;&#33021;&#20855;&#26377;&#20854;&#20182;&#24212;&#29992;&#30340;&#26032;&#39062;&#30340;&#22810;&#32500;&#28388;&#27874;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10416v1 Announce Type: new  Abstract: We study Gaussian sparse estimation tasks in Huber's contamination model with a focus on mean estimation, PCA, and linear regression. For each of these tasks, we give the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors. All prior efficient algorithms for these tasks incur quantitatively suboptimal error. Concretely, for Gaussian robust $k$-sparse mean estimation on $\mathbb{R}^d$ with corruption rate $\epsilon&gt;0$, our algorithm has sample complexity $(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$, runs in sample polynomial time, and approximates the target mean within $\ell_2$-error $O(\epsilon)$. Previous efficient algorithms inherently incur error $\Omega(\epsilon \sqrt{\log(1/\epsilon)})$. At the technical level, we develop a novel multidimensional filtering method in the sparse regime that may find other applications.
&lt;/p&gt;</description></item><item><title>SocialGenPod&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;AI&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;&#37096;&#32626;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#30340;Solid&#35268;&#33539;&#26469;&#35299;&#32806;&#29992;&#25143;&#25968;&#25454;&#19982;&#24212;&#29992;&#31243;&#24207;&#65292;&#23454;&#29616;&#29992;&#25143;&#22312;&#20010;&#20154;Pod&#20013;&#23433;&#20840;&#23384;&#20648;&#25152;&#26377;&#25968;&#25454;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.10408</link><description>&lt;p&gt;
SocialGenPod: &#38544;&#31169;&#21451;&#22909;&#30340;&#20998;&#24067;&#24335;&#20010;&#20154;&#25968;&#25454;&#23384;&#20648;&#30340;&#29983;&#25104;&#24335;AI&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10408
&lt;/p&gt;
&lt;p&gt;
SocialGenPod&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;AI&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;&#37096;&#32626;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#30340;Solid&#35268;&#33539;&#26469;&#35299;&#32806;&#29992;&#25143;&#25968;&#25454;&#19982;&#24212;&#29992;&#31243;&#24207;&#65292;&#23454;&#29616;&#29992;&#25143;&#22312;&#20010;&#20154;Pod&#20013;&#23433;&#20840;&#23384;&#20648;&#25152;&#26377;&#25968;&#25454;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SocialGenPod&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#19988;&#38544;&#31169;&#21451;&#22909;&#30340;&#26041;&#24335;&#29992;&#20110;&#37096;&#32626;&#29983;&#25104;&#24335;AI Web&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;Solid&#35268;&#33539;&#26469;&#23558;&#29992;&#25143;&#25968;&#25454;&#19982;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#31243;&#24207;&#35299;&#32806;&#65292;&#19982;&#20445;&#25345;&#29992;&#25143;&#25968;&#25454;&#19982;&#24212;&#29992;&#31243;&#24207;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#32465;&#23450;&#30340;&#38598;&#20013;&#24335;Web&#21644;&#25968;&#25454;&#26550;&#26500;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21407;&#22411;&#28436;&#31034;&#20102;SocialGenPod&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#65292;&#21487;&#36873;&#25321;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#29983;&#25104;&#20197;&#29992;&#25143;&#34987;&#20801;&#35768;&#30452;&#25509;&#25110;&#38388;&#25509;&#35775;&#38382;&#30340;&#20219;&#20309;Solid Pod&#20013;&#23384;&#20648;&#30340;&#31169;&#20154;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#31572;&#26696;&#12290;SocialGenPod&#21033;&#29992;Solid&#35775;&#38382;&#25511;&#21046;&#26426;&#21046;&#65292;&#35753;&#29992;&#25143;&#23436;&#20840;&#25511;&#21046;&#30830;&#23450;&#35841;&#21487;&#20197;&#35775;&#38382;&#23384;&#20648;&#22312;&#20182;&#20204;Pod&#20013;&#30340;&#25968;&#25454;&#12290;SocialGenPod&#23558;&#25152;&#26377;&#29992;&#25143;&#25968;&#25454;&#65288;&#32842;&#22825;&#35760;&#24405;&#65292;&#24212;&#29992;&#31243;&#24207;&#37197;&#32622;&#65292;&#20010;&#20154;&#25991;&#26723;&#31561;&#65289;&#23433;&#20840;&#22320;&#23384;&#20648;&#22312;&#29992;&#25143;&#30340;&#20010;&#20154;Pod&#20013;&#65307;&#19982;&#29305;&#23450;&#27169;&#22411;&#20998;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10408v1 Announce Type: cross  Abstract: We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid -- a decentralised Web specification -- to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#21644;&#22320;&#36136;&#22810;&#26679;&#24615;&#30340;&#38075;&#23380;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#27169;&#22411;&#29992;&#20110;&#20934;&#30830;&#22320;&#22312;&#23454;&#38469;&#38567;&#36947;&#26045;&#24037;&#29615;&#22659;&#20013;&#23545;&#23721;&#20307;&#36136;&#37327;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#20379;&#20851;&#38190;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.10404</link><description>&lt;p&gt;
&#22522;&#20110;&#38075;&#20117;&#25968;&#25454;&#30340;&#23721;&#20307;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study on machine learning approaches for rock mass classification using drilling data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#21644;&#22320;&#36136;&#22810;&#26679;&#24615;&#30340;&#38075;&#23380;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#27169;&#22411;&#29992;&#20110;&#20934;&#30830;&#22320;&#22312;&#23454;&#38469;&#38567;&#36947;&#26045;&#24037;&#29615;&#22659;&#20013;&#23545;&#23721;&#20307;&#36136;&#37327;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#20379;&#20851;&#38190;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22312;&#25970;&#20987;&#29190;&#30772;&#38567;&#36947;&#30340;&#23721;&#30707;&#24037;&#31243;&#35774;&#35745;&#20027;&#35201;&#20381;&#36182;&#20110;&#24037;&#31243;&#24072;&#30340;&#35266;&#27979;&#35780;&#20272;&#12290;&#22312;&#38567;&#36947;&#25366;&#25496;&#26399;&#38388;&#25910;&#38598;&#30340;&#39640;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#25968;&#25454;&#8212;&#8212;&#38075;&#36827;&#26102;&#27979;&#37327;&#65288;MWD&#65289;&#25968;&#25454;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#29992;&#20110;&#22320;&#36136;&#21487;&#35270;&#21270;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#33258;&#21160;&#23558;MWD&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#23721;&#30707;&#24037;&#31243;&#25351;&#26631;&#12290;&#23427;&#26088;&#22312;&#23558;&#25968;&#25454;&#19982;&#29305;&#23450;&#30340;&#24037;&#31243;&#34892;&#21160;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#38567;&#36947;&#25512;&#36827;&#21069;&#38754;&#30340;&#22320;&#36136;&#25361;&#25112;&#25552;&#20379;&#33267;&#20851;&#37325;&#35201;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#21033;&#29992;&#26469;&#33258;15&#20010;&#38567;&#36947;&#30340;50&#19975;&#20010;&#38075;&#23380;&#30340;&#22823;&#35268;&#27169;&#21644;&#22320;&#36136;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#22312;&#23454;&#38469;&#38567;&#36947;&#26045;&#24037;&#29615;&#22659;&#20013;&#20026;&#20934;&#30830;&#30340;&#23721;&#20307;&#36136;&#37327;&#20998;&#31867;&#24341;&#20837;&#20102;&#27169;&#22411;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;MWD&#25968;&#25454;&#20998;&#31867;&#20026;Q&#31867;&#21644;Q&#20540;&#65292;&#36825;&#20123;&#25351;&#26631;&#25551;&#36848;&#20102;&#23721;&#30707;&#36136;&#37327;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10404v1 Announce Type: new  Abstract: Current rock engineering design in drill and blast tunnelling primarily relies on engineers' observational assessments. Measure While Drilling (MWD) data, a high-resolution sensor dataset collected during tunnel excavation, is underutilised, mainly serving for geological visualisation. This study aims to automate the translation of MWD data into actionable metrics for rock engineering. It seeks to link data to specific engineering actions, thus providing critical decision support for geological challenges ahead of the tunnel face. Leveraging a large and geologically diverse dataset of 500,000 drillholes from 15 tunnels, the research introduces models for accurate rock mass quality classification in a real-world tunnelling context. Both conventional machine learning and image-based deep learning are explored to classify MWD data into Q-classes and Q-values, examples of metrics describing the stability of the rock mass, using both tabular 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#20064;&#20869;&#37096;&#20998;&#24067;&#29305;&#24449;&#23494;&#24230;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10403</link><description>&lt;p&gt;
&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Energy Correction Model in the Feature Space for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10403
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#20064;&#20869;&#37096;&#20998;&#24067;&#29305;&#24449;&#23494;&#24230;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#30740;&#31350;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#23398;&#20064;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#29305;&#24449;&#30340;&#23494;&#24230;&#65292;&#21457;&#29616;&#22312;EBM&#35757;&#32451;&#36807;&#31243;&#20013;MCMC&#37319;&#26679;&#30340;&#38750;&#28151;&#21512;&#24615;&#20250;&#21066;&#24369;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28151;&#21512;&#31867;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#32452;&#25104;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#65292;&#19982;CIFAR-10/CIFAR-100 OOD&#26816;&#27979;&#22522;&#20934;&#19978;&#30340;&#24378;&#22522;&#32447;KNN&#26816;&#27979;&#22120;&#30456;&#27604;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10403v1 Announce Type: cross  Abstract: In this work, we study the out-of-distribution (OOD) detection problem through the use of the feature space of a pre-trained deep classifier. We show that learning the density of in-distribution (ID) features with an energy-based models (EBM) leads to competitive detection results. However, we found that the non-mixing of MCMC sampling during the EBM's training undermines its detection performance. To overcome this an energy-based correction of a mixture of class-conditional Gaussian distributions. We obtains favorable results when compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100 OOD detection benchmarks.
&lt;/p&gt;</description></item><item><title>Isotropic3D&#26159;&#19968;&#20010;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22270;&#20687;CLIP&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#35753;&#20248;&#21270;&#30456;&#23545;&#20110;&#26041;&#20301;&#35282;&#26159;&#21508;&#21521;&#21516;&#24615;&#65292;&#36991;&#20813;&#36807;&#20998;&#20381;&#36182;&#22270;&#20687;&#23548;&#33268;&#29983;&#25104;&#30340;&#25153;&#24179;&#25110;&#25197;&#26354;&#12290;</title><link>https://arxiv.org/abs/2403.10395</link><description>&lt;p&gt;
Isotropic3D&#65306;&#22522;&#20110;&#21333;&#20010;CLIP&#23884;&#20837;&#30340;&#22270;&#20687;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10395
&lt;/p&gt;
&lt;p&gt;
Isotropic3D&#26159;&#19968;&#20010;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22270;&#20687;CLIP&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#35753;&#20248;&#21270;&#30456;&#23545;&#20110;&#26041;&#20301;&#35282;&#26159;&#21508;&#21521;&#21516;&#24615;&#65292;&#36991;&#20813;&#36807;&#20998;&#20381;&#36182;&#22270;&#20687;&#23548;&#33268;&#29983;&#25104;&#30340;&#25153;&#24179;&#25110;&#25197;&#26354;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40723;&#33310;&#20110;&#36234;&#26469;&#36234;&#22810;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#36827;&#34892;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#27491;&#22312;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;&#26032;&#35270;&#35282;&#25552;&#21319;&#21040;2D&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#24120;&#20197;&#21442;&#32771;&#22270;&#20687;&#20316;&#20026;&#26465;&#20214;&#65292;&#21516;&#26102;&#22312;&#21442;&#32771;&#35270;&#22270;&#19978;&#24212;&#29992;&#20005;&#26684;&#30340;L2&#22270;&#20687;&#30417;&#30563;&#12290;&#28982;&#32780;&#36807;&#20998;&#20381;&#36182;&#22270;&#20687;&#23481;&#26131;&#30772;&#22351;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#24402;&#32435;&#30693;&#35782;&#65292;&#32463;&#24120;&#23548;&#33268;&#29983;&#25104;&#24179;&#22374;&#25110;&#25197;&#26354;&#30340;3D&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22270;&#20687;&#21040;3D&#65292;&#24182;&#21576;&#29616;Isotropic3D&#65292;&#19968;&#20010;&#20165;&#20197;&#22270;&#20687;CLIP&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#30340;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#27969;&#27700;&#32447;&#12290;Isotropic3D&#20801;&#35768;&#20248;&#21270;&#30456;&#23545;&#20110;&#26041;&#20301;&#35282;&#26159;&#21508;&#21521;&#21516;&#24615;&#65292;&#22240;&#20026;&#20165;&#20381;&#36182;&#20110;SDS&#25439;&#22833;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#26680;&#24515;&#22312;&#20110;&#20004;&#38454;&#27573;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26367;&#25442; fine-tune&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;3D&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10395v1 Announce Type: cross  Abstract: Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D diffusion model by subst
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;DEC&#24182;&#27714;&#35299;&#30456;&#24212;&#30340;&#26497;&#23567;-&#26497;&#22823;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;Estimation-To-Decisions&#65288;E2D&#65289;&#31639;&#27861;&#30340;&#20219;&#20309;&#26102;&#20505;&#21464;&#20307;&#65292;&#25104;&#21151;&#23558;&#22312;&#32447;&#20248;&#21270;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#36716;&#21270;&#20026;&#23454;&#29992;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10379</link><description>&lt;p&gt;
&#36890;&#36807;&#38797;&#28857;&#20248;&#21270;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regret Minimization via Saddle Point Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10379
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;DEC&#24182;&#27714;&#35299;&#30456;&#24212;&#30340;&#26497;&#23567;-&#26497;&#22823;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;Estimation-To-Decisions&#65288;E2D&#65289;&#31639;&#27861;&#30340;&#20219;&#20309;&#26102;&#20505;&#21464;&#20307;&#65292;&#25104;&#21151;&#23558;&#22312;&#32447;&#20248;&#21270;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#36716;&#21270;&#20026;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31995;&#21015;&#20316;&#21697;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#31243;&#24207;&#34920;&#24449;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#22312;&#30456;&#24212;&#30340;&#38797;&#28857;&#21338;&#24328;&#20013;&#65292;&#26497;&#23567;&#29609;&#23478;&#38024;&#23545;&#36873;&#25321;&#24341;&#21457;&#22823;&#36951;&#25022;&#30340;&#28151;&#20081;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#12290;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26032;&#23454;&#20363;&#26159;&#20915;&#31574;-&#20272;&#35745;&#31995;&#25968;&#65288;DEC&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#20960;&#20046;&#25552;&#20379;&#20102;&#26368;&#32039;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#12290;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#24102;&#26377;&#32622;&#20449;&#21322;&#24452;&#30340;DEC&#20559;&#31227;&#37327;&#65292;&#24182;&#35299;&#20915;&#30456;&#24212;&#30340;&#26497;&#23567;-&#26497;&#22823;&#31243;&#24207;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;Estimation-To-Decisions&#65288;E2D&#65289;&#31639;&#27861;&#30340;&#20219;&#20309;&#26102;&#20505;&#21464;&#20307;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#31639;&#27861;&#22312;&#32447;&#20248;&#21270;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20998;&#26512;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#23548;&#33268;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26377;&#38480;&#27169;&#22411;&#31867;&#21644;&#32447;&#24615;&#21453;&#39304;&#27169;&#22411;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10379v1 Announce Type: new  Abstract: A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;XAI&#26041;&#27861;&#33258;&#21160;&#25913;&#21892;&#39044;&#20808;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2403.10373</link><description>&lt;p&gt;
&#36890;&#36807;XAI&#26041;&#27861;&#25913;&#21892;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a general framework for improving the performance of classifiers using XAI methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;XAI&#26041;&#27861;&#33258;&#21160;&#25913;&#21892;&#39044;&#20808;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38590;&#20197;&#29702;&#35299;&#20854;&#20869;&#37096;&#36816;&#20316;&#65292;&#22240;&#27492;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#25361;&#25112;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26816;&#26597;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25552;&#20379;&#26377;&#20851;&#20854;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;XAI&#26041;&#27861;&#33258;&#21160;&#25913;&#21892;&#39044;&#20808;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#65292;&#20998;&#21035;&#31216;&#20026;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10373v1 Announce Type: new  Abstract: Modern Artificial Intelligence (AI) systems, especially Deep Learning (DL) models, poses challenges in understanding their inner workings by AI researchers. eXplainable Artificial Intelligence (XAI) inspects internal mechanisms of AI models providing explanations about their decisions. While current XAI research predominantly concentrates on explaining AI systems, there is a growing interest in using XAI techniques to automatically improve the performance of AI systems themselves. This paper proposes a general framework for automatically improving the performance of pre-trained DL classifiers using XAI methods, avoiding the computational overhead associated with retraining complex models from scratch. In particular, we outline the possibility of two different learning strategies for implementing this architecture, which we will call auto-encoder-based and encoder-decoder-based, and discuss their key aspects.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#33410;&#33021;&#38598;&#25104;&#26041;&#27861;ENAMLE&#65292;&#26088;&#22312;&#35299;&#20915;SECOE&#30340;&#33021;&#28304;&#29942;&#39048;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10371</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#33410;&#33021;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness in IoT Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10371
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#33410;&#33021;&#38598;&#25104;&#26041;&#27861;ENAMLE&#65292;&#26088;&#22312;&#35299;&#20915;SECOE&#30340;&#33021;&#28304;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#21160;&#24577;&#24615;&#21644;&#20020;&#26102;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#65292;&#21363;&#32570;&#22833;&#30340;&#20256;&#24863;&#22120;&#35835;&#25968;&#12290;&#35768;&#22810;&#22240;&#32032;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;/&#25110;&#32593;&#32476;&#20013;&#26029;&#65292;&#37117;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29289;&#32852;&#32593;&#31995;&#32479;&#21463;&#21040;&#20005;&#37325;&#30340;&#30005;&#21147;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26500;&#24314;&#38024;&#23545;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#40065;&#26834;&#19988;&#33410;&#33021;&#30340;&#29289;&#32852;&#32593;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#23545;SECOE&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;-&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#29289;&#32852;&#32593;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20851;&#27880;&#20854;&#33021;&#28304;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;SECOE&#30340;&#33021;&#28304;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ENAMLE-&#19968;&#31181;&#20027;&#21160;&#30340;&#12289;&#33021;&#28304;&#24863;&#30693;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#21516;&#26102;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;ENAMLE&#22312;&#36825;&#26679;&#30340;&#24847;&#20041;&#19978;&#26159;&#29420;&#29305;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10371v1 Announce Type: cross  Abstract: Machine Learning (ML) is becoming increasingly important for IoT-based applications. However, the dynamic and ad-hoc nature of many IoT ecosystems poses unique challenges to the efficacy of ML algorithms. One such challenge is data incompleteness, which is manifested as missing sensor readings. Many factors, including sensor failures and/or network disruption, can cause data incompleteness. Furthermore, most IoT systems are severely power-constrained. It is important that we build IoT-based ML systems that are robust against data incompleteness while simultaneously being energy efficient. This paper presents an empirical study of SECOE - a recent technique for alleviating data incompleteness in IoT - with respect to its energy bottlenecks. Towards addressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive, energy-aware technique for mitigating the impact of concurrent missing data. ENAMLE is unique in the sense that it
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#20989;&#25968;&#21644;&#23450;&#20041;&#36866;&#24212;&#24615;&#23433;&#20840;&#38598;&#65292;&#23558;&#21487;&#25193;&#23637;&#20998;&#31867;&#22120;&#21644;&#36866;&#24212;&#24615;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#35774;&#35745;&#21021;&#26399;&#23601;&#20026;&#20998;&#31867;&#25552;&#20379;&#31283;&#20581;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10368</link><description>&lt;p&gt;
&#38754;&#21521;&#27010;&#29575;&#40065;&#26834;&#21487;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#30340;&#36866;&#24212;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Predictions for Probabilistically Robust Scalable Machine Learning Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10368
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#20989;&#25968;&#21644;&#23450;&#20041;&#36866;&#24212;&#24615;&#23433;&#20840;&#38598;&#65292;&#23558;&#21487;&#25193;&#23637;&#20998;&#31867;&#22120;&#21644;&#36866;&#24212;&#24615;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#35774;&#35745;&#21021;&#26399;&#23601;&#20026;&#20998;&#31867;&#25552;&#20379;&#31283;&#20581;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#39044;&#27979;&#21487;&#20197;&#23450;&#20041;&#21487;&#38752;&#19988;&#31283;&#20581;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#35774;&#35745;&#21021;&#26399;&#23601;&#20026;&#20998;&#31867;&#23450;&#20041;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#20998;&#31867;&#22120;&#30340;&#27010;&#24565;&#19982;&#32479;&#35745;&#25490;&#24207;&#29702;&#35770;&#21644;&#27010;&#29575;&#23398;&#20064;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#20989;&#25968;&#30340;&#26032;&#23450;&#20041;&#21644;&#23450;&#20041;&#19968;&#32452;&#29305;&#27530;&#30340;&#36755;&#20837;&#21464;&#37327;&#65292;&#21363;&#36866;&#24212;&#24615;&#23433;&#20840;&#38598;&#65292;&#26469;&#20998;&#26512;&#21487;&#25193;&#23637;&#20998;&#31867;&#22120;&#21644;&#36866;&#24212;&#24615;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#35813;&#23433;&#20840;&#38598;&#33021;&#22815;&#35782;&#21035;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#28385;&#36275;&#35823;&#24046;&#35206;&#30422;&#20445;&#35777;&#30340;&#27169;&#24335;&#65292;&#21363;&#23545;&#20110;&#23646;&#20110;&#35813;&#38598;&#21512;&#30340;&#28857;&#35266;&#23519;&#21040;&#38169;&#35823;&#65288;&#21487;&#33021;&#19981;&#23433;&#20840;&#65289;&#26631;&#31614;&#30340;&#27010;&#29575;&#21463;&#21040;&#39044;&#23450;&#20041;&#30340;$\varepsilon$&#38169;&#35823;&#27700;&#24179;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10368v1 Announce Type: cross  Abstract: Conformal predictions make it possible to define reliable and robust learning algorithms. But they are essentially a method for evaluating whether an algorithm is good enough to be used in practice. To define a reliable learning framework for classification from the very beginning of its design, the concept of scalable classifier was introduced to generalize the concept of classical classifier by linking it to statistical order theory and probabilistic learning theory. In this paper, we analyze the similarities between scalable classifiers and conformal predictions by introducing a new definition of a score function and defining a special set of input variables, the conformal safety set, which can identify patterns in the input space that satisfy the error coverage guarantee, i.e., that the probability of observing the wrong (possibly unsafe) label for points belonging to this set is bounded by a predefined $\varepsilon$ error level. W
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20010;&#20154;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#33719;&#24471;&#20102; $O(\log n)$-IP &#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.10365</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20010;&#20154;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Algorithms for Individual Preference Stable Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10365
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20010;&#20154;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#33719;&#24471;&#20102; $O(\log n)$-IP &#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#20154;&#20559;&#22909;&#65288;IP&#65289;&#31283;&#23450;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#25429;&#25417;&#32858;&#31867;&#20013;&#20010;&#20154;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#20010;&#35774;&#23450;&#20013;&#65292;&#24403;&#27599;&#20010;&#25968;&#25454;&#28857;&#21040;&#20854;&#31751;&#30340;&#24179;&#22343;&#36317;&#31163;&#19981;&#36229;&#36807;&#20854;&#21040;&#20219;&#20309;&#20854;&#20182;&#31751;&#30340;&#24179;&#22343;&#36317;&#31163;&#30340; $\alpha$ &#20493;&#26102;&#65292;&#19968;&#20010;&#32858;&#31867;&#26159; $\alpha$-IP &#31283;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110; IP &#31283;&#23450;&#32858;&#31867;&#30340;&#33258;&#28982;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;&#27492;&#31639;&#27861;&#30340; $O(\log n)$-IP &#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20854;&#20013; $n$ &#34920;&#31034;&#36755;&#20837;&#20013;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25913;&#36827;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#34920;&#26126;&#20854;&#36816;&#34892;&#26102;&#38388;&#20960;&#20046;&#26159;&#32447;&#24615;&#30340;&#65292;&#20026; $\tilde{O}(nk)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10365v1 Announce Type: cross  Abstract: In this paper, we study the individual preference (IP) stability, which is an notion capturing individual fairness and stability in clustering. Within this setting, a clustering is $\alpha$-IP stable when each data point's average distance to its cluster is no more than $\alpha$ times its average distance to any other cluster. In this paper, we study the natural local search algorithm for IP stable clustering. Our analysis confirms a $O(\log n)$-IP stability guarantee for this algorithm, where $n$ denotes the number of points in the input. Furthermore, by refining the local search approach, we show it runs in an almost linear time, $\tilde{O}(nk)$.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#30740;&#31350;&#20219;&#21153;&#38590;&#24230;&#65292;&#21457;&#29616;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10348</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Task Difficulty-based Curriculum for Training Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10348
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#30740;&#31350;&#20219;&#21153;&#38590;&#24230;&#65292;&#21457;&#29616;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#23545;&#21508;&#20010;&#26102;&#38388;&#27493;&#38271;&#21644;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#21435;&#22122;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21435;&#22122;&#20219;&#21153;&#30340;&#30456;&#23545;&#38590;&#24230;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#25910;&#25947;&#34892;&#20026;&#21644;&#26102;&#38388;&#27493;&#38271;&#38388;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#30340;&#30456;&#23545;&#29109;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#26174;&#31034;&#65292;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#23384;&#22312;&#25910;&#25947;&#32531;&#24930;&#21644;&#36739;&#39640;&#30340;&#30456;&#23545;&#29109;&#65292;&#34920;&#26126;&#22312;&#36825;&#20123;&#36739;&#20302;&#26102;&#38388;&#27493;&#38271;&#19978;&#20219;&#21153;&#38590;&#24230;&#22686;&#21152;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#30001;&#26131;&#21040;&#38590;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#20511;&#37492;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10348v1 Announce Type: cross  Abstract: Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learn
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24341;&#20837;&#31867;&#21516;&#36136;&#24615;&#26041;&#24046;&#36825;&#19968;&#26032;&#24230;&#37327;&#65292;&#22312;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#25552;&#20986;&#20102;Homophily Edge Generation Graph Neural Network (HedGe)&#27169;&#22411;&#65292;&#24378;&#35843;&#29983;&#25104;&#26032;&#20851;&#31995;&#20197;&#38477;&#20302;&#31867;&#21516;&#36136;&#24615;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.10339</link><description>&lt;p&gt;
&#27604;&#36215;&#20462;&#25913;&#65292;&#29983;&#25104;&#26356;&#22909;&#65306;&#22312;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#23545;&#25239;&#39640;&#31867;&#21516;&#24615;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10339
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24341;&#20837;&#31867;&#21516;&#36136;&#24615;&#26041;&#24046;&#36825;&#19968;&#26032;&#24230;&#37327;&#65292;&#22312;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#25552;&#20986;&#20102;Homophily Edge Generation Graph Neural Network (HedGe)&#27169;&#22411;&#65292;&#24378;&#35843;&#29983;&#25104;&#26032;&#20851;&#31995;&#20197;&#38477;&#20302;&#31867;&#21516;&#36136;&#24615;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#21516;&#36136;&#24615;&#20998;&#24067;&#24046;&#24322;&#26174;&#33879;&#22823;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#20013;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#31867;&#21516;&#36136;&#24615;&#26041;&#24046;&#30340;&#26032;&#24230;&#37327;&#65292;&#23450;&#37327;&#25551;&#36848;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#20854;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#36136;&#24615;&#36793;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HedGe&#65289;&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20391;&#37325;&#20110;&#23545;&#21407;&#22987;&#20851;&#31995;&#36827;&#34892;&#20462;&#21098;&#12289;&#36873;&#25321;&#25110;&#36830;&#25509;&#65292;&#25105;&#20204;&#31216;&#36825;&#20123;&#26041;&#27861;&#20026;&#20462;&#25913;&#12290;&#19982;&#36825;&#20123;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#29983;&#25104;&#26032;&#30340;&#20851;&#31995;&#65292;&#20351;&#31867;&#21516;&#36136;&#24615;&#26041;&#24046;&#36739;&#20302;&#65292;&#21516;&#26102;&#20351;&#29992;&#21407;&#22987;&#20851;&#31995;&#20316;&#20026;&#36741;&#21161;&#12290;HedGe&#20351;&#29992;&#33258;&#20851;&#27880;&#26426;&#21046;&#20174;&#22836;&#24320;&#22987;&#23545;&#21516;&#36136;&#24615;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#21033;&#29992;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10339v1 Announce Type: new  Abstract: Graph-based anomaly detection is currently an important research topic in the field of graph neural networks (GNNs). We find that in graph anomaly detection, the homophily distribution differences between different classes are significantly greater than those in homophilic and heterophilic graphs. For the first time, we introduce a new metric called Class Homophily Variance, which quantitatively describes this phenomenon. To mitigate its impact, we propose a novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe). Previous works typically focused on pruning, selecting or connecting on original relationships, and we refer to these methods as modifications. Different from these works, our method emphasizes generating new relationships with low class homophily variance, using the original relationships as an auxiliary. HedGe samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverage
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10332</link><description>&lt;p&gt;
GreedyML&#65306;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GreedyML: A Parallel Algorithm for Maximizing Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#22312;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#35299;&#20915;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38656;&#27714;&#30340;&#21551;&#21457;&#65292;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25968;&#25454;&#25688;&#35201;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;Barbosa&#12289;Ene&#12289;Nguyen&#21644;Ward&#65288;2015&#65289;&#25552;&#20986;&#30340;&#38543;&#26426;&#20998;&#24067;&#24335;RandGreedI&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#25968;&#25454;&#38543;&#26426;&#20998;&#21306;&#21040;&#25152;&#26377;&#22788;&#29702;&#22120;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#21333;&#20010;&#32047;&#31215;&#27493;&#39588;&#35745;&#31639;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#26377;&#22788;&#29702;&#22120;&#23558;&#23427;&#20204;&#30340;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#21457;&#36865;&#32473;&#19968;&#20010;&#22788;&#29702;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#38382;&#39064;&#65292;&#32047;&#31215;&#27493;&#39588;&#21487;&#33021;&#36229;&#36807;&#22788;&#29702;&#22120;&#19978;&#21487;&#29992;&#30340;&#20869;&#23384;&#65292;&#24182;&#19988;&#25191;&#34892;&#32047;&#31215;&#30340;&#22788;&#29702;&#22120;&#21487;&#33021;&#25104;&#20026;&#35745;&#31639;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10332v1 Announce Type: cross  Abstract: We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data summarization, machine learning, and graph sparsification. Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck.   Here, we propose a generalization of the R
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#65292;&#37325;&#35201;&#24615;&#33719;&#24471;&#19981;&#20855;&#26377;&#23545;&#25239;&#24615;&#29305;&#24449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.10330</link><description>&lt;p&gt;
&#26397;&#21521;&#38750;&#23545;&#25239;&#24615;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Towards Non-Adversarial Algorithmic Recourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10330
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#65292;&#37325;&#35201;&#24615;&#33719;&#24471;&#19981;&#20855;&#26377;&#23545;&#25239;&#24615;&#29305;&#24449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#30740;&#31350;&#26041;&#21521;&#22522;&#26412;&#19978;&#26159;&#29420;&#31435;&#22686;&#38271;&#30340;&#12290; &#36825;&#23548;&#33268;&#26368;&#36817;&#26377;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#38416;&#26126;&#23427;&#20204;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#12290; &#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26377;&#20154;&#35748;&#20026;&#65292;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#30456;&#21453;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#21363;&#19982;&#22522;&#26412;&#20107;&#23454;&#30456;&#27604;&#65292;&#23427;&#20204;&#20250;&#23548;&#33268;&#35823;&#20998;&#31867;&#12290; &#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#20013;&#25152;&#20351;&#29992;&#30340;&#35745;&#31639;&#30446;&#26631;&#21644;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#19982;&#27492;&#35201;&#27714;&#30340;&#23545;&#40784;&#12290; &#20351;&#29992;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38750;&#23545;&#25239;&#24615;&#30340;&#31639;&#27861;&#34917;&#20607;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#19981;&#20855;&#26377;&#23545;&#25239;&#24615;&#29305;&#24449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#25105;&#20204;&#38543;&#21518;&#35843;&#26597;&#20102;&#23458;&#20307;&#20013;&#19981;&#21516;&#32452;&#20214;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10330v1 Announce Type: new  Abstract: The streams of research on adversarial examples and counterfactual explanations have largely been growing independently. This has led to several recent works trying to elucidate their similarities and differences. Most prominently, it has been argued that adversarial examples, as opposed to counterfactual explanations, have a unique characteristic in that they lead to a misclassification compared to the ground truth. However, the computational goals and methodologies employed in existing counterfactual explanation and adversarial example generation methods often lack alignment with this requirement. Using formal definitions of adversarial examples and counterfactual explanations, we introduce non-adversarial algorithmic recourse and outline why in high-stakes situations, it is imperative to obtain counterfactual explanations that do not exhibit adversarial characteristics. We subsequently investigate how different components in the objec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;PLM&#22686;&#24378;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10326</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#22635;&#31354;&#24178;&#25200;&#39033;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;PLM&#22686;&#24378;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35774;&#35745;&#22635;&#31354;&#27979;&#35797;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#38169;&#35823;&#36873;&#39033;&#65288;&#24178;&#25200;&#39033;&#65289;&#30340;&#36873;&#25321;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#24178;&#25200;&#39033;&#25552;&#39640;&#20102;&#23398;&#20064;&#32773;&#33021;&#21147;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#30340;&#24819;&#27861;&#24212;&#36816;&#32780;&#29983;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24212;&#29992;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#30740;&#31350;&#22635;&#31354;&#24178;&#25200;&#39033;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PLM&#22686;&#24378;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#23558;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20174;14.94&#25552;&#21319;&#33267;34.17&#65288;NDCG@10&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/AndyChiangSH/CDGP &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10326v1 Announce Type: cross  Abstract: Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.
&lt;/p&gt;</description></item><item><title>ATLAS&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#35774;&#35745;&#30340;&#20219;&#24847;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#36807;&#28388;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#35757;&#32451;-free&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26550;&#26500;&#35780;&#20272;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.10318</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20219;&#24847;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Anytime Neural Architecture Search on Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10318
&lt;/p&gt;
&lt;p&gt;
ATLAS&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#35774;&#35745;&#30340;&#20219;&#24847;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#36807;&#28388;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#35757;&#32451;-free&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26550;&#26500;&#35780;&#20272;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20174;&#25163;&#21160;&#26550;&#26500;&#35774;&#35745;&#36716;&#21464;&#20026;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(NAS)&#12290;&#36825;&#31181;&#36716;&#21464;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#19988;&#21709;&#24212;&#28789;&#25935;&#30340;&#20219;&#24847;&#26102;&#38388;NAS&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#36820;&#22238;&#24403;&#21069;&#30340;&#26368;&#20339;&#26550;&#26500;&#65292;&#24182;&#38543;&#30528;&#39044;&#31639;&#20998;&#37197;&#30340;&#22686;&#21152;&#36880;&#28176;&#25552;&#39640;&#26550;&#26500;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20219;&#24847;&#26102;&#38388;NAS&#39046;&#22495;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ATLAS&#65292;&#31532;&#19968;&#20010;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#20219;&#24847;&#26102;&#38388;NAS&#26041;&#27861;&#12290;ATLAS&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#36807;&#28388;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#35757;&#32451;-free&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26550;&#26500;&#35780;&#20272;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36807;&#28388;&#38454;&#27573;&#65292;ATLAS&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#20026;&#34920;&#26684;&#25968;&#25454;&#19987;&#38376;&#35774;&#35745;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#20505;&#36873;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10318v1 Announce Type: new  Abstract: The increasing demand for tabular data analysis calls for transitioning from manual architecture design to Neural Architecture Search (NAS). This transition demands an efficient and responsive anytime NAS approach that is capable of returning current optimal architectures within any given time budget while progressively enhancing architecture quality with increased budget allocation. However, the area of research on Anytime NAS for tabular data remains unexplored. To this end, we introduce ATLAS, the first anytime NAS approach tailored for tabular data. ATLAS introduces a novel two-phase filtering-and-refinement optimization scheme with joint optimization, combining the strengths of both paradigms of training-free and training-based architecture evaluation. Specifically, in the filtering phase, ATLAS employs a new zero-cost proxy specifically designed for tabular data to efficiently estimate the performance of candidate architectures, th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31895;&#31961;Transformer&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#30340;&#36755;&#20837;&#24207;&#21015;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#24773;&#22659;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.10288</link><description>&lt;p&gt;
&#29992;&#20110;&#36830;&#32493;&#21644;&#39640;&#25928;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#31895;&#31961;Transformer
&lt;/p&gt;
&lt;p&gt;
Rough Transformers for Continuous and Efficient Time-Series Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10288
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31895;&#31961;Transformer&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#30340;&#36755;&#20837;&#24207;&#21015;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#24773;&#22659;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#20197;&#19981;&#22343;&#21248;&#38388;&#38548;&#35266;&#23519;&#21040;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#24490;&#29615;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#29992;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#27169;&#22411;&#26367;&#25442;&#24490;&#29615;&#26550;&#26500;&#26469;&#24314;&#27169;&#38750;&#22343;&#21248;&#37319;&#26679;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Transformer&#26550;&#26500;&#26469;&#32771;&#34385;&#38271;&#31243;&#20381;&#36182;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#20013;&#31561;&#38271;&#24230;&#21450;&#26356;&#38271;&#36755;&#20837;&#24207;&#21015;&#65292;&#20004;&#32773;&#37117;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31895;&#31961;Transformer&#65292;&#36825;&#26159;Transformer&#27169;&#22411;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#22312;&#36755;&#20837;&#24207;&#21015;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#24773;&#22659;&#20013;&#24120;&#35265;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#31614;&#21517;&#27880;&#24847;&#21147;&#65292;&#21033;&#29992;&#36335;&#24452;&#31614;&#21517;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10288v1 Announce Type: cross  Abstract: Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attent
&lt;/p&gt;</description></item><item><title>Team Trifecta&#22312;Factify 5WQA&#19978;&#20197;Fine-Tuning&#21462;&#24471;&#20102;&#39318;&#35201;&#22320;&#20301;&#65292;&#25104;&#21151;&#36229;&#36234;&#22522;&#20934;&#20934;&#30830;&#29575;103&#65285;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#32773;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.10281</link><description>&lt;p&gt;
Team Trifecta&#22312;Factify 5WQA&#19978;&#35774;&#23450;&#20102;&#32454;&#21270;&#35843;&#25972;&#20013;&#20107;&#23454;&#39564;&#35777;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10281
&lt;/p&gt;
&lt;p&gt;
Team Trifecta&#22312;Factify 5WQA&#19978;&#20197;Fine-Tuning&#21462;&#24471;&#20102;&#39318;&#35201;&#22320;&#20301;&#65292;&#25104;&#21151;&#36229;&#36234;&#22522;&#20934;&#20934;&#30830;&#29575;103&#65285;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#32773;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Pre-CoFactv3&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#32452;&#20214;&#32452;&#25104;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#23454;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;FakeNet&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20107;&#23454;&#39564;&#35777;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24341;&#20837;&#20102;FakeNet&#65292;&#24182;&#23454;&#26045;&#20102;&#21508;&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;Trifecta&#22312;AAAI-24 Factify 3.0&#30740;&#35752;&#20250;&#19978;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#27604;&#22522;&#20934;&#20934;&#30830;&#29575;&#39640;&#20986;103%&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#23545;&#25163;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;&#36825;&#19968;&#25104;&#21151;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#23545;&#25512;&#36827;&#20107;&#23454;&#39564;&#35777;&#30740;&#31350;&#30340;&#28508;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10281v1 Announce Type: cross  Abstract: In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;&#20998;&#31867;&#27169;&#22411;&#21644;LSTM&#27169;&#22411;&#22312;&#24037;&#19994;&#20013;&#36827;&#34892;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#23545;&#26426;&#22120;&#25925;&#38556;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.10259</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#31867;&#27169;&#22411;&#21644;LSTM&#27169;&#22411;&#22312;&#24037;&#19994;&#20013;&#36827;&#34892;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;&#20998;&#31867;&#27169;&#22411;&#21644;LSTM&#27169;&#22411;&#22312;&#24037;&#19994;&#20013;&#36827;&#34892;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#23545;&#26426;&#22120;&#25925;&#38556;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25216;&#26415;&#39537;&#21160;&#30340;&#26102;&#20195;&#65292;&#23545;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#20808;&#36827;&#35786;&#26029;&#30340;&#36843;&#20999;&#38656;&#27714;&#19981;&#20165;&#20165;&#38480;&#20110;&#33322;&#31354;&#39046;&#22495;&#65292;&#36824;&#21253;&#25324;&#23545;&#26059;&#36716;&#21644;&#31227;&#21160;&#26426;&#22120;&#20013;&#25439;&#22351;&#12289;&#25925;&#38556;&#21644;&#25805;&#20316;&#32570;&#38519;&#30340;&#35782;&#21035;&#12290;&#23454;&#26045;&#36825;&#26679;&#30340;&#26381;&#21153;&#19981;&#20165;&#21487;&#20197;&#32553;&#20943;&#32500;&#25252;&#25104;&#26412;&#65292;&#36824;&#21487;&#20197;&#24310;&#38271;&#26426;&#22120;&#23551;&#21629;&#65292;&#30830;&#20445;&#21331;&#36234;&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36825;&#20063;&#26159;&#19968;&#31181;&#38450;&#33539;&#28508;&#22312;&#20107;&#25925;&#25110;&#28798;&#38590;&#24615;&#20107;&#20214;&#30340;&#39044;&#38450;&#25514;&#26045;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#34892;&#19994;&#30340;&#32500;&#25252;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#22120;&#25925;&#38556;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#33410;&#32422;&#20102;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#25216;&#26415;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#36923;&#36753;&#22238;&#24402;&#20197;&#21450;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;LSTM&#30340;&#26426;&#22120;&#24615;&#33021;&#39044;&#27979;&#19982;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10259v1 Announce Type: cross  Abstract: In today's technology-driven era, the imperative for predictive maintenance and advanced diagnostics extends beyond aviation to encompass the identification of damages, failures, and operational defects in rotating and moving machines. Implementing such services not only curtails maintenance costs but also extends machine lifespan, ensuring heightened operational efficiency. Moreover, it serves as a preventive measure against potential accidents or catastrophic events. The advent of Artificial Intelligence (AI) has revolutionized maintenance across industries, enabling more accurate and efficient prediction and analysis of machine failures, thereby conserving time and resources. Our proposed study aims to delve into various machine learning classification techniques, including Support Vector Machine (SVM), Random Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for predicting and analyzing machine performance. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#29615;&#22659;&#19979;&#30340;&#25345;&#32493;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#32467;&#21512;&#25345;&#32493;&#23398;&#20064;&#21644;&#31890;&#29699;&#35745;&#31639;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#24182;&#20419;&#36827;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.10253</link><description>&lt;p&gt;
&#24320;&#25918;&#30340;&#31890;&#29699;&#30693;&#35782;&#36801;&#31227;&#19979;&#30340;&#25345;&#32493;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Open Continual Feature Selection via Granular-Ball Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#29615;&#22659;&#19979;&#30340;&#25345;&#32493;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#32467;&#21512;&#25345;&#32493;&#23398;&#20064;&#21644;&#31890;&#29699;&#35745;&#31639;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#24182;&#20419;&#36827;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#20013;&#25345;&#32493;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65288;CFS&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#20010;&#24320;&#25918;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26410;&#30693;&#31867;&#21035;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#31890;&#29699;&#35745;&#31639;&#65288;GBC&#65289;&#30340;&#20248;&#21183;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#31890;&#29699;&#30693;&#35782;&#24211;&#20197;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#65292;&#24182;&#20419;&#36827;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#30340;&#36716;&#31227;&#65292;&#36827;&#19968;&#27493;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10253v1 Announce Type: new  Abstract: This paper presents a novel framework for continual feature selection (CFS) in data preprocessing, particularly in the context of an open and dynamic environment where unknown classes may emerge. CFS encounters two primary challenges: the discovery of unknown knowledge and the transfer of known knowledge. To this end, the proposed CFS method combines the strengths of continual learning (CL) with granular-ball computing (GBC), which focuses on constructing a granular-ball knowledge base to detect unknown classes and facilitate the transfer of previously learned knowledge for further feature selection. CFS consists of two stages: initial learning and open learning. The former aims to establish an initial knowledge base through multi-granularity representation using granular-balls. The latter utilizes prior granular-ball knowledge to identify unknowns, updates the knowledge base for granular-ball knowledge transfer, reinforces old knowledge
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#36879;&#26126;&#24230;&#21644;&#20844;&#24179;&#24615;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#31526;&#21512;&#25968;&#23398;&#21407;&#29702;&#30340;&#29305;&#24449;&#24433;&#21709;&#21644;&#39118;&#38505;&#22240;&#32032;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10250</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for Survival Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10250
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#36879;&#26126;&#24230;&#21644;&#20844;&#24179;&#24615;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#31526;&#21512;&#25968;&#23398;&#21407;&#29702;&#30340;&#29305;&#24449;&#24433;&#21709;&#21644;&#39118;&#38505;&#22240;&#32032;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20256;&#25773;&#21644;&#24555;&#36895;&#36827;&#27493;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#39046;&#22495;&#25110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290; &#36825;&#22312;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#20013;&#37319;&#29992;IML&#25216;&#26415;&#20419;&#36827;&#20102;&#36879;&#26126;&#24230;&#12289;&#38382;&#36131;&#21046;&#21644;&#20844;&#24179;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#12289;&#26377;&#38024;&#23545;&#24615;&#30103;&#27861;&#30340;&#24320;&#21457;&#12289;&#24178;&#39044;&#25110;&#20854;&#20182;&#21307;&#23398;&#25110;&#19982;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#30340;&#29615;&#22659;&#20013;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#25581;&#31034;&#29983;&#23384;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#26356;&#31526;&#21512;&#25968;&#23398;&#21407;&#29702;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#21738;&#20123;&#29305;&#24449;&#23545;&#39044;&#27979;&#26377;&#24433;&#21709;&#25110;&#26500;&#25104;&#39118;&#38505;&#22240;&#32032;&#12290; &#28982;&#32780;&#65292;&#32570;&#20047;&#21363;&#26102;&#21487;&#29992;&#30340;IML&#26041;&#27861;&#21487;&#33021;&#24050;&#32463;&#38459;&#30861;&#20102;&#21307;&#23398;&#20174;&#19994;&#32773;&#21644;&#20844;&#20849;&#21355;&#29983;&#25919;&#31574;&#21046;&#23450;&#32773;&#20805;&#20998;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10250v1 Announce Type: cross  Abstract: With the spread and rapid advancement of black box machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade. This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and fairness in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts. More specifically, explainability can uncover a survival model's potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors. However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine lea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25511;&#21046;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;&#30697;&#38453;&#34917;&#20840;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10232</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#30340;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10232
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25511;&#21046;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;&#30697;&#38453;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#36890;&#36807;&#20551;&#35774;&#30697;&#38453;&#20855;&#26377;&#20302;&#31209;&#26469;&#36924;&#36817;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#32570;&#22833;&#20540;&#30340;&#32447;&#24615;&#36924;&#36817;&#12290;&#24050;&#32463;&#34920;&#26126;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20272;&#35745;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FCNN&#65289;&#26159;&#30697;&#38453;&#34917;&#20840;&#26368;&#36866;&#21512;&#30340;&#26550;&#26500;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#39640;&#23481;&#37327;&#32780;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#36827;&#32780;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20013;&#38388;&#34920;&#31034;&#30340; $\ell_{1}$ &#33539;&#25968;&#21644;&#26435;&#37325;&#30697;&#38453;&#30340;&#26680;&#33539;&#25968;&#26041;&#38754;&#23545;FCNN&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#25511;&#21046;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#20989;&#25968;&#21464;&#24471;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#65292;&#21363;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#24182;&#30740;&#31350;&#20854;&#25910;&#25947;&#21040;&#20020;&#30028;&#28857;&#12290;&#22312;FCNN&#30340;&#21021;&#22987;&#26102;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10232v1 Announce Type: cross  Abstract: Conventional matrix completion methods approximate the missing values by assuming the matrix to be low-rank, which leads to a linear approximation of missing values. It has been shown that enhanced performance could be attained by using nonlinear estimators such as deep neural networks. Deep fully connected neural networks (FCNNs), one of the most suitable architectures for matrix completion, suffer from over-fitting due to their high capacity, which leads to low generalizability. In this paper, we control over-fitting by regularizing the FCNN model in terms of the $\ell_{1}$ norm of intermediate representations and nuclear norm of weight matrices. As such, the resulting regularized objective function becomes nonsmooth and nonconvex, i.e., existing gradient-based methods cannot be applied to our model. We propose a variant of the proximal gradient method and investigate its convergence to a critical point. In the initial epochs of FCNN
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10231</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#25512;&#23548;&#26032;&#30340;&#20107;&#23454;&#65292;&#38142;&#25509;&#39044;&#27979;&#22120;&#20174;&#22270;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#25910;&#38598;&#23616;&#37096;&#35777;&#25454;&#20197;&#25214;&#21040;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#21033;&#29992;&#25972;&#20010;KG&#36827;&#34892;&#39044;&#27979;&#32780;&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;KG&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36890;&#36807;&#24120;&#35268;&#25277;&#26679;&#26041;&#27861;&#35299;&#20915;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#12290; &#35774;&#35745;&#21407;&#21017;&#26159;&#65292;&#39044;&#27979;&#36807;&#31243;&#19981;&#30452;&#25509;&#20316;&#29992;&#20110;&#25972;&#20010;KG&#65292;&#32780;&#26159;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#65288;i&#65289;&#26681;&#25454;&#26597;&#35810;&#20165;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#21644;&#65288;ii&#65289;&#22312;&#36825;&#20010;&#21333;&#19968;&#30340;&#12289;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20010;&#24615;&#21270;PageRank&#65288;PPR&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#31572;&#26696;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10231v1 Announce Type: cross  Abstract: To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders their promise on large scale KGs and cannot be directly addressed by vanilla sampling methods. In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole KG, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence. With efficient subgraph-b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AERO&#65292;&#19968;&#20010;&#19987;&#20026;&#22825;&#25991;&#35266;&#27979;&#20013;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#25797;&#38271;&#22788;&#29702;&#22825;&#25991;&#35266;&#27979;&#20013;&#29420;&#31435;&#20294;&#21463;&#21040;&#38543;&#26426;&#24182;&#21457;&#22122;&#22768;&#24178;&#25200;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.10220</link><description>&lt;p&gt;
&#20174;&#28151;&#27788;&#21040;&#28165;&#26224;&#65306;&#22825;&#25991;&#35266;&#27979;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AERO&#65292;&#19968;&#20010;&#19987;&#20026;&#22825;&#25991;&#35266;&#27979;&#20013;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#25797;&#38271;&#22788;&#29702;&#22825;&#25991;&#35266;&#27979;&#20013;&#29420;&#31435;&#20294;&#21463;&#21040;&#38543;&#26426;&#24182;&#21457;&#22122;&#22768;&#24178;&#25200;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22825;&#25991;&#35774;&#26045;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#35774;&#26045;&#35266;&#27979;&#21040;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#25910;&#38598;&#36215;&#26469;&#12290;&#20998;&#26512;&#36825;&#20123;&#22825;&#25991;&#35266;&#27979;&#20013;&#30340;&#24322;&#24120;&#23545;&#20110;&#25581;&#31034;&#28508;&#22312;&#30340;&#22825;&#20307;&#20107;&#20214;&#21644;&#29289;&#29702;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#25512;&#21160;&#31185;&#23398;&#30740;&#31350;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22312;&#22788;&#29702;&#22825;&#25991;&#35266;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#20854;&#20013;&#27599;&#39063;&#26143;&#20307;&#26412;&#36136;&#19978;&#26159;&#29420;&#31435;&#30340;&#65292;&#20294;&#21463;&#21040;&#38543;&#26426;&#24182;&#21457;&#22122;&#22768;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#20551;&#35686;&#25253;&#29575;&#36739;&#39640;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AERO&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#22825;&#25991;&#35266;&#27979;&#20013;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23398;&#20064;&#27599;&#20010;&#21464;&#37327;&#65288;&#21363;&#26143;&#20307;&#65289;&#19978;&#30340;&#27491;&#24120;&#26102;&#38388;&#27169;&#24335;&#65292;&#20197;&#19982;&#21464;&#37327;&#29420;&#31435;&#24615;&#30340;&#29305;&#24449;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10220v1 Announce Type: cross  Abstract: With the development of astronomical facilities, large-scale time series data observed by these facilities is being collected. Analyzing anomalies in these astronomical observations is crucial for uncovering potential celestial events and physical phenomena, thus advancing the scientific research process. However, existing time series anomaly detection methods fall short in tackling the unique characteristics of astronomical observations where each star is inherently independent but interfered by random concurrent noise, resulting in a high rate of false alarms. To overcome the challenges, we propose AERO, a novel two-stage framework tailored for unsupervised anomaly detection in astronomical observations. In the first stage, we employ a Transformer-based encoder-decoder architecture to learn the normal temporal patterns on each variate (i.e., star) in alignment with the characteristic of variate independence. In the second stage, we e
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;JPEG-LDPC&#21387;&#32553;&#22270;&#20687;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LDPC&#30721;&#30340;&#20869;&#37096;&#20195;&#30721;&#32467;&#26500;&#65292;&#20351;&#29992;GRU&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.10202</link><description>&lt;p&gt;
&#22312;JPEG-LDPC&#21387;&#32553;&#22270;&#20687;&#19978;&#23398;&#20064;&#65306;&#21033;&#29992;&#32508;&#21512;&#30151;&#29366;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;JPEG-LDPC&#21387;&#32553;&#22270;&#20687;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LDPC&#30721;&#30340;&#20869;&#37096;&#20195;&#30721;&#32467;&#26500;&#65292;&#20351;&#29992;GRU&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#30446;&#26631;&#30340;&#36890;&#20449;&#20013;&#65292;&#25509;&#25910;&#32773;&#30340;&#30446;&#26631;&#36890;&#24120;&#26159;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#37325;&#24314;&#21407;&#22987;&#25968;&#25454;&#12290; &#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22312;&#25509;&#25910;&#22120;&#19978;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20808;&#21069;&#35299;&#30721;&#21363;&#21487;&#30452;&#25509;&#23545;&#21387;&#32553;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#26377;&#26395;&#22686;&#24378;&#25512;&#26029;&#27169;&#22411;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20854;&#20013;&#32463;&#29109;&#32534;&#30721;&#26159;&#29992;&#20302;&#23494;&#24230;&#22855;&#20598;&#26657;&#39564;&#65288;LDPC&#65289;&#30721;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#20551;&#35774;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;LDPC&#30721;&#30340;&#20869;&#37096;&#20195;&#30721;&#32467;&#26500;&#12290; &#22312;&#25509;&#25910;&#22120;&#31471;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#36827;&#34892;&#35757;&#32451;&#12290; &#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LDPC&#30340;&#20998;&#31867;&#26377;&#26395;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#32508;&#21512;&#30151;&#29366;&#30340;&#22270;&#20687;&#20998;&#31867;&#26126;&#26174;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10202v1 Announce Type: cross  Abstract: In goal-oriented communications, the objective of the receiver is often to apply a Deep-Learning model, rather than reconstructing the original data. In this context, direct learning over compressed data, without any prior decoding, holds promise for enhancing the time-efficient execution of inference models at the receiver. However, conventional entropic-coding methods like Huffman and Arithmetic break data structure, rendering them unsuitable for learning without decoding. In this paper, we propose an alternative approach in which entropic coding is realized with Low-Density Parity Check (LDPC) codes. We hypothesize that Deep Learning models can more effectively exploit the internal code structure of LDPC codes. At the receiver, we leverage a specific class of Recurrent Neural Networks (RNNs), specifically Gated Recurrent Unit (GRU), trained for image classification. Our numerical results indicate that classification based on LDPC-co
&lt;/p&gt;</description></item><item><title>&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#26080;&#27861;&#24212;&#23545;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#35757;&#32451;&#26041;&#27861;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;</title><link>https://arxiv.org/abs/2403.10190</link><description>&lt;p&gt;
&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#27169;&#22411;&#35757;&#32451;&#22312;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#19979;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality-based Model Training under Annotator Label Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10190
&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#26080;&#27861;&#24212;&#23545;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#35757;&#32451;&#26041;&#27861;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10190v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;. &#26631;&#27880;&#32773;&#22312;&#25968;&#25454;&#26631;&#35760;&#36807;&#31243;&#20013;&#23384;&#22312;&#20998;&#27495;&#65292;&#21487;&#31216;&#20026;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#34920;&#29616;&#20026;&#26631;&#35760;&#36136;&#37327;&#30340;&#21464;&#21270;&#12290;&#27599;&#20010;&#26679;&#26412;&#20351;&#29992;&#21333;&#20010;&#20302;&#36136;&#37327;&#26631;&#27880;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#21487;&#38752;&#24615;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#32771;&#23519;&#20102;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20250;&#38543;&#30528;&#20302;&#36136;&#37327;&#30340;&#22024;&#26434;&#26631;&#31614;&#30340;&#23384;&#22312;&#32780;&#38477;&#20302;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#34920;&#26126;&#23427;&#20204;&#26080;&#27861;&#24212;&#23545;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#26126;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#26631;&#27880;&#32773;&#25910;&#38598;&#30340;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10190v1 Announce Type: cross  Abstract: Annotators exhibit disagreement during data labeling, which can be termed as annotator label uncertainty. Annotator label uncertainty manifests in variations of labeling quality. Training with a single low-quality annotation per sample induces model reliability degradations. In this work, we first examine the effects of annotator label uncertainty in terms of the model's generalizability and prediction uncertainty. We observe that the model's generalizability and prediction uncertainty degrade with the presence of low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty estimation algorithms indicates their incapability in response to annotator label uncertainty. To mitigate performance degradation, prior methods show that training models with labels collected from multiple independent annotators can enhance generalizability. However, they require massive annotations. Hence, we introduce a novel perceptual quality-ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;TAPG&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31574;&#30053;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#22312;&#25235;&#21462;&#20219;&#24847;&#29289;&#20307;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10187</link><description>&lt;p&gt;
&#25235;&#20303;&#19968;&#20999;&#65306;&#23558;&#25945;&#24072;&#22686;&#24378;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#19982;&#23454;&#20363;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25235;&#21462;&#20219;&#24847;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10187
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;TAPG&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31574;&#30053;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#22312;&#25235;&#21462;&#20219;&#24847;&#29289;&#20307;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20174;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#29289;&#20307;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#28789;&#24039;&#65292;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#23384;&#22312;&#24050;&#20037;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#25361;&#25112;&#28304;&#20110;&#35270;&#35273;&#24863;&#30693;&#30340;&#22797;&#26434;&#24615;&#65292;&#23545;&#31934;&#20934;&#36816;&#21160;&#25216;&#33021;&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#20004;&#32773;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Teacher-Augmented Policy Gradient (TAPG)&#65292;&#35813;&#26694;&#26550;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31574;&#30053;&#33976;&#39311;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25945;&#24072;&#31574;&#30053;&#26469;&#25484;&#25569;&#22522;&#20110;&#29289;&#20307;&#20301;&#32622;&#20449;&#24687;&#30340;&#36816;&#21160;&#25511;&#21046;&#65292;TAPG&#20419;&#36827;&#20102;&#22522;&#20110;&#29289;&#20307;&#20998;&#21106;&#30340;&#24863;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#25351;&#23548;&#24615;&#20294;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Segment Anything&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20174;&#20223;&#30495;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29289;&#20307;&#30340;&#29087;&#32451;&#25235;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#22522;&#20110;&#20154;&#31867;&#21487;&#29702;&#35299;&#25552;&#31034;&#30340;&#28151;&#20081;&#22330;&#26223;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#29087;&#32451;&#22320;&#25235;&#21462;&#21508;&#31181;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10187v1 Announce Type: cross  Abstract: Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;</title><link>https://arxiv.org/abs/2403.10175</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Short Survey on Importance Weighting for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10175
&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#31243;&#24207;&#65292;&#26681;&#25454;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#29992;&#30340;&#24605;&#24819;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#35768;&#22810;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25454;&#30693;&#65292;&#22312;&#20851;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#20551;&#35774;&#19979;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#21487;&#20197;&#20445;&#35777;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;&#12290;&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20851;&#30740;&#31350;&#20013;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10175v1 Announce Type: cross  Abstract: Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#26694;&#26550;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23450;&#20301;&#20026;XAI&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#36755;&#20986;&#32467;&#26524;&#26102;&#26159;&#21542;&#24212;&#35813;&#20449;&#20219;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10168</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#65306;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#20449;&#36182;&#30340;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Explainability through uncertainty: Trustworthy decision-making with neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#26694;&#26550;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23450;&#20301;&#20026;XAI&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#36755;&#20986;&#32467;&#26524;&#26102;&#26159;&#21542;&#24212;&#35813;&#20449;&#20219;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#65292;&#23588;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#19981;&#30830;&#23450;&#24615;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#20559;&#31163;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24708;&#26080;&#22768;&#24687;&#22320;&#19979;&#38477;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25552;&#20379;&#20102;&#35299;&#20915;&#36807;&#20110;&#33258;&#20449;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25351;&#31034;&#20309;&#26102;&#24212;&#35813;&#65288;&#19981;&#24212;&#35813;&#65289;&#20449;&#20219;&#36755;&#20986;&#32467;&#26524;&#12290;&#34429;&#28982;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#21457;&#23637;&#65292;&#20294;&#23578;&#26410;&#26126;&#30830;&#23450;&#20041;&#19982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36816;&#31609;&#23398;&#39046;&#22495;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#25805;&#20316;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26410;&#32771;&#34385;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#26694;&#26550;&#65292;&#36129;&#29486;&#20027;&#35201;&#20307;&#29616;&#22312;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;i&#65289;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23450;&#20301;&#20026;XAI&#25216;&#26415;&#65292;&#25552;&#20379;&#23616;&#37096;&#30340;&#65292;&#27169;&#22411;&#29305;&#23450;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10168v1 Announce Type: new  Abstract: Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident. This overconfidence is worrying under distribution shifts, where the model performance silently degrades as the data distribution diverges from the training data distribution. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI). Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider distribution shifts. This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific expla
&lt;/p&gt;</description></item><item><title>CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.10164</link><description>&lt;p&gt;
CoReEcho: 2D+&#26102;&#38388;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10164
&lt;/p&gt;
&lt;p&gt;
CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#21253;&#25324;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#21516;&#26102;&#12290;&#28982;&#32780;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#36229;&#22768;&#24515;&#21160;&#22270;&#29255;&#27573;&#20043;&#38388;&#30340;&#36830;&#32493;&#20851;&#31995;&#65292;&#23548;&#33268;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23545;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoReEcho&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#35843;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;CoReEcho&#65306;1&#65289;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#65288;EchoNet-Dynamic&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.90&#21644;R2 o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10164v1 Announce Type: cross  Abstract: Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 &amp; R2 o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20559;&#22909;&#21644;&#34394;&#25311;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10160</link><description>&lt;p&gt;
&#20174;&#31163;&#32447;&#20559;&#22909;&#20013;&#23398;&#20064;&#22312;&#32447;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Online Policy Learning from Offline Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10160
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20559;&#22909;&#21644;&#34394;&#25311;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#20013;&#65292;&#20154;&#31867;&#21453;&#39304;&#34987;&#31216;&#20026;&#20559;&#22909;&#65292;&#29992;&#20110;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#20026;&#20102;&#21152;&#24555;&#20559;&#22909;&#25910;&#38598;&#30340;&#36895;&#24230;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#31163;&#32447;&#20559;&#22909;&#65292;&#21363;&#25910;&#38598;&#29992;&#20110;&#26576;&#20123;&#31163;&#32447;&#25968;&#25454;&#30340;&#20559;&#22909;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#36866;&#21512;&#20110;&#31163;&#32447;&#25968;&#25454;&#12290;&#22914;&#26524;&#23398;&#20064;&#20195;&#29702;&#23637;&#29616;&#20986;&#19982;&#31163;&#32447;&#25968;&#25454;&#19981;&#37325;&#21472;&#30340;&#34892;&#20026;&#65292;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#33021;&#20250;&#36935;&#21040;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20559;&#22909;&#21644;&#34394;&#25311;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;PbRL&#65292;&#34394;&#25311;&#20559;&#22909;&#26159;&#20195;&#29702;&#30340;&#34892;&#20026;&#19982;&#31163;&#32447;&#25968;&#25454;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#20851;&#38190;&#26159;&#65292;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#34394;&#25311;&#20559;&#22909;&#36861;&#36394;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#20026;&#20195;&#29702;&#25552;&#20379;&#33391;&#22909;&#23545;&#40784;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10160v1 Announce Type: new  Abstract: In preference-based reinforcement learning (PbRL), a reward function is learned from a type of human feedback called preference. To expedite preference collection, recent works have leveraged \emph{offline preferences}, which are preferences collected for some offline data. In this scenario, the learned reward function is fitted on the offline data. If a learning agent exhibits behaviors that do not overlap with the offline data, the learned reward function may encounter generalizability issues. To address this problem, the present study introduces a framework that consolidates offline preferences and \emph{virtual preferences} for PbRL, which are comparisons between the agent's behaviors and the offline data. Critically, the reward function can track the agent's behaviors using the virtual preferences, thereby offering well-aligned guidance to the agent. Through experiments on continuous control tasks, this study demonstrates the effect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.10158</link><description>&lt;p&gt;
&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20419;&#36827;&#20581;&#24247;&#21644;&#31038;&#20250;&#20851;&#24576;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;funGCN&#65289;&#26694;&#26550;&#65292;&#23558;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#20581;&#24247;&#35299;&#20915;&#26041;&#26696;&#23545;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#21644;&#31038;&#20250;&#25903;&#25345;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#38271;&#65292;&#30830;&#20445;&#21508;&#24180;&#40836;&#27573;&#30340;&#20581;&#24247;&#29983;&#27963;&#21644;&#20419;&#36827;&#24184;&#31119;&#24863;&#65292;funGCN&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#20010;&#23454;&#20307;&#30340;&#22810;&#20803;&#32437;&#21521;&#25968;&#25454;&#65292;&#24182;&#30830;&#20445;&#21363;&#20351;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#20063;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#31649;&#29702;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#20197;&#33719;&#21462;&#27934;&#23519;&#24615;&#25968;&#25454;&#35299;&#37322;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#39564;&#35777;&#20102;funGCN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10158v1 Announce Type: cross  Abstract: This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.
&lt;/p&gt;</description></item><item><title>eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.10153</link><description>&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#19987;&#23478;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Multi-modal Contrastive Learning with Expert Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10153
&lt;/p&gt;
&lt;p&gt;
eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;CLIP&#27169;&#22411;&#8212;&#8212;eCLIP&#65292;&#23427;&#38598;&#25104;&#20102;&#25918;&#23556;&#31185;&#21307;&#29983;&#30524;&#29699;&#27880;&#35270;&#28909;&#22270;&#24418;&#24335;&#30340;&#19987;&#23478;&#27880;&#37322;&#12290;&#23427;&#35299;&#20915;&#20102;&#23545;&#27604;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#25968;&#25454;&#31232;&#32570;&#21644;&#8220;&#27169;&#24577;&#24046;&#36317;&#8221;&#8212;&#8212;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#38477;&#20302;&#20102;&#34920;&#31034;&#30340;&#36136;&#37327;&#24182;&#38459;&#30861;&#20102;&#36328;&#27169;&#24577;&#20114;&#25805;&#20316;&#24615;&#12290;eCLIP&#38598;&#25104;&#20102;&#19968;&#20010;&#28909;&#22270;&#22788;&#29702;&#22120;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#22686;&#24378;&#26469;&#26377;&#25928;&#21033;&#29992;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;eCLIP&#35774;&#35745;&#20026;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;CLIP&#21464;&#20307;&#65292;&#26080;&#38656;&#20462;&#25913;&#26680;&#24515;&#26550;&#26500;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#35814;&#32454;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#25512;&#26029;&#12289;&#32447;&#24615;&#25506;&#38024;&#12289;&#36328;&#27169;&#24577;&#26816;&#32034;&#20197;&#21450;&#20351;&#29992;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#65292;eCLIP&#23637;&#31034;&#20102;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10153v1 Announce Type: cross  Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.10144</link><description>&lt;p&gt;
NLP&#39564;&#35777;&#65306;&#36208;&#21521;&#19968;&#31181;&#36890;&#29992;&#30340;&#29992;&#20110;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
NLP Verification: Towards a General Methodology for Certifying Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#23545;&#21464;&#21270;&#25110;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#23545;&#20854;&#36755;&#20986;&#32473;&#20986;&#20445;&#35777;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;NLP&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26041;&#27861;&#35770;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#29486;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NLP&#39564;&#35777;&#30340;&#23454;&#29992;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#19981;&#28145;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#25552;&#28860;&#21644;&#35780;&#20272;&#19968;&#20010;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27969;&#31243;&#26469;&#28304;&#20110;&#36804;&#20170;&#20026;&#27490;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#35821;&#20041;&#27867;&#21270;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26102;&#36827;&#34892;&#26377;&#25928;&#26356;&#26032;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.10123</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regularization-Based Efficient Continual Learning in Deep State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26102;&#36827;&#34892;&#26377;&#25928;&#26356;&#26032;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#23545;&#21160;&#24577;&#31995;&#32479;&#20855;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;DSSMs&#65289;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DSSM&#24037;&#20316;&#23616;&#38480;&#20110;&#21333;&#20219;&#21153;&#24314;&#27169;&#65292;&#36825;&#38656;&#35201;&#22312;&#37325;&#26032;&#35775;&#38382;&#20043;&#21069;&#30340;&#20219;&#21153;&#26102;&#21033;&#29992;&#21382;&#21490;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#23398;&#20064;DSSMs&#65288;CLDSSMs&#65289;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CLDSSMs&#38598;&#25104;&#20102;&#20027;&#27969;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#30830;&#20445;&#22312;&#23545;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26102;&#39640;&#25928;&#26356;&#26032;&#65292;&#20445;&#25345;&#19981;&#21464;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#23545;&#24212;&#29992;&#20110;&#21508;&#33258;CLDSSMs&#30340;&#27599;&#31181;CL&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25104;&#26412;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#26469;&#23637;&#31034;CLDSSMs&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#35777;&#23454;&#65292;&#34429;&#28982;&#21508;&#31181;&#31454;&#20105;&#30340;CL&#26041;&#27861;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#28857;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;CLDSSMs&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10123v1 Announce Type: new  Abstract: Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistentl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#31639;&#23376;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#22797;&#26434;&#26597;&#35810;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10110</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19978;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#20803;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Meta Operator for Complex Query Answering on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#31639;&#23376;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#22797;&#26434;&#26597;&#35810;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#26377;&#20449;&#24687;&#24615;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#20026;&#20102;&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#19979;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#27169;&#22411;&#65292;&#30452;&#25509;&#20174;&#26597;&#35810;-&#31572;&#26696;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#36991;&#20813;&#30452;&#25509;&#36941;&#21382;&#19981;&#23436;&#25972;&#30340;&#22270;&#25968;&#25454;&#12290;&#29616;&#26377;&#30740;&#31350;&#23558;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#30340;&#35757;&#32451;&#21046;&#23450;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22797;&#26434;&#26597;&#35810;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#35748;&#20026;&#19981;&#21516;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#31867;&#22411;&#65292;&#32780;&#19981;&#26159;&#19981;&#21516;&#30340;&#22797;&#26434;&#26597;&#35810;&#31867;&#22411;&#65292;&#26159;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#23398;&#20064;&#20803;&#36816;&#31639;&#31526;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#22797;&#26434;&#26597;&#35810;&#19979;&#30340;&#19981;&#21516;&#36816;&#31639;&#31526;&#23454;&#20363;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#20803;&#31639;&#23376;&#27604;&#23398;&#20064;&#21407;&#22987;CQA&#25110;&#20803;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10110v1 Announce Type: cross  Abstract: Knowledge graphs contain informative factual knowledge but are considered incomplete. To answer complex queries under incomplete knowledge, learning-based Complex Query Answering (CQA) models are proposed to directly learn from the query-answer samples to avoid the direct traversal of incomplete graph data. Existing works formulate the training of complex query answering models as multi-task learning and require a large number of training samples. In this work, we explore the compositional structure of complex queries and argue that the different logical operator types, rather than the different complex query types, are the key to improving generalizability. Accordingly, we propose a meta-learning algorithm to learn the meta-operators with limited data and adapt them to different instances of operators under various complex queries. Empirical results show that learning meta-operators is more effective than learning original CQA or meta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;BNBRL+&#31639;&#27861;&#65292;&#32467;&#21512;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21487;&#35266;&#23519;&#21306;&#22495;&#35780;&#20272;&#39118;&#38505;&#21644;&#21046;&#23450;&#31227;&#21160;&#31574;&#30053;&#30340;&#30446;&#30340;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21160;&#24577;&#20851;&#31995;&#21644;&#31038;&#20250;&#35268;&#33539;&#65292;&#23454;&#29616;&#20102;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.10105</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#20449;&#24565;&#36741;&#21161;&#23548;&#33322;&#20197;&#36991;&#20813;&#30450;&#28857;&#20013;&#30340;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10105
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;BNBRL+&#31639;&#27861;&#65292;&#32467;&#21512;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21487;&#35266;&#23519;&#21306;&#22495;&#35780;&#20272;&#39118;&#38505;&#21644;&#21046;&#23450;&#31227;&#21160;&#31574;&#30053;&#30340;&#30446;&#30340;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21160;&#24577;&#20851;&#31995;&#21644;&#31038;&#20250;&#35268;&#33539;&#65292;&#23454;&#29616;&#20102;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#26368;&#26032;&#30740;&#31350;&#38598;&#20013;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#35201;&#27714;&#26469;&#33258;&#20840;&#21521;&#20256;&#24863;&#22120;&#30340;&#20934;&#30830;&#20301;&#32622;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#31639;&#27861;BNBRL+&#65292;&#22522;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#35780;&#20272;&#19981;&#21487;&#35266;&#23519;&#21306;&#22495;&#30340;&#39118;&#38505;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#21046;&#23450;&#31227;&#21160;&#31574;&#30053;&#12290;BNBRL+&#23558;&#20449;&#24565;&#31639;&#27861;&#19982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#26681;&#25454;&#20154;&#31867;&#30340;&#20301;&#32622;&#25968;&#25454;&#27010;&#29575;&#25512;&#26029;&#20449;&#24565;&#12290;&#23427;&#36827;&#19968;&#27493;&#25972;&#21512;&#20102;&#26426;&#22120;&#20154;&#12289;&#20154;&#31867;&#21644;&#25512;&#26029;&#20449;&#24565;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#65292;&#30830;&#23450;&#23548;&#33322;&#36335;&#24452;&#65292;&#24182;&#22312;&#22870;&#21169;&#20989;&#25968;&#20869;&#23884;&#20837;&#31038;&#20250;&#35268;&#33539;&#65292;&#20174;&#32780;&#20419;&#36827;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;&#36890;&#36807;&#19981;&#21516;&#39118;&#38505;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10105v1 Announce Type: cross  Abstract: Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaRand&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#29305;&#24449;&#21521;&#37327;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#38656;&#35201;&#36741;&#21161;&#28304;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.10097</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#29305;&#24449;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaRand&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#29305;&#24449;&#21521;&#37327;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#38656;&#35201;&#36741;&#21161;&#28304;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24494;&#35843;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#20107;&#23454;&#26631;&#20934;&#26041;&#27861;&#65292;&#20294;&#22312;&#20351;&#29992;&#23567;&#22411;&#30446;&#26631;&#25968;&#25454;&#38598;&#26102;&#20173;&#28982;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#20445;&#25345;&#23545;&#28304;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#25110;&#24341;&#20837;&#35832;&#22914;&#23545;&#27604;&#25439;&#22833;&#20043;&#31867;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36741;&#21161;&#28304;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#28304;&#26631;&#31614;&#25110;&#25968;&#25454;&#38598;&#65289;&#25110;&#37325;&#22797;&#38468;&#21152;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#36866;&#24212;&#38543;&#26426;&#29305;&#24449;&#27491;&#21017;&#21270;&#65288;AdaRand&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;AdaRand&#21487;&#20197;&#24110;&#21161;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#22312;&#27809;&#26377;&#36741;&#21161;&#28304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24230;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#21521;&#37327;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;AdaRand&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#24449;&#21521;&#37327;&#21644;&#20174;&#31867;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38543;&#26426;&#21442;&#32771;&#21521;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10097v1 Announce Type: cross  Abstract: While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamicall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.10089</link><description>&lt;p&gt;
&#29992;&#20110;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximation and bounding techniques for the Fisher-Rao distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#30340;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#34987;&#23450;&#20041;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#35825;&#23548;&#30340;Riemannian&#27979;&#22320;&#36317;&#31163;&#12290;&#20026;&#20102;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;Fisher-Rao&#36317;&#31163;&#65292;&#25105;&#20204;&#38656;&#35201;&#65288;1&#65289;&#25512;&#23548;&#20986;Fisher-Rao&#27979;&#22320;&#32447;&#30340;&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;2&#65289;&#27839;&#30528;&#36825;&#20123;&#27979;&#22320;&#32447;&#31215;&#20998;Fisher&#38271;&#24230;&#20803;&#32032;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#23376;&#27169;&#22411;&#30340;&#38381;&#21512;&#24418;&#24335;1D Fisher-Rao&#36317;&#31163;&#25253;&#21578;&#20102;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#31181;&#36890;&#29992;&#30340;&#36817;&#20284;&#26041;&#26696;&#65292;&#21462;&#20915;&#20110;Fisher-Rao&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#33021;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#25552;&#20379;Fisher-Rao&#39044;&#27979;&#27979;&#22320;&#32447;&#21644;&#20005;&#26684;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#26102;&#36817;&#20284;&#20135;&#29983;&#20219;&#24847;&#23567;&#30340;&#38468;&#21152;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10089v1 Announce Type: cross  Abstract: The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22522;&#20110;&#36924;&#30495;3D&#22270;&#24418;&#24314;&#27169;&#12289;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#12289;&#24046;&#20998;&#31070;&#32463;&#28210;&#26579;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10075</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey of synthetic data augmentation methods in computer vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22522;&#20110;&#36924;&#30495;3D&#22270;&#24418;&#24314;&#27169;&#12289;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#12289;&#24046;&#20998;&#31070;&#32463;&#28210;&#26579;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#20351;&#29992;&#20195;&#34920;&#30446;&#26631;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#35757;&#32451;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#33719;&#21462;&#36275;&#22815;&#30340;&#30446;&#26631;&#20219;&#21153;&#22270;&#20687;&#25968;&#25454;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#26126;&#30830;&#22320;&#20197;&#25152;&#38656;&#30340;&#26041;&#24335;&#36716;&#25442;&#29616;&#26377;&#22270;&#20687;&#65292;&#20197;&#21019;&#24314;&#23454;&#29616;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#24615;&#12290;&#22312;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#20174;&#38646;&#24320;&#22987;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;--&#21363;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24191;&#27867;&#35780;&#20272;&#12290;&#23427;&#28085;&#30422;&#20102;&#22522;&#20110;&#36924;&#30495;3D&#22270;&#24418;&#24314;&#27169;&#12289;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#65288;NST&#65289;&#12289;&#24046;&#20998;&#31070;&#32463;&#28210;&#26579;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10075v1 Announce Type: cross  Abstract: The standard approach to tackling computer vision problems is to train deep convolutional neural network (CNN) models using large-scale image datasets which are representative of the target task. However, in many scenarios, it is often challenging to obtain sufficient image data for the target task. Data augmentation is a way to mitigate this challenge. A common practice is to explicitly transform existing images in desired ways so as to create the required volume and variability of training data necessary to achieve good generalization performance. In situations where data for the target domain is not accessible, a viable workaround is to synthesize training data from scratch--i.e., synthetic data augmentation. This paper presents an extensive review of synthetic data augmentation techniques. It covers data synthesis approaches based on realistic 3D graphics modeling, neural style transfer (NST), differential neural rendering, and gen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#25299;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#20540;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10070</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#20445;&#32467;&#26500;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Structure-Preserving Kernel Method for Learning Hamiltonian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#25299;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#20540;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#21253;&#21547;&#21704;&#23494;&#39039;&#21521;&#37327;&#22330;&#30340;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#35299;&#65292;&#22312;&#36825;&#19968;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#25216;&#26415;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#38656;&#35201;&#21253;&#21547;&#26799;&#24230;&#32447;&#24615;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#35777;&#26126;&#20102;&#24494;&#20998;&#20877;&#29616;&#23646;&#24615;&#21644;&#34920;&#31034;&#23450;&#29702;&#12290;&#20998;&#26512;&#20102;&#20445;&#32467;&#26500;&#26680;&#20272;&#35745;&#22120;&#21644;&#39640;&#26031;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#25552;&#20379;&#20351;&#29992;&#22266;&#23450;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#33391;&#24615;&#33021;&#24471;&#21040;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10070v1 Announce Type: cross  Abstract: A structure-preserving kernel ridge regression method is presented that allows the recovery of potentially high-dimensional and nonlinear Hamiltonian functions out of datasets made of noisy observations of Hamiltonian vector fields. The method proposes a closed-form solution that yields excellent numerical performances that surpass other techniques proposed in the literature in this setup. From the methodological point of view, the paper extends kernel regression methods to problems in which loss functions involving linear functions of gradients are required and, in particular, a differential reproducing property and a Representer Theorem are proved in this context. The relation between the structure-preserving kernel estimator and the Gaussian posterior mean estimator is analyzed. A full error analysis is conducted that provides convergence rates using fixed and adaptive regularization parameters. The good performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#24182;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.10063</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#24182;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;&#36830;&#32493;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#20840;&#20449;&#24687;&#21644;&#65288;&#21322;&#65289;&#24378;&#25932;&#21453;&#39304;&#12289;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#12289;&#19981;&#21516;&#32422;&#26463;&#20197;&#21450;&#31867;&#22411;&#30340;&#38543;&#26426;&#26597;&#35810;&#31561;&#22330;&#26223;&#12290;&#22312;&#38750;&#21333;&#35843;&#35774;&#32622;&#20013;&#32771;&#34385;&#30340;&#27599;&#20010;&#38382;&#39064;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#35201;&#20040;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#35777;&#26126;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#30340;&#31639;&#27861;&#65292;&#35201;&#20040;&#20855;&#26377;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#20854;&#20013; $\alpha$ &#26159;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#30456;&#24212;&#36817;&#20284;&#19978;&#30028;&#12290;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;8&#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340;7&#31181;&#20013;&#26159;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#21516;&#26102;&#19982;&#21097;&#20313;&#24773;&#20917;&#30340;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#30340;&#21322;&#24378;&#25932;&#21644;&#24378;&#25932;&#21453;&#39304;&#65292;&#25512;&#36827;&#20102;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10063v1 Announce Type: cross  Abstract: This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\alpha$-regret bounds or have better $\alpha$-regret bounds than the state of the art, where $\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10045</link><description>&lt;p&gt;
&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Dataset Distillation by Curvature Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20801;&#35768;&#23558;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#21407;&#22987;&#22823;&#23567;&#30340;&#20998;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20016;&#23500;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#33410;&#30465;&#26174;&#33879;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#39640;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;DD&#30340;&#19968;&#31181;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20351;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#26354;&#29575;&#27491;&#21017;&#21270;&#32435;&#20837;&#21040;&#31934;&#28860;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#27604;&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#35201;&#23569;&#24471;&#22810;&#12290;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#33021;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10045v1 Announce Type: new  Abstract: Dataset distillation (DD) allows datasets to be distilled to fractions of their original size while preserving the rich distributional information so that models trained on the distilled datasets can achieve a comparable accuracy while saving significant computational loads. Recent research in this area has been focusing on improving the accuracy of models trained on distilled datasets. In this paper, we aim to explore a new perspective of DD. We study how to embed adversarial robustness in distilled datasets, so that models trained on these datasets maintain the high accuracy and meanwhile acquire better adversarial robustness. We propose a new method that achieves this goal by incorporating curvature regularization into the distillation process with much less computational overhead than standard adversarial training. Extensive empirical experiments suggest that our method not only outperforms standard adversarial training on both accur
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#32467;&#21512;&#22312;&#24494;&#22411;X&#23556;&#32447;&#34893;&#23556;&#30456;&#20301;&#35782;&#21035;&#20013;&#21462;&#24471;&#31361;&#30772;&#65292;&#27169;&#22411;&#35757;&#32451;&#20943;&#23569;&#20102;&#23454;&#39564;&#25968;&#25454;&#26631;&#35760;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;CNN&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.10042</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#20934;&#30830;&#39640;&#25928;&#22320;&#35782;&#21035;&#24494;&#22411;XRD&#30456;&#20301;&#65306;&#20197;&#28909;&#28082;&#27969;&#20307;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Accurate and Data-Efficient Micro-XRD Phase Identification Using Multi-Task Learning: Application to Hydrothermal Fluids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10042
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#32467;&#21512;&#22312;&#24494;&#22411;X&#23556;&#32447;&#34893;&#23556;&#30456;&#20301;&#35782;&#21035;&#20013;&#21462;&#24471;&#31361;&#30772;&#65292;&#27169;&#22411;&#35757;&#32451;&#20943;&#23569;&#20102;&#23454;&#39564;&#25968;&#25454;&#26631;&#35760;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;CNN&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#39640;&#24230;&#25197;&#26354;&#30340;&#24494;&#22411;X&#23556;&#32447;&#34893;&#23556;&#65288;&#956;-XRD&#65289;&#22270;&#26679;&#30340;&#20998;&#26512;&#22312;&#28909;&#28082;&#29615;&#22659;&#20013;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26631;&#35760;&#30340;&#23454;&#39564;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26550;&#26500;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;MTL&#27169;&#22411;&#26469;&#35782;&#21035;&#956;-XRD&#22270;&#26679;&#20013;&#30340;&#30456;&#20301;&#20449;&#24687;&#65292;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#23454;&#39564;&#25968;&#25454;&#21644;&#25513;&#30422;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#38656;&#35201;&#12290; &#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MTL&#27169;&#22411;&#30456;&#27604;&#20110;&#20108;&#20803;&#20998;&#31867;CNN&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#23450;&#21046;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;MTL&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#35843;&#25972;&#20026;&#20998;&#26512;&#21407;&#22987;&#21644;&#26410;&#25513;&#30422;XRD&#22270;&#26679;&#30340;MTL&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;&#20998;&#26512;&#39044;&#22788;&#29702;&#25968;&#25454;&#30340;&#27169;&#22411;&#32039;&#23494;&#30340;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#24046;&#24322;&#26497;&#23567;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#20687;MTL&#36825;&#26679;&#30340;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10042v1 Announce Type: cross  Abstract: Traditional analysis of highly distorted micro-X-ray diffraction ({\mu}-XRD) patterns from hydrothermal fluid environments is a time-consuming process, often requiring substantial data preprocessing and labeled experimental data. This study demonstrates the potential of deep learning with a multitask learning (MTL) architecture to overcome these limitations. We trained MTL models to identify phase information in {\mu}-XRD patterns, minimizing the need for labeled experimental data and masking preprocessing steps. Notably, MTL models showed superior accuracy compared to binary classification CNNs. Additionally, introducing a tailored cross-entropy loss function improved MTL model performance. Most significantly, MTL models tuned to analyze raw and unmasked XRD patterns achieved close performance to models analyzing preprocessed data, with minimal accuracy differences. This work indicates that advanced deep learning architectures like MT
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MR-MT3&#27169;&#22411;&#26469;&#20943;&#23569;&#20048;&#22120;&#27844;&#28431;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#23384;&#20648;&#20445;&#30041;&#26426;&#21046;&#12289;&#20808;&#21069;&#20196;&#29260;&#37319;&#26679;&#21644;&#20196;&#29260;&#28151;&#27927;&#31561;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;Slakh2100&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.10024</link><description>&lt;p&gt;
MR-MT3:&#23384;&#20648;&#20445;&#30041;&#30340;&#22810;&#36712;&#38899;&#20048;&#36716;&#24405;&#20197;&#20943;&#23569;&#20048;&#22120;&#27844;&#28431;
&lt;/p&gt;
&lt;p&gt;
MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MR-MT3&#27169;&#22411;&#26469;&#20943;&#23569;&#20048;&#22120;&#27844;&#28431;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#23384;&#20648;&#20445;&#30041;&#26426;&#21046;&#12289;&#20808;&#21069;&#20196;&#29260;&#37319;&#26679;&#21644;&#20196;&#29260;&#28151;&#27927;&#31561;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;Slakh2100&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MT3&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;MT3&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20196;&#29260;&#30340;&#22810;&#20048;&#22120;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#27169;&#22411;&#12290;&#23613;&#31649;MT3&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#20294;&#23384;&#22312;&#20048;&#22120;&#27844;&#28431;&#38382;&#39064;&#65292;&#21363;&#36716;&#24405;&#22312;&#19981;&#21516;&#20048;&#22120;&#20043;&#38388;&#29255;&#27573;&#21270;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MR-MT3&#65292;&#20854;&#20013;&#21253;&#25324;&#23384;&#20648;&#20445;&#30041;&#26426;&#21046;&#12289;&#20808;&#21069;&#20196;&#29260;&#37319;&#26679;&#21644;&#20196;&#29260;&#28151;&#27927;&#31561;&#22686;&#24378;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;Slakh2100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#36215;&#22987;F1&#20998;&#25968;&#21644;&#20943;&#23569;&#30340;&#20048;&#22120;&#27844;&#28431;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22810;&#20048;&#22120;&#36716;&#24405;F1&#20998;&#25968;&#65292;&#36824;&#24341;&#20837;&#20102;&#35832;&#22914;&#20048;&#22120;&#27844;&#28431;&#27604;&#29575;&#21644;&#20048;&#22120;&#26816;&#27979;F1&#20998;&#25968;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#36716;&#24405;&#36136;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36890;&#36807;&#22312;&#21333;&#20048;&#22120;&#21333;&#22768;&#36947;&#25968;&#25454;&#38598;&#65288;&#22914;ComMU&#21644;NSynth&#65289;&#19978;&#35780;&#20272;MT3&#26469;&#35780;&#20272;&#22495;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10024v1 Announce Type: cross  Abstract: This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA) token-based multi-instrument automatic music transcription (AMT) model. Despite SOTA performance, MT3 has the issue of instrument leakage, where transcriptions are fragmented across different instruments. To mitigate this, we propose MR-MT3, with enhancements including a memory retention mechanism, prior token sampling, and token shuffling are proposed. These methods are evaluated on the Slakh2100 dataset, demonstrating improved onset F1 scores and reduced instrument leakage. In addition to the conventional multi-instrument transcription F1 score, new metrics such as the instrument leakage ratio and the instrument detection F1 score are introduced for a more comprehensive assessment of transcription quality. The study also explores the issue of domain overfitting by evaluating MT3 on single-instrument monophonic datasets such as ComMU and NSynth. The findings,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#38598;&#20998;&#31867;&#30340;&#32447;&#24615;&#26368;&#20248;&#36755;&#36816;&#23376;&#31354;&#38388;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#23884;&#20837;&#38598;&#21512;&#32467;&#26500;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#31354;&#38388;&#21464;&#24418;&#20013;&#30340;&#33021;&#21147;&#65292;&#20197;&#31616;&#21270;&#28857;&#38598;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10015</link><description>&lt;p&gt;
&#28857;&#38598;&#20998;&#31867;&#30340;&#32447;&#24615;&#26368;&#20248;&#36755;&#36816;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Linear optimal transport subspaces for point set classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#38598;&#20998;&#31867;&#30340;&#32447;&#24615;&#26368;&#20248;&#36755;&#36816;&#23376;&#31354;&#38388;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#23884;&#20837;&#38598;&#21512;&#32467;&#26500;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#31354;&#38388;&#21464;&#24418;&#20013;&#30340;&#33021;&#21147;&#65292;&#20197;&#31616;&#21270;&#28857;&#38598;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28857;&#38598;&#20013;&#23398;&#20064;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#21407;&#29983;&#30340;&#12289;&#26080;&#24207;&#30340;&#12289;&#32622;&#25442;&#19981;&#21464;&#30340;&#38598;&#21512;&#32467;&#26500;&#31354;&#38388;&#24456;&#38590;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#24418;&#21464;&#19979;&#36827;&#34892;&#28857;&#38598;&#20998;&#31867;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#32463;&#21382;&#29305;&#23450;&#31867;&#22411;&#31354;&#38388;&#24418;&#21464;&#30340;&#28857;&#38598;&#65292;&#29305;&#21035;&#24378;&#35843;&#21253;&#21547;&#20223;&#23556;&#24418;&#21464;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32447;&#24615;&#26368;&#20248;&#36755;&#36816;&#65288;LOT&#65289;&#21464;&#25442;&#26469;&#33719;&#24471;&#38598;&#21512;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32447;&#24615;&#23884;&#20837;&#12290;&#21033;&#29992;LOT&#21464;&#25442;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#36866;&#24212;&#28857;&#38598;&#21464;&#21270;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20984;&#25968;&#25454;&#31354;&#38388;&#65292;&#26377;&#25928;&#31616;&#21270;&#20102;&#28857;&#38598;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LOT&#31354;&#38388;&#20013;&#37319;&#29992;&#26368;&#36817;&#23376;&#31354;&#38388;&#31639;&#27861;&#65292;&#34920;&#29616;&#20986;&#26631;&#31614;&#25928;&#29575;&#12289;&#38750;&#36845;&#20195;&#34892;&#20026;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10015v1 Announce Type: cross  Abstract: Learning from point sets is an essential component in many computer vision and machine learning applications. Native, unordered, and permutation invariant set structure space is challenging to model, particularly for point set classification under spatial deformations. Here we propose a framework for classifying point sets experiencing certain types of spatial deformations, with a particular emphasis on datasets featuring affine deformations. Our approach employs the Linear Optimal Transport (LOT) transform to obtain a linear embedding of set-structured data. Utilizing the mathematical properties of the LOT transform, we demonstrate its capacity to accommodate variations in point sets by constructing a convex data space, effectively simplifying point set classification problems. Our method, which employs a nearest-subspace algorithm in the LOT space, demonstrates label efficiency, non-iterative behavior, and requires no hyper-parameter
&lt;/p&gt;</description></item><item><title>LyZNet&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;Python&#24037;&#20855;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31070;&#32463;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;SMT&#27714;&#35299;&#22120;&#39564;&#35777;&#65292;&#33021;&#22815;&#25552;&#20379;&#39564;&#35777;&#33539;&#22260;&#25509;&#36817;&#21560;&#24341;&#21147;&#22495;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.10013</link><description>&lt;p&gt;
LyZNet: &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#21644;&#39564;&#35777;&#31070;&#32463;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#21644;&#21560;&#24341;&#21147;&#21306;&#22495;&#30340;&#36731;&#37327;&#32423;Python&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LyZNet: A Lightweight Python Tool for Learning and Verifying Neural Lyapunov Functions and Regions of Attraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10013
&lt;/p&gt;
&lt;p&gt;
LyZNet&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;Python&#24037;&#20855;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31070;&#32463;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;SMT&#27714;&#35299;&#22120;&#39564;&#35777;&#65292;&#33021;&#22815;&#25552;&#20379;&#39564;&#35777;&#33539;&#22260;&#25509;&#36817;&#21560;&#24341;&#21147;&#22495;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;Python&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#31070;&#32463;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#23398;&#20064;&#21644;&#39564;&#35777;&#65292;&#29992;&#20110;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25552;&#20986;&#30340;&#24037;&#20855;&#21517;&#20026;LyZNet&#65292;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#23398;&#20064;&#31070;&#32463;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#26469;&#35299;&#20915;&#31062;&#21338;&#22827;&#26041;&#31243;&#65292;&#24182;&#20351;&#29992;&#21487;&#28385;&#36275;&#24615;&#27169;&#29702;&#35770;&#65288;SMT&#65289;&#27714;&#35299;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#24037;&#20855;&#19982;&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#21306;&#21035;&#22312;&#20110;&#20854;&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#21560;&#24341;&#21147;&#22495;&#38468;&#36817;&#30340;&#39564;&#35777;&#21306;&#22495;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#32534;&#30721;&#21040;PINN&#26041;&#27861;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#36890;&#36807;&#25317;&#25265;&#28508;&#22312;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#36136;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20984;&#20248;&#21270;&#22833;&#36133;&#25429;&#25417;&#21560;&#24341;&#21147;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#35777;&#26126;&#26356;&#25104;&#21151;&#12290;&#35813;&#24037;&#20855;&#36824;&#25552;&#20379;&#32806;&#21512;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#33258;&#21160;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10013v1 Announce Type: cross  Abstract: In this paper, we describe a lightweight Python framework that provides integrated learning and verification of neural Lyapunov functions for stability analysis. The proposed tool, named LyZNet, learns neural Lyapunov functions using physics-informed neural networks (PINNs) to solve Zubov's equation and verifies them using satisfiability modulo theories (SMT) solvers. What distinguishes this tool from others in the literature is its ability to provide verified regions of attraction close to the domain of attraction. This is achieved by encoding Zubov's partial differential equation (PDE) into the PINN approach. By embracing the non-convex nature of the underlying optimization problems, we demonstrate that in cases where convex optimization, such as semidefinite programming, fails to capture the domain of attraction, our neural network framework proves more successful. The tool also offers automatic decomposition of coupled nonlinear sy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21327;&#20316;&#38382;&#39064;&#35299;&#20915;&#20013;&#24418;&#25104;&#26377;&#25928;&#32676;&#32452;&#65292;&#24182;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#21644;&#20914;&#31361;&#20943;&#23569;&#30340;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.10006</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#20316;&#38382;&#39064;&#35299;&#20915;&#20013;&#26377;&#25928;&#32676;&#32452;&#24418;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Enhanced Reinforcement Learning for Effective Group Formation in Collaborative Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21327;&#20316;&#38382;&#39064;&#35299;&#20915;&#20013;&#24418;&#25104;&#26377;&#25928;&#32676;&#32452;&#65292;&#24182;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#21644;&#20914;&#31361;&#20943;&#23569;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#30340;&#29615;&#22659;&#20013;&#24418;&#25104;&#26377;&#25928;&#32676;&#32452;&#30340;&#25361;&#25112;&#12290;&#35748;&#35782;&#21040;&#20154;&#38469;&#20114;&#21160;&#30340;&#22797;&#26434;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#24517;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20174;&#25968;&#25454;&#38598;&#26500;&#24314;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#21442;&#19982;&#32773;&#65292;&#36793;&#34920;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#21442;&#19982;&#32773;&#27010;&#24565;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#20195;&#29702;&#65292;&#26088;&#22312;&#23398;&#20064;&#21453;&#26144;&#26377;&#25928;&#32676;&#20307;&#21160;&#24577;&#30340;&#26368;&#20339;&#22270;&#32467;&#26500;&#12290;&#32858;&#31867;&#25216;&#26415;&#34987;&#29992;&#26469;&#26681;&#25454;&#23398;&#20064;&#30340;&#22270;&#30028;&#23450;&#28165;&#26224;&#30340;&#32676;&#32452;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#22522;&#20110;&#35780;&#20272;&#25351;&#26631;&#21644;&#22270;&#27979;&#37327;&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#32676;&#32452;&#25928;&#29575;&#30340;&#28508;&#22312;&#25913;&#36827;&#21644;&#20914;&#31361;&#20107;&#20214;&#30340;&#20943;&#23569;&#25552;&#20379;&#35265;&#35299;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10006v1 Announce Type: cross  Abstract: This study addresses the challenge of forming effective groups in collaborative problem-solving environments. Recognizing the complexity of human interactions and the necessity for efficient collaboration, we propose a novel approach leveraging graph theory and reinforcement learning. Our methodology involves constructing a graph from a dataset where nodes represent participants, and edges signify the interactions between them. We conceptualize each participant as an agent within a reinforcement learning framework, aiming to learn an optimal graph structure that reflects effective group dynamics. Clustering techniques are employed to delineate clear group structures based on the learned graph. Our approach provides theoretical solutions based on evaluation metrics and graph measurements, offering insights into potential improvements in group effectiveness and reductions in conflict incidences. This research contributes to the fields of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AD3&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#21160;&#20316;&#29983;&#25104;&#22120;(IAG)&#23398;&#20064;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#38544;&#24335;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#21306;&#20998;&#20219;&#21153;&#19981;&#30456;&#20851;&#32452;&#20214;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09976</link><description>&lt;p&gt;
AD3:&#38544;&#24335;&#21160;&#20316;&#26159;&#19990;&#30028;&#27169;&#22411;&#21306;&#20998;&#19981;&#21516;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AD3&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#21160;&#20316;&#29983;&#25104;&#22120;(IAG)&#23398;&#20064;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#38544;&#24335;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#21306;&#20998;&#20219;&#21153;&#19981;&#30456;&#20851;&#32452;&#20214;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#21306;&#20998;&#35270;&#35273;&#25511;&#21046;&#20013;&#30340;&#20219;&#21153;&#19981;&#30456;&#20851;&#24178;&#25200;&#22240;&#32032;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24322;&#36136;&#24178;&#25200;&#22240;&#32032;&#65292;&#22914;&#22024;&#26434;&#30340;&#32972;&#26223;&#35270;&#39057;&#19978;&#65292;&#23545;&#23494;&#20999;&#31867;&#20284;&#21487;&#25511;&#21046;&#20195;&#29702;&#30340;&#22343;&#36136;&#24178;&#25200;&#22240;&#32032;&#30340;&#30740;&#31350;&#24456;&#23569;&#65292;&#36825;&#32473;&#29616;&#26377;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38544;&#24335;&#21160;&#20316;&#29983;&#25104;&#22120;&#65288;IAG&#65289;&#26469;&#23398;&#20064;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#30340;&#38544;&#24335;&#21160;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#24335;&#21160;&#20316;&#36890;&#30693;&#30340;&#22810;&#26679;&#21270;&#35270;&#35273;&#24178;&#25200;&#22240;&#32032;&#21306;&#20998;&#22120;&#65288;AD3&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;IAG&#25512;&#26029;&#30340;&#21160;&#20316;&#26469;&#35757;&#32451;&#20998;&#31163;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#38544;&#24335;&#21160;&#20316;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32972;&#26223;&#24178;&#25200;&#22240;&#32032;&#30340;&#34892;&#20026;&#65292;&#26377;&#21161;&#20110;&#21306;&#20998;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#20248;&#21270;&#20219;&#21153;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#20869;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09976v1 Announce Type: new  Abstract: Model-based methods have significantly contributed to distinguishing task-irrelevant distractors for visual control. However, prior research has primarily focused on heterogeneous distractors like noisy background videos, leaving homogeneous distractors that closely resemble controllable agents largely unexplored, which poses significant challenges to existing methods. To tackle this problem, we propose Implicit Action Generator (IAG) to learn the implicit actions of visual distractors, and present a new algorithm named implicit Action-informed Diverse visual Distractors Distinguisher (AD3), that leverages the action inferred by IAG to train separated world models. Implicit actions effectively capture the behavior of background distractors, aiding in distinguishing the task-irrelevant components, and the agent can optimize the policy within the task-relevant state space. Our method achieves superior performance on various visual control 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MKDE&#21644;&#32858;&#31867;&#25552;&#21462;&#33337;&#33334;&#21040;&#36798;&#36718;&#24275;&#65292;&#34701;&#21512;&#22810;&#25968;&#25454;&#28304;&#65292;&#24182;&#37319;&#29992;TCN&#26694;&#26550;&#23398;&#20064;&#38544;&#34255;&#21040;&#36798;&#27169;&#24335;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#33337;&#33334;&#21040;&#24341;&#33322;&#21306;&#30340;&#21040;&#36798;&#26102;&#38388;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09969</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#25968;&#25454;&#34701;&#21512;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#33337;&#33334;&#21040;&#24341;&#33322;&#21306;&#30340;&#21040;&#36798;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data Fusion and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09969
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MKDE&#21644;&#32858;&#31867;&#25552;&#21462;&#33337;&#33334;&#21040;&#36798;&#36718;&#24275;&#65292;&#34701;&#21512;&#22810;&#25968;&#25454;&#28304;&#65292;&#24182;&#37319;&#29992;TCN&#26694;&#26550;&#23398;&#20064;&#38544;&#34255;&#21040;&#36798;&#27169;&#24335;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#33337;&#33334;&#21040;&#24341;&#33322;&#21306;&#30340;&#21040;&#36798;&#26102;&#38388;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#25968;&#25454;&#34701;&#21512;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#33337;&#33334;&#21040;&#36798;&#24341;&#33322;&#21306;&#30340;&#21040;&#36798;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#22810;&#20803;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;MKDE&#65289;&#21644;&#32858;&#31867;&#25552;&#21462;&#33337;&#33334;&#21040;&#36798;&#36718;&#24275;&#12290;&#20854;&#27425;&#65292;&#22312;&#28508;&#22312;&#29305;&#24449;&#25552;&#21462;&#20043;&#21069;&#34701;&#21512;&#20102;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#21253;&#25324;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#65288;AIS&#65289;&#12289;&#24341;&#33322;&#39044;&#35746;&#20449;&#24687;&#21644;&#27668;&#35937;&#25968;&#25454;&#12290;&#31532;&#19977;&#65292;&#22312;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#21512;&#27531;&#24046;&#26426;&#21046;&#30340;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#33337;&#33334;&#30340;&#38544;&#34255;&#21040;&#36798;&#27169;&#24335;&#12290;&#22312;&#26032;&#21152;&#22369;&#30340;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#20197;&#19979;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65306;1&#65289;&#24341;&#33322;&#39044;&#35746;&#20449;&#24687;&#21644;&#27668;&#35937;&#25968;&#25454;&#30340;&#34701;&#21512;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;&#24341;&#33322;&#39044;&#35746;&#20449;&#24687;&#24433;&#21709;&#26356;&#26174;&#33879;&#65307;2&#65289;&#20351;&#29992;&#31163;&#25955;&#23884;&#20837;&#36827;&#34892;&#27668;&#35937;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09969v1 Announce Type: new  Abstract: This paper investigates the prediction of vessels' arrival time to the pilotage area using multi-data fusion and deep learning approaches. Firstly, the vessel arrival contour is extracted based on Multivariate Kernel Density Estimation (MKDE) and clustering. Secondly, multiple data sources, including Automatic Identification System (AIS), pilotage booking information, and meteorological data, are fused before latent feature extraction. Thirdly, a Temporal Convolutional Network (TCN) framework that incorporates a residual mechanism is constructed to learn the hidden arrival patterns of the vessels. Extensive tests on two real-world data sets from Singapore have been conducted and the following promising results have been obtained: 1) fusion of pilotage booking information and meteorological data improves the prediction accuracy, with pilotage booking information having a more significant impact; 2) using discrete embedding for the meteoro
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#25554;&#20540;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31354;&#38388;&#25554;&#20540;&#31639;&#27861;&#65292;&#26500;&#24314;&#20102;&#32654;&#22269;&#26412;&#22303;&#28201;&#24230;-&#28145;&#24230;&#22320;&#22270;&#65292;&#21516;&#26102;&#31934;&#30830;&#39044;&#27979;&#22320;&#19979;&#28201;&#24230;&#12289;&#22320;&#34920;&#28909;&#27969;&#21644;&#23721;&#30707;&#23548;&#28909;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09961</link><description>&lt;p&gt;
&#21033;&#29992;&#25554;&#20540;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;InterPIGNN&#65289;&#24314;&#31435;&#32654;&#22269;&#26412;&#22303;&#28909;&#22320;&#29699;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Thermal Earth Model for the Conterminous United States Using an Interpolative Physics-Informed Graph Neural Network (InterPIGNN)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09961
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#25554;&#20540;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31354;&#38388;&#25554;&#20540;&#31639;&#27861;&#65292;&#26500;&#24314;&#20102;&#32654;&#22269;&#26412;&#22303;&#28201;&#24230;-&#28145;&#24230;&#22320;&#22270;&#65292;&#21516;&#26102;&#31934;&#30830;&#39044;&#27979;&#22320;&#19979;&#28201;&#24230;&#12289;&#22320;&#34920;&#28909;&#27969;&#21644;&#23721;&#30707;&#23548;&#28909;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31354;&#38388;&#25554;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#32654;&#22269;&#26412;&#22303;&#30340;&#28201;&#24230;-&#28145;&#24230;&#22320;&#22270;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21516;&#26102;&#39044;&#27979;&#22320;&#19979;&#28201;&#24230;&#12289;&#22320;&#34920;&#28909;&#27969;&#21644;&#23721;&#30707;&#23548;&#28909;&#29575;&#65292;&#20197;&#36817;&#20284;&#28385;&#36275;&#19977;&#32500;&#20256;&#28909;&#23450;&#24459;&#12290;&#38500;&#20102;&#24213;&#23380;&#28201;&#24230;&#27979;&#37327;&#20540;&#65292;&#25105;&#20204;&#36824;&#23558;&#20854;&#20182;&#29289;&#29702;&#37327;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#65292;&#20363;&#22914;&#28145;&#24230;&#12289;&#22320;&#29702;&#22352;&#26631;&#12289;&#28023;&#25300;&#12289;&#27785;&#31215;&#29289;&#21402;&#24230;&#12289;&#30913;&#24322;&#24120;&#12289;&#37325;&#21147;&#24322;&#24120;&#12289;&#25918;&#23556;&#24615;&#20803;&#32032;&#30340;&#947;&#23556;&#32447;&#36890;&#37327;&#12289;&#22320;&#38663;&#27963;&#21160;&#21644;&#30005;&#23548;&#29575;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#28145;&#24230;&#20026;0-7&#20844;&#37324;&#65292;&#38388;&#38548;&#20026;1&#20844;&#37324;&#65292;&#31354;&#38388;&#20998;&#36776;&#29575;&#20026;&#27599;&#20010;18&#24179;&#26041;&#20844;&#37324;&#26684;&#32593;&#21333;&#20803;&#30340;&#22320;&#34920;&#28909;&#27969;&#12289;&#28201;&#24230;&#21644;&#23548;&#28909;&#29575;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#28201;&#24230;&#12289;&#22320;&#34920;&#28909;&#27969;&#21644;&#23548;&#28909;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09961v1 Announce Type: cross  Abstract: This study presents a data-driven spatial interpolation algorithm based on physics-informed graph neural networks used to develop national temperature-at-depth maps for the conterminous United States. The model was trained to approximately satisfy the three-dimensional heat conduction law by simultaneously predicting subsurface temperature, surface heat flow, and rock thermal conductivity. In addition to bottomhole temperature measurements, we incorporated other physical quantities as model inputs, such as depth, geographic coordinates, elevation, sediment thickness, magnetic anomaly, gravity anomaly, gamma-ray flux of radioactive elements, seismicity, and electric conductivity. We constructed surface heat flow, and temperature and thermal conductivity predictions for depths of 0-7 km at an interval of 1 km with spatial resolution of 18 km$^2$ per grid cell. Our model showed superior temperature, surface heat flow and thermal conductiv
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;GNN&#35780;&#20272;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27979;&#35797;&#26102;&#38388;&#22270;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#34913;&#37327;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#26080;&#26631;&#31614;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09953</link><description>&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#22270;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#22312;&#32447;GNN&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Online GNN Evaluation Under Test-time Graph Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09953
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;GNN&#35780;&#20272;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27979;&#35797;&#26102;&#38388;&#22270;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#34913;&#37327;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#26080;&#26631;&#31614;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#30340;&#34920;&#29616;&#26159;&#21487;&#38752;&#30340;GNN&#22312;&#32447;&#37096;&#32626;&#21644;&#26381;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30001;&#20110;&#32570;&#20047;&#27979;&#35797;&#33410;&#28857;&#26631;&#31614;&#21644;&#26410;&#30693;&#28508;&#22312;&#30340;&#35757;&#32451;-&#27979;&#35797;&#22270;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#65292;&#20256;&#32479;&#27169;&#22411;&#35780;&#20272;&#22312;&#35745;&#31639;&#24615;&#33021;&#25351;&#26631;&#65288;&#22914;&#27979;&#35797;&#38169;&#35823;&#65289;&#21644;&#27979;&#37327;&#22270;&#25968;&#25454;&#32423;&#21035;&#24046;&#24322;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#27979;&#35797;&#26102;&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#22270;&#20173;&#26410;&#34987;&#35266;&#23519;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#22312;&#32447;GNN&#35780;&#20272;&#65292;&#26088;&#22312;&#20026;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;GNN&#22312;&#27979;&#35797;&#26102;&#38388;&#22270;&#20998;&#24067;&#36716;&#31227;&#19979;&#26377;&#25928;&#25512;&#24191;&#21040;&#30495;&#23454;&#19990;&#30028;&#26080;&#26631;&#31614;&#22270;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#34892;&#20026;&#24046;&#24322;&#35780;&#20998;&#65292;&#31216;&#20026;LeBeD&#65292;&#26469;&#20272;&#35745;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#27867;&#21270;&#38169;&#35823;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#37325;&#26032;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09953v1 Announce Type: new  Abstract: Evaluating the performance of a well-trained GNN model on real-world graphs is a pivotal step for reliable GNN online deployment and serving. Due to a lack of test node labels and unknown potential training-test graph data distribution shifts, conventional model evaluation encounters limitations in calculating performance metrics (e.g., test error) and measuring graph data-level discrepancies, particularly when the training graph used for developing GNNs remains unobserved during test time. In this paper, we study a new research problem, online GNN evaluation, which aims to provide valuable insights into the well-trained GNNs's ability to effectively generalize to real-world unlabeled graphs under the test-time graph distribution shifts. Concretely, we develop an effective learning behavior discrepancy score, dubbed LeBeD, to estimate the test-time generalization errors of well-trained GNN models. Through a novel GNN re-training strategy
&lt;/p&gt;</description></item><item><title>&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#29305;&#24449;&#32858;&#21512;&#32593;&#32476;GLIMS&#36827;&#34892;3D&#33041;&#32959;&#30244;&#20998;&#21106;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;Swin Transformer&#22359;&#25913;&#21892;&#20102;&#20840;&#23616;&#29305;&#24449;&#32858;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.09942</link><description>&lt;p&gt;
&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#29305;&#24449;&#32858;&#21512;&#32593;&#32476;&#29992;&#20110;3D&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#29305;&#24449;&#32858;&#21512;&#32593;&#32476;GLIMS&#36827;&#34892;3D&#33041;&#32959;&#30244;&#20998;&#21106;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;Swin Transformer&#22359;&#25913;&#21892;&#20102;&#20840;&#23616;&#29305;&#24449;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Glioblastoma&#26159;&#19968;&#31181;&#39640;&#24230;&#20405;&#30053;&#24615;&#21644;&#24694;&#24615;&#30340;&#33041;&#32959;&#30244;&#31867;&#22411;&#65292;&#38656;&#35201;&#21450;&#26089;&#35786;&#26029;&#21644;&#21450;&#26102;&#24178;&#39044;&#12290;&#30001;&#20110;&#20854;&#22806;&#35266;&#19978;&#30340;&#24322;&#36136;&#24615;&#65292;&#24320;&#21457;&#33258;&#21160;&#26816;&#27979;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#21307;&#30103;&#20445;&#20581;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#20197;&#26377;&#25928;&#35786;&#26029;&#21644;&#35780;&#20272;&#33041;&#32959;&#30244;&#12290;Brain Tumor Segmentation Challenge (BraTS)&#26159;&#19968;&#20010;&#24320;&#21457;&#21644;&#35780;&#20272;&#20351;&#29992;&#39640;&#36136;&#37327;&#20020;&#24202;&#33719;&#24471;&#30340;MRI&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#20998;&#26512;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#30340;&#24179;&#21488;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23610;&#24230;&#12289;&#27880;&#24847;&#21147;&#24341;&#23548;&#21644;&#28151;&#21512;U-Net&#24418;&#29366;&#27169;&#22411;--GLIMS--&#22312;&#19977;&#20010;&#21306;&#22495;&#65306;&#22686;&#24378;&#32959;&#30244;&#65288;ET&#65289;&#12289;&#32959;&#30244;&#26680;&#24515;&#65288;TC&#65289;&#21644;&#25972;&#20010;&#32959;&#30244;&#65288;WT&#65289;&#20013;&#25191;&#34892;3D&#33041;&#32959;&#30244;&#20998;&#21106;&#12290;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#22312;&#39640;&#20998;&#36776;&#29575;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#32858;&#21512;&#65292;&#32780;Swin Transformer&#22359;&#25913;&#21892;&#20102;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09942v1 Announce Type: cross  Abstract: Glioblastoma is a highly aggressive and malignant brain tumor type that requires early diagnosis and prompt intervention. Due to its heterogeneity in appearance, developing automated detection approaches is challenging. To address this challenge, Artificial Intelligence (AI)-driven approaches in healthcare have generated interest in efficiently diagnosing and evaluating brain tumors. The Brain Tumor Segmentation Challenge (BraTS) is a platform for developing and assessing automated techniques for tumor analysis using high-quality, clinically acquired MRI data. In our approach, we utilized a multi-scale, attention-guided and hybrid U-Net-shaped model -- GLIMS -- to perform 3D brain tumor segmentation in three regions: Enhancing Tumor (ET), Tumor Core (TC), and Whole Tumor (WT). The multi-scale feature extraction provides better contextual feature aggregation in high resolutions and the Swin Transformer blocks improve the global feature 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#23545;&#25163;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#23545;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09940</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25163;&#30340;&#32852;&#37030;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#23545;&#25163;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#23545;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated Reinforcement Learning (FRL)&#20801;&#35768;&#22810;&#20010;&#20195;&#29702;&#20849;&#21516;&#26500;&#24314;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#21407;&#22987;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#20195;&#29702;&#20013;&#21482;&#26377;&#23569;&#37096;&#20998;&#26159;&#23545;&#25163;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#23545;&#25163;&#20195;&#29702;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#20219;&#24847;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24418;&#25104;&#20102;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#21270;&#30340;&#39318;&#20010;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#25163;&#30340;&#24377;&#24615;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;$\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$&#65292;&#20854;&#20013;$N$&#26159;&#20195;&#29702;&#30340;&#24635;&#25968;&#65292;$f$&#26159;&#23545;&#25163;&#20195;&#29702;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09940v1 Announce Type: cross  Abstract: Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories. However, if a small fraction of these agents are adversarial, it can lead to catastrophic results. We propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server. Under this setting, our results form the first global convergence guarantees with general parametrization. These results demonstrate resilience with adversaries, while achieving sample complexity of order $\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$, where $N$ is the total number of agents and $f$ is the number of adversarial agents.
&lt;/p&gt;</description></item><item><title>QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.09930</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#36890;&#36807;&#20540;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09930
&lt;/p&gt;
&lt;p&gt;
QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#34920;&#29616;&#20986;&#36866;&#24212;&#24847;&#22806;&#24773;&#20917;&#30340;&#24191;&#27867;&#34892;&#20026;&#35889;&#12290;&#36807;&#21435;&#21313;&#24180;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#36820;&#22238;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;QDAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28436;&#21592;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#25191;&#34892;&#22810;&#26679;&#24615;&#25216;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#26080;&#32541;&#32479;&#19968;&#20102;&#20004;&#20010;&#35780;&#35770;&#23478;&#12290;&#19982;&#20854;&#20182;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;QDAC&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#36816;&#21160;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#26679;&#24615;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>FedComLoc&#21033;&#29992;Scaffnew&#31639;&#27861;&#30340;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#21387;&#32553;&#21644;&#26412;&#22320;&#35757;&#32451;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2403.09904</link><description>&lt;p&gt;
FedComLoc: &#31232;&#30095;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09904
&lt;/p&gt;
&lt;p&gt;
FedComLoc&#21033;&#29992;Scaffnew&#31639;&#27861;&#30340;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#21387;&#32553;&#21644;&#26412;&#22320;&#35757;&#32451;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#20854;&#20801;&#35768;&#24322;&#26500;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#22788;&#29702;&#20854;&#31169;&#26377;&#25968;&#25454;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20114;&#21160;&#65292;&#21516;&#26102;&#23562;&#37325;&#38544;&#31169;&#30340;&#29420;&#29305;&#29305;&#28857;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#20102;&#21019;&#26032;&#30340;Scaffnew&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#22312;FL&#20013;&#22823;&#22823;&#25512;&#21160;&#20102;&#36890;&#20449;&#22797;&#26434;&#24615;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FedComLoc&#65288;&#32852;&#37030;&#21387;&#32553;&#21644;&#26412;&#22320;&#35757;&#32451;&#65289;&#65292;&#23558;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#21387;&#32553;&#38598;&#25104;&#21040;Scaffnew&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#27969;&#34892;&#30340;TopK&#21387;&#32553;&#22120;&#21644;&#37327;&#21270;&#65292;&#23427;&#22312;&#22823;&#24133;&#20943;&#23569;&#24322;&#26500;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09904v1 Announce Type: cross  Abstract: Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is \emph{Local Training}, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative \emph{Scaffnew} algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into \emph{Scaffnew} to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHERD&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26816;&#27979;&#26131;&#21463;&#25915;&#20987;&#33410;&#28857;&#65292;&#20174;&#32780;&#22312;&#22270;&#36755;&#20837;&#20013;&#23454;&#29616;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09901</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#25511;&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#23376;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Subgraph Learning by Monitoring Early Training Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHERD&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26816;&#27979;&#26131;&#21463;&#25915;&#20987;&#33410;&#28857;&#65292;&#20174;&#32780;&#22312;&#22270;&#36755;&#20837;&#20013;&#23454;&#29616;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#25991;:2403.09901v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22240;&#22312;&#22270;&#23398;&#20064;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26131;&#21463;&#25915;&#20987;&#30340;&#33410;&#28857;&#65292;&#32473;&#20915;&#31574;&#21046;&#23450;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#40065;&#26834;&#30340;&#22270;&#25688;&#35201;&#38656;&#27714;&#22312;&#20110;&#23545;&#25239;&#24615;&#25361;&#25112;&#20250;&#23548;&#33268;&#25915;&#20987;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#25216;&#26415;SHERD (&#36890;&#36807;&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#36317;&#31163;&#36827;&#34892;&#23376;&#22270;&#23398;&#20064;)&#26469;&#35299;&#20915;&#22270;&#36755;&#20837;&#20013;&#30340;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;SHERD&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#23618;&#20449;&#24687;&#65292;&#36890;&#36807;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26469;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#26399;&#38388;&#26131;&#21463;&#25915;&#20987;&#30340;&#33410;&#28857;&#12290;&#35813;&#26041;&#27861;&#35782;&#21035;&#20986;"&#26131;&#21463;&#25915;&#20987;&#30340;(&#22351;)"&#33410;&#28857;&#24182;&#31227;&#38500;&#36825;&#20123;&#33410;&#28857;&#65292;&#24418;&#25104;&#19968;&#20010;&#40065;&#26834;&#30340;&#23376;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09901v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have attracted significant attention for their outstanding performance in graph learning and node classification tasks. However, their vulnerability to adversarial attacks, particularly through susceptible nodes, poses a challenge in decision-making. The need for robust graph summarization is evident in adversarial challenges resulting from the propagation of attacks throughout the entire graph. In this paper, we address both performance and adversarial robustness in graph input by introducing the novel technique SHERD (Subgraph Learning Hale through Early Training Representation Distances). SHERD leverages information from layers of a partially trained graph convolutional network (GCN) to detect susceptible nodes during adversarial attacks using standard distance metrics. The method identifies "vulnerable (bad)" nodes and removes such nodes to form a robust subgraph while maintaining node classification perf
&lt;/p&gt;</description></item><item><title>TimeMachine&#26159;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;Mamba&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36890;&#36807;&#20840;&#38754;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#39044;&#27979;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09898</link><description>&lt;p&gt;
TimeMachine: &#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20215;&#20540;&#30456;&#24403;&#20110;4&#26465;&#30524;&#38236;&#34503;
&lt;/p&gt;
&lt;p&gt;
TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09898
&lt;/p&gt;
&lt;p&gt;
TimeMachine&#26159;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;Mamba&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36890;&#36807;&#20840;&#38754;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#39044;&#27979;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30001;&#20110;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12289;&#23454;&#29616;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#32780;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TimeMachine&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;Mamba&#65292;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;TimeMachine&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#20135;&#29983;&#26174;&#33879;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#24182;&#21033;&#29992;&#21019;&#26032;&#30340;&#25972;&#21512;&#22235;&#37325;Mamba&#26550;&#26500;&#26469;&#32479;&#19968;&#22788;&#29702;&#36890;&#36947;&#28151;&#21512;&#21644;&#36890;&#36947;&#29420;&#31435;&#24773;&#20917;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20840;&#23616;&#21644;&#26412;&#22320;&#24773;&#22659;&#22312;&#19981;&#21516;&#23610;&#24230;&#19979;&#36827;&#34892;&#26377;&#25928;&#20869;&#23481;&#36873;&#25321;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TimeMachine&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09898v1 Announce Type: new  Abstract: Long-term time-series forecasting remains challenging due to the difficulty in capturing long-term dependencies, achieving linear scalability, and maintaining computational efficiency. We introduce TimeMachine, an innovative model that leverages Mamba, a state-space model, to capture long-term dependencies in multivariate time series data while maintaining linear scalability and small memory footprints. TimeMachine exploits the unique properties of time series data to produce salient contextual cues at multi-scales and leverage an innovative integrated quadruple-Mamba architecture to unify the handling of channel-mixing and channel-independence situations, thus enabling effective selection of contents for prediction against global and local contexts at different scales. Experimentally, TimeMachine achieves superior performance in prediction accuracy, scalability, and memory efficiency, as extensively validated using benchmark datasets. C
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#26080;&#38480;&#28145;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#19979;&#30340;&#32553;&#25918;ResNet&#65292;&#25512;&#23548;&#20986;&#20102;&#22312;&#22343;&#22330;&#26497;&#38480;&#20013;&#27867;&#21270;&#30028;&#38480;&#30340;&#20840;&#23616;&#19979;&#30028;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#21160;&#24577;&#36319;&#36394;&#65292;&#20026;&#28145;&#24230;ResNet&#30340;&#27867;&#21270;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09889</link><description>&lt;p&gt;
&#22312;&#22343;&#22330;&#26497;&#38480;&#20013;&#32553;&#25918;&#30340;&#28145;&#24230;ResNets&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of Scaled Deep ResNets in the Mean-Field Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#26080;&#38480;&#28145;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#19979;&#30340;&#32553;&#25918;ResNet&#65292;&#25512;&#23548;&#20986;&#20102;&#22312;&#22343;&#22330;&#26497;&#38480;&#20013;&#27867;&#21270;&#30028;&#38480;&#30340;&#20840;&#23616;&#19979;&#30028;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#21160;&#24577;&#36319;&#36394;&#65292;&#20026;&#28145;&#24230;ResNet&#30340;&#27867;&#21270;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;ResNet&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#28145;&#24230;ResNet&#30340;&#27867;&#21270;&#29305;&#24615;&#22312;&#25042;&#24816;&#35757;&#32451;&#38454;&#27573;&#20043;&#22806;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#28145;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#19979;&#30340;\emph{&#32553;&#25918;}ResNet&#65292;&#20854;&#20013;&#26799;&#24230;&#27969;&#34987;&#25551;&#36848;&#20026;&#22823;&#31070;&#32463;&#32593;&#32476;&#26497;&#38480;&#19979;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21363;\emph{&#22343;&#22330;}&#26497;&#38480;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#25512;&#23548;&#20986;&#27867;&#21270;&#30028;&#38480;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#38656;&#35201;&#20174;&#25042;&#24816;&#35757;&#32451;&#38454;&#27573;&#37319;&#29992;&#30340;&#20256;&#32479;&#26102;&#38388;&#19981;&#21464;Gram&#30697;&#38453;&#36716;&#21464;&#20026;&#19968;&#20010;&#26102;&#38388;&#21464;&#37327;&#12289;&#20381;&#36182;&#20110;&#20998;&#24067;&#30340;&#29256;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#20840;&#23616;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36861;&#36394;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#21160;&#24577;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#32463;&#39564;&#35823;&#24046;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#20272;&#35745;&#20102;KL&#25955;&#24230;&#22312;&#21442;&#25968;&#19978;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09889v1 Announce Type: new  Abstract: Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate \emph{scaled} ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a partial differential equation in the large-neural network limit, i.e., the \emph{mean-field} regime. To derive the generalization bounds under this setting, our analysis necessitates a shift from the conventional time-invariant Gram matrix employed in the lazy training regime to a time-variant, distribution-dependent version. To this end, we provide a global lower bound on the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides, for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we establish the linear convergence of the empirical error and estimate the upper bound of the KL divergence over param
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#26063;&#32452;&#24863;&#30693;&#20808;&#39564;&#20998;&#24067;&#65292;&#21487;&#20197;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#37325;&#26032;&#35757;&#32451;&#38750;&#40065;&#26834;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#20351;&#29992;&#36825;&#31181;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#20063;&#33021;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09869</link><description>&lt;p&gt;
&#23545;&#20122;&#32676;&#20307;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#65306;&#20351;&#29992;&#32452;&#24863;&#30693;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09869
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#26063;&#32452;&#24863;&#30693;&#20808;&#39564;&#20998;&#24067;&#65292;&#21487;&#20197;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#37325;&#26032;&#35757;&#32451;&#38750;&#40065;&#26834;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#20351;&#29992;&#36825;&#31181;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#20063;&#33021;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#33021;&#22815;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#36825;&#31181;&#20559;&#31227;&#30340;&#26041;&#27861;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#32452;&#24863;&#30693;&#20808;&#39564;&#65288;GAP&#65289;&#20998;&#24067;&#65292;&#26126;&#30830;&#25903;&#25345;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#27867;&#21270;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32452;&#24863;&#30693;&#20808;&#39564;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#19968;&#23567;&#37096;&#20998;&#21253;&#21547;&#32452;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#22312;&#27492;&#20808;&#39564;&#19979;&#35757;&#32451;&#20250;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#8212;&#8212;&#21363;&#20351;&#21482;&#37325;&#26032;&#35757;&#32451;&#20808;&#21069;&#35757;&#32451;&#30340;&#38750;&#40065;&#26834;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#32452;&#24863;&#30693;&#20808;&#39564;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#23646;&#24615;&#20266;&#26631;&#35760;&#21644;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#65289;&#20114;&#34917;&#65292;&#20026;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20197;&#23454;&#29616;&#23545;&#20122;&#32676;&#20307;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#24320;&#36767;&#20102;&#26377;&#21069;&#26223;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09869v1 Announce Type: cross  Abstract: Machine learning models often perform poorly under subpopulation shifts in the data distribution. Developing methods that allow machine learning models to better generalize to such shifts is crucial for safe deployment in real-world settings. In this paper, we develop a family of group-aware prior (GAP) distributions over neural network parameters that explicitly favor models that generalize well under subpopulation shifts. We design a simple group-aware prior that only requires access to a small set of data with group information and demonstrate that training with this prior yields state-of-the-art performance -- even when only retraining the final layer of a previously trained non-robust model. Group aware-priors are conceptually simple, complementary to existing approaches, such as attribute pseudo labeling and data reweighting, and open up promising new avenues for harnessing Bayesian inference to enable robustness to subpopulation
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24179;&#34913;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09867</link><description>&lt;p&gt;
iBRF: &#25913;&#36827;&#30340;&#24179;&#34913;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
iBRF: Improved Balanced Random Forest Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24179;&#34913;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#22312;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#25968;&#25454;&#37325;&#37319;&#26679;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#25110;&#20174;&#25968;&#25454;&#20013;&#28040;&#38500;&#26679;&#26412;&#26469;&#24179;&#34913;&#31867;&#21035;&#20998;&#24067;&#12290;&#22810;&#24180;&#26469;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#37319;&#26679;&#25216;&#26415;&#20063;&#21487;&#20197;&#32435;&#20837;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#33719;&#24471;&#26356;&#24191;&#20041;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#24179;&#34913;&#38543;&#26426;&#26862;&#26519;&#65288;BRF&#65289;&#21644;SMOTE-Bagging&#26159;&#19968;&#20123;&#27969;&#34892;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;BRF&#20998;&#31867;&#22120;&#30340;&#20462;&#25913;&#20197;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#21407;&#22987;&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#27424;&#37319;&#26679;&#65288;RUS&#65289;&#25216;&#26415;&#26469;&#24179;&#34913;&#33258;&#20030;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09867v1 Announce Type: new  Abstract: Class imbalance poses a major challenge in different classification tasks, which is a frequently occurring scenario in many real-world applications. Data resampling is considered to be the standard approach to address this issue. The goal of the technique is to balance the class distribution by generating new samples or eliminating samples from the data. A wide variety of sampling techniques have been proposed over the years to tackle this challenging problem. Sampling techniques can also be incorporated into the ensemble learning framework to obtain more generalized prediction performance. Balanced Random Forest (BRF) and SMOTE-Bagging are some of the popular ensemble approaches. In this study, we propose a modification to the BRF classifier to enhance the prediction performance. In the original algorithm, the Random Undersampling (RUS) technique was utilized to balance the bootstrap samples. However, randomly eliminating too many sampl
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>MAMBA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#39046;&#22495;&#19978;&#23454;&#29616;&#26356;&#22823;&#30340;&#22238;&#25253;&#21644;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#21482;&#38656;&#24456;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2403.09859</link><description>&lt;p&gt;
MAMBA&#65306;&#19968;&#31181;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#19990;&#30028;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09859
&lt;/p&gt;
&lt;p&gt;
MAMBA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#39046;&#22495;&#19978;&#23454;&#29616;&#26356;&#22823;&#30340;&#22238;&#25253;&#21644;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#21482;&#38656;&#24456;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meta&#24378;&#21270;&#23398;&#20064;(meta-RL)&#26159;&#35299;&#20915;&#38656;&#35201;&#39640;&#25928;&#25506;&#32034;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;meta-RL&#31639;&#27861;&#20197;&#20302;&#26679;&#26412;&#25928;&#29575;&#20026;&#29305;&#24449;&#65292;&#20027;&#35201;&#20851;&#27880;&#20302;&#32500;&#20219;&#21153;&#20998;&#24067;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26041;&#27861;&#22312;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;MDP&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20854;&#20013;meta-RL&#26159;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#25104;&#21151;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;meta-RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;meta-RL&#26041;&#27861;&#30340;&#20803;&#32032;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#30340;meta-RL&#22522;&#20934;&#39046;&#22495;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#22823;&#22238;&#25253;&#21644;&#26356;&#22909;&#26679;&#26412;&#25928;&#29575;(&#26368;&#39640;&#25552;&#21319;$15\times$)&#65292;&#21516;&#26102;&#38656;&#35201;&#38750;&#24120;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12289;&#26356;&#39640;&#32500;&#24230;&#30340;&#39046;&#22495;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36808;&#20986;&#20102;&#36890;&#21521;&#30495;&#23454;&#19990;&#30028;&#27867;&#21270;&#20195;&#29702;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09859v1 Announce Type: new  Abstract: Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;DECAF&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#22312;&#22810;&#20010;&#30456;&#20851;&#29615;&#22659;&#20013;&#36827;&#34892;&#32452;&#21512;&#65292;&#36890;&#36807;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;CRL&#26041;&#27861;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09830</link><description>&lt;p&gt;
&#26088;&#22312;&#23454;&#29616;&#22240;&#26524;&#34920;&#31034;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#21487;&#32452;&#21512;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards the Reusability and Compositionality of Causal Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09830
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;DECAF&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#22312;&#22810;&#20010;&#30456;&#20851;&#29615;&#22659;&#20013;&#36827;&#34892;&#32452;&#21512;&#65292;&#36890;&#36807;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;CRL&#26041;&#27861;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning&#65288;CRL&#65289;&#26088;&#22312;&#20174;&#39640;&#32500;&#35266;&#27979;&#20013;&#35782;&#21035;&#39640;&#32423;&#22240;&#26524;&#22240;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;CRL&#20316;&#21697;&#20391;&#37325;&#20110;&#22312;&#21333;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#21521;&#65292;&#21363;&#20174;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#25110;&#32773;&#36328;&#22810;&#20010;&#30456;&#20851;&#29615;&#22659;&#36827;&#34892;&#32452;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DECAF&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#22240;&#26524;&#22240;&#32032;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#65292;&#21738;&#20123;&#38656;&#35201;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#22240;&#26524;&#34920;&#31034;&#20013;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#24178;&#39044;&#30446;&#26631;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#20123;&#30446;&#26631;&#25351;&#31034;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#34987;&#25200;&#21160;&#30340;&#21464;&#37327;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;CRL&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09830v1 Announce Type: cross  Abstract: Causal Representation Learning (CRL) aims at identifying high-level causal factors and their relationships from high-dimensional observations, e.g., images. While most CRL works focus on learning causal representations in a single environment, in this work we instead propose a first step towards learning causal representations from temporal sequences of images that can be adapted in a new environment, or composed across multiple related environments. In particular, we introduce DECAF, a framework that detects which causal factors can be reused and which need to be adapted from previously learned causal representations. Our approach is based on the availability of intervention targets, that indicate which variables are perturbed at each time step. Experiments on three benchmark datasets show that integrating our framework with four state-of-the-art CRL approaches leads to accurate representations in a new environment with only a few sam
&lt;/p&gt;</description></item><item><title>&#23558;OC20&#35757;&#32451;&#30340;EquiformerV2&#27169;&#22411;&#25104;&#21151;&#35843;&#25972;&#24182;&#24494;&#35843;&#65292;&#29992;&#20110;&#25512;&#26029;&#39640;&#29109;&#21512;&#37329;&#19978;*OH&#21644;*O&#30340;&#21560;&#38468;&#33021;&#65292;&#36890;&#36807;&#33021;&#37327;&#36807;&#28388;&#22120;&#21644;&#23569;&#37327;&#24494;&#35843;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09811</link><description>&lt;p&gt;
&#23558;OC20&#35757;&#32451;&#30340;EquiformerV2&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#29109;&#26448;&#26009;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09811
&lt;/p&gt;
&lt;p&gt;
&#23558;OC20&#35757;&#32451;&#30340;EquiformerV2&#27169;&#22411;&#25104;&#21151;&#35843;&#25972;&#24182;&#24494;&#35843;&#65292;&#29992;&#20110;&#25512;&#26029;&#39640;&#29109;&#21512;&#37329;&#19978;*OH&#21644;*O&#30340;&#21560;&#38468;&#33021;&#65292;&#36890;&#36807;&#33021;&#37327;&#36807;&#28388;&#22120;&#21644;&#23569;&#37327;&#24494;&#35843;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39640;&#36890;&#37327;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#23545;&#39640;&#29109;&#26448;&#26009;&#21644;&#20652;&#21270;&#21058;&#30340;&#30740;&#31350;&#65292;&#21463;&#21040;&#39640;&#32500;&#32452;&#25104;&#31354;&#38388;&#21644;&#22810;&#26679;&#30340;&#32467;&#26500;&#24494;&#29366;&#24577;&#30340;&#38459;&#30861;&#12290;&#23427;&#20204;&#23545;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#35745;&#31639;&#30340;&#20256;&#32479;&#20351;&#29992;&#26500;&#25104;&#20102;&#29942;&#39048;&#65292;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#21183;&#22312;&#21407;&#23376;&#32467;&#26500;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#26222;&#36941;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#24320;&#25918;&#20652;&#21270;&#21058;&#39033;&#30446;&#30340;&#39044;&#35757;&#32451;EquiformerV2&#27169;&#22411;&#35843;&#25972;&#21644;&#24494;&#35843;&#20197;&#25512;&#26029;&#20986;&#22495;&#39640;&#29109;&#21512;&#37329;Ag-Ir-Pd-Pt-Ru&#19978;*OH&#21644;*O&#30340;&#21560;&#38468;&#33021;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#32467;&#21512;&#20301;&#28857;&#23616;&#37096;&#29615;&#22659;&#30340;&#33021;&#37327;&#36807;&#28388;&#22120;&#65292;&#38646;-shot&#25512;&#26029;&#26174;&#30528;&#25552;&#39640;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#24494;&#35843;&#65292;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36824;&#21457;&#29616;EquiformerV2&#65292;&#20316;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#35282;&#33394;&#65292;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09811v1 Announce Type: cross  Abstract: Computational high-throughput studies, especially in research on high-entropy materials and catalysts, are hampered by high-dimensional composition spaces and myriad structural microstates. They present bottlenecks to the conventional use of density functional theory calculations, and consequently, the use of machine-learned potentials is becoming increasingly prevalent in atomic structure simulations. In this communication, we show the results of adjusting and fine-tuning the pretrained EquiformerV2 model from the Open Catalyst Project to infer adsorption energies of *OH and *O on the out-of-domain high-entropy alloy Ag-Ir-Pd-Pt-Ru. By applying an energy filter based on the local environment of the binding site the zero-shot inference is markedly improved and through few-shot fine-tuning the model yields state-of-the-art accuracy. It is also found that EquiformerV2, assuming the role of general machine learning potential, is able to i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#21363;&#26102;AI&#24178;&#39044;&#26469;&#25552;&#21319;&#20247;&#21253;&#24179;&#21488;&#20013;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#20171;&#32461;&#20102;LabelAId&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38169;&#35823;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09810</link><description>&lt;p&gt;
LabelAId: &#29992;&#20110;&#25913;&#21892;&#20247;&#21253;&#31995;&#32479;&#20013;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#21450;&#26102;AI&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#21363;&#26102;AI&#24178;&#39044;&#26469;&#25552;&#21319;&#20247;&#21253;&#24179;&#21488;&#20013;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#20171;&#32461;&#20102;LabelAId&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38169;&#35823;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#24179;&#21488;&#24050;&#32463;&#25913;&#21464;&#20102;&#20998;&#24067;&#24335;&#38382;&#39064;&#35299;&#20915;&#65292;&#20294;&#36136;&#37327;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#36136;&#37327;&#25511;&#21046;&#25514;&#26045;&#65292;&#22914;&#23545;&#24037;&#20316;&#32773;&#36827;&#34892;&#39044;&#31579;&#36873;&#21644;&#23436;&#21892;&#35828;&#26126;&#65292;&#36890;&#24120;&#21482;&#19987;&#27880;&#20110;&#20248;&#21270;&#32463;&#27982;&#20135;&#20986;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21363;&#26102;AI&#24178;&#39044;&#65292;&#20197;&#22686;&#24378;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LabelAId&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#25512;&#26029;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#31243;&#24207;&#21270;&#24369;&#30417;&#30563;&#65288;PWS&#65289;&#21644;FT-Transformers&#65292;&#26681;&#25454;&#29992;&#25143;&#34892;&#20026;&#21644;&#39046;&#22495;&#30693;&#35782;&#25512;&#26029;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;LabelAId&#31649;&#36947;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;ML&#22522;&#32447;&#65292;&#20351;&#29992;50&#20010;&#19979;&#28216;&#26679;&#26412;&#25552;&#39640;&#20102;36.7%&#30340;&#38169;&#35823;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;LabelAId&#23454;&#29616;&#21040;Project Sidewalk&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#22478;&#24066;&#21487;&#35775;&#38382;&#24615;&#30340;&#24320;&#28304;&#20247;&#21253;&#24179;&#21488;&#12290;&#19968;&#39033;&#28041;&#21450;34&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09810v1 Announce Type: cross  Abstract: Crowdsourcing platforms have transformed distributed problem-solving, yet quality control remains a persistent challenge. Traditional quality control measures, such as prescreening workers and refining instructions, often focus solely on optimizing economic output. This paper explores just-in-time AI interventions to enhance both labeling quality and domain-specific knowledge among crowdworkers. We introduce LabelAId, an advanced inference model combining Programmatic Weak Supervision (PWS) with FT-Transformers to infer label correctness based on user behavior and domain knowledge. Our technical evaluation shows that our LabelAId pipeline consistently outperforms state-of-the-art ML baselines, improving mistake inference accuracy by 36.7% with 50 downstream samples. We then implemented LabelAId into Project Sidewalk, an open-source crowdsourcing platform for urban accessibility. A between-subjects study with 34 participants demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23545;&#27604;&#21644;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#27934;&#23519;&#65292;&#24182;&#20026;&#36873;&#25321;&#21512;&#36866;&#30340;SSL&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09809</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#23545;&#27604;&#25110;&#29983;&#25104;&#65311;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series: Contrastive or Generative?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23545;&#27604;&#21644;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#27934;&#23519;&#65292;&#24182;&#20026;&#36873;&#25321;&#21512;&#36866;&#30340;SSL&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#20027;&#27969;&#65306;&#23545;&#27604;&#21644;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23545;&#27604;&#21644;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#21035;&#20171;&#32461;&#20102;&#23545;&#27604;&#21644;&#29983;&#25104;SSL&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#33719;&#24471;&#25351;&#23548;&#27169;&#22411;&#20248;&#21270;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;&#27599;&#31181;&#31867;&#22411;&#23454;&#29616;&#20102;&#32463;&#20856;&#31639;&#27861;&#65288;SimCLR vs. MAE&#65289;&#65292;&#24182;&#22312;&#20844;&#24179;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#20026;&#36873;&#25321;&#21512;&#36866;&#30340;SSL&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#26356;&#24191;&#27867;&#30340;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09809v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has recently emerged as a powerful approach to learning representations from large-scale unlabeled data, showing promising results in time series analysis. The self-supervised representation learning can be categorized into two mainstream: contrastive and generative. In this paper, we will present a comprehensive comparative study between contrastive and generative methods in time series. We first introduce the basic frameworks for contrastive and generative SSL, respectively, and discuss how to obtain the supervision signal that guides the model optimization. We then implement classical algorithms (SimCLR vs. MAE) for each type and conduct a comparative analysis in fair settings. Our results provide insights into the strengths and weaknesses of each approach and offer practical recommendations for choosing suitable SSL methods. We also discuss the implications of our findings for the broader field of rep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HandFormer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#65292;&#29992;&#20110;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09805</link><description>&lt;p&gt;
&#20851;&#20110;3D&#25163;&#37096;&#23039;&#21183;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Utility of 3D Hand Poses for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HandFormer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#65292;&#29992;&#20110;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#25163;&#37096;&#23039;&#21183;&#26159;&#19968;&#31181;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#21160;&#20316;&#35782;&#21035;&#27169;&#24577;&#12290;&#23039;&#21183;&#26082;&#32039;&#20945;&#21448;&#20449;&#24687;&#20016;&#23500;&#65292;&#24182;&#19988;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#35745;&#31639;&#39044;&#31639;&#26377;&#38480;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#30340;&#23039;&#21183;&#19981;&#33021;&#23436;&#20840;&#29702;&#35299;&#20154;&#31867;&#19982;&#20043;&#20132;&#20114;&#30340;&#29289;&#20307;&#21644;&#29615;&#22659;&#12290;&#20026;&#20102;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HandFormer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;Transformer&#12290;HandFormer&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#65292;&#29992;&#20110;&#31934;&#32454;&#36816;&#21160;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#26469;&#32534;&#30721;&#22330;&#26223;&#35821;&#20041;&#12290;&#35266;&#23519;&#25163;&#37096;&#23039;&#21183;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#25105;&#20204;&#23545;&#25163;&#37096;&#24314;&#27169;&#36827;&#34892;&#20102;&#26102;&#38388;&#20998;&#35299;&#65292;&#24182;&#36890;&#36807;&#20854;&#30701;&#26399;&#36712;&#36857;&#34920;&#31034;&#27599;&#20010;&#20851;&#33410;&#28857;&#12290;&#36825;&#31181;&#34987;&#20998;&#35299;&#30340;&#23039;&#21183;&#34920;&#31034;&#19982;&#31232;&#30095;&#30340;RGB&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#25928;&#29575;&#38750;&#24120;&#39640;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20165;&#26377;&#25163;&#37096;&#23039;&#21183;&#30340;&#21333;&#27169;HandFormer&#22312;5&#20493;&#26356;&#23569;&#30340;FLO&#19979;&#32988;&#36807;&#29616;&#26377;&#22522;&#20110;&#39592;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09805v1 Announce Type: cross  Abstract: 3D hand poses are an under-explored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. To efficiently model hand-object interactions, we propose HandFormer, a novel multimodal transformer. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and achieves high accuracy. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5x fewer FLO
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25991;&#26412;&#20013;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#31934;&#24230;&#12289;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#39044;&#27979;&#20197;&#21450;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#24773;&#32490;&#26234;&#33021;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25991;&#26412;&#20013;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#31934;&#24230;&#12289;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#39044;&#27979;&#20197;&#21450;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35780;&#20272;&#19982;&#21307;&#30103;&#30456;&#20851;&#25991;&#26412;&#20013;&#24773;&#32490;&#26041;&#38754;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#35770;&#26816;&#26597;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#35768;&#22810;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#24773;&#24863;&#20998;&#26512;&#12289;&#23545;&#24773;&#32490;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#22522;&#20110;&#20020;&#24202;&#21465;&#20107;&#12289;&#24739;&#32773;&#23545;&#33647;&#29289;&#30340;&#21453;&#39304;&#21644;&#22312;&#32447;&#20581;&#24247;&#35752;&#35770;&#25152;&#33719;&#24471;&#30340;&#25991;&#26412;&#20449;&#24687;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#30340;&#30740;&#31350;&#12290;&#32508;&#36848;&#23637;&#31034;&#20102;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#31639;&#27861;&#31934;&#24230;&#12289;&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;AI&#27169;&#22411;&#39044;&#27979;&#33021;&#21147;&#20197;&#21450;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;AI&#31995;&#32479;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21033;&#29992;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#35745;&#21010;&#65292;&#36890;&#36807;&#25972;&#21512;&#24739;&#32773;&#24773;&#32490;&#24182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09762v1 Announce Type: cross  Abstract: This manuscript presents a methodical examination of the utilization of Artificial Intelligence in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing and deep learning technologies. We scrutinize numerous research studies that employ AI to augment sentiment analysis, categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feedback on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification, the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI-powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#21294;&#20047;&#29615;&#22659;&#20013;&#23454;&#29616;&#20960;&#20046;&#23454;&#26102;&#30340;&#34880;&#27969;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.09758</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#21294;&#20047;&#29615;&#22659;&#20013;&#37325;&#24314;&#34880;&#27969;&#65306;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#34880;&#31649;&#32593;&#32476;&#26680;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network Kernel for Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#21294;&#20047;&#29615;&#22659;&#20013;&#23454;&#29616;&#20960;&#20046;&#23454;&#26102;&#30340;&#34880;&#27969;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#65292;&#34880;&#27969;&#37325;&#24314;&#23545;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#29615;&#22659;&#20013;&#21487;&#29992;&#30340;&#25968;&#25454;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#21294;&#20047;&#29615;&#22659;&#20013;&#20960;&#20046;&#23454;&#26102;&#37325;&#24314;&#34880;&#27969;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#34880;&#31649;&#32593;&#32476;&#20869;&#37325;&#24314;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#34880;&#31649;&#32593;&#32476;&#26159;&#19968;&#20010;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25152;&#25552;&#20986;&#30340;&#26680;&#32534;&#30721;&#20102;&#26102;&#31354;&#21644;&#34880;&#31649;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#25968;&#25454;&#21294;&#20047;&#29615;&#22659;&#20013;&#30340;&#34880;&#27969;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09758v1 Announce Type: cross  Abstract: Blood flow reconstruction in the vasculature is important for many clinical applications. However, in clinical settings, the available data are often quite limited. For instance, Transcranial Doppler ultrasound (TCD) is a noninvasive clinical tool that is commonly used in the clinical settings to measure blood velocity waveform at several locations on brain's vasculature. This amount of data is grossly insufficient for training machine learning surrogate models, such as deep neural networks or Gaussian process regression. In this work, we propose a Gaussian process regression approach based on physics-informed kernels, enabling near-real-time reconstruction of blood flow in data-poor regimes. We introduce a novel methodology to reconstruct the kernel within the vascular network, which is a non-Euclidean space. The proposed kernel encodes both spatiotemporal and vessel-to-vessel correlations, thus enabling blood flow reconstruction in v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20854;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.09755</link><description>&lt;p&gt;
&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#30340;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
Estimating the history of a random recursive tree
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20854;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#65306;&#22343;&#21248;&#36830;&#25509;&#27169;&#22411;&#21644;&#32447;&#24615;&#20248;&#20808;&#36830;&#25509;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#26063;&#39118;&#38505;&#24230;&#37327;&#26469;&#37327;&#21270;&#25490;&#24207;&#36807;&#31243;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24314;&#31435;&#20102;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20248;&#20110;&#22522;&#20110;&#24230;&#25968;&#21644;&#35889;&#25490;&#24207;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09755v1 Announce Type: cross  Abstract: This paper studies the problem of estimating the order of arrival of the vertices in a random recursive tree. Specifically, we study two fundamental models: the uniform attachment model and the linear preferential attachment model. We propose an order estimator based on the Jordan centrality measure and define a family of risk measures to quantify the quality of the ordering procedure. Moreover, we establish a minimax lower bound for this problem, and prove that the proposed estimator is nearly optimal. Finally, we numerically demonstrate that the proposed estimator outperforms degree-based and spectral ordering procedures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;SoM-TP&#65292;&#36890;&#36807;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#23454;&#29616;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#26102;&#38388;&#27744;&#21270;&#65292;&#22312;&#21333;&#20010;&#20998;&#31867;&#22120;&#20869;&#23454;&#29616;&#38750;&#36845;&#20195;&#27744;&#21270;&#38598;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.09749</link><description>&lt;p&gt;
&#23454;&#29616;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#65306;&#22522;&#20110;&#22810;&#20010;&#26102;&#38388;&#27744;&#21270;&#30340;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09749
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;SoM-TP&#65292;&#36890;&#36807;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#23454;&#29616;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#26102;&#38388;&#27744;&#21270;&#65292;&#22312;&#21333;&#20010;&#20998;&#31867;&#22120;&#20869;&#23454;&#29616;&#38750;&#36845;&#20195;&#27744;&#21270;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#39034;&#24207;&#20449;&#24687;&#30340;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#20010;&#26102;&#38388;&#27744;&#21270;&#20855;&#26377;&#19981;&#21516;&#30340;&#26426;&#21046;&#65292;&#24182;&#19988;&#26681;&#25454;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19981;&#21516;&#24773;&#20917;&#21487;&#33021;&#25928;&#26524;&#22909;&#22351;&#19981;&#19968;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22266;&#23450;&#27744;&#21270;&#26426;&#21046;&#31216;&#20026;&#21333;&#19968;&#35270;&#35282;&#30340;&#26102;&#38388;&#27744;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#30340;&#26032;&#22411;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;&#65306;&#36873;&#25321;&#22810;&#20010;&#26102;&#38388;&#27744;&#21270;&#65288;SoM-TP&#65289;&#12290;SoM-TP&#36890;&#36807;&#27880;&#24847;&#21147;&#21160;&#24577;&#36873;&#25321;&#22810;&#31181;&#26041;&#27861;&#20013;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#30340;&#26368;&#20339;&#26102;&#38388;&#27744;&#21270;&#12290;SoM-TP&#30340;&#21160;&#24577;&#27744;&#21270;&#36873;&#25321;&#21463;&#21040;&#22810;&#36873;&#23398;&#20064;&#65288;MCL&#65289;&#38598;&#25104;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#35813;&#27010;&#24565;&#20174;&#22810;&#20010;&#36755;&#20986;&#20013;&#36873;&#25321;&#26368;&#20339;&#36755;&#20986;&#12290;SoM-TP&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#36873;&#25321;&#23454;&#29616;&#20102;&#21333;&#20010;&#20998;&#31867;&#22120;&#20869;&#30340;&#38750;&#36845;&#20195;&#27744;&#21270;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35270;&#35282;&#25439;&#22833;&#21644;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#32593;&#32476;&#65288;DPLN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09749v1 Announce Type: cross  Abstract: In Time Series Classification (TSC), temporal pooling methods that consider sequential information have been proposed. However, we found that each temporal pooling has a distinct mechanism, and can perform better or worse depending on time series data. We term this fixed pooling mechanism a single perspective of temporal poolings. In this paper, we propose a novel temporal pooling method with diverse perspective learning: Selection over Multiple Temporal Poolings (SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among multiple methods for each data by attention. The dynamic pooling selection is motivated by the ensemble concept of Multiple Choice Learning (MCL), which selects the best among multiple outputs. The pooling selection by SoM-TP's attention enables a non-iterative pooling ensemble within a single classifier. Additionally, we define a perspective loss and Diverse Perspective Learning Network (DPLN). The loss w
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20154;&#31867;&#22240;&#32032;&#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20854;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09743</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#20026;&#22240;&#32032;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09743
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20154;&#31867;&#22240;&#32032;&#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20854;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#22312;2022&#24180;11&#26376;&#25512;&#20986;&#30340;ChatGPT&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20027;&#27969;&#65292;&#24182;&#22312;&#29992;&#25143;&#37319;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#35760;&#24405;&#12290;&#23588;&#20854;&#26159;ChatGPT&#65292;&#32463;&#36807;&#24191;&#27867;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#35757;&#32451;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#26263;&#31034;&#23545;&#21171;&#21160;&#21147;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;-&#8220;&#24187;&#35273;&#8221;&#21644;&#36951;&#28431;&#65292;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#20449;&#24687;&#12290;&#36825;&#22312;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#29615;&#22659;&#20013;&#23588;&#20026;&#21361;&#38505;&#65292;&#27604;&#22914;&#27861;&#24459;&#21512;&#35268;&#12289;&#21307;&#23398;&#25110;&#31934;&#32454;&#30340;&#27969;&#31243;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09743v1 Announce Type: cross  Abstract: The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - "hallucinations" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.   There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essen
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#22238;&#39038;&#20102;&#35299;&#20915;&#26368;&#22823;&#22242;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#27979;&#35797;&#36825;&#20123;&#31639;&#27861;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.09742</link><description>&lt;p&gt;
&#26368;&#22823;&#22242;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#31616;&#35201;&#22238;&#39038;&#65306;&#20174;&#32463;&#20856;&#31639;&#27861;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Short Review on Novel Approaches for Maximum Clique Problem: from Classical algorithms to Graph Neural Networks and Quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#22238;&#39038;&#20102;&#35299;&#20915;&#26368;&#22823;&#22242;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#27979;&#35797;&#36825;&#20123;&#31639;&#27861;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25163;&#31295;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#22823;&#22242;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#22312;&#22270;&#20013;&#25214;&#21040;&#25152;&#26377;&#20004;&#20004;&#30456;&#37051;&#30340;&#39030;&#28857;&#23376;&#38598;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#25163;&#31295;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#28085;&#30422;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#24182;&#21253;&#25324;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;&#26368;&#36817;&#21457;&#23637;&#30340;&#23457;&#26597;&#12290;&#35813;&#32508;&#36848;&#20197;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#32463;&#20856;&#20197;&#21450;&#26032;&#30340;&#23398;&#20064;&#21644;&#37327;&#23376;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09742v1 Announce Type: new  Abstract: This manuscript provides a comprehensive review of the Maximum Clique Problem, a computational problem that involves finding subsets of vertices in a graph that are all pairwise adjacent to each other. The manuscript covers in a simple way classical algorithms for solving the problem and includes a review of recent developments in graph neural networks and quantum algorithms. The review concludes with benchmarks for testing classical as well as new learning, and quantum algorithms.
&lt;/p&gt;</description></item><item><title>RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.</title><link>https://arxiv.org/abs/2403.09727</link><description>&lt;p&gt;
&#25506;&#31350;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#24494;&#35843;&#22312;&#21457;&#23637;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30693;&#35782;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09727
&lt;/p&gt;
&lt;p&gt;
RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09727v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#39046;&#22495; &#25688;&#35201;: &#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(G-LLM)&#30340;&#21457;&#23637;&#20026;&#31867;&#20284;ChatGPT&#12289;Bing&#25110;Gemini&#30340;&#26032;&#22411;&#30693;&#35782;&#31995;&#32479;&#30340;&#24320;&#21457;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#24494;&#35843;(FN)&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#21487;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;G-LLM&#30340;&#30693;&#35782;&#31995;&#32479;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;ROUGE&#12289;BLEU&#12289;METEOR&#20998;&#25968;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#27604;&#36739;&#24182;&#26816;&#39564;&#20102;GPT-J-6B&#12289;OPT-6.7B&#12289;LlaMA&#12289;LlaMA-2&#35821;&#35328;&#27169;&#22411;&#30340;RAG&#21644;FN&#30340;&#34920;&#29616;&#12290;&#22522;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;RAG&#30340;&#26500;&#24314;&#27604;&#20351;&#29992;FN&#20135;&#29983;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#25351;&#20986;&#23558;RAG&#21644;FN&#36830;&#25509;&#36215;&#26469;&#24182;&#19981;&#26159;&#36731;&#32780;&#26131;&#20030;&#30340;&#65292;&#22240;&#20026;&#23558;FN&#27169;&#22411;&#19982;RAG&#36830;&#25509;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;RAG&#30340;&#26550;&#26500;&#65292;&#24179;&#22343;&#22312;RO&#26041;&#38754;&#20248;&#20110;FN&#27169;&#22411;16%
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09727v1 Announce Type: cross  Abstract: The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the RO
&lt;/p&gt;</description></item><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#20998;&#26512;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#65292;&#35782;&#21035;&#28508;&#22312;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20026;&#35299;&#20915;EULA&#36807;&#38271;&#19988;&#38590;&#20197;&#29702;&#35299;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.09715</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#26512;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#29992;&#20110;&#26631;&#35760;&#28508;&#22312;&#24694;&#24847;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Textual analysis of End User License Agreement for red-flagging potentially malicious software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#20998;&#26512;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#65292;&#35782;&#21035;&#28508;&#22312;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20026;&#35299;&#20915;EULA&#36807;&#38271;&#19988;&#38590;&#20197;&#29702;&#35299;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#29992;&#25143;&#20250;&#19979;&#36733;&#26032;&#36719;&#20214;&#21644;&#26356;&#26032;&#65292;&#27599;&#20010;&#19979;&#36733;&#30340;&#36719;&#20214;&#37117;&#38468;&#24102;&#19968;&#20221;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#65288;EULA&#65289;&#65292;&#20294;&#36825;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;EULA&#21253;&#21547;&#30340;&#20449;&#24687;&#26159;&#20026;&#20102;&#36991;&#20813;&#27861;&#24459;&#36131;&#20219;&#65292;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#24102;&#26469;&#19968;&#31995;&#21015;&#28508;&#22312;&#38382;&#39064;&#65292;&#27604;&#22914;&#38388;&#35853;&#36719;&#20214;&#25110;&#23545;&#30446;&#26631;&#31995;&#32479;&#20135;&#29983;&#38750;&#39044;&#26399;&#24433;&#21709;&#12290;&#29992;&#25143;&#19981;&#35835;EULA&#26159;&#22240;&#20026;&#25991;&#26723;&#22826;&#38271;&#65292;&#38590;&#20197;&#29702;&#35299;&#65292;&#25991;&#26412;&#25688;&#35201;&#26159;&#36825;&#31181;&#38382;&#39064;&#30340;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27010;&#25324;EULA&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#8220;&#33391;&#24615;&#8221;&#25110;&#8220;&#24694;&#24847;&#8221;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#19981;&#21516;&#36719;&#20214;&#30340;EULA&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#20843;&#31181;&#30417;&#30563;&#20998;&#31867;&#22120;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#23558;EULA&#20998;&#31867;&#20026;&#33391;&#24615;&#25110;&#24694;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09715v1 Announce Type: cross  Abstract: New software and updates are downloaded by end users every day. Each dowloaded software has associated with it an End Users License Agreements (EULA), but this is rarely read. An EULA includes information to avoid legal repercussions. However,this proposes a host of potential problems such as spyware or producing an unwanted affect in the target system. End users do not read these EULA's because of length of the document and users find it extremely difficult to understand. Text summarization is one of the relevant solution to these kind of problems. This require a solution which can summarize the EULA and classify the EULA as "Benign" or "Malicious". We propose a solution in which we have summarize the EULA and classify the EULA as "Benign" or "Malicious". We extract EULA text of different sofware's then we classify the text using eight different supervised classifiers. we use ensemble learning to classify the EULA as benign or malicio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#65292;&#30740;&#31350;&#23545;108,280&#20221;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;ICIs&#27835;&#30103;&#24739;&#32773;&#20013;IrAEs&#30340;&#21457;&#29983;&#24773;&#20917;&#65292;&#24182;&#36827;&#34892;&#20102;&#27835;&#30103;&#20013;&#26029;&#29575;&#21644;&#29983;&#23384;&#26354;&#32447;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.09708</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#23545;&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;IrAEs&#36827;&#34892;&#26426;&#26500;&#32423;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs Using a Novel Natural Language Processing Algorithmic Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09708
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#65292;&#30740;&#31350;&#23545;108,280&#20221;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;ICIs&#27835;&#30103;&#24739;&#32773;&#20013;IrAEs&#30340;&#21457;&#29983;&#24773;&#20917;&#65292;&#24182;&#36827;&#34892;&#20102;&#27835;&#30103;&#20013;&#26029;&#29575;&#21644;&#29983;&#23384;&#26354;&#32447;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;&#65288;ICIs&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#30284;&#30151;&#27835;&#30103;&#65292;&#20294;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20813;&#30123;&#30456;&#20851;&#19981;&#33391;&#20107;&#20214;&#65288;IrAEs&#65289;&#12290;&#30417;&#27979;IrAEs&#30340;&#21457;&#29983;&#23545;&#20110;&#20010;&#24615;&#21270;&#39118;&#38505;&#35780;&#20272;&#20197;&#21450;&#21327;&#21161;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25509;&#21463;ICIs&#27835;&#30103;&#30340;Tel Aviv Sourasky&#21307;&#30103;&#20013;&#24515;&#24739;&#32773;&#30340;&#20020;&#24202;&#35760;&#24405;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#31995;&#32479;&#24615;&#22320;&#35782;&#21035;&#20102;&#19971;&#31181;&#24120;&#35265;&#25110;&#20005;&#37325;&#30340;IrAEs&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30382;&#36136;&#31867;&#22266;&#37255;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#21450;IrAEs&#21518;&#30340;&#27835;&#30103;&#20013;&#26029;&#29575;&#65292;&#24182;&#26500;&#24314;&#20102;&#29983;&#23384;&#26354;&#32447;&#20197;&#21487;&#35270;&#21270;&#27835;&#30103;&#36807;&#31243;&#20013;&#19981;&#33391;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09708v1 Announce Type: new  Abstract: Background: Immune checkpoint inhibitors (ICIs) have revolutionized cancer treatment but can result in severe immune-related adverse events (IrAEs). Monitoring IrAEs on a large scale is essential for personalized risk profiling and assisting in treatment decisions.   Methods: In this study, we conducted an analysis of clinical notes from patients who received ICIs at the Tel Aviv Sourasky Medical Center. By employing a Natural Language Processing algorithmic pipeline, we systematically identified seven common or severe IrAEs. We examined the utilization of corticosteroids, treatment discontinuation rates following IrAEs, and constructed survival curves to visualize the occurrence of adverse events during treatment.   Results: Our analysis encompassed 108,280 clinical notes associated with 1,635 patients who had undergone ICI therapy. The detected incidence of IrAEs was consistent with previous reports, exhibiting substantial variation ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Alignment Studio &#26550;&#26500;&#65292;&#20351;&#24212;&#29992;&#24320;&#21457;&#32773;&#33021;&#22815;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20182;&#20204;&#29305;&#23450;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#65292;&#24182;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09704</link><description>&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#29305;&#23450;&#24773;&#22659;&#35268;&#33539;&#30340; Alignment Studio
&lt;/p&gt;
&lt;p&gt;
Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Alignment Studio &#26550;&#26500;&#65292;&#20351;&#24212;&#29992;&#24320;&#21457;&#32773;&#33021;&#22815;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20182;&#20204;&#29305;&#23450;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#65292;&#24182;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#36890;&#24120;&#30001;&#27169;&#22411;&#25552;&#20379;&#32773;&#36827;&#34892;&#65292;&#20197;&#28155;&#21152;&#25110;&#25511;&#21046;&#36328;&#29992;&#20363;&#21644;&#24773;&#22659;&#20013;&#36890;&#29992;&#25110;&#26222;&#36941;&#29702;&#35299;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21644;&#26550;&#26500;&#65292;&#36171;&#20104;&#24212;&#29992;&#24320;&#21457;&#32773;&#35843;&#25972;&#27169;&#22411;&#33267;&#20854;&#29305;&#23450;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24773;&#22659;&#20013;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#36825;&#31181;&#23545;&#40784;&#24037;&#20316;&#23460;&#26550;&#26500;&#30340;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#26500;&#26550;&#32773;&#12289;&#25351;&#23548;&#32773;&#21644;&#23457;&#26680;&#32773;&#20849;&#21516;&#20316;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20225;&#19994;&#20869;&#37096;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#40784;&#21040;&#19994;&#21153;&#34892;&#20026;&#20934;&#21017;&#30340;&#23454;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09704v1 Announce Type: cross  Abstract: The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#22312;&#32447;&#31639;&#27861;&#30340;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#36827;&#34892;&#21551;&#21160;&#65292;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#30340;&#21487;&#35777;&#26126;&#25910;&#30410;&#65292;&#21363;&#20351;&#31163;&#32447;&#25968;&#25454;&#38598;&#27809;&#26377;&#21333;&#19968;&#31574;&#30053;&#21487;&#38598;&#20013;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09701</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#26377;&#38480;&#35206;&#30422;&#30340;&#28151;&#21512;RL&#22312;&#32447;&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09701
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#22312;&#32447;&#31639;&#27861;&#30340;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#36827;&#34892;&#21551;&#21160;&#65292;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#30340;&#21487;&#35777;&#26126;&#25910;&#30410;&#65292;&#21363;&#20351;&#31163;&#32447;&#25968;&#25454;&#38598;&#27809;&#26377;&#21333;&#19968;&#31574;&#30053;&#21487;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32467;&#21512;&#22312;&#32447;&#21644;&#31163;&#32447;&#25968;&#25454;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20851;&#20110;&#20854;&#21487;&#35777;&#26126;&#30410;&#22788;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#28151;&#21512;RL&#31639;&#27861;&#23545;&#31163;&#32447;&#25968;&#25454;&#38598;&#26045;&#21152;&#35206;&#30422;&#20551;&#35774;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#22312;&#32447;&#31639;&#27861;&#24212;&#35813;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#8220;&#22635;&#34917;&#31354;&#30333;&#8221;&#65292;&#25506;&#32034;&#34892;&#20026;&#31574;&#30053;&#26410;&#25506;&#32034;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#20272;&#35745;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20197;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#26631;&#20934;&#20048;&#35266;&#22312;&#32447;&#31639;&#27861;&#30340;&#19968;&#20010;&#33258;&#28982;&#25193;&#23637;&#8212;&#8212;&#36890;&#36807;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#26469;&#21551;&#21160;&#23427;&#20204;&#8212;&#8212;&#21363;&#20351;&#31163;&#32447;&#25968;&#25454;&#38598;&#27809;&#26377;&#21333;&#19968;&#31574;&#30053;&#21487;&#38598;&#20013;&#24615;&#65292;&#20063;&#21487;&#23454;&#29616;&#28151;&#21512;&#25968;&#25454;&#30340;&#31867;&#20284;&#21487;&#35777;&#26126;&#25910;&#30410;&#12290;&#25105;&#20204;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09701v1 Announce Type: new  Abstract: Hybrid Reinforcement Learning (RL), leveraging both online and offline data, has garnered recent interest, yet research on its provable benefits remains sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on the offline dataset, but we show that this is unnecessary. A well-designed online algorithm should "fill in the gaps" in the offline dataset, exploring states and actions that the behavior policy did not explore. Unlike previous approaches that focus on estimating the offline data distribution to guide online exploration (Li et al., 2023b), we show that a natural extension to standard optimistic online algorithms -- warm-starting them by including the offline dataset in the experience replay buffer -- achieves similar provable gains from hybrid data even when the offline dataset does not have single-policy concentrability. We accomplish
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20107;&#23454;&#22270;&#20687;&#32534;&#36753;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;i.i.d.&#22270;&#20687;&#26679;&#26412;&#21644;&#26631;&#31614;&#36827;&#34892;&#23545;&#20107;&#23454;&#32534;&#36753;&#30340;&#19981;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#21363;&#20351;&#30693;&#26195;&#28508;&#22312;&#29983;&#25104;&#22240;&#32032;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20063;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#20219;&#20309;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.09683</link><description>&lt;p&gt;
&#23545;&#20107;&#23454;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20107;&#23454;&#22270;&#20687;&#32534;&#36753;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;i.i.d.&#22270;&#20687;&#26679;&#26412;&#21644;&#26631;&#31614;&#36827;&#34892;&#23545;&#20107;&#23454;&#32534;&#36753;&#30340;&#19981;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#21363;&#20351;&#30693;&#26195;&#28508;&#22312;&#29983;&#25104;&#22240;&#32032;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20063;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20107;&#23454;&#22270;&#20687;&#32534;&#36753;&#26159;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#35810;&#38382;&#20102;&#22914;&#26524;&#26576;&#20123;&#29305;&#24449;&#19981;&#21516;&#65292;&#22270;&#20687;&#23558;&#20250;&#21576;&#29616;&#24590;&#26679;&#30340;&#22806;&#35266;&#12290;&#30446;&#21069;&#20851;&#20110;&#36825;&#20010;&#35805;&#39064;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21464;&#21333;&#20010;&#29305;&#24449;&#19978;&#65292;&#21364;&#19981;&#35848;&#35770;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#32780;&#36825;&#26159;&#30495;&#23454;&#19990;&#30028;&#20013;&#25152;&#23384;&#22312;&#30340;&#12290;&#26412;&#25991;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#23545;&#23545;&#20107;&#23454;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#21152;&#20197;&#24418;&#24335;&#21270;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#22686;&#24378;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;ASCMs&#65289;&#30340;&#29305;&#27530;&#27169;&#22411;&#26469;&#23545;&#28508;&#22312;&#29983;&#25104;&#22240;&#32032;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#20010;&#22522;&#26412;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65306;&#65288;1&#65289;&#20165;&#20165;&#20381;&#38752;i.i.d.&#22270;&#20687;&#26679;&#26412;&#21450;&#20854;&#23545;&#24212;&#26631;&#31614;&#26159;&#19981;&#21487;&#33021;&#36827;&#34892;&#23545;&#20107;&#23454;&#32534;&#36753;&#30340;&#65307;&#65288;2&#65289;&#21363;&#20351;&#28508;&#22312;&#29983;&#25104;&#22240;&#32032;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#21487;&#29992;&#30340;&#65292;&#20063;&#26080;&#27861;&#20445;&#35777;&#27169;&#22411;&#36755;&#20986;&#30340;&#32467;&#26524;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09683v1 Announce Type: cross  Abstract: Counterfactual image editing is an important task in generative AI, which asks how an image would look if certain features were different. The current literature on the topic focuses primarily on changing individual features while remaining silent about the causal relationships between these features, as present in the real world. In this paper, we formalize the counterfactual image editing task using formal language, modeling the causal relationships between latent generative factors and images through a special type of model called augmented structural causal models (ASCMs). Second, we show two fundamental impossibility results: (1) counterfactual editing is impossible from i.i.d. image samples and their corresponding labels alone; (2) even when the causal relationships between the latent generative factors and images are available, no guarantees regarding the output of the model can be provided. Third, we propose a relaxation for th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#26368;&#26032;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#22312;Vision Transformers&#19978;&#30340;&#23454;&#39564;&#65292;&#20026;Vision Transformers&#23450;&#21046;&#30340;&#26426;&#22120;&#36951;&#24536;&#65288;MUL&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#32447;&#30740;&#31350;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.09681</link><description>&lt;p&gt;
ViT-MUL&#65306;&#26368;&#36817;&#24212;&#29992;&#20110;&#35270;&#35273;Transformer&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#30340;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied to Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09681
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#26368;&#26032;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#22312;Vision Transformers&#19978;&#30340;&#23454;&#39564;&#65292;&#20026;Vision Transformers&#23450;&#21046;&#30340;&#26426;&#22120;&#36951;&#24536;&#65288;MUL&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#32447;&#30740;&#31350;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MUL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#23398;&#20064;&#20449;&#24687;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#23545;MUL&#30340;&#30740;&#31350;&#36739;&#20026;&#27963;&#36291;&#65292;&#20294;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#22522;&#20110;ResNet&#30340;&#27169;&#22411;&#19978;&#12290;&#37492;&#20110;Vision Transformer&#65288;ViT&#65289;&#24050;&#32463;&#25104;&#20026;&#20027;&#35201;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#29305;&#23450;&#20110;ViT&#30340;MUL&#30340;&#35814;&#32454;&#30740;&#31350;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;MUL&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#23545;ViTs&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;&#25105;&#20204;&#39044;&#26399;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#12289;&#28040;&#34701;&#30740;&#31350;&#21644;&#21457;&#29616;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#28608;&#21457;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09681v1 Announce Type: cross  Abstract: Machine unlearning (MUL) is an arising field in machine learning that seeks to erase the learned information of specific training data points from a trained model. Despite the recent active research in MUL within computer vision, the majority of work has focused on ResNet-based models. Given that Vision Transformers (ViT) have become the predominant model architecture, a detailed study of MUL specifically tailored to ViT is essential. In this paper, we present comprehensive experiments on ViTs using recent MUL algorithms and datasets. We anticipate that our experiments, ablation studies, and findings could provide valuable insights and inspire further research in this field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.09680</link><description>&lt;p&gt;
&#39044;&#25490;&#24207;Tsetlin&#26426;&#22120;&#65288;&#22522;&#22240;K-Medoid&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#20174;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;N&#20010;&#25968;&#25454;&#28857;&#65292;&#20197;&#35299;&#20915;&#26368;&#22823;&#31163;&#25955;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34987;&#29992;&#20316;&#36816;&#34892;K-Medoid&#32858;&#31867;&#31639;&#27861;&#30340;&#21021;&#22987;&#25918;&#32622;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#27721;&#26126;&#36317;&#31163;&#26469;&#23545;&#40784;N&#20010;&#29420;&#31435;&#30340;Tsetlin Machines&#12290;&#23545;&#20110;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;&#39640;&#36798;10&#65285;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;383&#20493;&#65292;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;86&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09680v1 Announce Type: cross  Abstract: This paper proposes a machine learning pre-sort stage to traditional supervised learning using Tsetlin Machines. Initially, N data-points are identified from the dataset using an expedited genetic algorithm to solve the maximum dispersion problem. These are then used as the initial placement to run the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is used to align N independent Tsetlin Machines by maximising hamming distance. For MNIST level classification problems, results demonstrate up to 10% improvement in accuracy, approx. 383X reduction in training time and approx. 86X reduction in inference time.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#65288;ChatGPT-4&#65289;&#22914;&#20309;&#36731;&#26494;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20197;&#21046;&#36896;&#20986;&#19968;&#20010;&#23436;&#20840;&#34394;&#26500;&#30340;&#21307;&#23398;&#26696;&#20363;&#26469;&#35686;&#31034;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2403.09674</link><description>&lt;p&gt;
&#36991;&#24320;&#29983;&#25104;&#30340;&#26367;&#20195;&#20107;&#23454;&#30340;&#21361;&#38505;&#65306;&#20197;ChatGPT-4&#21046;&#36896;&#30340;&#937;&#21464;&#31181;&#26696;&#20363;&#20316;&#20026;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#35686;&#31034;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#65288;ChatGPT-4&#65289;&#22914;&#20309;&#36731;&#26494;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20197;&#21046;&#36896;&#20986;&#19968;&#20010;&#23436;&#20840;&#34394;&#26500;&#30340;&#21307;&#23398;&#26696;&#20363;&#26469;&#35686;&#31034;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#21307;&#23398;&#30740;&#31350;&#20132;&#32455;&#30340;&#26102;&#20195;&#65292;&#30495;&#30456;&#30340;&#25259;&#38706;&#21464;&#24471;&#26085;&#30410;&#22797;&#26434;&#12290;&#26412;&#30740;&#31350;&#34920;&#38754;&#19978;&#23457;&#26597;&#20102;&#19968;&#31181;&#25152;&#35859;&#30340;&#26032;&#22411;SARS-CoV-2&#21464;&#31181;&#65292;&#34987;&#31216;&#20026;&#937;&#21464;&#31181;&#65292;&#23637;&#31034;&#22312;S&#22522;&#22240;&#21306;&#22495;&#20013;&#26377;31&#20010;&#29420;&#29305;&#31361;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25925;&#20107;&#30340;&#30495;&#27491;&#28508;&#21488;&#35789;&#26159;&#23637;&#31034;&#20102;AI&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;ChatGPT-4&#65289;&#21487;&#20197;&#22914;&#20309;&#36731;&#26494;&#22320;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#12290;&#25152;&#35859;&#30340;&#937;&#21464;&#31181;&#22312;&#19968;&#20010;&#23436;&#20840;&#25509;&#31181;&#30123;&#33495;&#12289;&#20043;&#21069;&#24863;&#26579;&#36807;&#30340;35&#23681;&#30007;&#24615;&#20013;&#34987;&#37492;&#23450;&#20986;&#29616;&#20005;&#37325;COVID-19&#30151;&#29366;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#65292;&#23613;&#31649;&#26159;&#34394;&#25311;&#30340;&#65292;&#22522;&#22240;&#32452;&#20998;&#26512;&#21644;&#25509;&#35302;&#32773;&#36861;&#36394;&#65292;&#26412;&#30740;&#31350;&#27169;&#25311;&#20102;&#30495;&#23454;&#30149;&#20363;&#25253;&#21578;&#30340;&#20005;&#35880;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#20294;&#23436;&#20840;&#26500;&#36896;&#30340;&#21465;&#36848;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25972;&#20010;&#30149;&#20363;&#30740;&#31350;&#26159;&#30001;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-4&#29983;&#25104;&#30340;&#937;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09674v1 Announce Type: new  Abstract: In an era where artificial intelligence (AI) intertwines with medical research, the delineation of truth becomes increasingly complex. This study ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega variant, showcasing 31 unique mutations in the S gene region. However, the real undercurrent of this narrative is a demonstration of the ease with which AI, specifically ChatGPT-4, can fabricate convincing yet entirely fictional scientific data. The so-called Omega variant was identified in a fully vaccinated, previously infected 35-year-old male presenting with severe COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and contact tracing, this study mirrors the rigorous methodology of genuine case reports, thereby setting the stage for a compelling but entirely constructed narrative. The entire case study was generated by ChatGPT-4, a large language model by OpenAI. The fabricated Omega variant f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09673</link><description>&lt;p&gt;
FoldToken&#65306;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21450;&#26356;&#22810;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
FoldToken: Learning Protein Language via Vector Quantization and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09673
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#21516;&#26102;&#25551;&#36848;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#22806;&#35821;&#65311;&#30001;&#20110;&#36830;&#32493;3D&#28857;&#34920;&#31034;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#31163;&#25955;&#24207;&#21015;&#30340;&#23545;&#27604;&#24314;&#27169;&#26041;&#24335;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{FoldTokenizer}&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#28041;&#21450;&#23558;&#27531;&#22522;&#31867;&#22411;&#21644;&#32467;&#26500;&#25237;&#23556;&#21040;&#19968;&#20010;&#31163;&#25955;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#20449;&#24687;&#20445;&#23384;&#30340;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#31163;&#25955;&#31526;&#21495;&#31216;&#20026;\textbf{FoldToken}&#65292;&#32780;FoldTokens&#30340;&#24207;&#21015;&#21017;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#24418;&#24577;&#12290;&#25105;&#20204;&#23558;&#21019;&#24314;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#24212;&#29992;&#20110;&#26222;&#36890;&#20027;&#24178;&#20462;&#34917;&#21644;&#25239;&#20307;&#35774;&#35745;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#39318;&#20010;GPT&#39118;&#26684;&#27169;&#22411;(\textbf{FoldGPT})&#29992;&#20110;&#20855;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#12290;&#25105;&#20204;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#26174;&#33879;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09673v1 Announce Type: cross  Abstract: Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancem
&lt;/p&gt;</description></item><item><title>COMPRER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#30446;&#26631;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#34920;&#24449;&#65292;&#24182;&#36890;&#36807;&#22810;&#30446;&#26631;&#35757;&#32451;&#25552;&#39640;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09672</link><description>&lt;p&gt;
COMPRER&#65306;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#34920;&#24449;&#30340;&#22810;&#27169;&#24577;&#22810;&#30446;&#26631;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced Medical Image Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09672
&lt;/p&gt;
&lt;p&gt;
COMPRER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#30446;&#26631;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#34920;&#24449;&#65292;&#24182;&#36890;&#36807;&#22810;&#30446;&#26631;&#35757;&#32451;&#25552;&#39640;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#20419;&#36827;&#20102;&#23558;&#19981;&#21516;&#21307;&#23398;&#27169;&#24577;&#30456;&#32467;&#21512;&#20197;&#23454;&#29616;&#25972;&#20307;&#20581;&#24247;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;COMPRER&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#30446;&#26631;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#34920;&#24449;&#12289;&#35786;&#26029;&#25512;&#26029;&#21644;&#30142;&#30149;&#39044;&#21518;&#12290;COMPRER&#37319;&#29992;&#22810;&#30446;&#26631;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#21521;&#27169;&#22411;&#24341;&#20837;&#19981;&#21516;&#30340;&#30693;&#35782;&#12290;&#20854;&#20013;&#21253;&#25324;&#25972;&#21512;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#25439;&#22833;&#65307;&#36171;&#20104;&#27169;&#22411;&#35782;&#21035;&#26102;&#38388;&#27169;&#24335;&#30340;&#26102;&#38388;&#25439;&#22833;&#65307;&#28155;&#21152;&#36866;&#24403;&#30340;&#21307;&#23398;&#27934;&#35265;&#30340;&#21307;&#23398;&#27979;&#37327;&#39044;&#27979;&#65307;&#26368;&#21518;&#65292;&#36890;&#36807;&#37325;&#24314;&#25439;&#22833;&#20445;&#35777;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#22270;&#20687;&#32467;&#26500;&#23436;&#25972;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#22810;&#20010;&#30446;&#26631;&#21487;&#33021;&#21066;&#24369;&#20219;&#21153;&#24615;&#33021;&#30340;&#25285;&#24551;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#32452;&#21512;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09672v1 Announce Type: cross  Abstract: Substantial advances in multi-modal Artificial Intelligence (AI) facilitate the combination of diverse medical modalities to achieve holistic health assessments. We present COMPRER , a novel multi-modal, multi-objective pretraining framework which enhances medical-image representation, diagnostic inferences, and prognosis of diseases. COMPRER employs a multi-objective training framework, where each objective introduces distinct knowledge to the model. This includes a multimodal loss that consolidates information across different imaging modalities; A temporal loss that imparts the ability to discern patterns over time; Medical-measure prediction adds appropriate medical insights; Lastly, reconstruction loss ensures the integrity of image structure within the latent space. Despite the concern that multiple objectives could weaken task performance, our findings show that this combination actually boosts outcomes on certain tasks. Here, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20013;&#19968;&#20010;&#32463;&#20856;&#24037;&#20316;CycleGAN&#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#22833;&#36133;&#19982;GAN&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2403.09646</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;GAN&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Unsupervised Image-to-image translation and GAN stability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20013;&#19968;&#20010;&#32463;&#20856;&#24037;&#20316;CycleGAN&#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#22833;&#36133;&#19982;GAN&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#30340;&#38382;&#39064;&#26082;&#26377;&#36259;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#23545;&#20854;&#20182;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#65288;&#22914;&#30528;&#33394;&#12289;&#20462;&#34917;&#12289;&#20998;&#21106;&#31561;&#65289;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#65288;&#38750;&#37197;&#23545;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#39640;&#24230;&#22797;&#26434;&#30340;&#25216;&#26415;&#20174;&#19968;&#20010;&#39046;&#22495;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#25104;&#21151;&#24212;&#29992;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#26159;&#20854;&#20013;&#19968;&#20010;&#39318;&#27425;&#25104;&#21151;&#24212;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21462;&#24471;&#20102;&#23454;&#38469;&#24433;&#21709;&#32780;&#38750;&#20165;&#20165;&#26159;&#29702;&#35770;&#23454;&#21147;&#23637;&#31034;&#30340;&#24778;&#20154;&#32467;&#26524;&#65292;&#36825;&#31181;&#32467;&#26524;&#20027;&#23548;&#20102;GAN&#30340;&#19990;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#32463;&#20856;&#24037;&#20316;CycleGAN [1] &#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#22833;&#36133;&#19982;GAN&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09646v1 Announce Type: cross  Abstract: The problem of image-to-image translation is one that is intruiging and challenging at the same time, for the impact potential it can have on a wide variety of other computer vision applications like colorization, inpainting, segmentation and others. Given the high-level of sophistication needed to extract patterns from one domain and successfully applying them to another, especially, in a completely unsupervised (unpaired) manner, this problem has gained much attention as of the last few years. It is one of the first problems where successful applications to deep generative models, and especially Generative Adversarial Networks achieved astounding results that are actually of realworld impact, rather than just a show of theoretical prowess; the such that has been dominating the GAN world. In this work, we study some of the failure cases of a seminal work in the field, CycleGAN [1] and hypothesize that they are GAN-stability related, a
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>VISA&#26041;&#27861;&#36890;&#36807;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#36817;&#20284;&#25512;&#26029;&#65292;&#33021;&#22815;&#22312;&#20445;&#23432;&#36873;&#25321;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#20197;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#36798;&#21040;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#24403;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.09429</link><description>&lt;p&gt;
&#20855;&#26377;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Sequential Sample-Average Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09429
&lt;/p&gt;
&lt;p&gt;
VISA&#26041;&#27861;&#36890;&#36807;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#36817;&#20284;&#25512;&#26029;&#65292;&#33021;&#22815;&#22312;&#20445;&#23432;&#36873;&#25321;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#20197;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#36798;&#21040;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#24403;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#65288;VISA&#65289;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#36817;&#20284;&#25512;&#26029;&#65292;&#20363;&#22914;&#22522;&#20110;&#25968;&#20540;&#27169;&#25311;&#30340;&#27169;&#22411;&#12290;VISA&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#26469;&#25193;&#23637;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#21069;&#21521;KL&#21464;&#20998;&#25512;&#26029;&#65292;&#36825;&#20123;&#36924;&#36817;&#22312;&#20449;&#20219;&#21306;&#22495;&#20869;&#34987;&#35270;&#20026;&#26377;&#25928;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#22810;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#37325;&#22797;&#20351;&#29992;&#27169;&#22411;&#35780;&#20272;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#12289;Lotka-Volterra&#21160;&#21147;&#23398;&#21644;Pickover&#21560;&#24341;&#23376;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;VISA&#21487;&#20197;&#22312;&#36873;&#25321;&#20445;&#23432;&#30340;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20004;&#20493;&#25110;&#26356;&#39640;&#30340;&#35745;&#31639;&#33410;&#32422;&#36798;&#21040;&#19982;&#26631;&#20934;&#37325;&#35201;&#24615;&#21152;&#26435;&#21069;&#21521;KL&#21464;&#20998;&#25512;&#26029;&#30456;&#24403;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09429v1 Announce Type: cross  Abstract: We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.09303</link><description>&lt;p&gt;
&#29992;&#29702;&#35770;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#20165;&#20351;&#29992;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#24322;&#24120;&#21457;&#29616;&#65292;&#23545;&#20581;&#24247;&#31579;&#26597;&#21644;&#35782;&#21035;&#32597;&#35265;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#65288;AEs&#65289;&#30340;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#23427;&#20204;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#24037;&#20316;&#65306;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#35757;&#32451;&#30340;AEs&#19981;&#33021;&#24456;&#22909;&#22320;&#37325;&#24314;&#30475;&#19981;&#35265;&#30340;&#24322;&#24120;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#22522;&#20110;&#37325;&#24314;&#38169;&#35823;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#24314;&#35757;&#32451;&#30446;&#26631;&#19982;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#19981;&#22815;&#21512;&#29702;&#12290;&#35813;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#22522;&#20110;AE&#30340;&#37325;&#24314;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21407;&#21017;&#65292;&#24182;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09303v1 Announce Type: new  Abstract: Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the informati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08838</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33337;&#33334;&#36712;&#36857;&#32858;&#31867;&#26088;&#22312;&#23547;&#25214;&#30456;&#20284;&#30340;&#36712;&#36857;&#27169;&#24335;&#65292;&#22312;&#28023;&#19978;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#21644;&#38408;&#20540;&#26469;&#35782;&#21035;&#31163;&#25955;&#30340;&#33337;&#33334;&#34892;&#20026;&#65292;&#20294;&#23384;&#22312;&#26080;&#27861;&#34920;&#31034;&#28436;&#21464;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#65288;PC-HiV&#65289;&#30340;&#26041;&#27861;&#12290;PC-HiV&#39318;&#20808;&#20351;&#29992;&#20998;&#23618;&#34920;&#31034;&#23558;&#27599;&#26465;&#36712;&#36857;&#36716;&#25442;&#20026;&#34892;&#20026;&#24207;&#21015;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#25139;&#39044;&#27979;&#28436;&#21270;&#12290;&#36890;&#36807;&#24212;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;PC-HiV&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#22312;&#30495;&#23454;AIS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PC-HiV&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25429;&#25417;&#33337;&#33334;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08838v1 Announce Type: cross  Abstract: Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturin
&lt;/p&gt;</description></item><item><title>&#35813;&#27169;&#22411;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#21407;&#23376;&#31867;&#22411;&#21644;&#20960;&#20309;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#33647;&#20998;&#23376;&#26500;&#35937;&#30340;&#39640;&#31934;&#24230;&#29983;&#25104;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07925</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#31867;&#33647;&#20998;&#23376;&#26500;&#35937;
&lt;/p&gt;
&lt;p&gt;
Physics-informed generative model for drug-like molecule conformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#27169;&#22411;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#21407;&#23376;&#31867;&#22411;&#21644;&#20960;&#20309;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#33647;&#20998;&#23376;&#26500;&#35937;&#30340;&#39640;&#31934;&#24230;&#29983;&#25104;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#35937;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20391;&#37325;&#20110;&#37325;&#29616;&#25104;&#38190;&#32467;&#26500;&#65292;&#24182;&#19988;&#26159;&#20174;&#20256;&#32479;&#21147;&#22330;&#20013;&#36890;&#24120;&#25214;&#21040;&#30340;&#30456;&#20851;&#39033;&#26500;&#24314;&#30340;&#65292;&#20197;&#30830;&#20445;&#29289;&#29702;&#30456;&#20851;&#24615;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#34987;&#29992;&#26469;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#21407;&#23376;&#31867;&#22411;&#21644;&#20960;&#20309;&#21442;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#25193;&#25955;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#23454;&#29616;&#26500;&#35937;&#37319;&#26679;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#31867;&#33647;&#20998;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20248;&#21270;&#20102;&#21322;&#32463;&#39564;GFN2-xTB&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25104;&#38190;&#21442;&#25968;&#30340;&#39640;&#31934;&#24230;&#39044;&#27979;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#36824;&#19982;&#34507;&#30333;&#36136;&#25968;&#25454;&#24211;&#65288;PDB&#65289;&#21644;&#21073;&#26725;&#32467;&#26500;&#25968;&#25454;&#24211;&#65288;CSD&#65289;&#20013;&#30340;&#23454;&#39564;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07925v1 Announce Type: cross  Abstract: We present a diffusion-based, generative model for conformer generation. Our model is focused on the reproduction of bonded structure and is constructed from the associated terms traditionally found in classical force fields to ensure a physically relevant representation. Techniques in deep learning are used to infer atom typing and geometric parameters from a training set. Conformer sampling is achieved by taking advantage of recent advancements in diffusion-based generation. By training on large, synthetic data sets of diverse, drug-like molecules optimized with the semiempirical GFN2-xTB method, high accuracy is achieved for bonded parameters, exceeding that of conventional, knowledge-based methods. Results are also compared to experimental structures from the Protein Databank (PDB) and Cambridge Structural Database (CSD).
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#65288;FBI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#28857;&#22312;&#29942;&#39048;&#23618;&#20043;&#21069;&#30340;&#29305;&#24449;&#33539;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;XAI&#26041;&#27861;&#33267;&#23569;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#28857;&#20113;&#25110;&#22823;&#35268;&#27169;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.07706</link><description>&lt;p&gt;
&#28857;&#20113;&#32593;&#32476;&#30340;&#24555;&#36895;&#31616;&#21333;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fast and Simple Explainability for Point Cloud Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#65288;FBI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#28857;&#22312;&#29942;&#39048;&#23618;&#20043;&#21069;&#30340;&#29305;&#24449;&#33539;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;XAI&#26041;&#27861;&#33267;&#23569;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#28857;&#20113;&#25110;&#22823;&#35268;&#27169;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28857;&#20113;&#25968;&#25454;&#30340;&#24555;&#36895;&#31616;&#21333;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#23427;&#35745;&#31639;&#20102;&#30456;&#23545;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#19979;&#28216;&#20219;&#21153;&#30340;&#27599;&#20010;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#35843;&#35797;&#21644;&#21487;&#35270;&#21270;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#26377;&#21161;&#20110;&#22312;&#32447;&#21453;&#39304;&#21040;&#32593;&#32476;&#36827;&#34892;&#25512;&#26029;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#8221;&#65288;FBI&#65289;&#65292;&#22312;&#29942;&#39048;&#23618;&#20043;&#21069;&#35745;&#31639;&#27599;&#20010;&#28857;&#30340;&#29305;&#24449;&#33539;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#30340;&#20351;&#29992;&#20197;&#21450;&#21518;&#29942;&#39048;&#21644;&#21069;&#29942;&#39048;&#31574;&#30053;&#65292;&#32467;&#26524;&#26174;&#31034;&#21069;&#29942;&#39048;&#26356;&#21463;&#38738;&#30544;&#65292;&#20174;&#24179;&#28369;&#24230;&#21644;&#25490;&#21517;&#35282;&#24230;&#26469;&#30475;&#12290;&#19982;&#24403;&#21069;&#30340;XAI&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33267;&#23569;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#22823;&#22411;&#28857;&#20113;&#25110;&#22823;&#35268;&#27169;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;SOTA&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07706v1 Announce Type: cross  Abstract: We propose a fast and simple explainable AI (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA re
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>AdaNovo &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#20809;&#35889;&#21644;&#27599;&#20010;&#27688;&#22522;&#37240;/&#32957;&#27573;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.07013</link><description>&lt;p&gt;
AdaNovo&#65306;&#20855;&#26377;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;\emph{De Novo}&#32957;&#29255;&#27573;&#27979;&#24207;
&lt;/p&gt;
&lt;p&gt;
AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07013
&lt;/p&gt;
&lt;p&gt;
AdaNovo &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#20809;&#35889;&#21644;&#27599;&#20010;&#27688;&#22522;&#37240;/&#32957;&#27573;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#35889;&#32852;&#29992;&#24050;&#22312;&#20419;&#36827;&#34507;&#30333;&#36136;&#32452;&#23398;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20351;&#24471;&#21487;&#20197;&#20998;&#26512;&#29983;&#29289;&#26679;&#26412;&#20013;&#30340;&#34507;&#30333;&#36136;&#32452;&#25104;&#12290;&#23613;&#31649;&#24050;&#24320;&#21457;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#23548;&#33268;&#35266;&#23519;&#20809;&#35889;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#65288;&#32957;&#27573;&#65289;&#65292;&#20294;\emph{de novo}&#32957;&#27573;&#27979;&#24207;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaNovo&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#35745;&#31639;&#20102;&#20809;&#35889;&#21644;&#27599;&#20010;&#27688;&#22522;&#37240;/&#32957;&#27573;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#65292;&#24182;&#21033;&#29992;CMI&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07013v1 Announce Type: cross  Abstract: Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological samples. Despite the development of various deep learning methods for identifying amino acid sequences (peptides) responsible for observed spectra, challenges persist in \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with post-translational modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in decreased peptide-level identification precision. Secondly, diverse types of noise and missing peaks in mass spectra reduce the reliability of training data (peptide-spectrum matches, PSMs). To address these challenges, we propose AdaNovo, a novel framework that calculates conditional mutual information (CMI) between the spectrum and each amino acid/peptide, using CMI for adaptive model training. Extens
&lt;/p&gt;</description></item><item><title>$\mathtt{tsGT}$&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#36229;&#36807;&#20854;&#38543;&#26426;&#21516;&#34892;&#65292;&#29305;&#21035;&#22312;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#21644;&#36793;&#38469;&#20998;&#20301;&#20540;&#39044;&#27979;&#26041;&#38754;&#20855;&#22791;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05713</link><description>&lt;p&gt;
$\mathtt{tsGT}$&#65306;&#20855;&#26377;Transformer&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05713
&lt;/p&gt;
&lt;p&gt;
$\mathtt{tsGT}$&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#36229;&#36807;&#20854;&#38543;&#26426;&#21516;&#34892;&#65292;&#29305;&#21035;&#22312;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#21644;&#36793;&#38469;&#20998;&#20301;&#20540;&#39044;&#27979;&#26041;&#38754;&#20855;&#22791;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#22312;&#20960;&#20046;&#25152;&#26377;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#22823;&#25209;&#20855;&#26377;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#26550;&#26500;&#20559;&#35265;&#30340;&#30830;&#23450;&#24615;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#24341;&#20837;&#20102;$\mathtt{tsGT}$&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#28378;&#21160;&#31383;&#21475;&#22238;&#27979;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;$\mathtt{tsGT}$&#22312;&#22235;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#22312;MAD&#21644;RMSE&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#22312;QL&#21644;CRPS&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#38543;&#26426;&#21516;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;$\mathtt{tsGT}$&#22312;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#21644;&#39044;&#27979;&#36793;&#38469;&#20998;&#20301;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#26469;&#34917;&#20805;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05713v1 Announce Type: new  Abstract: Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$'s ability to model the data distribution and predict marginal quantile values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#23398;&#20064;&#21463;&#38480;&#21046;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#23545;&#20598;&#35299;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#21407;&#22987;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20598;&#21487;&#34892;&#35299;&#23545;&#65292;&#21516;&#26102;&#36845;&#20195;&#21521;&#21407;&#22987;&#21487;&#34892;&#24615;&#65292;&#27169;&#25311;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03454</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23398;&#20064;&#21463;&#38480;&#21046;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Constrained Optimization with Deep Augmented Lagrangian Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#23398;&#20064;&#21463;&#38480;&#21046;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#23545;&#20598;&#35299;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#21407;&#22987;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20598;&#21487;&#34892;&#35299;&#23545;&#65292;&#21516;&#26102;&#36845;&#20195;&#21521;&#21407;&#22987;&#21487;&#34892;&#24615;&#65292;&#27169;&#25311;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#65288;LtO&#65289;&#26159;&#19968;&#20010;&#38382;&#39064;&#35774;&#32622;&#65292;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#27169;&#25311;&#19968;&#20010;&#21463;&#38480;&#21046;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;&#23398;&#20064;&#20135;&#29983;&#26368;&#20248;&#21644;&#31526;&#21512;&#22797;&#26434;&#32422;&#26463;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#31354;&#38388;&#38480;&#21046;&#20026;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#22823;&#22810;&#25968;LtO&#26041;&#27861;&#20391;&#37325;&#20110;&#30452;&#25509;&#23398;&#20064;&#21407;&#22987;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24212;&#29992;&#26657;&#27491;&#26041;&#26696;&#25110;&#25439;&#22833;&#20989;&#25968;&#24809;&#32602;&#26469;&#40723;&#21169;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;ML&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#23545;&#20598;&#35299;&#20272;&#35745;&#65292;&#20174;&#32780;&#26500;&#24314;&#21407;&#22987;&#20272;&#35745;&#20197;&#24418;&#25104;&#23545;&#20598;&#21487;&#34892;&#35299;&#23545;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#26696;&#65292;&#22312;&#36825;&#31181;&#26041;&#26696;&#20013;&#23545;&#20598;&#30446;&#26631;&#34987;&#26368;&#22823;&#21270;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#26041;&#26696;&#20272;&#35745;&#21521;&#21407;&#22987;&#21487;&#34892;&#24615;&#36845;&#20195;&#65292;&#27169;&#25311;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03454v1 Announce Type: new  Abstract: Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.03359</link><description>&lt;p&gt;
RACE-SM:&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#24335;&#21277;&#36947;&#21512;&#27969;&#33258;&#20027;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#22312;&#20154;&#25511;&#36710;&#36742;&#20132;&#36890;&#20013;&#20173;&#28982;&#26159;&#33258;&#20027;&#36710;&#36742;&#25511;&#21046;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#38750;&#23398;&#20064;&#22411;&#36710;&#36742;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20381;&#36182;&#35268;&#21017;&#21644;&#20248;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#23637;&#29616;&#20102;&#24076;&#26395;&#65292;&#24182;&#21463;&#21040;&#20102;&#37325;&#35201;&#23398;&#26415;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#20854;&#20182;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#20851;&#27880;&#19981;&#36275;&#65292;&#19988;&#32463;&#24120;&#20381;&#36182;&#19981;&#20934;&#30830;&#30340;&#36947;&#36335;&#20132;&#36890;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#24182;&#34892;&#24335;&#24773;&#20917;&#24456;&#23569;&#34987;&#32771;&#34385;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#21464;&#36947;&#20915;&#31574;&#21046;&#23450;&#65292;&#35813;&#27169;&#22411;&#26126;&#30830;&#32771;&#34385;&#20102;&#23545;&#20110;&#36710;&#36742;&#26412;&#36523;&#21450;&#20854;&#21608;&#22260;&#36710;&#36742;&#65288;&#21487;&#33021;&#21512;&#20316;&#25110;&#19981;&#21512;&#20316;&#65289;&#30340;&#25928;&#29992;&#65292;&#20197;&#20135;&#29983;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#20989;&#25968;&#21033;&#29992;&#31038;&#20132;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03359v1 Announce Type: new  Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02598</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#19981;&#24179;&#34913;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Pooling Image Datasets With Multiple Covariate Shift and Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#31185;&#20013;&#24120;&#35265;&#23567;&#26679;&#26412;&#22823;&#23567;&#65292;&#36825;&#38656;&#35201;&#36328;&#22810;&#20010;&#26426;&#26500;&#27719;&#24635;&#22823;&#33268;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#22270;&#20687;&#19982;&#30142;&#30149;&#32467;&#26524;&#20043;&#38388;&#30340;&#24369;&#20294;&#30456;&#20851;&#20851;&#32852;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20307;&#29616;&#20986;&#21327;&#21464;&#37327;&#65288;&#21363;&#27425;&#35201;&#30340;&#38750;&#25104;&#20687;&#25968;&#25454;&#65289;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#22312;&#26631;&#20934;&#32479;&#35745;&#20998;&#26512;&#20013;&#25511;&#21046;&#36825;&#20123;&#26080;&#29992;&#21464;&#37327;&#26159;&#24120;&#35265;&#30340;&#65292;&#20294;&#36825;&#20123;&#24605;&#24819;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#21442;&#25968;&#36807;&#22810;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20174;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#36215;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#24211;&#20165;&#38480;&#20110;&#19968;&#27425;&#32771;&#34385;&#20960;&#20010;&#21327;&#21464;&#37327;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#30475;&#24453;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#21407;&#26412;&#38656;&#35201;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02598v1 Announce Type: new  Abstract: Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effect
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.01759</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#65306;&#22238;&#39038;&#19982;&#26032;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Open-world Machine Learning: A Review and New Outlooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#21363;&#20551;&#23450;&#29615;&#22659;&#26159;&#38745;&#24577;&#30340;&#65292;&#27169;&#22411;&#19968;&#26086;&#37096;&#32626;&#23601;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#22522;&#26412;&#19988;&#30456;&#24403;&#24188;&#31258;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#24320;&#25918;&#29615;&#22659;&#22797;&#26434;&#12289;&#21160;&#24577;&#19988;&#20805;&#28385;&#26410;&#30693;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25298;&#32477;&#26410;&#30693;&#12289;&#21457;&#29616;&#26032;&#22855;&#28857;&#65292;&#28982;&#21518;&#36880;&#27493;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#20687;&#29983;&#29289;&#31995;&#32479;&#19968;&#26679;&#23433;&#20840;&#22320;&#24182;&#25345;&#32493;&#36827;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#22312;&#32479;&#19968;&#33539;&#24335;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25361;&#25112;&#12289;&#21407;&#21017;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01759v1 Announce Type: new  Abstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15656</link><description>&lt;p&gt;
&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#65306;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#30340;&#32479;&#19968;&#36882;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#31639;&#23376;&#65288;NOs&#65289;&#29702;&#35770;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35745;&#31639;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#35299;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;NO&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#26102;&#31354;PDE&#26102;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#21069;&#30340;NO&#29702;&#35770;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#20415;&#26681;&#25454;&#31232;&#30095;&#37319;&#26679;&#30340;&#22024;&#26434;&#27979;&#37327;&#26377;&#25928;&#22320;&#32416;&#27491;PDE&#35299;&#30340;&#28436;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#26469;&#35745;&#31639;&#26080;&#38480;&#32500;&#21322;&#32447;&#24615;PDE&#30340;&#35299;&#31639;&#23376;&#12290;&#21033;&#29992;&#21322;&#32447;&#24615;PDE&#30340;&#32467;&#26500;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#35266;&#27979;&#32773;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#65292;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15656v1 Announce Type: cross  Abstract: Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with spatio-temporal PDEs over long time scales. Specifically, the current theory of NOs does not present a systematic framework to perform data assimilation and efficiently correct the evolution of PDE solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear PDEs. Exploiting the structure of semilinear PDEs and the theory of nonlinear observers in function spaces, we develop a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. The proposed 
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.14982</link><description>&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#22312;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#23637;&#29616;&#20986;&#19981;&#21516;&#27169;&#24335;&#65306;&#21021;&#27493;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14982
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#24182;&#27809;&#26377;&#26174;&#31034;&#20986;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#20043;&#38388;&#30340;&#28165;&#26224;&#19981;&#21516;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22823;&#33041;&#27963;&#21160;&#65292;&#36890;&#36807; EEG &#27979;&#37327;&#65292;&#22312;&#20010;&#20307;&#25509;&#35302;&#34394;&#20551;&#19982;&#30495;&#23454;&#38899;&#39057;&#26102;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#36825;&#20123;&#21021;&#27493;&#35777;&#25454;&#20026;&#26410;&#26469;&#22312;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14982v1 Announce Type: cross  Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MolEdit3D&#65292;&#23558;3D&#20998;&#23376;&#29983;&#25104;&#19982;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#32534;&#36753;&#20998;&#23376;&#26102;&#36873;&#25321;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14315</link><description>&lt;p&gt;
&#36890;&#36807;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21644;&#37319;&#26679;&#36827;&#34892;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MolEdit3D&#65292;&#23558;3D&#20998;&#23376;&#29983;&#25104;&#19982;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#32534;&#36753;&#20998;&#23376;&#26102;&#36873;&#25321;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#26088;&#22312;&#22312;&#20808;&#39564;&#30693;&#35782;&#19979;&#29983;&#25104;&#20855;&#26377;&#39640;&#20146;&#21644;&#21147;&#30340;&#37197;&#20307;&#65292;&#24182;&#20102;&#35299;3D&#38774;&#26631;&#32467;&#26500;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#32473;&#23450;&#30446;&#26631;&#32467;&#21512;&#20301;&#28857;&#30340;3D&#37197;&#20307;&#20998;&#24067;&#65292;&#35201;&#20040;&#36845;&#20195;&#20462;&#25913;&#20998;&#23376;&#20197;&#20248;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#27963;&#24615;&#20272;&#35745;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;3D&#20998;&#23376;&#29983;&#25104;&#19982;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#32534;&#36753;&#20998;&#23376;&#26102;&#36873;&#25321;&#22312;2D&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#20998;&#23376;&#23545;&#25509;&#26469;&#20272;&#35745;&#27963;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14315v1 Announce Type: cross  Abstract: Structure-based drug design aims at generating high affinity ligands with prior knowledge of 3D target structures. Existing methods either use conditional generative model to learn the distribution of 3D ligands given target binding sites, or iteratively modify molecules to optimize a structure-based activity estimator. The former is highly constrained by data quantity and quality, which leaves optimization-based approaches more promising in practical scenario. However, existing optimization-based approaches choose to edit molecules in 2D space, and use molecular docking to estimate the activity using docking predicted 3D target-ligand complexes. The misalignment between the action space and the objective hinders the performance of these models, especially for those employ deep learning for acceleration. In this work, we propose MolEdit3D to combine 3D molecular generation with optimization frameworks. We develop a novel 3D graph editi
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10242</link><description>&lt;p&gt;
&#26377;&#31526;&#21495;&#22810;&#26679;&#21270;&#22810;&#37325;&#32593;&#32476;&#65306;&#32858;&#31867;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Signed Diverse Multiplex Networks: Clustering and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10242
&lt;/p&gt;
&lt;p&gt;
&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#31526;&#21495;&#30340;&#24191;&#20041;&#38543;&#26426;&#28857;&#31215;&#22270;&#65288;SGRDPG&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#24191;&#20041;&#38543;&#26426;&#28857;&#31215;&#22270;&#65288;GRDPG&#65289;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#36793;&#21487;&#20197;&#26159;&#27491;&#30340;&#20063;&#21487;&#20197;&#26159;&#36127;&#30340;&#12290;&#35813;&#35774;&#32622;&#34987;&#25193;&#23637;&#20026;&#22810;&#37325;&#32593;&#32476;&#29256;&#26412;&#65292;&#20854;&#20013;&#25152;&#26377;&#23618;&#20855;&#26377;&#30456;&#21516;&#30340;&#33410;&#28857;&#38598;&#21512;&#24182;&#36981;&#24490;SGRDPG&#12290;&#32593;&#32476;&#23618;&#30340;&#21807;&#19968;&#20844;&#20849;&#29305;&#24449;&#26159;&#23427;&#20204;&#21487;&#20197;&#34987;&#21010;&#20998;&#20026;&#20855;&#26377;&#20849;&#21516;&#23376;&#31354;&#38388;&#32467;&#26500;&#30340;&#32452;&#65292;&#32780;&#20854;&#20182;&#24773;&#20917;&#19979;&#25152;&#26377;&#36830;&#25509;&#27010;&#29575;&#30697;&#38453;&#21487;&#33021;&#26159;&#23436;&#20840;&#19981;&#21516;&#30340;&#12290;&#19978;&#36848;&#35774;&#32622;&#38750;&#24120;&#28789;&#27963;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#29616;&#26377;&#22810;&#37325;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#20854;&#29305;&#20363;&#12290;&#35770;&#25991;&#23454;&#29616;&#20102;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#34920;&#26126;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#24212;&#23545;&#35832;&#22914;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#20043;&#31867;&#30340;&#29616;&#23454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10242v1 Announce Type: cross  Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise all matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models as its particular cases. The paper fulfills two objectives. First, it shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as analysis of brain networks. Second, b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21644;I/O&#35268;&#33539;&#23545;&#40784;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#31185;&#23398;&#32534;&#31243;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08073</link><description>&lt;p&gt;
&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#26469;&#25903;&#25745;&#25968;&#25454;&#31185;&#23398;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Grounding Data Science Code Generation with Input-Output Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21644;I/O&#35268;&#33539;&#23545;&#40784;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#31185;&#23398;&#32534;&#31243;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;(NL)&#25552;&#31034;&#29983;&#25104;&#20195;&#30721;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;NL&#24448;&#24448;&#36807;&#20110;&#27169;&#31946;&#65292;&#26080;&#27861;&#25429;&#25417;&#32534;&#31243;&#38382;&#39064;&#32972;&#21518;&#30340;&#30495;&#23454;&#24847;&#22270;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#36755;&#20837;&#36755;&#20986;(I/O)&#35268;&#33539;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#21487;&#33021;&#38590;&#20197;&#23558;&#20854;&#36755;&#20986;&#19982;NL&#25552;&#31034;&#21644;I/O&#35268;&#33539;&#23545;&#40784;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#25968;&#25454;&#31185;&#23398;&#32534;&#31243;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#38656;&#35201;&#26126;&#30830;&#30340;I/O&#35268;&#33539;&#20197;&#20445;&#35777;&#28165;&#26224;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIFT4Code&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;I/O&#35268;&#33539;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#30340;LLM&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#26412;&#36523;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#27966;&#29983;&#30340;&#21453;&#39304;&#20316;&#20026;&#20851;&#38190;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#23558;&#35813;&#21453;&#39304;&#20197;&#31243;&#24207;I/O&#35268;&#33539;&#30340;&#24418;&#24335;&#25552;&#20379;&#32473;LLM&#20197;&#20419;&#36827;&#25351;&#23548;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data scien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03388</link><description>&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#34892;&#20026;&#29992;&#25143;&#20998;&#21106;&#20013;&#30340;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03388
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#32447;&#34892;&#20026;&#36275;&#36857;&#21487;&#20197;&#20351;&#20844;&#21496;&#21457;&#29616;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#32454;&#20998;&#65292;&#24182;&#21521;&#29992;&#25143;&#21457;&#36865;&#29305;&#23450;&#32454;&#20998;&#30340;&#20449;&#24687;&#12290;&#22312;&#21457;&#29616;&#32454;&#20998;&#20043;&#21518;&#65292;&#36890;&#36807;&#20687;Facebook&#21644;Google&#36825;&#26679;&#30340;&#39318;&#36873;&#23186;&#20307;&#28192;&#36947;&#21521;&#29992;&#25143;&#21457;&#36865;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21482;&#26377;&#37096;&#20998;&#34892;&#20026;&#32454;&#20998;&#20013;&#30340;&#29992;&#25143;&#22312;&#23186;&#20307;&#19978;&#25214;&#21040;&#21305;&#37197;&#65292;&#24182;&#19988;&#21482;&#26377;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#30475;&#21040;&#28040;&#24687;&#65288;&#26333;&#20809;&#65289;&#12290;&#21363;&#20351;&#39640;&#36136;&#37327;&#30340;&#21457;&#29616;&#20063;&#20250;&#22312;&#20256;&#36882;&#22833;&#36133;&#26102;&#21464;&#24471;&#26080;&#29992;&#12290;&#35768;&#22810;&#22797;&#26434;&#30340;&#31639;&#27861;&#29992;&#20110;&#21457;&#29616;&#34892;&#20026;&#32454;&#20998;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#20256;&#36882;&#32452;&#20214;&#12290;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#26159;&#22240;&#20026;&#65288;i&#65289;&#21457;&#29616;&#26159;&#22312;&#20844;&#21496;&#25968;&#25454;&#65288;&#20363;&#22914;&#29992;&#25143;&#28857;&#20987;&#65289;&#30340;&#34892;&#20026;&#25968;&#25454;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#20256;&#36882;&#21017;&#26159;&#22522;&#20110;&#23186;&#20307;&#23450;&#20041;&#30340;&#38745;&#24577;&#25968;&#25454;&#31354;&#38388;&#65288;&#20363;&#22914;&#22320;&#29702;&#20301;&#32622;&#65292;&#24180;&#40836;&#65289;&#36827;&#34892;&#30340;&#65307;&#65288;ii&#65289;&#20844;&#21496;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36816;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;CT&#30340;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#20013;&#30340;&#35299;&#21078;&#20998;&#21106;&#25552;&#20379;&#20102;&#38024;&#23545;3D U-shaped&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03230</link><description>&lt;p&gt;
&#22522;&#20110;CT&#30340;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#30340;&#35299;&#21078;&#20998;&#21106;&#65306;&#38024;&#23545;3D U-shaped&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;CT&#30340;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#20013;&#30340;&#35299;&#21078;&#20998;&#21106;&#25552;&#20379;&#20102;&#38024;&#23545;3D U-shaped&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#24739;&#32773;&#29305;&#23450;&#33016;&#37096;&#25163;&#26415;&#35268;&#21010;&#21644;&#20223;&#30495;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#38656;&#35201;&#20174;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#20013;&#39640;&#25928;&#12289;&#31283;&#20581;&#22320;&#21019;&#24314;&#25968;&#23383;&#35299;&#21078;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;(DL)&#29616;&#22312;&#26159;&#21508;&#31181;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#32780;U-shaped DL&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33258;2D UNet&#20197;&#26469;&#23601;&#19968;&#30452;&#22914;&#27492;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32593;&#32476;&#37197;&#32622;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;U-shaped&#27169;&#22411;&#30340;&#21464;&#20307;&#12290;&#20511;&#21161;&#26368;&#36817;&#22823;&#22411;&#22810;&#26631;&#31614;&#25968;&#25454;&#24211;&#30340;&#21457;&#23637;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#31995;&#32479;&#22522;&#20934;&#30740;&#31350;&#21487;&#20197;&#20026;&#20020;&#24202;&#37096;&#32626;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20294;&#27492;&#31867;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;3D U-shaped&#27169;&#22411;(3DUNet&#12289;STUNet&#12289;AttentionUNet&#12289;SwinUNETR&#12289;FocalSegNet&#21644;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#22235;&#20010;&#21464;&#20307;&#30340;3D SwinUnet)&#30340;&#31532;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#22522;&#20110;CT&#30340;&#33016;&#37096;&#35299;&#21078;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#35270;&#35273;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#22495;&#24182;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.02340</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#25552;&#31034;&#20013;&#23398;&#20064;&#35821;&#20041;&#20195;&#29702;&#65292;&#20026;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#35270;&#35273;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#22495;&#24182;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;(DML)&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20851;&#27880;&#30340;&#37325;&#28857;&#30446;&#26631;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#38598;&#20013;&#20110;&#23545;&#20256;&#32479;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#30001;&#20110;&#26368;&#36817;&#20174;&#26356;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#65292;&#23558;&#35813;&#27169;&#22411;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#22495;&#30340;DML&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;DML&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;(ViT)&#20013;&#30340;&#35270;&#35273;&#25552;&#31034;(VPT)&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;DML&#33539;&#20363;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21644;ViT&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#20195;&#29702;&#20013;&#26469;&#20248;&#21270;&#27599;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#36924;&#36817;&#26041;&#27861;&#22312;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#20248;&#20110;&#20195;&#34920;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models trained from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#33041;&#32959;&#30244;&#65292;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;98%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00038</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#33041;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
Detecting Brain Tumors through Multimodal Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00038
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#33041;&#32959;&#30244;&#65292;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;98%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32959;&#30244;&#21487;&#20197;&#20197;&#21508;&#31181;&#24418;&#24335;&#20986;&#29616;&#22312;&#20154;&#20307;&#30340;&#19981;&#21516;&#37096;&#20301;&#12290;&#30001;&#20110;&#33041;&#32452;&#32455;&#30340;&#22797;&#26434;&#24615;&#65292;&#33041;&#32959;&#30244;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#29305;&#21035;&#22256;&#38590;&#12290;&#21450;&#26102;&#26816;&#27979;&#32959;&#30244;&#21487;&#20197;&#38477;&#20302;&#27515;&#20129;&#39118;&#38505;&#65292;&#24182;&#20026;&#24739;&#32773;&#30340;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#20415;&#21033;&#12290;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36890;&#36807;&#25104;&#20687;&#25216;&#26415;&#33719;&#21462;&#22270;&#20687;&#26469;&#21457;&#29616;&#21644;&#35782;&#21035;&#32959;&#30244;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#23558;&#22788;&#29702;&#25104;&#28784;&#24230;&#22270;&#20687;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#29992;&#20110;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#24182;&#19982;&#31867;&#20284;&#30740;&#31350;&#19968;&#33268;&#65292;&#27169;&#22411;&#20934;&#30830;&#29575;&#32422;&#20026;98&#65285;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#20154;&#31867;&#25511;&#21046;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tumors can manifest in various forms and in different areas of the human body. Brain tumors are specifically hard to diagnose and treat because of the complexity of the organ in which they develop. Detecting them in time can lower the chances of death and facilitate the therapy process for patients. The use of Artificial Intelligence (AI) and, more specifically, deep learning, has the potential to significantly reduce costs in terms of time and resources for the discovery and identification of tumors from images obtained through imaging techniques. This research work aims to assess the performance of a multimodal model for the classification of Magnetic Resonance Imaging (MRI) scans processed as grayscale images. The results are promising, and in line with similar works, as the model reaches an accuracy of around 98\%. We also highlight the need for explainability and transparency to ensure human control and safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.12467</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#25509;&#35302;&#32593;&#26684;&#21464;&#25442;&#22120;&#23398;&#20064;&#28789;&#27963;&#36523;&#20307;&#30896;&#25758;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#39640;&#32500;&#29289;&#29702;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#23601;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27714;&#35299;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#30340;&#26159;&#23427;&#20204;&#26159;&#21542;&#26377;&#25928;&#22320;&#24212;&#23545;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#30340;&#25361;&#25112;&#65292;&#21363;&#30636;&#26102;&#30896;&#25758;&#21457;&#29983;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#36523;&#20307;&#31354;&#38388;&#20301;&#32622;&#20043;&#38388;&#65288;&#30001;&#30896;&#25758;&#24341;&#36215;&#30340;&#65289;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;--&#22312;&#26356;&#39640;&#32423;&#21035;&#32593;&#26684;&#20013;&#30340;&#20004;&#20010;&#25509;&#36817;&#20301;&#32622;&#23545;&#24212;&#20110;&#36523;&#20307;&#20013;&#30340;&#20004;&#20010;&#36828;&#36317;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12467v2 Announce Type: replace-cross  Abstract: Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body -- two close positions in a higher-level mesh corresponds to two distant posi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;$k$-&#24418;&#24335;&#36827;&#34892;&#21333;&#32431;&#22797;&#21512;&#29289;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#21363;&#21487;&#33719;&#21462;&#20960;&#20309;&#20449;&#24687;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#21644;&#20960;&#20309;&#19968;&#33268;&#24615;&#65292;&#24182;&#33021;&#24212;&#29992;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#23454;&#29616;&#36890;&#29992;&#36924;&#36817;&#12290;</title><link>https://arxiv.org/abs/2312.08515</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;$k$-&#24418;&#24335;&#36827;&#34892;&#21333;&#32431;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simplicial Representation Learning with Neural $k$-Forms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;$k$-&#24418;&#24335;&#36827;&#34892;&#21333;&#32431;&#22797;&#21512;&#29289;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#21363;&#21487;&#33719;&#21462;&#20960;&#20309;&#20449;&#24687;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#21644;&#20960;&#20309;&#19968;&#33268;&#24615;&#65292;&#24182;&#33021;&#24212;&#29992;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#23454;&#29616;&#36890;&#29992;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Geometric deep learning&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#65292;&#33021;&#22815;&#34701;&#21512;&#20851;&#20110;&#20960;&#20309;&#21644;&#25299;&#25169;&#25968;&#25454;&#30340;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#39046;&#22495;&#65292;&#22914;&#22270;&#24418;&#20013;&#12290;&#23613;&#31649;&#28040;&#24687;&#20256;&#36882;&#22312;&#36825;&#19968;&#39046;&#22495;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23384;&#22312;&#35832;&#22914;&#38656;&#35201;&#37325;&#36830;&#22270;&#12289;&#25968;&#25454;&#35299;&#37322;&#27169;&#31946;&#21644;&#36807;&#24230;&#24179;&#28369;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#21033;&#29992;&#23884;&#20837;&#22312;$\mathbb{R}^n$&#20013;&#30340;&#21333;&#32431;&#22797;&#21512;&#29289;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#21033;&#29992;&#33410;&#28857;&#22352;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;$\mathbb{R}^n$&#20013;&#30340;&#24494;&#20998;k-&#24418;&#24335;&#26469;&#21019;&#24314;&#23545;&#21333;&#32431;&#20307;&#30340;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#20960;&#20309;&#19968;&#33268;&#24615;&#65292;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#12290;&#35813;&#26041;&#27861;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#24212;&#29992;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#65292;&#24182;&#23454;&#29616;&#20102;&#36890;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#12289;&#22810;&#21151;&#33021;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#36755;&#20837;&#22797;&#21512;&#29289;&#65292;&#21253;&#25324;&#22270;&#24418;&#12289;&#21333;&#32431;&#22797;&#21512;&#29289;&#21644;&#32990;&#22797;&#21512;&#29289;&#12290;&#23427;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08515v2 Announce Type: replace  Abstract: Geometric deep learning extends deep learning to incorporate information about the geometry and topology data, especially in complex domains like graphs. Despite the popularity of message passing in this field, it has limitations such as the need for graph rewiring, ambiguity in interpreting data, and over-smoothing. In this paper, we take a different approach, focusing on leveraging geometric information from simplicial complexes embedded in $\mathbb{R}^n$ using node coordinates. We use differential k-forms in \mathbb{R}^n to create representations of simplices, offering interpretability and geometric consistency without message passing. This approach also enables us to apply differential geometry tools and achieve universal approximation. Our method is efficient, versatile, and applicable to various input complexes, including graphs, simplicial complexes, and cell complexes. It outperforms existing message passing neural networks i
&lt;/p&gt;</description></item><item><title>&#31163;&#23376;&#26799;&#24230;&#21487;&#33021;&#20351;&#32454;&#32990;&#24418;&#25104;&#21160;&#24577;&#22810;&#21151;&#33021;&#30340;&#29983;&#29289;&#31995;&#32479;&#65292;&#20419;&#20351;&#32454;&#32990;&#33719;&#21462;&#12289;&#20998;&#26512;&#21644;&#21709;&#24212;&#29615;&#22659;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.07977</link><description>&lt;p&gt;
&#20351;&#29992;&#20648;&#23618;&#35745;&#31639;&#27169;&#22411;&#32454;&#32990;&#20013;&#30340;&#38750;&#36951;&#20256;&#20449;&#24687;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Modeling non-genetic information dynamics in cells using reservoir computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07977
&lt;/p&gt;
&lt;p&gt;
&#31163;&#23376;&#26799;&#24230;&#21487;&#33021;&#20351;&#32454;&#32990;&#24418;&#25104;&#21160;&#24577;&#22810;&#21151;&#33021;&#30340;&#29983;&#29289;&#31995;&#32479;&#65292;&#20419;&#20351;&#32454;&#32990;&#33719;&#21462;&#12289;&#20998;&#26512;&#21644;&#21709;&#24212;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#25152;&#26377;&#32454;&#32990;&#37117;&#20351;&#29992;&#33021;&#37327;&#21644;&#31163;&#23376;&#29305;&#24322;&#24615;&#33180;&#27893;&#26469;&#32500;&#25345;Na$^+$&#12289;K$^+$&#12289;Cl$^-$&#12289;Mg$^{++}$&#21644;Ca$^{++}$&#30340;&#36328;&#33180;&#26799;&#24230;&#12290;&#34429;&#28982;&#23427;&#20204;&#28040;&#32791;&#22810;&#36798;&#32454;&#32990;&#33021;&#37327;&#39044;&#31639;&#30340;1/3&#65292;&#20294;&#36328;&#33180;&#31163;&#23376;&#26799;&#24230;&#30340;&#30456;&#24212;&#36827;&#21270;&#20248;&#21183;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#31163;&#23376;&#26799;&#24230;&#20351;&#24471;&#19968;&#20010;&#21160;&#24577;&#19988;&#22810;&#21151;&#33021;&#30340;&#29983;&#29289;&#31995;&#32479;&#25104;&#20026;&#21487;&#33021;&#65292;&#35813;&#31995;&#32479;&#33719;&#21462;&#12289;&#20998;&#26512;&#24182;&#21709;&#24212;&#29615;&#22659;&#20449;&#24687;&#12290;&#25105;&#20204;&#20551;&#35774;&#29615;&#22659;&#20449;&#21495;&#36890;&#36807;&#27839;&#30528;&#39044;&#20808;&#23384;&#22312;&#30340;&#26799;&#24230;&#30340;&#31163;&#23376;&#36890;&#37327;&#36890;&#36807;&#38376;&#25511;&#31163;&#23376;&#29305;&#24322;&#24615;&#33180;&#36890;&#36947;&#20256;&#20837;&#32454;&#32990;&#20869;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32454;&#32990;&#36136;&#31163;&#23376;&#27987;&#24230;&#30340;&#25913;&#21464;&#21487;&#20197;&#20135;&#29983;&#23616;&#37096;&#21709;&#24212;&#65292;&#24182;&#36890;&#36807;&#27839;&#30528;&#39044;&#20808;&#23384;&#22312;&#30340;&#21644;&#33258;&#32452;&#35013;&#30340;&#32454;&#32990;&#39592;&#26550;&#36827;&#34892;&#32447;&#29366;&#31163;&#23376;&#27969;&#19982;&#20869;&#36136;&#32593;&#12289;&#32447;&#31890;&#20307;&#21644;&#32454;&#32990;&#26680;&#20132;&#20114;&#20197;&#32534;&#25490;&#20840;&#23616;&#25110;&#21306;&#22495;&#24615;&#21453;&#24212;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#20934;&#29289;&#29702;&#20551;&#35774;&#26469;&#34920;&#36798;&#25105;&#20204;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07977v2 Announce Type: replace-cross  Abstract: Virtually all cells use energy and ion-specific membrane pumps to maintain large transmembrane gradients of Na$^+$, K$^+$, Cl$^-$, Mg$^{++}$, and Ca$^{++}$. Although they consume up to 1/3 of a cell's energy budget, the corresponding evolutionary benefit of transmembrane ion gradients remain unclear. Here, we propose that ion gradients enable a dynamic and versatile biological system that acquires, analyzes, and responds to environmental information. We hypothesize environmental signals are transmitted into the cell by ion fluxes along pre-existing gradients through gated ion-specific membrane channels. The consequent changes of cytoplasmic ion concentration can generate a local response and orchestrate global or regional responses through wire-like ion fluxes along pre-existing and self-assembling cytoskeleton to engage the endoplasmic reticulum, mitochondria, and nucleus.   Here, we frame our hypothesis through a quasi-physic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#22914;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2312.05742</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Generalization Gap in Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#22914;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31163;&#32447;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#26159;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#31163;&#32447;RL&#12289;&#24207;&#21015;&#24314;&#27169;&#21644;&#34892;&#20026;&#20811;&#38534;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#19981;&#22914;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#31163;&#32447;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;Procgen&#65288;2D&#35270;&#39057;&#28216;&#25103;&#65289;&#21644;WebShop&#65288;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#65289;&#25910;&#38598;&#20102;&#22810;&#31181;&#22823;&#23567;&#21644;&#25216;&#33021;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#28216;&#25103;&#20851;&#21345;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#36712;&#36857;&#65292;&#27979;&#35797;&#26102;&#65292;&#20195;&#29702;&#35201;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#20851;&#21345;&#25110;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#38590;&#19982;&#22312;&#32447;RL&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05742v2 Announce Type: replace-cross  Abstract: Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and behavioral cloning. Our experiments show that offline learning algorithms perform worse on new environments than online learning ones. We also introduce the first benchmark for evaluating generalization in offline learning, collecting datasets of varying sizes and skill-levels from Procgen (2D video games) and WebShop (e-commerce websites). The datasets contain trajectories for a limited number of game levels or natural language instructions and at test time, the agent has to generalize to new levels or instructions. Our experiments reveal that existing offline learning algorithms struggle to match the performance of online RL on both 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#31070;&#32463;&#32593;&#32476;&#26381;&#35013;&#25104;&#34915;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#20960;&#20309;&#32422;&#26463;&#65292;&#20351;&#24471;&#26381;&#35013;&#21482;&#26377;&#22312;&#35206;&#30422;&#26356;&#22823;&#30340;&#36523;&#20307;&#26102;&#25165;&#21457;&#29983;&#20280;&#23637;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#19981;&#30495;&#23454;&#12289;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.01490</link><description>&lt;p&gt;
GAPS: &#20960;&#20309;&#24847;&#35782;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#12289;&#33258;&#30417;&#30563;&#30340;&#31070;&#32463;&#26381;&#35013;&#25104;&#34915;
&lt;/p&gt;
&lt;p&gt;
GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment Draping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#31070;&#32463;&#32593;&#32476;&#26381;&#35013;&#25104;&#34915;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#20960;&#20309;&#32422;&#26463;&#65292;&#20351;&#24471;&#26381;&#35013;&#21482;&#26377;&#22312;&#35206;&#30422;&#26356;&#22823;&#30340;&#36523;&#20307;&#26102;&#25165;&#21457;&#29983;&#20280;&#23637;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#19981;&#30495;&#23454;&#12289;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#32593;&#32476;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#23637;&#31034;&#26381;&#35013;&#21464;&#24418;&#65292;&#24182;&#19988;&#30475;&#36215;&#26469;&#26356;&#32654;&#35266;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#26448;&#36136;&#21442;&#25968;&#25511;&#21046;&#26381;&#35013;&#30340;&#19981;&#21487;&#20280;&#32553;&#24615;&#65292;&#36825;&#31181;&#37197;&#26041;&#20135;&#29983;&#20102;&#20855;&#26377;&#29289;&#29702;&#19981;&#21487;&#20449;&#20280;&#23637;&#30340;&#19981;&#30495;&#23454;&#32467;&#26524;&#12290;&#32463;&#24120;&#34987;&#36974;&#25377;&#30340;&#26381;&#35013;&#34987;&#25512;&#21040;&#20154;&#20307;&#20869;&#37096;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#26114;&#36149;&#30340;&#21518;&#26399;&#22788;&#29702;&#34987;&#32416;&#27491;&#65292;&#20174;&#32780;&#22686;&#21152;&#19981;&#19968;&#33268;&#30340;&#20280;&#23637;&#65307;&#35201;&#20040;&#36890;&#36807;&#20026;&#27599;&#31181;&#20307;&#22411;&#37096;&#32626;&#29420;&#31435;&#30340;&#35757;&#32451;&#21046;&#24230;&#26469;&#36827;&#34892;&#20462;&#27491;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#37096;&#32626;&#30340;&#26377;&#32570;&#38519;&#30340;&#30382;&#32932;&#22788;&#29702;&#36807;&#31243;&#22312;&#26494;&#25955;&#30340;&#26381;&#35013;&#19978;&#20135;&#29983;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21521;&#29616;&#26377;&#30340;&#37197;&#26041;&#24341;&#20837;&#20102;&#19968;&#20010;&#20960;&#20309;&#32422;&#26463;&#65292;&#23427;&#26159;&#30896;&#25758;&#24863;&#30693;&#30340;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#24378;&#21046;&#26381;&#35013;&#19981;&#33021;&#20280;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#29616;&#23454;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#36974;&#25377;&#30340;&#26381;&#35013;&#21482;&#22312;&#35206;&#30422;&#26356;&#22823;&#30340;&#36523;&#20307;&#26102;&#20250;&#20280;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01490v2 Announce Type: replace-cross  Abstract: Recent neural, physics-based modeling of garment deformations allows faster and visually aesthetic results as opposed to the existing methods. Material-specific parameters are used by the formulation to control the garment inextensibility. This delivers unrealistic results with physically implausible stretching. Oftentimes, the draped garment is pushed inside the body which is either corrected by an expensive post-processing, thus adding to further inconsistent stretching; or by deploying a separate training regime for each body type, restricting its scalability. Additionally, the flawed skinning process deployed by existing methods produces incorrect results on loose garments. In this paper, we introduce a geometrical constraint to the existing formulation that is collision-aware and imposes garment inextensibility wherever possible. Thus, we obtain realistic results where draped clothes stretch only while covering bigger body
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#35268;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#26435;&#37325;&#27874;&#21160;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#65292;&#20854;&#20013;&#21452;&#23618;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21463;&#21040;&#23618;&#38388;&#32806;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21576;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2311.14120</link><description>&lt;p&gt;
(&#28145;)&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21644;&#36870;&#26041;&#24046;&#24179;&#30452;&#20851;&#31995;&#30340;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#35268;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#26435;&#37325;&#27874;&#21160;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#65292;&#20854;&#20013;&#21452;&#23618;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21463;&#21040;&#23618;&#38388;&#32806;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21576;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#21512;&#25104;&#39640;&#26031;&#25968;&#25454;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#36830;&#32493;&#26497;&#38480;&#20869;&#65292;&#30740;&#31350;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#27424;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#65288;&#26411;&#24577;&#65289;&#35757;&#32451;&#35268;&#21017;&#12290;&#23545;&#20110; schwach&#27424;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#30340;&#21333;&#23618;&#32593;&#32476;&#65292;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#26126;&#26174;&#20559;&#31163;Hessian&#65292;&#21487;&#20197;&#24402;&#22240;&#20110;SGD&#21160;&#24577;&#30340;&#30772;&#22351;&#35814;&#32454;&#24179;&#34913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26435;&#37325;&#27874;&#21160;&#36890;&#24120;&#26159;&#21508;&#21521;&#24322;&#24615;&#30340;&#65292;&#20294;&#21463;&#21508;&#21521;&#21516;&#24615;&#25439;&#22833;&#38480;&#21046;&#12290;&#23545;&#20110;&#21452;&#23618;&#32593;&#32476;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#20851;&#30340;&#31283;&#23450;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23618;&#38388;&#32806;&#21512;&#20316;&#20026;&#26435;&#37325;&#27874;&#21160;&#30340;&#21508;&#21521;&#24322;&#24615;&#30340;&#26032;&#26469;&#28304;&#12290;&#19982;&#21333;&#23618;&#24773;&#20917;&#30456;&#21453;&#65292;&#26435;&#37325;&#27874;&#21160;&#32463;&#21382;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#65292;&#20854;&#24179;&#30452;&#24230;&#19982;&#27874;&#21160;&#30340;&#26041;&#24046;&#25104;&#21453;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14120v2 Announce Type: replace  Abstract: We investigate the stationary (late-time) training regime of single- and two-layer linear underparameterized neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly underparameterized regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but are subject to an isotropic loss. For a two-layer network, we obtain the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a new source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations experience an anisotropic loss, the flatness of which is inversely related to the fluctuation varian
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20113;&#20809;&#23398;&#21402;&#24230;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22320;&#29699;&#35266;&#27979;&#32972;&#26223;&#19979;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.14024</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20113;&#20809;&#23398;&#21402;&#24230;&#27979;&#37327;&#30340;&#22810;&#20809;&#35889;&#25104;&#20687;&#20202;&#20113;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness Measures for Cloud Detection in MSI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20113;&#20809;&#23398;&#21402;&#24230;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22320;&#29699;&#35266;&#27979;&#32972;&#26223;&#19979;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#22242;&#36890;&#24120;&#20250;&#36974;&#34109;&#22320;&#29699;&#34920;&#38754;&#30340;&#20809;&#23398;&#21355;&#26143;&#30417;&#27979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#12289;&#28023;&#27915;&#33394;&#24425;&#20998;&#26512;&#21644;&#20892;&#30000;&#30417;&#27979;&#31561;&#22320;&#29699;&#35266;&#27979;&#27963;&#21160;&#12290; &#22312;&#36965;&#24863;&#39046;&#22495;&#20869;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20113;&#26816;&#27979;&#21644;&#36807;&#28388;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#12290; ML&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#22320;&#29699;&#35266;&#27979;&#32972;&#26223;&#19979;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#65292;&#36825;&#22312;&#20113;&#20809;&#23398;&#21402;&#24230;&#65288;COT&#65289;&#20272;&#31639;&#26041;&#38754;&#23588;&#20026;&#26126;&#26174;&#12290; &#21487;&#38752;&#30340;COT&#20272;&#35745;&#30456;&#27604;&#20351;&#29992;&#24120;&#35268;&#20113;&#31867;&#21035;&#33021;&#26356;&#31934;&#32454;&#21644;&#24212;&#29992;&#30456;&#20851;&#22320;&#25511;&#21046;&#12290; &#20026;&#20102;&#32531;&#35299;COT&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14024v2 Announce Type: replace-cross  Abstract: Cloud formations often obscure optical satellite-based monitoring of the Earth's surface, thus limiting Earth observation (EO) activities such as land cover mapping, ocean color analysis, and cropland monitoring. The integration of machine learning (ML) methods within the remote sensing domain has significantly improved performance on a wide range of EO tasks, including cloud detection and filtering, but there is still much room for improvement. A key bottleneck is that ML methods typically depend on large amounts of annotated data for training, which is often difficult to come by in EO contexts. This is especially true when it comes to cloud optical thickness (COT) estimation. A reliable estimation of COT enables more fine-grained and application-dependent control compared to using pre-specified cloud categories, as is commonly done in practice. To alleviate the COT data scarcity problem, in this work we propose a novel synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#20851;&#31995;&#22312;&#24314;&#27169;&#26102;&#33021;&#20855;&#26377;&#30456;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.10112</link><description>&lt;p&gt;
zrLLM&#65306;&#22312;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#20851;&#31995;&#22312;&#24314;&#27169;&#26102;&#33021;&#20855;&#26377;&#30456;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21270;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#30693;&#35782;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;(TKGs)&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#28861;&#28909;&#35805;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;TKGs&#19978;&#30340;&#38142;&#25509;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#65292;&#20854;&#20013;&#23398;&#20064;&#38544;&#34255;&#34920;&#31034;&#20197;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#22270;&#19978;&#19979;&#25991;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;(KG)&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;TKG&#39044;&#27979;(TKGF)&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#24314;&#27169;&#27809;&#26377;&#20808;&#21069;&#22270;&#19978;&#19979;&#25991;&#30340;&#26410;&#35265;&#36807;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#19978;&#38754;&#20020;&#24378;&#28872;&#25361;&#25112;&#12290;&#26412;&#25991;&#23581;&#35797;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#22914;&#19979;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;KG&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#36755;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20197;&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#12290;LLM&#22686;&#24378;&#30340;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#20851;&#31995;&#65292;&#26080;&#35770;&#26159;&#24050;&#35265;&#36824;&#26159;&#26410;&#35265;&#30340;&#65292;&#37117;&#33021;&#22815;&#33719;&#24471;&#31867;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10112v2 Announce Type: replace  Abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic mean
&lt;/p&gt;</description></item><item><title>&#23558;&#20844;&#24179;&#24615;&#35270;&#20026;&#20998;&#24067;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#21644;&#35843;&#25972;DFT&#20004;&#39033;&#25216;&#26415;&#36129;&#29486;&#65292;&#26174;&#33879;&#20943;&#23569;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20132;&#21449;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2311.07604</link><description>&lt;p&gt;
&#20026;&#20844;&#24179;&#24615;&#35843;&#25972;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Finetuning Text-to-Image Diffusion Models for Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07604
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20844;&#24179;&#24615;&#35270;&#20026;&#20998;&#24067;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#21644;&#35843;&#25972;DFT&#20004;&#39033;&#25216;&#26415;&#36129;&#29486;&#65292;&#26174;&#33879;&#20943;&#23569;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20132;&#21449;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#37319;&#29992;&#20984;&#26174;&#20102;&#35299;&#20915;&#20854;&#20559;&#35265;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22914;&#26524;&#19981;&#36827;&#34892;&#24178;&#39044;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20256;&#25773;&#20986;&#25197;&#26354;&#30340;&#19990;&#30028;&#35266;&#65292;&#24182;&#38480;&#21046;&#23569;&#25968;&#32676;&#20307;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20844;&#24179;&#24615;&#35270;&#20026;&#19968;&#20010;&#20998;&#24067;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#65306;(1)&#19968;&#20010;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#65292;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#29305;&#23450;&#29305;&#24449;&#24341;&#21521;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#20197;&#21450;(2)&#35843;&#25972;&#20102;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#30340;&#30452;&#25509;&#24494;&#35843;&#65288;&#35843;&#25972;DFT&#65289;&#65292;&#23427;&#21033;&#29992;&#35843;&#25972;&#21518;&#30340;&#26799;&#24230;&#30452;&#25509;&#20248;&#21270;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#19978;&#23450;&#20041;&#30340;&#25439;&#22833;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#32844;&#19994;&#25552;&#31034;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21450;&#20854;&#20132;&#21449;&#20559;&#35265;&#12290;&#21363;&#20351;&#21482;&#23545;&#20116;&#20010;&#36719;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#24615;&#21035;&#20559;&#35265;&#20063;&#22823;&#22823;&#20943;&#23569;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#22810;&#20803;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07604v2 Announce Type: replace-cross  Abstract: The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;Follow-up Differential Descriptions&#65288;FuDD&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#27169;&#31946;&#31867;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#31867;&#25551;&#36848;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#30446;&#26631;&#31867;&#12290;</title><link>https://arxiv.org/abs/2311.07593</link><description>&lt;p&gt;
&#36319;&#36827;&#24046;&#20998;&#25551;&#36848;&#65306;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07593
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;Follow-up Differential Descriptions&#65288;FuDD&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#27169;&#31946;&#31867;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#31867;&#25551;&#36848;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#30446;&#26631;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#25193;&#23637;&#31867;&#25551;&#36848;&#65288;&#21363;&#25552;&#31034;&#65289;&#30340;&#30456;&#20851;&#23646;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26837;&#33394;&#40635;&#38592;&#20195;&#26367;&#40635;&#38592;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26080;&#35770;&#30446;&#26631;&#31867;&#20043;&#38388;&#30340;&#20849;&#21516;&#20043;&#22788;&#22914;&#20309;&#65292;&#37117;&#20250;&#36873;&#25321;&#19968;&#32452;&#23646;&#24615;&#65292;&#21487;&#33021;&#25552;&#20379;&#27809;&#26377;&#24110;&#21161;&#21306;&#20998;&#23427;&#20204;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Follow-up Differential Descriptions&#65288;FuDD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#31867;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#21306;&#20998;&#30446;&#26631;&#31867;&#30340;&#38468;&#21152;&#23646;&#24615;&#12290;FuDD&#39318;&#20808;&#20026;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#27169;&#31946;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#26032;&#30340;&#31867;&#25551;&#36848;&#65292;&#20197;&#21306;&#20998;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07593v2 Announce Type: replace  Abstract: A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between t
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>CLIP&#22312;&#32463;&#36807;&#37325;&#29616;ImageNet&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#30340;&#21098;&#26525;LAION&#20998;&#21106;&#37325;&#26032;&#35757;&#32451;&#21518;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#22522;&#20934;&#19978;&#34920;&#29616;&#26377;&#25152;&#19979;&#38477;&#65292;&#20294;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#24456;&#39640;</title><link>https://arxiv.org/abs/2310.09562</link><description>&lt;p&gt;
CLIP&#30340;&#27867;&#21270;&#24615;&#33021;&#20027;&#35201;&#28304;&#20110;&#35757;&#32451;-&#27979;&#35797;&#20043;&#38388;&#30340;&#39640;&#30456;&#20284;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09562
&lt;/p&gt;
&lt;p&gt;
CLIP&#22312;&#32463;&#36807;&#37325;&#29616;ImageNet&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#30340;&#21098;&#26525;LAION&#20998;&#21106;&#37325;&#26032;&#35757;&#32451;&#21518;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#22522;&#20934;&#19978;&#34920;&#29616;&#26377;&#25152;&#19979;&#38477;&#65292;&#20294;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; CLIP &#31561;&#22522;&#30784;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#25968;&#20159;&#26679;&#26412;&#19978;&#65292;&#33021;&#22815;&#36731;&#26494;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#21644;&#36755;&#20837;&#12290;CLIP &#20986;&#33394;&#22320;&#23637;&#31034;&#20102;&#22312;&#24191;&#27867;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#22522;&#20934;&#19978;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#20854;&#24402;&#22240;&#20110;&#24403;&#20170;&#30340;&#22823;&#35268;&#27169;&#21644;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914; LAION&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110; CLIP &#26469;&#35828;&#65292;&#20687;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#36825;&#26679;&#30340;&#26415;&#35821;&#26159;&#21542;&#20855;&#26377;&#24847;&#20041;&#26159;&#20540;&#24471;&#24576;&#30097;&#30340;&#65292;&#22240;&#20026;&#20687; LAION &#36825;&#26679;&#30340;&#32593;&#39029;&#35268;&#27169;&#25968;&#25454;&#38598;&#21487;&#33021;&#21482;&#26159;&#21253;&#21547;&#35768;&#22810;&#19982;&#26368;&#21021;&#20026; ImageNet &#35774;&#35745;&#30340;&#24120;&#35265; OOD &#22522;&#20934;&#30456;&#20284;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#22797;&#21046; ImageNet &#30340;&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#30456;&#23545;&#20110;&#24120;&#35265; OOD &#22522;&#20934;&#30340;&#21098;&#26525; LAION &#20998;&#21106;&#19978;&#37325;&#26032;&#35757;&#32451; CLIP&#12290;&#34429;&#28982;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#19968;&#20123;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;CLIP &#30340;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#24456;&#39640;&#12290;&#36825;&#34920;&#26126;&#39640;&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#26159;&#19981;&#36275;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09562v2 Announce Type: replace-cross  Abstract: Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;iable Euler Characteristic Transform&#65288;DECT&#65289;&#35745;&#31639;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;ECT&#65292;&#23637;&#29616;&#20986;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.07630</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#27431;&#25289;&#29305;&#24449;&#21464;&#25442;&#29992;&#20110;&#24418;&#29366;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentiable Euler Characteristic Transforms for Shape Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07630
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;iable Euler Characteristic Transform&#65288;DECT&#65289;&#35745;&#31639;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;ECT&#65292;&#23637;&#29616;&#20986;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#25289;&#29305;&#24449;&#21464;&#25442;&#65288;ECT&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#24418;&#29366;&#21644;&#22270;&#24418;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;ECT&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#23618;&#65292;&#21487;&#20197;&#20351;ECT&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Differentiable Euler Characteristic Transform&#65288;DECT&#65289;&#65292;&#36895;&#24230;&#24555;&#65292;&#35745;&#31639;&#39640;&#25928;&#65292;&#21516;&#26102;&#22312;&#22270;&#24418;&#21644;&#28857;&#20113;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#30475;&#20284;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#19982;&#26356;&#22797;&#26434;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23618;&#30456;&#21516;&#30340;&#25299;&#25169;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07630v2 Announce Type: replace  Abstract: The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion. Our method, the Differentiable Euler Characteristic Transform (DECT), is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks. Moreover, we show that this seemingly simple statistic provides the same topological expressivity as more complex topological deep learning layers.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#21644;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#38024;&#23545;&#20799;&#31185;&#24739;&#32773;&#30340;&#33041;&#30244;&#31934;&#30830;&#20998;&#21106;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2308.07212</link><description>&lt;p&gt;
&#20799;&#31461;&#33041;&#30244;&#20998;&#21106;&#30340;&#33258;&#21160;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automated ensemble method for pediatric brain tumor segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07212
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#21644;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#38024;&#23545;&#20799;&#31185;&#24739;&#32773;&#30340;&#33041;&#30244;&#31934;&#30830;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30244;&#20173;&#28982;&#26159;&#20840;&#29699;&#20581;&#24247;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#35786;&#26029;&#25216;&#26415;&#21644;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#27169;&#24577;&#65292;&#20197;&#38024;&#23545;&#20799;&#31185;&#24739;&#32773;&#29305;&#23450;&#24180;&#40836;&#27573;&#30340;&#20998;&#21106;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#24341;&#20837;&#20351;&#29992;ONet&#21644;UNet&#20462;&#25913;&#29256;&#26412;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#30740;&#31350;&#23454;&#29616;&#20102;&#23545;BraTS-PEDs 2023&#25361;&#25112;&#30340;&#31934;&#30830;&#20998;&#21106;&#27169;&#22411;&#12290;&#25968;&#25454;&#22686;&#24378;&#21253;&#25324;&#21333;&#19968;&#21644;&#22797;&#21512;&#36716;&#25442;&#65292;&#30830;&#20445;&#27169;&#22411;&#22312;&#19981;&#21516;&#25195;&#25551;&#21327;&#35758;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#38598;&#25104;&#31574;&#30053;&#23558;ONet&#21644;UNet&#27169;&#22411;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07212v2 Announce Type: replace-cross  Abstract: Brain tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. A tumor or its recurrence often needs to be identified in imaging studies and differentiated from normal brain tissue. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2308.07061</link><description>&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65306;&#35299;&#20915;&#26041;&#26696;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning: Solutions and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#35760;&#20303;&#25935;&#24863;&#12289;&#26410;&#32463;&#25480;&#26435;&#25110;&#24694;&#24847;&#25968;&#25454;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#23433;&#20840;&#28431;&#27934;&#21644;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26426;&#22120;&#36951;&#24536;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#28040;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20998;&#20026;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#24433;&#21709;&#30340;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#22238;&#39038;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#24182;&#23558;&#20854;&#24314;&#31435;&#20026;&#20540;&#24471;&#20449;&#36182;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07061v2 Announce Type: replace-cross  Abstract: Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy breaches, security vulnerabilities, and performance degradation. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of the solutions in machine unlearning. We categorize existing solutions into exact unlearning approaches that remove data influence thoroughly and approximate unlearning approaches that efficiently minimize data influence. By comprehensively reviewing solutions, we identify and discuss their strengths and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning models. This paper provides researchers with a roadmap
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#23545;&#25968;&#25454;&#36136;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#20294;&#26356;&#31934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2305.08752</link><description>&lt;p&gt;
&#27880;&#37322;&#38382;&#39064;&#65306;&#26469;&#33258;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#21407;&#20301;&#21644;&#33258;&#25105;&#22238;&#24518;&#27963;&#21160;&#27880;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Matter of Annotation: An Empirical Study on In Situ and Self-Recall Activity Annotations from Wearable Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08752
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#23545;&#25968;&#25454;&#36136;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#20294;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#20174;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20013;&#26816;&#27979;&#20154;&#31867;&#27963;&#21160;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#20351;&#35768;&#22810;&#24212;&#29992;&#21463;&#30410;&#65292;&#20174;&#36890;&#36807;&#20581;&#24247;&#25252;&#29702;&#24739;&#32773;&#30340;&#27493;&#34892;&#30417;&#27979;&#21040;&#20581;&#36523;&#25351;&#23548;&#20877;&#21040;&#31616;&#21270;&#25163;&#24037;&#20316;&#19994;&#27969;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22312;&#37326;&#22806;&#25968;&#25454;&#29992;&#25143;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;4&#31181;&#19981;&#21516;&#24120;&#29992;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#29992;&#25143;&#39537;&#21160;&#30340;&#12289;&#21407;&#20301;&#27880;&#37322;-&#21363;&#22312;&#35760;&#24405;&#27963;&#21160;&#20043;&#21069;&#25110;&#26399;&#38388;&#25191;&#34892;&#30340;&#27880;&#37322;-&#21644;&#22238;&#24518;&#26041;&#27861;-&#21442;&#19982;&#32773;&#22312;&#24403;&#22825;&#32467;&#26463;&#26102;&#36861;&#28335;&#22320;&#23545;&#20854;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#30452;&#25509;&#24433;&#21709;&#27880;&#37322;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#30456;&#24212;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#65292;&#20294;&#26356;&#31934;&#30830;&#65292;&#32780;&#22238;&#24518;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#22810;&#65292;&#20294;&#19981;&#22815;&#31934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#26412;&#27963;&#21160;&#26085;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08752v2 Announce Type: replace-cross  Abstract: Research into the detection of human activities from wearable sensors is a highly active field, benefiting numerous applications, from ambulatory monitoring of healthcare patients via fitness coaching to streamlining manual work processes. We present an empirical study that compares 4 different commonly used annotation methods utilized in user studies that focus on in-the-wild data. These methods can be grouped in user-driven, in situ annotations - which are performed before or during the activity is recorded - and recall methods - where participants annotate their data in hindsight at the end of the day. Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively. We noticed that in situ methods produce less but more precise labels than recall methods. Furthermore, we combined an activity diary
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#32467;&#21512;&#29289;&#29702;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23454;&#26102;&#36830;&#32493;&#26816;&#27979;&#28369;&#21160;&#65292;&#20174;&#28369;&#21160;&#20107;&#20214;&#20013;&#25552;&#21462;&#19981;&#22343;&#21248;&#29305;&#24449;&#35299;&#20915;&#28369;&#21160;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2303.00935</link><description>&lt;p&gt;
&#36890;&#36807;&#25509;&#35302;&#21147;&#22330;&#21644;&#29109;&#30340;&#35302;&#35273;&#20272;&#35745;&#23398;&#20064;&#26816;&#27979;&#28369;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning to Detect Slip through Tactile Estimation of the Contact Force Field and its Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00935
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#32467;&#21512;&#29289;&#29702;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23454;&#26102;&#36830;&#32493;&#26816;&#27979;&#28369;&#21160;&#65292;&#20174;&#28369;&#21160;&#20107;&#20214;&#20013;&#25552;&#21462;&#19981;&#22343;&#21248;&#29305;&#24449;&#35299;&#20915;&#28369;&#21160;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#25805;&#20316;&#36807;&#31243;&#20013;&#26816;&#27979;&#28369;&#21160;&#23545;&#20110;&#29289;&#20307;&#22788;&#29702;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#23454;&#26102;&#36830;&#32493;&#26816;&#27979;&#28369;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;GelSight Mini&#65292;&#19968;&#31181;&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#36830;&#25509;&#21040;&#33258;&#23450;&#20041;&#35774;&#35745;&#30340;&#22841;&#20855;&#19978;&#20197;&#25910;&#38598;&#35302;&#35273;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#28369;&#21160;&#20107;&#20214;&#26399;&#38388;&#35302;&#35273;&#20256;&#24863;&#22120;&#35835;&#25968;&#30340;&#19981;&#22343;&#21248;&#24615;&#26469;&#24320;&#21457;&#29420;&#29305;&#29305;&#24449;&#65292;&#23558;&#28369;&#21160;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00935v3 Announce Type: replace-cross  Abstract: Detection of slip during object grasping and manipulation plays a vital role in object handling. Existing solutions primarily rely on visual information to devise a strategy for grasping. However, for robotic systems to attain a level of proficiency comparable to humans, especially in consistently handling and manipulating unfamiliar objects, integrating artificial tactile sensing is increasingly essential. We introduce a novel physics-informed, data-driven approach to detect slip continuously in real time. We employ the GelSight Mini, an optical tactile sensor, attached to custom-designed grippers to gather tactile data. Our work leverages the inhomogeneity of tactile sensor readings during slip events to develop distinctive features and formulates slip detection as a classification problem. To evaluate our approach, we test multiple data-driven models on 10 common objects under different loading conditions, textures, and mate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.10164</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#22686;&#24378;&#24555;&#25346;&#30340;&#26397;&#21521;&#30340;&#25856;&#23721;&#20013;&#19979;&#38477;&#34892;&#20026;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lowering Detection in Sport Climbing Based on Orientation of the Sensor Enhanced Quickdraw
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#25856;&#23721;&#32773;&#30340;&#27963;&#21160;&#20197;&#25913;&#21892;&#26381;&#21153;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#26159;&#25856;&#23721;&#20581;&#36523;&#25151;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#24517;&#39035;&#20174;&#24320;&#22987;&#20998;&#26512;&#27599;&#20010;&#25856;&#23721;&#27963;&#21160;&#30452;&#21040;&#25856;&#30331;&#32773;&#38477;&#19979;&#26469;&#12290;&#22240;&#27492;&#65292;&#21457;&#29616;&#25856;&#23721;&#32773;&#19979;&#38477;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#26631;&#24535;&#30528;&#25856;&#30331;&#32467;&#26463;&#12290;&#24517;&#39035;&#22312;&#20445;&#25252;&#25856;&#23721;&#32773;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#38544;&#31169;&#21644;&#20415;&#21033;&#24615;&#30340;&#21516;&#26102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30828;&#20214;&#21407;&#22411;&#65292;&#20351;&#29992;&#38468;&#22312;&#22681;&#19978;&#30340;&#25856;&#23721;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#65292;&#31216;&#20026;&#24555;&#25346;&#65292;&#23427;&#36830;&#25509;&#25856;&#23721;&#32499;&#21644;&#34746;&#26643;&#38170;&#28857;&#12290;&#30456;&#24212;&#30340;&#20256;&#24863;&#22120;&#34987;&#37197;&#32622;&#20026;&#33410;&#33021;&#65292;&#22240;&#27492;&#22312;&#25856;&#23721;&#20581;&#36523;&#25151;&#22823;&#37327;&#20351;&#29992;&#26102;&#22312;&#36153;&#29992;&#21644;&#26356;&#25442;&#25152;&#38656;&#26102;&#38388;&#26041;&#38754;&#21464;&#24471;&#23454;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#30828;&#20214;&#35268;&#26684;&#65292;&#24182;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;&#27979;&#24471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10164v2 Announce Type: replace-cross  Abstract: Tracking climbers' activity to improve services and make the best use of their infrastructure is a concern for climbing gyms. Each climbing session must be analyzed from beginning till lowering of the climber. Therefore, spotting the climbers descending is crucial since it indicates when the ascent has come to an end. This problem must be addressed while preserving privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence become practical in terms of expenses and time consumption for replacement when using in large quantity in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPAR&#30340;&#20998;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23454;&#29616;&#23545;GNNs&#36827;&#34892;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#20174;&#32780;&#20445;&#25252;&#33410;&#28857;&#21450;&#20854;&#36793;&#32536;&#12290;</title><link>https://arxiv.org/abs/2210.04442</link><description>&lt;p&gt;
DPAR: &#20855;&#26377;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.04442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPAR&#30340;&#20998;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23454;&#29616;&#23545;GNNs&#36827;&#34892;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#20174;&#32780;&#20445;&#25252;&#33410;&#28857;&#21450;&#20854;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290; &#36824;&#25552;&#20986;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26292;&#38706;&#22270;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290; &#26412;&#25991;&#26088;&#22312;&#23454;&#29616;&#23545;GNNs&#36827;&#34892;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65292;&#20197;&#20445;&#25252;&#33410;&#28857;&#21450;&#20854;&#36793;&#32536;&#12290; GNNs&#30340;&#33410;&#28857;DP&#22312;&#26412;&#36136;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#25152;&#26377;&#30452;&#25509;&#21644;&#22810;&#36339;&#37051;&#23621;&#36890;&#36807;&#36880;&#23618;&#28040;&#24687;&#20256;&#36882;&#21442;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#19988;&#33410;&#28857;&#21487;&#20197;&#20855;&#26377;&#22810;&#23569;&#30452;&#25509;&#21644;&#22810;&#36339;&#37051;&#23621;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#23558;&#23548;&#33268;&#24456;&#39640;&#30340;&#38544;&#31169;&#25104;&#26412;&#25110;&#30001;&#20110;&#33410;&#28857;&#25935;&#24863;&#24615;&#39640;&#32780;&#25928;&#29992;&#19981;&#20339;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#24322;&#24615;&#31169;&#20154;&#21270;&#35843;&#25972;&#39029;&#38754;&#25490;&#21517;&#65288;DPAR&#65289;&#30340;\textbf{D}ecoupled GNN&#65292;&#29992;&#20110;&#35757;&#32451;&#24102;&#26377;&#22686;&#24378;&#38544;&#31169;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.04442v2 Announce Type: replace  Abstract: Graph Neural Networks (GNNs) have achieved great success in learning with graph-structured data. Privacy concerns have also been raised for the trained models which could expose the sensitive information of graphs including both node features and the structure information. In this paper, we aim to achieve node-level differential privacy (DP) for training GNNs so that a node and its edges are protected. Node DP is inherently difficult for GNNs because all direct and multi-hop neighbors participate in the calculation of gradients for each node via layer-wise message passing and there is no bound on how many direct and multi-hop neighbors a node can have, so existing DP methods will result in high privacy cost or poor utility due to high node sensitivity. We propose a \textbf{D}ecoupled GNN with Differentially \textbf{P}rivate \textbf{A}pproximate Personalized Page\textbf{R}ank (DPAR) for training GNNs with an enhanced privacy-utility t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diana&#30340;&#21160;&#24577;&#26550;&#26500;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2208.14602</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#25345;&#32493;&#38382;&#31572;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continuous QA Learning with Structured Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14602
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diana&#30340;&#21160;&#24577;&#26550;&#26500;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#32456;&#36523;&#23398;&#20064;&#65288;LL&#65289;&#33021;&#21147;&#30340;QA&#27169;&#22411;&#23545;&#20110;&#23454;&#38469;&#30340;QA&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22522;&#20110;&#26550;&#26500;&#30340;LL&#26041;&#27861;&#34987;&#25253;&#21578;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#20808;&#21069;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;QA&#20219;&#21153;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#22312;&#27979;&#35797;&#38454;&#27573;&#38656;&#35201;&#35775;&#38382;&#20219;&#21153;&#26631;&#35782;&#65292;&#35201;&#20040;&#19981;&#26126;&#30830;&#22320;&#23545;&#26469;&#33258;&#26410;&#35265;&#20219;&#21153;&#30340;&#26679;&#26412;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diana&#65306;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#35797;&#22270;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#12290;&#22312;Diana&#20013;&#20351;&#29992;&#20102;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#20197;&#20445;&#25345;&#39640;LL&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#23454;&#20363;&#32423;&#25552;&#31034;&#26469;&#23398;&#20064;&#36328;&#19981;&#21516;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14602v3 Announce Type: replace-cross  Abstract: QA models with lifelong learning (LL) abilities are important for practical QA applications, and architecture-based LL methods are reported to be an effective implementation for these models. However, it is non-trivial to extend previous approaches to QA tasks since they either require access to task identities in the testing phase or do not explicitly model samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong QA model that tries to learn a sequence of QA tasks with a prompt enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture QA knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across different input samples to improve the model's generalization performance. Moreover, we dedi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StyleTalker&#65292;&#19968;&#31181;&#33021;&#22815;&#20174;&#21333;&#20010;&#21442;&#32771;&#22270;&#20687;&#21512;&#25104;&#20855;&#26377;&#20934;&#30830;&#38899;&#39057;&#21516;&#27493;&#30340;&#35828;&#35805;&#20154;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#20960;&#20010;&#26032;&#35774;&#35745;&#30340;&#32452;&#20214;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2208.10922</link><description>&lt;p&gt;
StyleTalker: &#19968;&#27425;&#26679;&#24335;&#39537;&#21160;&#30340;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#22836;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.10922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StyleTalker&#65292;&#19968;&#31181;&#33021;&#22815;&#20174;&#21333;&#20010;&#21442;&#32771;&#22270;&#20687;&#21512;&#25104;&#20855;&#26377;&#20934;&#30830;&#38899;&#39057;&#21516;&#27493;&#30340;&#35828;&#35805;&#20154;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#20960;&#20010;&#26032;&#35774;&#35745;&#30340;&#32452;&#20214;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;StyleTalker&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#22836;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#21442;&#32771;&#22270;&#20687;&#21512;&#25104;&#19968;&#20010;&#35828;&#35805;&#20154;&#30340;&#35270;&#39057;&#65292;&#20854;&#20013;&#21253;&#21547;&#20934;&#30830;&#38899;&#39057;&#21516;&#27493;&#30340;&#21767;&#24418;&#12289;&#36924;&#30495;&#30340;&#22836;&#37096;&#23039;&#24577;&#21644;&#30504;&#30524;&#21160;&#20316;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#20272;&#31639;&#20102;&#35328;&#35821;&#22836;&#37096;&#35270;&#39057;&#30340;&#28508;&#22312;&#20195;&#30721;&#65292;&#24544;&#23454;&#22320;&#21453;&#26144;&#20102;&#32473;&#23450;&#38899;&#39057;&#12290;&#36825;&#24471;&#30410;&#20110;&#20960;&#20010;&#26032;&#35774;&#35745;&#30340;&#32452;&#20214;&#65306;1&#65289;&#29992;&#20110;&#20934;&#30830;&#21767;&#37096;&#21516;&#27493;&#30340;&#23545;&#27604;&#24230;&#21767;&#21516;&#27493;&#37492;&#21035;&#22120;&#65292;2&#65289;&#23398;&#20064;&#19982;&#21767;&#37096;&#36816;&#21160;&#20998;&#31163;&#30340;&#28508;&#22312;&#36816;&#21160;&#31354;&#38388;&#30340;&#26377;&#26465;&#20214;&#24207;&#21015;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#29420;&#31435;&#22320;&#25805;&#32437;&#36816;&#21160;&#21644;&#22068;&#21767;&#36816;&#21160;&#65292;&#21516;&#26102;&#20445;&#25345;&#36523;&#20221;&#12290;3&#65289;&#37197;&#22791;&#20102;&#27491;&#35268;&#21270;&#27969;&#30340;&#33258;&#22238;&#24402;&#20808;&#39564;&#65292;&#23398;&#20064;&#20102;&#22797;&#26434;&#30340;&#38899;&#39057;&#21040;&#36816;&#21160;&#22810;&#27169;&#28508;&#22312;&#31354;&#38388;&#12290;&#20511;&#21161;&#36825;&#20123;&#32452;&#20214;&#65292;StyleTalker&#21487;&#20197;&#29983;&#25104;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.10922v2 Announce Type: replace-cross  Abstract: We propose StyleTalker, a novel audio-driven talking head generation model that can synthesize a video of a talking person from a single reference image with accurately audio-synced lip shapes, realistic head poses, and eye blinks. Specifically, by leveraging a pretrained image generator and an image encoder, we estimate the latent codes of the talking head video that faithfully reflects the given audio. This is made possible with several newly devised components: 1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A conditional sequential variational autoencoder that learns the latent motion space disentangled from the lip movements, such that we can independently manipulate the motions and lip movements while preserving the identity. 3) An auto-regressive prior augmented with normalizing flow to learn a complex audio-to-motion multi-modal latent space. Equipped with these components, StyleTalker can g
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#39062;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#30340;&#20805;&#20998;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#36870;&#27169;&#22411;&#20272;&#35745;&#21644;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#30340;&#23454;&#29992;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#22870;&#21169;&#20449;&#21495;&#20294;&#21487;&#20197;&#21033;&#29992;&#22870;&#21169;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2106.04379</link><description>&lt;p&gt;
&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#25277;&#35937;&#20197;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Markov State Abstractions for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.04379
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#39062;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#30340;&#20805;&#20998;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#36870;&#27169;&#22411;&#20272;&#35745;&#21644;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#30340;&#23454;&#29992;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#22870;&#21169;&#20449;&#21495;&#20294;&#21487;&#20197;&#21033;&#29992;&#22870;&#21169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#30456;&#20851;&#30340;&#20915;&#31574;&#36807;&#31243;&#23454;&#38469;&#19978;&#26159;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;MDPs&#20855;&#26377;&#20016;&#23500;&#30340;&#35266;&#27979;&#26102;&#65292;&#20195;&#29702;&#36890;&#24120;&#36890;&#36807;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#31181;&#34920;&#31034;&#26410;&#24517;&#33021;&#20445;&#25345;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#36275;&#20197;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#36870;&#27169;&#22411;&#20272;&#35745;&#21644;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#35757;&#32451;&#30446;&#26631;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35757;&#32451;&#65306;&#23427;&#19981;&#38656;&#35201;&#22870;&#21169;&#20449;&#21495;&#65292;&#20294;&#24403;&#21487;&#29992;&#26102;&#65292;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#22870;&#21169;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#35273;&#26684;&#23376;&#19990;&#30028;&#22495;&#21644;&#19968;&#32452;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.04379v4 Announce Type: replace-cross  Abstract: A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.16433</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#36827;&#34892;&#31726;&#20869;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Within-basket Recommendation via Neural Pattern Associator. (arXiv:2401.16433v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31726;&#20869;&#25512;&#33616;&#65288;WBR&#65289;&#26159;&#25351;&#22312;&#36141;&#29289;&#36807;&#31243;&#20013;&#20026;&#20102;&#23436;&#25104;&#19968;&#20010;&#38750;&#31354;&#36141;&#29289;&#31726;&#32780;&#25512;&#33616;&#21830;&#21697;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#21019;&#26032;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#23454;&#38469;&#29992;&#25143;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#65292;&#27604;&#22914;1&#65289;&#22810;&#20010;&#36141;&#29289;&#24847;&#22270;&#30340;&#20849;&#23384;&#65292;2&#65289;&#36825;&#20123;&#24847;&#22270;&#30340;&#22810;&#31890;&#24230;&#21644;3&#65289;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#20132;&#32455;&#34892;&#20026;&#65288;&#20999;&#25442;&#24847;&#22270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#19978;&#36848;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#21040;&#21521;&#37327;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;NPA&#27169;&#22411;&#23398;&#20064;&#23558;&#24120;&#35265;&#30340;&#29992;&#25143;&#24847;&#22270;&#65288;&#25110;&#21830;&#21697;&#32452;&#21512;&#27169;&#24335;&#65289;&#32534;&#30721;&#20026;&#37327;&#21270;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#30721;&#26412;&#65289;&#65292;&#36825;&#20801;&#35768;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;&#36825;&#26679;&#20135;&#29983;&#30340;&#25512;&#33616;&#32467;&#26524;&#36830;&#36143;&#19988;&#33258;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within-basket recommendation (WBR) refers to the task of recommending items to the end of completing a non-empty shopping basket during a shopping session. While the latest innovations in this space demonstrate remarkable performance improvement on benchmark datasets, they often overlook the complexity of user behaviors in practice, such as 1) co-existence of multiple shopping intentions, 2) multi-granularity of such intentions, and 3) interleaving behavior (switching intentions) in a shopping session. This paper presents Neural Pattern Associator (NPA), a deep item-association-mining model that explicitly models the aforementioned factors. Specifically, inspired by vector quantization, the NPA model learns to encode common user intentions (or item-combination patterns) as quantized representations (a.k.a. codebook), which permits identification of users's shopping intentions via attention-driven lookup during the reasoning phase. This yields coherent and self-interpretable recommendat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>Imperio&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20219;&#24847;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#25193;&#23637;&#20102;NLP&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01085</link><description>&lt;p&gt;
Imperio: &#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#21518;&#38376;&#25915;&#20987;&#23454;&#29616;&#20219;&#24847;&#27169;&#22411;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control. (arXiv:2401.01085v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01085
&lt;/p&gt;
&lt;p&gt;
Imperio&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20219;&#24847;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#25193;&#23637;&#20102;NLP&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;transformer&#26550;&#26500;&#30340;&#38761;&#21629;&#19979;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21463;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;NLP&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#20854;&#21518;&#38376;&#28431;&#27934;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#21518;&#38376;&#23041;&#32961;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Imperio&#65292;&#23427;&#21033;&#29992;NLP&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26469;&#20016;&#23500;&#21518;&#38376;&#25915;&#20987;&#12290;Imperio&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#25511;&#21046;&#20307;&#39564;&#65292;&#20351;&#23545;&#25163;&#36890;&#36807;&#35821;&#35328;&#24341;&#23548;&#30340;&#25351;&#20196;&#21487;&#20197;&#20219;&#24847;&#25511;&#21046;&#21463;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#39537;&#21160;&#26465;&#20214;&#35302;&#21457;&#29983;&#25104;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20197;&#25193;&#23637;&#20854;&#23545;&#21518;&#38376;&#25351;&#20196;&#35299;&#37322;&#21644;&#25191;&#34892;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#12289;&#20116;&#31181;&#25915;&#20987;&#21644;&#20061;&#31181;&#38450;&#24481;&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;Imperio&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#21487;&#20197;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#20135;&#29983;&#19978;&#19979;&#25991;&#36866;&#24212;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#25511;&#21046;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionized by the transformer architecture, natural language processing (NLP) has received unprecedented attention. While advancements in NLP models have led to extensive research into their backdoor vulnerabilities, the potential for these advancements to introduce new backdoor threats remains unexplored. This paper proposes Imperio, which harnesses the language understanding capabilities of NLP models to enrich backdoor attacks. Imperio provides a new model control experience. It empowers the adversary to control the victim model with arbitrary output through language-guided instructions. This is achieved using a language model to fuel a conditional trigger generator, with optimizations designed to extend its language understanding capabilities to backdoor instruction interpretation and execution. Our experiments across three datasets, five attacks, and nine defenses confirm Imperio's effectiveness. It can produce contextually adaptive triggers from text descriptions and control 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.00500</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Data Attribution on Diffusion Models. (arXiv:2311.00500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26088;&#22312;&#23558;&#27169;&#22411;&#36755;&#20986;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25968;&#25454;&#24402;&#22240;&#24050;&#25104;&#20026;&#19968;&#20010;&#29702;&#24819;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#20026;&#39640;&#36136;&#37327;&#25110;&#29256;&#26435;&#20445;&#25252;&#30340;&#35757;&#32451;&#26679;&#26412;&#27491;&#30830;&#20998;&#37197;&#20215;&#20540;&#65292;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#24471;&#21040;&#20844;&#24179;&#30340;&#34917;&#20607;&#25110;&#35748;&#21487;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25968;&#25454;&#24402;&#22240;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;CIFAR-10&#21644;CelebA&#19978;&#35757;&#32451;&#30340;DDPM&#20197;&#21450;&#22312;ArtBench&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;LoRA&#30340;&#24402;&#22240;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#23454;&#38469;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#22522;&#32447;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#24615;&#25968;&#25454;&#24314;&#27169;&#24471;&#20998;&#36824;&#26159;&#21453;&#20107;&#23454;&#35780;&#20272;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a signific
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13011</link><description>&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13011
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#21450;&#23545;&#20559;&#22909;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20174;&#19968;&#20010;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#36825;&#20123;&#29305;&#24449;&#30340;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#32858;&#21512;&#36825;&#20123;&#35780;&#20998;&#12290;CPMs&#20801;&#35768;&#25511;&#21046;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20351;&#29992;&#21738;&#20123;&#23646;&#24615;&#26469;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22522;&#30784;&#30340;&#29305;&#24449;&#26500;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPMs&#19981;&#20165;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#27604;&#26631;&#20934;&#20559;&#22909;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;CPMs&#33719;&#24471;&#30340;&#26368;&#20339;n&#20010;&#26679;&#26412;&#27604;&#20351;&#29992;&#26631;&#20934;PMs&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#21482;&#38656;&#35201;&#23569;&#37327;&#29420;&#31435;&#20219;&#21153;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#25509;&#36817;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#23545;ICL&#30340;&#32479;&#35745;&#22522;&#30784;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.08391</link><description>&lt;p&gt;
&#22810;&#23569;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#38656;&#35201;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?. (arXiv:2310.08391v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#21482;&#38656;&#35201;&#23569;&#37327;&#29420;&#31435;&#20219;&#21153;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#25509;&#36817;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#23545;ICL&#30340;&#32479;&#35745;&#22522;&#30784;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;Transformer&#23637;&#29616;&#20102;&#38750;&#20961;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20165;&#22522;&#20110;&#36755;&#20837;&#19978;&#19979;&#25991;&#35299;&#20915;&#26410;&#35265;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#20013;&#26368;&#31616;&#21333;&#35774;&#32622;&#30340;ICL&#65306;&#39044;&#35757;&#32451;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#21333;&#23618;&#32447;&#24615;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#20026;&#27880;&#24847;&#21147;&#27169;&#22411;&#39044;&#35757;&#32451;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#35745;&#20219;&#21153;&#22797;&#26434;&#24230;&#30028;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#21482;&#38656;&#35201;&#23569;&#37327;&#29420;&#31435;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#38750;&#24120;&#25509;&#36817;&#65292;&#21363;&#20960;&#20046;&#23454;&#29616;&#20102;&#22266;&#23450;&#19978;&#19979;&#25991;&#38271;&#24230;&#19979;&#26410;&#35265;&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#23545;&#20043;&#21069;&#30340;&#23454;&#39564;&#30740;&#31350;&#36827;&#34892;&#20102;&#34917;&#20805;&#65292;&#24182;&#20026;ICL&#30340;&#32479;&#35745;&#22522;&#30784;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#24471;&#20998;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#22312;D4RL&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.07297</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#34892;&#20026;&#23454;&#29616;&#24471;&#20998;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07297
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#24471;&#20998;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#22312;D4RL&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#20805;&#20998;&#23637;&#29616;&#20102;&#20854;&#22312;&#34920;&#36798;&#24322;&#36136;&#34892;&#20026;&#31574;&#30053;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25193;&#25955;&#31574;&#30053;&#20013;&#37319;&#26679;&#38750;&#24120;&#32531;&#24930;&#65292;&#22240;&#20026;&#38656;&#35201;&#25968;&#21313;&#21040;&#25968;&#30334;&#27425;&#36845;&#20195;&#25512;&#29702;&#27493;&#39588;&#26469;&#36827;&#34892;&#19968;&#27425;&#21160;&#20316;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35780;&#35770;&#23478;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#20013;&#25552;&#21462;&#39640;&#25928;&#30830;&#23450;&#24615;&#25512;&#29702;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#21518;&#32773;&#30452;&#25509;&#23545;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20351;&#29992;&#34892;&#20026;&#20998;&#24067;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#36807;&#31243;&#20013;&#20805;&#20998;&#21457;&#25381;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#23436;&#20840;&#32469;&#36807;&#20102;&#35745;&#31639;&#23494;&#38598;&#21644;&#32791;&#26102;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#12290;&#22312;D4RL&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#39640;&#20102;&#36229;&#36807;25&#20493;&#65292;&#30456;&#27604;&#20110;&#21508;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#22312;&#36816;&#21160;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta
&lt;/p&gt;</description></item><item><title>DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.06020</link><description>&lt;p&gt;
DyST&#65306;&#38754;&#21521;&#23454;&#38469;&#35270;&#39057;&#30340;&#21160;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06020
&lt;/p&gt;
&lt;p&gt;
DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#19990;&#30028;&#30340;&#35270;&#35273;&#29702;&#35299;&#36229;&#36234;&#20102;&#21333;&#20010;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#24179;&#38754;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#23454;&#38469;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;Dynamic Scene Transformer&#65288;DyST&#65289;&#27169;&#22411;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30740;&#31350;&#25104;&#26524;&#65292;&#23398;&#20064;&#20102;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#21253;&#25324;&#22330;&#26223;&#20869;&#23481;&#12289;&#27599;&#20010;&#35270;&#35282;&#30340;&#22330;&#26223;&#21160;&#24577;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#36890;&#36807;&#22312;&#21333;&#30446;&#35270;&#39057;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;DySO&#19978;&#36827;&#34892;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#20998;&#31163;&#12290;DyST&#23398;&#20064;&#21040;&#20102;&#21160;&#24577;&#22330;&#26223;&#30340;&#20855;&#20307;&#28508;&#22312;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#22330;&#26223;&#30340;&#30456;&#26426;&#21644;&#20869;&#23481;&#36827;&#34892;&#29420;&#31435;&#25511;&#21046;&#30340;&#35270;&#22270;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22312;&#28385;&#36275;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#20559;&#24046;&#20998;&#25968;&#30340;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05725</link><description>&lt;p&gt;
&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#23545;&#20844;&#24179;&#20998;&#31867;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
Post-hoc Bias Scoring Is Optimal For Fair Classification. (arXiv:2310.05725v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22312;&#28385;&#36275;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#20559;&#24046;&#20998;&#25968;&#30340;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#26159;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#24615;&#65288;DP&#65289;&#65292;&#26426;&#20250;&#22343;&#31561;&#65288;EOp&#65289;&#25110;&#31561;&#27010;&#29575;&#65288;EO&#65289;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#26126;&#30830;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#26159;&#19981;&#21463;&#32422;&#26463;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#20462;&#25913;&#35268;&#21017;&#12290;&#21363;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#30340;&#20559;&#24046;&#24230;&#37327;&#65292;&#31216;&#20026;&#20559;&#24046;&#20998;&#25968;&#65292;&#32780;&#20462;&#25913;&#35268;&#21017;&#21017;&#26159;&#22312;&#26377;&#38480;&#37327;&#30340;&#20559;&#24046;&#20998;&#25968;&#20043;&#19978;&#30340;&#31616;&#21333;&#32447;&#24615;&#35268;&#21017;&#12290;&#22522;&#20110;&#36825;&#20010;&#29305;&#24449;&#21270;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21518;&#39564;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#20844;&#24179;&#24615;&#32422;&#26463;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;DP&#21644;EOp&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#35268;&#21017;&#26159;&#22522;&#20110;&#21333;&#20010;&#20559;&#24046;&#20998;&#25968;&#30340;&#38408;&#20540;&#36873;&#25321;&#65292;&#32780;&#22312;EO&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#35843;&#25972;&#20855;&#26377;2&#20010;&#21442;&#25968;&#30340;&#32447;&#24615;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#29992;&#20110;&#21253;&#21547;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#22797;&#21512;&#32676;&#20307;&#20844;&#24179;&#24615;&#26631;&#20934;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive att
&lt;/p&gt;</description></item><item><title>Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04345</link><description>&lt;p&gt;
Neur2RO: &#31070;&#32463;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04345
&lt;/p&gt;
&lt;p&gt;
Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#24314;&#27169;&#21644;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#65288;&#20063;&#31216;&#20026;&#21487;&#35843;&#25972;&#40065;&#26834;&#20248;&#21270;&#65289;&#38382;&#39064;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20043;&#21069;&#21644;&#20043;&#21518;&#36827;&#34892;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20915;&#31574;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#35745;&#31639;&#19978;&#26469;&#35828;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#20915;&#31574;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Neur2RO&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#65288;CCG&#65289;&#30340;&#23454;&#20363;&#31639;&#27861;&#65292;CCG&#26159;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#30340;&#32463;&#20856;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#31181;&#26550;&#26500;&#26131;&#20110;&#20248;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;CCG&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#36825;&#22312;&#20004;&#20010;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#32972;&#21253;&#38382;&#39064;&#21644;&#36164;&#26412;&#39044;&#31639;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#12290;&#20182;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20960;&#20309;&#20248;&#21270;&#30340;&#36793;&#23646;&#24615;&#65292;&#24182;&#23558;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22788;&#29702;&#30456;&#21516;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#20182;&#20204;&#22312;&#22788;&#29702;3D&#28857;&#20113;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36873;&#25321;&#20102;$\mathbb{R}^3 {\times} S^2$&#20316;&#20026;&#26368;&#20339;&#30340;&#22788;&#29702;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.02970</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. (arXiv:2310.02970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#12290;&#20182;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20960;&#20309;&#20248;&#21270;&#30340;&#36793;&#23646;&#24615;&#65292;&#24182;&#23558;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22788;&#29702;&#30456;&#21516;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#20182;&#20204;&#22312;&#22788;&#29702;3D&#28857;&#20113;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36873;&#25321;&#20102;$\mathbb{R}^3 {\times} S^2$&#20316;&#20026;&#26368;&#20339;&#30340;&#22788;&#29702;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#25512;&#23548;&#20986;&#29992;&#20110;&#28789;&#27963;&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#8220;&#20960;&#20309;&#20248;&#21270;&#36793;&#23646;&#24615;&#8221;&#12290;&#25105;&#20204;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22320;&#22788;&#29702;&#24182;&#19988;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#30340;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#20849;&#20139;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#31561;&#20215;&#31867;&#65292;&#36825;&#20123;&#31561;&#20215;&#31867;&#22312;&#32676;&#20013;&#36827;&#34892;&#21464;&#25442;&#26102;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#25512;&#23548;&#20986;&#21807;&#19968;&#26631;&#35782;&#36825;&#20123;&#31867;&#21035;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;&#36825;&#20123;&#23646;&#24615;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#26435;&#37325;&#20849;&#20139;&#12290;&#20316;&#20026;&#35813;&#29702;&#35770;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#26469;&#22788;&#29702;3D&#28857;&#20113;&#12290;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#22914;&#20309;&#22312;&#20301;&#32622;$\mathbb{R}^3$&#12289;&#20301;&#32622;&#21644;&#26041;&#21521;$\mathbb{R}^3 {\times} S^2$&#30340;&#21516;&#24577;&#31354;&#38388;&#20197;&#21450;&#32676;SE$(3)$&#19978;&#30340;&#29305;&#24449;&#22270;&#19978;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;&#22312;&#36825;&#20123;&#36873;&#25321;&#20013;&#65292;$\mathbb{R}^3 {\times} S^2$&#26159;&#19968;&#20010;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22788;&#29702;&#26041;&#21521;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the theory of homogeneous spaces we derive \textit{geometrically optimal edge attributes} to be used within the flexible message passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\mathbb{R}^3$, position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$ itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to the ability to 
&lt;/p&gt;</description></item><item><title>SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02227</link><description>&lt;p&gt;
SNIP: &#29992;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#36830;&#25509;&#25968;&#23398;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02227
&lt;/p&gt;
&lt;p&gt;
SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26080;&#27861;&#32570;&#23569;&#31526;&#21495;&#25968;&#23398;&#26041;&#31243;&#26469;&#24314;&#27169;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#30340;&#26102;&#20195;&#65292;&#31185;&#23398;&#25506;&#31350;&#24448;&#24448;&#28041;&#21450;&#21040;&#25910;&#38598;&#35266;&#23519;&#25968;&#25454;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29305;&#21270;&#20110;&#25968;&#20540;&#39046;&#22495;&#25110;&#31526;&#21495;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#20026;&#29305;&#23450;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#31526;&#21495;&#26041;&#31243;&#21644;&#20854;&#25968;&#20540;&#23545;&#24212;&#29289;&#20043;&#38388;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#22823;&#22909;&#22788;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SNIP&#65292;&#19968;&#31181;&#31526;&#21495;-&#25968;&#20540;&#38598;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#23884;&#20837;&#20013;&#30340;&#30456;&#20114;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#28508;&#31354;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNIP&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic 
&lt;/p&gt;</description></item><item><title>DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02025</link><description>&lt;p&gt;
DeepZero: &#23558;&#38646;&#38454;&#20248;&#21270;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02025
&lt;/p&gt;
&lt;p&gt;
DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#27861;&#33719;&#21462;&#19968;&#38454;&#20449;&#24687;&#26102;&#65292;&#38646;&#38454;&#20248;&#21270;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20854;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;DeepZero&#65292;&#19968;&#20010;&#22522;&#20110;&#38646;&#38454;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#23558;&#38646;&#38454;&#20248;&#21270;&#25193;&#23637;&#21040;&#20174;&#38646;&#24320;&#22987;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00535</link><description>&lt;p&gt;
JoMA: &#36890;&#36807;MLP&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#26469;&#35299;&#23494;&#22810;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;Transformer&#20013;&#21435;&#38500;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#24471;&#21040;&#20165;&#21253;&#21547;MLP&#23618;&#30340;&#20462;&#25913;&#21518;&#21160;&#24577;&#12290;JoMA&#28040;&#38500;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#32570;&#20047;&#27531;&#24046;&#36830;&#25509;&#65289;&#65292;&#24182;&#39044;&#27979;&#27880;&#24847;&#21147;&#22312;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#24773;&#20917;&#19979;&#39318;&#20808;&#21464;&#24471;&#31232;&#30095;&#65288;&#20026;&#20102;&#23398;&#20064;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#21464;&#24471;&#23494;&#38598;&#65288;&#20026;&#20102;&#23398;&#20064;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#32780;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23427;&#19982;&#29616;&#26377;&#30740;&#31350;&#19968;&#33268;&#65292;&#26174;&#31034;&#20986;&#27880;&#24847;&#21147;&#38543;&#26102;&#38388;&#21464;&#24471;&#31232;&#30095;&#12290;&#25105;&#20204;&#21033;&#29992;JoMA&#23450;&#24615;&#22320;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#22914;&#20309;&#23558;&#26631;&#35760;&#32452;&#21512;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#24403;&#36755;&#20837;&#26631;&#35760;&#26159;&#30001;&#28508;&#22312;&#30340;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#22312;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;Wikitext2/Wikitext103&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;OPT&#65292;Pythia&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#31574;&#30053;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#22312;&#21508;&#31181;RL&#35774;&#32622;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16984</link><description>&lt;p&gt;
&#19968;&#31181;&#20316;&#20026;&#20016;&#23500;&#39640;&#25928;&#30340;&#31574;&#30053;&#31867;&#21035;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. (arXiv:2309.16984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#31574;&#30053;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#22312;&#21508;&#31181;RL&#35774;&#32622;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#22312;&#24314;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20174;&#22270;&#20687;&#29983;&#25104;&#21040;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#20250;&#24456;&#24930;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#22312;&#20855;&#26377;&#36845;&#20195;&#37319;&#26679;&#30340;RL&#20013;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#31574;&#30053;&#34920;&#31034;&#65292;&#21363;&#19968;&#33268;&#24615;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#28436;&#21592;-&#35780;&#35770;&#23478;&#39118;&#26684;&#30340;&#31639;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#20856;&#22411;&#30340;RL&#35774;&#32622;&#65306;&#31163;&#32447;&#12289;&#31163;&#32447;&#21040;&#22312;&#32447;&#21644;&#22312;&#32447;&#12290;&#23545;&#20110;&#31163;&#32447;RL&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23545;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;RL&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#26174;&#31034;&#20986;&#27604;&#25193;&#25955;&#31574;&#30053;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#24615;&#33021;&#21487;&#27604;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#19968;&#33268;&#24615;&#31574;&#30053;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#29978;&#33267;&#27604;&#25193;&#25955;&#31574;&#30053;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2309.15551</link><description>&lt;p&gt;
&#20351;&#29992;DeepRepViz&#26469;&#35782;&#21035;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identifying confounders in deep-learning-based model predictions using DeepRepViz. (arXiv:2309.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#25581;&#31034;&#22823;&#33041;&#12289;&#22823;&#33041;&#30149;&#29702;&#21644;&#24515;&#29702;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#21442;&#19982;&#32773;&#24180;&#40836;&#12289;&#24615;&#21035;&#25110;&#24433;&#20687;&#20266;&#24433;&#31561;&#22806;&#37096;&#30340;&#8220;&#28151;&#28102;&#22240;&#32032;&#8221;&#21464;&#37327;&#21487;&#33021;&#20250;&#20559;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#23398;&#20064;&#30456;&#20851;&#30340;&#33041;-&#34920;&#22411;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DeepRepViz&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#31995;&#32479;&#22320;&#26816;&#27979;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;(1)&#24230;&#37327;&#21487;&#33021;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#31243;&#24230;&#30340;&#25351;&#26631;&#21644;(2)&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#23450;&#24615;&#26816;&#26597;DL&#27169;&#22411;&#23398;&#20064;&#20869;&#23481;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#30340;&#30410;&#22788;&#12290;&#20363;&#22914;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#24615;&#21035;&#26159;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models are increasingly used to analyze neuroimaging data and uncover insights about the brain, brain pathologies, and psychological traits. However, extraneous `confounders' variables such as the age of the participants, sex, or imaging artifacts can bias model predictions, preventing the models from learning relevant brain-phenotype relationships. In this study, we provide a solution called the `DeepRepViz' framework that enables researchers to systematically detect confounders in their DL model predictions. The framework consists of (1) a metric that quantifies the effect of potential confounders and (2) a visualization tool that allows researchers to qualitatively inspect what the DL model is learning. By performing experiments on simulated and neuroimaging datasets, we demonstrate the benefits of using DeepRepViz in combination with DL models. For example, experiments on the neuroimaging datasets reveal that sex is a significant confounder in a DL model predicti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27010;&#29575;&#24615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#36716;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32763;&#35793;&#27719;&#32534;&#20195;&#30721;&#31243;&#24207;&#65292;&#20197;&#32553;&#30701;&#32500;&#25252;&#36951;&#30041;&#36719;&#20214;&#30340;&#26102;&#38388;&#21644;&#24037;&#31243;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.14396</link><description>&lt;p&gt;
&#29468;&#27979;&#19982;&#32472;&#22270;&#65306;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#36716;&#35793;
&lt;/p&gt;
&lt;p&gt;
Guess &amp; Sketch: Language Model Guided Transpilation. (arXiv:2309.14396v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27010;&#29575;&#24615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#36716;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32763;&#35793;&#27719;&#32534;&#20195;&#30721;&#31243;&#24207;&#65292;&#20197;&#32553;&#30701;&#32500;&#25252;&#36951;&#30041;&#36719;&#20214;&#30340;&#26102;&#38388;&#21644;&#24037;&#31243;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25252;&#36951;&#30041;&#36719;&#20214;&#38656;&#35201;&#22823;&#37327;&#30340;&#36719;&#20214;&#21644;&#31995;&#32479;&#24037;&#31243;&#26102;&#38388;&#12290;&#27719;&#32534;&#20195;&#30721;&#31243;&#24207;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#29305;&#21035;&#38590;&#20197;&#20998;&#26512;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#35745;&#31639;&#26426;&#26426;&#22120;&#29366;&#24577;&#38656;&#35201;&#20302;&#32423;&#21035;&#30340;&#25511;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#21464;&#37327;&#21517;&#31216;&#12290;&#29616;&#26377;&#30340;&#20256;&#32479;&#31243;&#24207;&#36716;&#25442;&#22120;&#20445;&#35777;&#27491;&#30830;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#38024;&#23545;&#29305;&#23450;&#30340;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#25163;&#24037;&#24037;&#31243;&#35774;&#35745;&#30340;&#12290;&#23398;&#20064;&#24335;&#36716;&#35793;&#65292;&#21363;&#20195;&#30721;&#30340;&#33258;&#21160;&#32763;&#35793;&#65292;&#25552;&#20379;&#20102;&#25163;&#21160;&#37325;&#20889;&#21644;&#24037;&#31243;&#21162;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#33258;&#21160;&#21270;&#30340;&#31526;&#21495;&#31243;&#24207;&#36716;&#25442;&#26041;&#27861;&#20445;&#35777;&#20102;&#27491;&#30830;&#24615;&#65292;&#20294;&#26159;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24456;&#38590;&#25193;&#23637;&#21040;&#36739;&#38271;&#30340;&#31243;&#24207;&#12290;&#23427;&#20204;&#21018;&#24615;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#21482;&#33021;&#25512;&#29702;&#20986;&#19968;&#23567;&#37096;&#20998;&#31243;&#24207;&#31354;&#38388;&#12290;&#27010;&#29575;&#24615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20026;&#27599;&#20010;&#36755;&#20837;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20986;&#65292;&#20294;&#26159;&#20197;&#30830;&#20445;&#27491;&#30830;&#24615;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#20102;LMs&#21644;&#31526;&#21495;&#21270;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08374</link><description>&lt;p&gt;
&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the limitations of self-supervised learning for tabular anomaly detection. (arXiv:2309.08374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25913;&#36827;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#26159;&#21542;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;26&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#28041;&#21450;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#19982;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#34920;&#31034;&#30456;&#27604;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it. This paper explores the limitations of self-supervision for tabular anomaly detection. We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case. Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data. We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors. However, we demonstrate that using a subspace of the neural network's representation can recover performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08045</link><description>&lt;p&gt;
&#26053;&#34892;&#27874;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#24182;&#22686;&#24378;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Traveling Waves Encode the Recent Past and Enhance Sequence Learning. (arXiv:2309.08045v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27963;&#21160;&#30340;&#26053;&#34892;&#27874;&#29616;&#35937;&#22312;&#22823;&#33041;&#30340;&#19981;&#21516;&#21306;&#22495;&#21644;&#23610;&#24230;&#19978;&#37117;&#26377;&#25152;&#35266;&#23519;&#21040;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#35282;&#33394;&#19978;&#30340;&#20855;&#20307;&#20316;&#29992;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#30382;&#36136;&#23618;&#21487;&#20197;&#20687;&#27874;&#21160;&#22330;&#19968;&#26679;&#65292;&#36890;&#36807;&#27839;&#30528;&#30382;&#36136;&#34920;&#38754;&#20256;&#25773;&#30340;&#27874;&#21160;&#26469;&#23384;&#20648;&#39034;&#24207;&#21050;&#28608;&#30340;&#30701;&#26399;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#31616;&#21333;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33021;&#22815;&#23637;&#29616;&#20986;&#36825;&#31181;&#27874;&#21160;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#24819;&#27861;&#30340;&#35745;&#31639;&#24847;&#20041;&#19968;&#30452;&#26159;&#20551;&#35774;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Wave-RNN (wRNN)&#65292;&#24182;&#23637;&#31034;&#20102;&#36830;&#36890;&#24615;&#32422;&#26463;&#21644;&#21021;&#22987;&#21270;&#22312;&#27874;&#21160;&#21160;&#21147;&#23398;&#20986;&#29616;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#20102;&#36825;&#26679;&#30340;&#26550;&#26500;&#30340;&#30830;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;wRNN&#27604;&#27874;&#21160;&#27169;&#22411;&#23398;&#20064;&#26356;&#24555;&#12289;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically grounded hypothesis suggests that the cortical sheet may act like a wave-field capable of storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how both connectivity constraints and initialization play a crucial role in the emergence of wave-like dynamics. We then empirically show how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and perform significantly better than wave-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04849</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#31934;&#28860;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#20018;&#35821;&#38899;&#20449;&#21495;&#26469;&#36827;&#34892;&#21333;&#27169;&#24577;SER&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#36991;&#20813;&#36816;&#34892;&#26102;&#30340;&#36716;&#24405;&#21644;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#38169;&#35823;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#23545;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#20013;&#30340;&#23884;&#20837;&#21644;&#36923;&#36753;&#23618;&#38754;&#33976;&#39311;&#20449;&#24687;&#12290;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#24182;&#36798;&#21040;&#20102;77.49&#65285;&#30340;&#26080;&#26435;&#37325;&#20934;&#30830;&#29575;&#21644;78.91&#65285;&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#30340;&#26368;&#26032;&#25104;&#32489;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
&lt;/p&gt;</description></item><item><title>&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04284</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304; - &#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier. (arXiv:2309.04284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04284
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#35768;&#22810;&#29702;&#35299;&#31639;&#27861;&#21487;&#20197;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#31034;&#20363;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#36825;&#20010;&#29983;&#25104;&#36807;&#31243;&#35270;&#20026;&#19968;&#31181;&#21019;&#36896;&#19968;&#23450;&#37327;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#23384;&#20648;&#24182;&#22312;&#20197;&#21518;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#26412;&#25991;&#22312;&#21152;&#27861;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#27492;&#30446;&#30340;&#19978;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.02427</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#22686;&#21152;&#20102;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20114;&#32852;&#32593;&#65289;&#25110;&#20869;&#37096;&#25511;&#21046;&#27969;&#65288;&#20363;&#22914;&#25552;&#31034;&#38142;&#65289;&#65292;&#29992;&#20110;&#38656;&#35201;&#22522;&#20110;&#35821;&#22659;&#25110;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#29616;&#26377;&#20195;&#29702;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20016;&#23500;&#21382;&#21490;&#65292;&#25552;&#20986;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;&#65288;CoALA&#65289;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#29992;&#20110;&#19982;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#36873;&#25321;&#34892;&#21160;&#30340;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;CoALA&#23545;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#32452;&#32455;&#65292;&#24182;&#23637;&#26395;&#20102;&#26356;&#24378;&#22823;&#20195;&#29702;&#30340;&#21487;&#34892;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CoALA&#23558;&#24403;&#20170;&#30340;&#35821;&#35328;&#20195;&#29702;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15594</link><description>&lt;p&gt;
&#21464;&#24418;&#37329;&#21018;&#26159;&#21542;&#33021;&#23398;&#20250;&#26368;&#22823;&#20844;&#32422;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#20004;&#20010;&#27491;&#25972;&#25968;&#30340;&#26368;&#22823;&#20844;&#32422;&#25968;&#65288;GCD&#65289;&#30340;&#33021;&#21147;&#12290;&#24403;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#20180;&#32454;&#36873;&#25321;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;98%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#27491;&#30830;&#39044;&#27979;&#21069;100&#20010;GCD&#20013;&#30340;91&#20010;&#12290;&#27169;&#22411;&#30340;&#39044;&#27979;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23398;&#20250;&#23558;&#20855;&#26377;&#30456;&#21516;GCD&#30340;&#36755;&#20837;&#23545;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20854;&#38500;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;&#22522;&#26412;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#22522;&#25968;&#32534;&#30721;&#30340;&#22343;&#21248;&#25805;&#20316;&#25968;&#20165;&#35745;&#31639;&#23569;&#25968;GCD&#65288;&#26368;&#22810;100&#20010;&#20013;&#30340;38&#20010;&#65289;&#65306;&#22522;&#25968;&#30340;&#38500;&#25968;&#20056;&#31215;&#12290;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#22522;&#25968;&#20801;&#35768;&#19968;&#20123;&#27169;&#22411;&#8220;&#20102;&#35299;&#8221;&#23567;&#30340;&#32032;&#25968;GCD&#12290;&#20351;&#29992;&#23545;&#25968;&#22343;&#21248;&#25805;&#20316;&#25968;&#36827;&#34892;&#35757;&#32451;&#23558;&#24615;&#33021;&#25552;&#21319;&#21040;&#27491;&#30830;&#30340;73&#20010;GCD&#65292;&#24182;&#36890;&#36807;&#20174;&#20498;&#25968;&#24179;&#26041;&#21040;&#23545;&#25968;&#22343;&#21248;&#30340;GCD&#35757;&#32451;&#20998;&#24067;&#30340;&#24179;&#34913;&#65292;&#20351;&#24615;&#33021;&#36798;&#21040;91&#20010;GCD&#12290;&#20174;GCD&#30340;&#22343;&#21248;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#30772;&#22351;&#20102;&#30830;&#23450;&#24615;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to "grok" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13838</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#23545;&#32852;&#21512;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21487;&#20197;&#24418;&#25104;&#20856;&#22411;&#30340;&#20080;&#26041;&#24066;&#22330;&#65292;&#20854;&#20013;PS/&#20080;&#23478;&#25968;&#37327;&#36828;&#36828;&#23569;&#20110;&#23458;&#25143;&#31471;/&#21334;&#23478;&#25968;&#37327;&#12290;&#20026;&#20102;&#25913;&#21892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#20026;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30456;&#21516;&#30340;&#26381;&#21153;&#23450;&#20215;&#12290;&#20215;&#26684;&#24046;&#24322;&#21270;&#22522;&#20110;&#23545;&#32852;&#21512;&#23398;&#20064;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#35745;&#31639;&#36890;&#20449;&#33021;&#21147;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#30446;&#26631;&#26435;&#34913;&#12289;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#28608;&#21169;&#26426;&#21046;&#12290;&#30001;&#20110;PDG&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#21322;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#21464;&#37327;&#23376;&#38598;&#19978;&#23398;&#20064;&#30340;&#22240;&#26524;&#22270;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#26816;&#27979;&#65292;&#26469;&#20266;&#35777;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09552</link><description>&lt;p&gt;
&#33258;&#25105;&#20860;&#23481;&#24615;&#65306;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Compatibility: Evaluating Causal Discovery without Ground Truth. (arXiv:2307.09552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#21464;&#37327;&#23376;&#38598;&#19978;&#23398;&#20064;&#30340;&#22240;&#26524;&#22270;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#26816;&#27979;&#65292;&#26469;&#20266;&#35777;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22240;&#26524;&#22522;&#26412;&#20107;&#23454;&#38750;&#24120;&#32597;&#35265;&#65292;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#36890;&#24120;&#21482;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27169;&#25311;&#21453;&#26144;&#20102;&#20851;&#20110;&#22122;&#22768;&#20998;&#24067;&#12289;&#27169;&#22411;&#31867;&#21035;&#31561;&#29983;&#25104;&#36807;&#31243;&#30340;&#24120;&#35265;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#36755;&#20986;&#36827;&#34892;&#20266;&#35777;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#23613;&#31649;&#32479;&#35745;&#23398;&#20064;&#23547;&#27714;&#25968;&#25454;&#28857;&#23376;&#38598;&#20043;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#22240;&#26524;&#23398;&#20064;&#24212;&#35813;&#23547;&#27714;&#21464;&#37327;&#23376;&#38598;&#20043;&#38388;&#30340;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#35265;&#35299;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#19981;&#21516;&#21464;&#37327;&#23376;&#38598;&#19978;&#23398;&#20064;&#30340;&#22240;&#26524;&#22270;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26816;&#27979;&#19981;&#20860;&#23481;&#24615;&#21487;&#20197;&#20266;&#35777;&#22240;&#26524;&#20851;&#31995;&#34987;&#38169;&#35823;&#25512;&#26029;&#30340;&#21407;&#22240;&#65292;&#36825;&#26159;&#22240;&#20026;&#20551;&#35774;&#36829;&#21453;&#25110;&#26377;&#38480;&#26679;&#26412;&#25928;&#24212;&#24102;&#26469;&#30340;&#38169;&#35823;&#12290;&#34429;&#28982;&#36890;&#36807;&#36825;&#31181;&#20860;&#23481;&#24615;&#27979;&#35797;&#21482;&#26159;&#23545;&#33391;&#22909;&#24615;&#33021;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.07516</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#26816;&#27979;&#27450;&#39575;&#32473;&#36825;&#19968;&#26087;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#20351;&#29992;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;&#23494;&#27463;&#26681;&#22823;&#23398;&#30340;&#30495;&#23454;&#35797;&#39564;&#25968;&#25454;&#38598;&#21644;&#36808;&#38463;&#23494;&#22823;&#23398;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#35270;&#39057;&#26679;&#26412;&#34987;&#20998;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25163;&#31295;&#30340;&#24103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#25237;&#31080;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#38899;&#39057;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;Mel&#39057;&#35889;&#22270;&#19978;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#31532;&#19977;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#25163;&#31295;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#19978;&#30340;Word2Vec&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#21462;&#24471;&#30340;&#26368;&#20339;&#32467;&#26524;&#20998;&#21035;&#20026;97&#65285;&#12289;96&#65285;&#12289;9
&lt;/p&gt;
&lt;p&gt;
Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14009</link><description>&lt;p&gt;
&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#21152;&#24378;&#22270;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#22270;&#39044;&#27979;&#12290;&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27492;&#38382;&#39064;&#65292;&#32771;&#34385;&#21516;&#26102;&#22312;&#22270;&#19978;&#39044;&#27979;&#22810;&#20010;&#33410;&#28857;&#26631;&#31614;&#20989;&#25968;&#12290;&#20026;&#20102;&#20855;&#20307;&#35828;&#26126;&#65292;&#32771;&#34385;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#65306;&#27599;&#20010;&#31038;&#21306;&#25104;&#21592;&#36523;&#20221;&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#37325;&#21472;&#27169;&#24335;&#65292;&#24403;&#25105;&#20204;&#23558;&#22810;&#20010;&#31038;&#21306;&#26816;&#27979;&#24212;&#29992;&#21040;naive&#22810;&#20219;&#21153;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36127;&#36801;&#31227;&#24456;&#26222;&#36941;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#30340;&#20219;&#21153;&#20851;&#31995;&#39640;&#24230;&#38750;&#32447;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22522;&#20110;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#27599;&#20010;&#20219;&#21153;&#32452;&#19978;&#25311;&#21512;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#20135;&#29983;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#30340;&#22686;&#24378;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#20272;&#35745;&#20026;&#39044;&#27979;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07261</link><description>&lt;p&gt;
&#21462;&#28040;&#19971;&#24180;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19971;&#24180;&#21069;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#20351;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#35823;&#24046;&#29575;&#30456;&#31561;&#12290;&#36825;&#39033;&#24037;&#20316;&#21551;&#21160;&#20102;&#25968;&#30334;&#31687;&#35770;&#25991;&#65292;&#22768;&#31216;&#33021;&#22815;&#25913;&#36827;&#21518;&#22788;&#29702;&#22522;&#32447;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#25968;&#21315;&#20010;&#27169;&#22411;&#35780;&#20272;&#30340;&#23454;&#35777;&#35780;&#20272;&#26469;&#35780;&#20272;&#36825;&#20123;&#22768;&#26126;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#35770;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#22256;&#25200;&#20102;&#20197;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#19968;&#20010;&#19982;&#20351;&#29992;&#19981;&#21516;&#30340;&#26080;&#32422;&#26463;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#26377;&#20851;&#12290;&#21478;&#19968;&#20010;&#28041;&#21450;&#23454;&#29616;&#19981;&#21516;&#30340;&#32422;&#26463;&#25918;&#26494;&#27700;&#24179;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21462;&#28040;&#22788;&#29702;&#65292;&#22823;&#33268;&#23545;&#24212;&#20110;&#21518;&#22788;&#29702;&#30340;&#21453;&#28436;&#12290;&#21462;&#28040;&#22788;&#29702;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#20351;&#29992;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#21644;&#25918;&#26494;&#32423;&#21035;&#30340;&#26041;&#27861;&#12290;&#35299;&#35835;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#36991;&#20813;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27969;&#22330;&#12290;</title><link>http://arxiv.org/abs/2306.01776</link><description>&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#22312;&#19977;&#32500;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Diffusion for 3D Turbulent Flows. (arXiv:2306.01776v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#36991;&#20813;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#27969;&#21160;&#36890;&#24120;&#38590;&#20197;&#39044;&#27979;&#65292;&#20294;&#20108;&#32500;&#21644;&#19977;&#32500;&#30340;&#28237;&#27969;&#27969;&#21160;&#24615;&#36136;&#19981;&#21516;&#12290;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#65292;&#28237;&#27969;&#20250;&#24418;&#25104;&#22823;&#30340;&#12289;&#36830;&#32493;&#30340;&#32467;&#26500;&#65292;&#32780;&#22312;&#19977;&#32500;&#24773;&#20917;&#19979;&#65292;&#26059;&#28065;&#32423;&#32852;&#25104;&#36234;&#26469;&#36234;&#23567;&#30340;&#23610;&#24230;&#65292;&#24418;&#25104;&#35768;&#22810;&#24555;&#36895;&#21464;&#21270;&#30340;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21152;&#21095;&#20102;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#38590;&#20197;&#20351;&#29992;&#22238;&#24402;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#27969;&#22330;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21482;&#20381;&#38752;&#20960;&#20309;&#20449;&#24687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#27604;&#24037;&#19994;&#32423;&#25968;&#20540;&#27714;&#35299;&#22120;&#26356;&#24555;&#22320;&#29983;&#25104;&#28237;&#27969;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulent flows are well known to be chaotic and hard to predict; however, their dynamics differ between two and three dimensions. While 2D turbulence tends to form large, coherent structures, in three dimensions vortices cascade to smaller and smaller scales. This cascade creates many fast-changing, small-scale structures and amplifies the unpredictability, making regression-based methods infeasible. We propose the first generative model for forced turbulence in arbitrary 3D geometries and introduce a sample quality metric for turbulent flows based on the Wasserstein distance of the generated velocity-vorticity distribution. In several experiments, we show that our generative diffusion model circumvents the unpredictability of turbulent flows and produces high-quality samples based solely on geometric information. Furthermore, we demonstrate that our model beats an industrial-grade numerical solver in the time to generate a turbulent flow field from scratch by an order of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#35843;&#25972;&#35757;&#32451;&#38598;&#20013;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#20351;PDE&#35299;&#30340;&#27531;&#20313;&#22312;&#26368;&#23567;&#21270;&#26102;&#33021;&#20445;&#25345;&#24179;&#28369;&#30340;&#36718;&#24275;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#25439;&#22833;&#39033;&#20248;&#21270;PINN&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31283;&#23450;&#30340;&#35299;&#12290;&#21516;&#26102;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#32435;&#20837;&#26368;&#20248;&#20256;&#36755;&#32422;&#26463;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#23558;PINN&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;PDE&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.18702</link><description>&lt;p&gt;
&#23545;&#25239;&#24335;&#33258;&#36866;&#24212;&#37319;&#26679;&#65306;&#23558;PINN&#21644;&#26368;&#20248;&#20256;&#36755;&#32479;&#19968;&#29992;&#20110;PDE&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs. (arXiv:2305.18702v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#35843;&#25972;&#35757;&#32451;&#38598;&#20013;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#20351;PDE&#35299;&#30340;&#27531;&#20313;&#22312;&#26368;&#23567;&#21270;&#26102;&#33021;&#20445;&#25345;&#24179;&#28369;&#30340;&#36718;&#24275;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#25439;&#22833;&#39033;&#20248;&#21270;PINN&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31283;&#23450;&#30340;&#35299;&#12290;&#21516;&#26102;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#32435;&#20837;&#26368;&#20248;&#20256;&#36755;&#32422;&#26463;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#23558;PINN&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;PDE&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#31185;&#23398;&#35745;&#31639;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;PDE&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20855;&#26377;&#26080;&#32593;&#26684;&#31163;&#25955;&#30340;&#28789;&#27963;&#24615;&#21644;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#35745;&#31639;&#22256;&#38590;&#26159;&#35757;&#32451;&#38598;&#20013;&#30340;&#38543;&#26426;&#26679;&#26412;&#24341;&#20837;&#20102;&#32479;&#35745;&#38169;&#35823;&#65292;&#21487;&#33021;&#25104;&#20026;&#26368;&#32456;&#36924;&#36817;&#20013;&#21344;&#20027;&#23548;&#30340;&#35823;&#24046;&#65292;&#20174;&#32780;&#25513;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;minmax&#20844;&#24335;&#65292;&#21516;&#26102;&#20248;&#21270;&#36817;&#20284;&#30340;&#35299;&#21644;&#30001;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#30340;&#35757;&#32451;&#38598;&#20013;&#30340;&#38543;&#26426;&#26679;&#26412;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#35843;&#25972;&#35757;&#32451;&#38598;&#20013;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20351;&#36817;&#20284;PDE&#35299;&#24341;&#36215;&#30340;&#27531;&#20313;&#22312;&#26368;&#23567;&#21270;&#26102;&#33021;&#20445;&#25345;&#24179;&#28369;&#30340;&#36718;&#24275;&#12290;&#36825;&#31181;&#24819;&#27861;&#26159;&#36890;&#36807;&#22312;PINN&#20248;&#21270;&#36807;&#31243;&#20013;&#24341;&#20837;&#23545;&#25239;&#24615;&#25439;&#22833;&#39033;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#25439;&#22833;&#39033;&#40723;&#21169;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31283;&#23450;&#30340;&#35299;&#65292;&#21363;&#20351;&#35757;&#32451;&#38598;&#20013;&#30340;&#26679;&#26412;&#26377;&#38480;&#25110;&#36755;&#20837;&#21547;&#22122;&#22768;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#32435;&#20837;&#26368;&#20248;&#20256;&#36755;&#32422;&#26463;&#65292;&#20174;&#32780;&#24418;&#25104;&#23558;PINN&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;PDE&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust random samples in the training set such that the residual induced by the approximate PDE solution can maintain a smooth profile when it is being minimized. Such an idea is ach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17297</link><description>&lt;p&gt;
&#26080;&#29420;&#31435;&#24615;&#30340;&#27867;&#21270;&#35823;&#24046;&#65306;&#21435;&#22122;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning. (arXiv:2305.17297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#19968;&#20123;&#37325;&#35201;&#24037;&#20316;&#39564;&#35777;&#20102;&#29702;&#35770;&#24037;&#20316;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#30001;&#20110;&#25216;&#26415;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#21253;&#25324;&#20855;&#26377;&#33391;&#22909;&#26465;&#20214;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20197;&#21450;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#19968;&#20123;&#20851;&#20110;&#20998;&#24067;&#20559;&#31227;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#25216;&#26415;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#23545;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#36890;&#36807;&#20998;&#31163;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#20551;&#35774;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20123;&#26494;&#24347;&#30340;&#20551;&#35774;&#19979;&#65292;&#30740;&#31350;&#20102;&#21435;&#22122;&#38382;&#39064;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the generalization abilities of linear models with real data is a central question in statistical learning. While there exist a limited number of prior important works (Loureiro et al. (2021A, 2021B), Wei et al. 2022) that do validate theoretical work with real data, these works have limitations due to technical assumptions. These assumptions include having a well-conditioned covariance matrix and having independent and identically distributed data. These assumptions are not necessarily valid for real data. Additionally, prior works that do address distributional shifts usually make technical assumptions on the joint distribution of the train and test data (Tripuraneni et al. 2021, Wu and Xu 2020), and do not test on real data.  In an attempt to address these issues and better model real data, we look at data that is not I.I.D. but has a low-rank structure. Further, we address distributional shift by decoupling assumptions on the training and test distribution. We provide anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03341</link><description>&lt;p&gt;
&#26080;Softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;(ViTs)&#22312;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#20013;&#36215;&#21040;&#20102;&#25512;&#21160;&#20316;&#29992;&#12290;ViTs&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#22312;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#36924;&#36817;&#33258;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#30340;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#35201;&#20040;&#22312;&#29702;&#35770;&#19978;&#26377;&#32570;&#38519;&#65292;&#35201;&#20040;&#22312;&#23454;&#36341;&#20013;&#26080;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#26469;&#28304;&#20110;&#22312;&#36924;&#36817;&#36807;&#31243;&#20013;&#32487;&#25215;&#20102;&#22522;&#20110;softmax&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#20351;&#29992;softmax&#20989;&#25968;&#23545;&#20196;&#29260;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;&#28857;&#31215;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#30001;&#20110;&#23384;&#22312;&#36825;&#20010;softmax&#25805;&#20316;&#65292;&#25361;&#25112;&#20102;&#20219;&#20309;&#21518;&#32493;&#30340;&#32447;&#24615;&#21270;&#24037;&#20316;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26080;softmax&#30340;&#21464;&#25442;&#22120;(SOFT)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#26367;&#20195;&#28857;&#31215;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#33258;&#27880;&#24847;&#30697;&#38453;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#22312;&#27969;&#24418;&#21464;&#24418;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#25512;&#24191;&#20102;&#22270;&#28388;&#27874;&#22120;&#21644;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2106.03725</link><description>&lt;p&gt;
&#12298;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#24418;&#31283;&#23450;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Stability to Deformations of Manifold Filters and Manifold Neural Networks. (arXiv:2106.03725v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#22312;&#27969;&#24418;&#21464;&#24418;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#25512;&#24191;&#20102;&#22270;&#28388;&#27874;&#22120;&#21644;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#24182;&#30740;&#31350;&#20102;&#27969;&#24418;&#65288;M&#65289;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#12290;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;MNN&#26159;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#25351;&#25968;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#22312;&#27969;&#24418;&#34987;&#37319;&#26679;&#26102;&#65292;&#21487;&#20197;&#24674;&#22797;&#20026;&#22270;&#65288;G&#65289;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#31163;&#25955;&#36817;&#20284;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#20855;&#26377;&#35889;&#34920;&#31034;&#65292;&#23427;&#26159;&#22270;&#28388;&#27874;&#22120;&#30340;&#35889;&#34920;&#31034;&#21644;&#36830;&#32493;&#26102;&#38388;&#20013;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#39057;&#29575;&#21709;&#24212;&#30340;&#25512;&#24191;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#20998;&#26512;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;MNN&#22312;&#27969;&#24418;&#20809;&#28369;&#21464;&#24418;&#19979;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#20998;&#26512;&#25512;&#24191;&#20102;&#22270;&#28388;&#27874;&#22120;&#21644;GNN&#30340;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#65292;&#24182;&#19988;&#20063;&#26159;&#36830;&#32493;&#26102;&#38388;&#20013;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#30340;&#25512;&#24191;&#12290;&#20174;&#36825;&#31181;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#65292;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#22270;&#28388;&#27874;&#22120;&#19968;&#26679;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper defines and studies manifold (M) convolutional filters and neural networks (NNs). \emph{Manifold} filters and MNNs are defined in terms of the Laplace-Beltrami operator exponential and are such that \emph{graph} (G) filters and neural networks (NNs) are recovered as discrete approximations when the manifold is sampled. These filters admit a spectral representation which is a generalization of both the spectral representation of graph filters and the frequency response of standard convolutional filters in continuous time. The main technical contribution of the paper is to analyze the stability of manifold filters and MNNs to smooth deformations of the manifold. This analysis generalizes known stability properties of graph filters and GNNs and it is also a generalization of known stability properties of standard convolutional filters and neural networks in continuous time. The most important observation that follows from this analysis is that manifold filters, same as graph fil
&lt;/p&gt;</description></item></channel></rss>