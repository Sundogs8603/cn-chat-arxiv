<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#65292;&#24182;&#33021;&#26377;&#25928;&#25269;&#25239;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00906</link><description>&lt;p&gt;
BrainLeaks: &#20851;&#20110;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#65292;&#24182;&#33021;&#26377;&#25928;&#25269;&#25239;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#30340;&#20027;&#27969;&#25972;&#21512;&#65292;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#24050;&#32463;&#21152;&#21095;&#12290;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#24050;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#27844;&#38706;&#25935;&#24863;&#25968;&#25454;&#30340;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#27169;&#22411;&#21453;&#36716;&#65288;MI&#65289;&#25915;&#20987;&#21487;&#20197;&#37325;&#26500;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#35745;&#31639;&#30340;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#23454;&#29616;&#20102;&#24322;&#27493;&#21644;&#33410;&#33021;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#23545;&#27169;&#22411;&#21453;&#36716;&#30340;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21040;&#30340;&#21551;&#31034;&#26159;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#19981;&#21487;&#24494;&#29305;&#24615;&#21487;&#33021;&#23548;&#33268;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#65292;&#23588;&#20854;&#25269;&#25239;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;SNNs&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the mainstream integration of machine learning into security-sensitive domains such as healthcare and finance, concerns about data privacy have intensified. Conventional artificial neural networks (ANNs) have been found vulnerable to several attacks that can leak sensitive data. Particularly, model inversion (MI) attacks enable the reconstruction of data samples that have been used to train the model. Neuromorphic architectures have emerged as a paradigm shift in neural computing, enabling asynchronous and energy-efficient computation. However, little to no existing work has investigated the privacy of neuromorphic architectures against model inversion. Our study is motivated by the intuition that the non-differentiable aspect of spiking neural networks (SNNs) might result in inherent privacy-preserving properties, especially against gradient-based attacks. To investigate this hypothesis, we propose a thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#26041;&#27861;&#24182;&#21033;&#29992;&#34920;&#31034;&#20998;&#24067;&#30340;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#22312;&#28145;&#24230;&#27169;&#22411;&#20013;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#20449;&#24687;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.00506</link><description>&lt;p&gt;
&#19982;&#26631;&#31614;&#26080;&#20851;&#30340;&#36951;&#24536;:&#28145;&#24230;&#27169;&#22411;&#20013;&#26080;&#30417;&#30563;&#30340;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#26041;&#27861;&#24182;&#21033;&#29992;&#34920;&#31034;&#20998;&#24067;&#30340;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#22312;&#28145;&#24230;&#27169;&#22411;&#20013;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#20449;&#24687;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#26088;&#22312;&#20174;&#24050;&#36951;&#24536;&#25968;&#25454;&#20013;&#21024;&#38500;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#20013;&#21097;&#20313;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25972;&#20010;&#21435;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#23436;&#20840;&#30417;&#30563;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#26631;&#27880;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#33719;&#24471;&#36825;&#31181;&#30417;&#30563;&#21487;&#33021;&#23454;&#38469;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#36825;&#20010;&#25361;&#25112;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22312;&#21435;&#23398;&#20064;&#36807;&#31243;&#20013;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#36817;&#20284;&#21097;&#20313;&#25968;&#25454;&#30340;&#34920;&#31034;&#20998;&#24067;&#12290;&#21033;&#29992;&#36825;&#31181;&#36817;&#20284;&#65292;&#25105;&#20204;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#20197;&#22312;&#34920;&#31034;&#32423;&#21035;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00506v1 Announce Type: new  Abstract: Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge prompts us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further a
&lt;/p&gt;</description></item><item><title>Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.16689</link><description>&lt;p&gt;
Synapse: &#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#20248;&#20808;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Synapse: Learning Preferential Concepts from Visual Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16689
&lt;/p&gt;
&lt;p&gt;
Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#23398;&#20064;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#65288;&#20363;&#22914;&#65292;&#8220;&#22909;&#20572;&#36710;&#20301;&#8221;&#65292;&#8220;&#26041;&#20415;&#30340;&#19979;&#36710;&#20301;&#32622;&#8221;&#65289;&#12290;&#23613;&#31649;&#19982;&#23398;&#20064;&#20107;&#23454;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#65289;&#30456;&#20284;&#65292;&#20294;&#20559;&#22909;&#23398;&#20064;&#26159;&#19968;&#20010;&#22522;&#26412;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#20027;&#35266;&#24615;&#36136;&#21644;&#20010;&#20154;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Synapse&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#12290;Synapse&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#22312;&#22270;&#20687;&#19978;&#36816;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#20013;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30340;&#26032;&#32452;&#21512;&#26469;&#23398;&#20064;&#20195;&#34920;&#20010;&#20154;&#20559;&#22909;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;Synapse&#65292;&#21253;&#25324;&#19968;&#20010;&#20851;&#27880;&#19982;&#31227;&#21160;&#30456;&#20851;&#30340;&#29992;&#25143;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16689v1 Announce Type: cross  Abstract: This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., "good parking spot", "convenient drop-off location") from visual input. Despite its similarity to learning factual concepts (e.g., "red cube"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03542</link><description>&lt;p&gt;
DPOT: &#33258;&#22238;&#24402;&#21435;&#22122;&#36816;&#31639;&#22120;&#21464;&#25442;&#22120;&#29992;&#20110;&#22823;&#35268;&#27169;PDE&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#26469;&#25552;&#39640;&#22312;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#20013;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#22914;&#38271;&#36712;&#36857;&#12289;&#22810;&#20010;&#23610;&#24230;&#21644;&#19981;&#21516;&#32500;&#24230;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#27867;&#21270;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#28789;&#27963;&#21487;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;10+&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20855;&#26377;&#36229;&#36807;0.5B&#21442;&#25968;&#30340;PDE&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#36229;&#36807;100k&#36712;&#36857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;SOTA&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03542v1 Announce Type: new  Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performanc
&lt;/p&gt;</description></item><item><title>&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02622</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
World Models for Autonomous Driving: An Initial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02622
&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#35780;&#20272;&#20854;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#20851;&#38190;&#22320;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#33021;&#22815;&#32508;&#21512;&#21644;&#35299;&#37322;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#28508;&#22312;&#30340;&#26410;&#26469;&#24773;&#26223;&#24182;&#24357;&#34917;&#20449;&#24687;&#32570;&#21475;&#12290;&#26412;&#25991;&#23545;&#33258;&#20027;&#39550;&#39542;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21457;&#23637;&#36827;&#34892;&#20102;&#21021;&#27493;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#24378;&#35843;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#22522;&#30784;&#21442;&#32771;&#65292;&#20415;&#20110;&#24555;&#36895;&#33719;&#24471;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#32852;&#21512;&#20272;&#35745;&#21644;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#22312;&#32447;&#35757;&#32451;&#21644;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.02215</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#23454;&#29616;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32852;&#21512;&#21442;&#25968;&#21644;&#21442;&#25968;&#21270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02215
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#32852;&#21512;&#20272;&#35745;&#21644;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#22312;&#32447;&#35757;&#32451;&#21644;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#34920;&#31034;&#25968;&#20540;&#27169;&#25311;&#20013;&#26410;&#30693;&#21644;&#20122;&#32593;&#26684;&#29289;&#29702;&#36807;&#31243;&#30340;&#21442;&#25968;&#21270;(&#25110;&#38381;&#21512;)&#24182;&#23545;&#20854;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#23545;&#20110;&#35299;&#26512;&#35768;&#22810;&#38382;&#39064;&#30340;&#31895;&#31890;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#20197;&#21450;&#28237;&#27969;&#27169;&#25311;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#30475;&#21040;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23545;&#36825;&#20123;&#20122;&#32593;&#26684;&#36807;&#31243;&#24314;&#27169;&#65292;&#23548;&#33268;&#20102;&#36890;&#36807;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#38598;&#25104;&#24320;&#21457;&#28151;&#21512;&#29289;&#29702;-ML&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#12290;&#36890;&#36807;&#22312;&#32447;&#35757;&#32451;&#21644;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#20511;&#21161;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#33021;&#21147;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02215v1 Announce Type: new  Abstract: Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations. Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12875</link><description>&lt;p&gt;
&#24605;&#32500;&#38142;&#28608;&#21457;&#21464;&#21387;&#22120;&#35299;&#20915;&#22266;&#26377;&#20018;&#34892;&#38382;&#39064;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12875
&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#27493;&#39588;&#65292;&#21363;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#20934;&#30830;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CoT&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#34920;&#36798;&#24615;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35299;&#30721;&#22120;&#19987;&#29992;&#21464;&#21387;&#22120;&#30340;CoT&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;CoT&#36171;&#20104;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#21464;&#21387;&#22120;&#20013;&#32570;&#20047;&#65292;&#29305;&#21035;&#26159;&#24403;&#28145;&#24230;&#36739;&#20302;&#26102;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;CoT&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26377;&#38480;&#31934;&#24230;$\mathsf{poly}(n)$&#23884;&#20837;&#23610;&#23544;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#21482;&#33021;&#22312;$\mathsf{TC}^0$&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20855;&#26377;&#24120;&#25968;&#20301;&#31934;&#24230;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#30340;&#26356;&#32039;&#23494;&#30340;&#34920;&#36798;&#24615;&#19978;&#30028;&#65292;&#23427;&#21482;&#33021;&#35299;&#20915;$\mathsf{AC}^0$&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;&#22914;&#22522;&#30784;&#27169;&#22411;&#65289;&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#65292;&#23545;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#31561;&#20851;&#38190;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10977</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#65306;&#19979;&#19968;&#20010;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Generative AI and Process Systems Engineering: The Next Frontier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10977
&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;&#22914;&#22522;&#30784;&#27169;&#22411;&#65289;&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#65292;&#23545;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#31561;&#20851;&#38190;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;&#20309;&#22686;&#24378;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#65288;PSE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36825;&#20123;&#26368;&#21069;&#27839;&#30340;GenAI&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#20026;&#28041;&#21450;&#26597;&#35810;&#21709;&#24212;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#22797;&#26434;&#20915;&#31574;&#31561;&#24191;&#27867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#12290;&#37492;&#20110;PSE&#30340;&#36827;&#23637;&#19982;&#35745;&#31639;&#21644;&#31995;&#32479;&#25216;&#26415;&#30340;&#21457;&#23637;&#20043;&#38388;&#23494;&#20999;&#20851;&#31995;&#65292;&#25506;&#32034;GenAI&#21644;PSE&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#32463;&#20856;&#21644;&#26032;&#20852;&#30340;GenAI&#27169;&#22411;&#65292;&#21253;&#25324;FMs&#30340;&#31616;&#35201;&#27010;&#36848;&#24320;&#22987;&#35752;&#35770;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#22312;&#20851;&#38190;PSE&#39046;&#22495;&#20869;&#30340;&#24212;&#29992;&#65306;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#65292;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#12290;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;GenAI&#27169;&#22411;&#22914;&#20309;&#21487;&#20197;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10977v1 Announce Type: new  Abstract: This article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.10665</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21450;&#20854;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#28982;&#32780;&#20854;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#25152;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#19968;&#20010;&#24120;&#35265;&#31574;&#30053;&#26159;&#21033;&#29992;&#22312;&#19981;&#21516;&#31181;&#32676;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22914;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#22312;&#20852;&#36259;&#31181;&#32676;&#19978;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#22411;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#21518;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#36731;&#39118;&#38505;&#12289;&#20943;&#23569;&#23545;&#19987;&#23478;&#30417;&#30563;&#20381;&#36182;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#30528;&#37325;&#20110;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#36816;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10665v1 Announce Type: new  Abstract: Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through expe
&lt;/p&gt;</description></item><item><title>&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.10517</link><description>&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#65306;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLM&#30340;&#20302;&#25104;&#26412;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10517
&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#65292;&#36825;&#20123;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#20307;&#31215;&#32780;&#23548;&#33268;&#37096;&#32626;&#25104;&#26412;&#39640;&#26114;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23613;&#31649;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#37096;&#32626;&#30340;&#25104;&#26412;&#22312;&#23454;&#38469;&#24847;&#20041;&#19978;&#24456;&#37325;&#35201;&#65292;&#20294;&#21364;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#20219;&#24847;&#31934;&#24230;LLM&#8221;&#65292;&#23558;&#20219;&#24847;&#31934;&#24230;DNN&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;LLMs&#12290;&#35299;&#20915;&#20102;&#20219;&#24847;&#31934;&#24230;LLM&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;LLMs&#20219;&#24847;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#36719;&#20214;&#24341;&#25806;&#26469;&#23454;&#29616;&#20854;&#26377;&#25928;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20197;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;&#8230;&#65292;n&#20301;&#65289;&#37327;&#21270;&#30340;LLMs&#21472;&#21152;&#21040;&#20869;&#23384;&#36275;&#21360;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#30340;&#39640;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;GraphCBAL&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#36873;&#25321;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;GNNs&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10074</link><description>&lt;p&gt;
GraphCBAL: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;GraphCBAL&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#36873;&#25321;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;GNNs&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;GNNs&#30340;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#26597;&#35810;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;GNNs&#20013;&#30340;&#24378;&#21270;&#20027;&#21160;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#24230;&#20542;&#26012;&#30340;&#31867;&#21035;&#22330;&#26223;&#19979;&#12290;&#36825;&#36827;&#19968;&#27493;&#23545;&#20998;&#31867;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;GraphCBAL&#65292;&#29992;&#20110;GNNs&#12290;&#23427;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#20197;&#33719;&#21462;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#26631;&#35760;&#33410;&#28857;&#35757;&#32451;&#30340;GNNs&#30340;&#24615;&#33021;&#12290;GraphCBAL&#35774;&#35745;&#20102;&#31867;&#24179;&#34913;&#24863;&#30693;&#29366;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#31867;&#24179;&#34913;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;GraphCBAL&#65292;&#24471;&#21040;GraphCBAL++&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10074v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have recently demonstrated significant success. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost. However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. This further adversely affects the classification performance. To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. We further upgrade GraphCBAL to GraphCBAL++ by intr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01810</link><description>&lt;p&gt;
&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Misspecification uncertainties in near-deterministic regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25439;&#22833;&#26159;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;PAC-Bayes&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#26368;&#23567;&#21270;&#34987;&#35748;&#20026;&#24573;&#30053;&#20102;&#38169;&#35823;&#35268;&#33539;&#21270;&#65292;&#21363;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#22797;&#21046;&#35266;&#27979;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#22823;&#25968;&#25454;&#25110;&#27424;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#23545;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#33879;&#20302;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#36817;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#35268;&#33539;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24191;&#27867;&#30456;&#20851;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#24517;&#39035;&#35206;&#30422;&#27599;&#20010;&#35757;&#32451;&#28857;&#65292;&#20197;&#36991;&#20813;&#21457;&#25955;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#31526;&#21512;&#36825;&#20010;&#32422;&#26463;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39069;&#22806;&#24320;&#38144;&#26368;&#23567;&#12290;&#36825;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#27169;&#22411;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.01705</link><description>&lt;p&gt;
&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#34920;&#24449;&#20260;&#23475;&#65306;&#24230;&#37327;&#21644;&#20943;&#36731;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20260;&#23475;&#36890;&#24120;&#34987;&#20998;&#20026;&#37197;&#32622;&#24615;&#25110;&#34920;&#24449;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#21518;&#32773;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#24403;&#21069;&#34920;&#24449;&#24615;&#20260;&#23475;&#23450;&#20041;&#30340;&#23457;&#26597;&#65292;&#20197;&#30830;&#23450;&#20854;&#20013;&#21253;&#21547;&#20160;&#20040;&#21644;&#19981;&#21253;&#21547;&#20160;&#20040;&#12290;&#36825;&#20010;&#20998;&#26512;&#20419;&#20351;&#25105;&#20204;&#25193;&#23637;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#21253;&#25324;&#23545;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#30340;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24230;&#37327;&#30340;&#39640;&#32423;&#35201;&#27714;&#65306;&#30830;&#23450;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#34920;&#24449;&#24615;&#20260;&#23475;&#26102;&#30340;&#29420;&#29305;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#20260;&#23475;&#26410;&#34987;&#24230;&#37327;&#21644;&#20943;&#36731;&#26102;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20943;&#36731;&#25514;&#26045;&#24182;&#30028;&#23450;&#20309;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#32467;&#26463;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26694;&#26550;&#65292;&#25193;&#22823;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#20844;&#24179;&#30740;&#31350;&#30340;&#35265;&#35299;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#36896;&#21147;&#39046;&#22495;&#30340;&#21382;&#21490;&#12289;&#29616;&#29366;&#65292;&#20197;&#21450;&#20851;&#38190;&#30340;&#36129;&#29486;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2104.02726</link><description>&lt;p&gt;
&#21019;&#24847;&#19982;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Creativity and Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.02726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#36896;&#21147;&#39046;&#22495;&#30340;&#21382;&#21490;&#12289;&#29616;&#29366;&#65292;&#20197;&#21450;&#20851;&#38190;&#30340;&#36129;&#29486;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#24847;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#24863;&#20852;&#36259;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#35745;&#31639;&#21019;&#36896;&#21147;&#29702;&#35770;&#30340;&#21382;&#21490;&#21644;&#29616;&#29366;&#12289;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#21253;&#25324;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#23545;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#36129;&#29486;&#36827;&#34892;&#25209;&#21028;&#24615;&#35752;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#30740;&#31350;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#20852;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.02726v4 Announce Type: replace  Abstract: There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CascadedGaze&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#20013;&#20840;&#23616;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21367;&#31215;&#23618;&#20043;&#38388;&#24341;&#20837;&#23567;&#30340;&#21367;&#31215;&#26680;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#20840;&#23616;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.15235</link><description>&lt;p&gt;
CascadedGaze: &#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CascadedGaze: Efficiency in Global Context Extraction for Image Restoration. (arXiv:2401.15235v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CascadedGaze&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#20013;&#20840;&#23616;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21367;&#31215;&#23618;&#20043;&#38388;&#24341;&#20837;&#23567;&#30340;&#21367;&#31215;&#26680;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#20840;&#23616;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21367;&#31215;&#36816;&#31639;&#31526;&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#24456;&#38590;&#25429;&#25417;&#21040;&#20840;&#23616;&#20449;&#24687;&#12290;Transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20248;&#21183;&#22312;&#20110;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#21364;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#22270;&#20687;&#24674;&#22797;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#21464;&#31181;Transformer&#35299;&#20915;&#24615;&#33021;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CascadedGaze&#32593;&#32476;&#65288;CGNet&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#65288;GCE&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#22270;&#20687;&#24674;&#22797;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;GCE&#27169;&#22359;&#36890;&#36807;&#22312;&#21367;&#31215;&#23618;&#20043;&#38388;&#20351;&#29992;&#23567;&#30340;&#21367;&#31215;&#26680;&#26469;&#23398;&#20064;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21435;&#22122;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65288;&#21253;&#25324;&#30495;&#23454;&#22270;&#20687;&#21435;&#22122;&#21644;&#65289;&#30340;&#24615;&#33021;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our approach outperforms a range of state-of-the-art methods on denoising benchmark datasets including both real image denoising and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#23436;&#20840;&#31561;&#21464;&#37327;&#21152;&#36895;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#28857;&#36317;&#31163;&#20998;&#24067;(PDD)&#20316;&#20026;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#31034;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#21033;&#29992;PDD&#12290;</title><link>http://arxiv.org/abs/2401.15089</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#29992;&#23436;&#20840;&#31561;&#21464;&#37327;&#21152;&#36895;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accelerating Material Property Prediction using Generically Complete Isometry Invariants. (arXiv:2401.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#23436;&#20840;&#31561;&#21464;&#37327;&#21152;&#36895;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#28857;&#36317;&#31163;&#20998;&#24067;(PDD)&#20316;&#20026;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#31034;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#21033;&#29992;PDD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26448;&#26009;&#25110;&#26230;&#20307;&#23646;&#24615;&#39044;&#27979;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#23545;&#20256;&#32479;&#27169;&#25311;&#26041;&#27861;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#30340;&#20851;&#38190;&#31532;&#19968;&#27493;&#26159;&#21608;&#26399;&#24615;&#26230;&#20307;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#31561;&#29289;&#20307;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#21407;&#23376;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#21487;&#20197;&#22522;&#20110;&#26377;&#38480;&#28857;&#20113;&#36827;&#34892;&#35299;&#37322;&#65292;&#20294;&#26159;&#21608;&#26399;&#24615;&#26230;&#20307;&#30340;&#23610;&#23544;&#26159;&#26080;&#38480;&#30340;&#65292;&#25152;&#20197;&#23427;&#20204;&#30340;&#34920;&#31034;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#28857;&#36317;&#31163;&#20998;&#24067;(PDD)&#65292;&#36825;&#26159;&#19968;&#31181;&#36830;&#32493;&#19988;&#36890;&#29992;&#30340;&#23436;&#20840;&#31561;&#21464;&#37327;&#65292;&#29992;&#20316;&#25105;&#20204;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;PDD&#22312;&#21306;&#20998;&#21608;&#26399;&#24615;&#28857;&#38598;&#30340;&#31561;&#21464;&#24615;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20854;&#27809;&#26377;&#32771;&#34385;&#22522;&#30784;&#26448;&#26009;&#30340;&#32452;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#21033;&#29992;PDD&#21644;...
&lt;/p&gt;
&lt;p&gt;
Material or crystal property prediction using machine learning has grown popular in recent years as it provides a computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a finite number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous and generically complete isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and inc
&lt;/p&gt;</description></item><item><title>&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;Transformer-based&#21435;&#22122;&#27169;&#22411;&#21644;&#22270;&#20381;&#36182;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#32422;&#26463;&#19979;&#20934;&#30830;&#22320;&#29983;&#25104;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.13858</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Inverse Molecular Design with Multi-Conditional Diffusion Guidance. (arXiv:2401.13858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13858
&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;Transformer-based&#21435;&#22122;&#27169;&#22411;&#21644;&#22270;&#20381;&#36182;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#32422;&#26463;&#19979;&#20934;&#30830;&#22320;&#29983;&#25104;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36870;&#20998;&#23376;&#35774;&#35745;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#26080;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23558;&#21512;&#25104;&#35780;&#20998;&#21644;&#27668;&#20307;&#28183;&#36879;&#24615;&#31561;&#22810;&#20010;&#23646;&#24615;&#20316;&#20026;&#26465;&#20214;&#32422;&#26463;&#38598;&#25104;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#21435;&#22122;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#25968;&#20540;&#21644;&#20998;&#31867;&#26465;&#20214;&#30340;&#34920;&#31034;&#12290;&#32452;&#25104;&#32467;&#26500;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#21435;&#22122;&#27169;&#22411;&#22312;&#26465;&#20214;&#34920;&#31034;&#19979;&#36827;&#34892;&#21435;&#22122;&#35757;&#32451;&#12290;&#25193;&#25955;&#36807;&#31243;&#21464;&#24471;&#20381;&#36182;&#20110;&#22270;&#26469;&#20934;&#30830;&#20272;&#35745;&#20998;&#23376;&#20013;&#19982;&#22270;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#32780;&#19981;&#20687;&#20197;&#21069;&#30340;&#27169;&#22411;&#20165;&#20851;&#27880;&#21407;&#23376;&#25110;&#38190;&#30340;&#36793;&#32536;&#20998;&#24067;&#12290;&#25105;&#20204;&#24191;&#27867;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#22312;&#20998;&#24067;&#24230;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31038;&#20132;&#38899;&#20048;&#25512;&#33616;&#20013;&#24433;&#21709;&#38899;&#20048;&#20114;&#21160;&#30340;&#31038;&#20132;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#38899;&#20048;&#21697;&#21619;&#30456;&#20284;&#12289;&#20998;&#20139;&#30340;&#38899;&#36712;&#36866;&#21512;&#25509;&#25910;&#32773;&#30340;&#21697;&#21619;&#12289;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#20855;&#26377;&#26356;&#24378;&#21644;&#26356;&#20146;&#23494;&#30340;&#32852;&#31995;&#20197;&#21450;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#22312;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#20013;&#21463;&#27426;&#36814;&#65292;&#36825;&#20123;&#22240;&#32032;&#37117;&#20250;&#22686;&#21152;&#25509;&#25910;&#32773;&#19982;&#26032;&#33402;&#26415;&#23478;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.08818</link><description>&lt;p&gt;
Link Me Baby One More Time: &#22312; Spotify &#19978;&#30340;&#31038;&#20132;&#38899;&#20048;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Link Me Baby One More Time: Social Music Discovery on Spotify. (arXiv:2401.08818v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31038;&#20132;&#38899;&#20048;&#25512;&#33616;&#20013;&#24433;&#21709;&#38899;&#20048;&#20114;&#21160;&#30340;&#31038;&#20132;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#38899;&#20048;&#21697;&#21619;&#30456;&#20284;&#12289;&#20998;&#20139;&#30340;&#38899;&#36712;&#36866;&#21512;&#25509;&#25910;&#32773;&#30340;&#21697;&#21619;&#12289;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#20855;&#26377;&#26356;&#24378;&#21644;&#26356;&#20146;&#23494;&#30340;&#32852;&#31995;&#20197;&#21450;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#22312;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#20013;&#21463;&#27426;&#36814;&#65292;&#36825;&#20123;&#22240;&#32032;&#37117;&#20250;&#22686;&#21152;&#25509;&#25910;&#32773;&#19982;&#26032;&#33402;&#26415;&#23478;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#24433;&#21709;&#20010;&#20154;&#20043;&#38388;&#38899;&#20048;&#25512;&#33616;&#21644;&#21457;&#29616;&#32467;&#26524;&#30340;&#31038;&#20132;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992; Spotify &#30340;&#25968;&#25454;&#26469;&#30740;&#31350;&#29992;&#25143;&#20043;&#38388;&#21457;&#36865;&#38142;&#25509;&#23548;&#33268;&#25509;&#25910;&#32773;&#19982;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#30340;&#38899;&#20048;&#20114;&#21160;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#21487;&#33021;&#24433;&#21709;&#36825;&#19968;&#36807;&#31243;&#30340;&#22240;&#32032;&#65292;&#22914;&#21457;&#36865;&#32773;&#19982;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#24378;&#24230;&#65292;&#29992;&#25143;&#22312; Spotify &#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35282;&#33394;&#65292;&#20182;&#20204;&#30340;&#38899;&#20048;&#31038;&#20132;&#20957;&#32858;&#21147;&#65292;&#20197;&#21450;&#26032;&#33402;&#26415;&#23478;&#19982;&#25509;&#25910;&#32773;&#30340;&#21697;&#21619;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#30340;&#38899;&#20048;&#21697;&#21619;&#30456;&#20284;&#19988;&#20998;&#20139;&#30340;&#38899;&#36712;&#36866;&#21512;&#20182;&#20204;&#30340;&#21697;&#21619;&#26102;&#65292;&#20182;&#20204;&#26356;&#26377;&#21487;&#33021;&#19982;&#26032;&#33402;&#26415;&#23478;&#20114;&#21160;&#65307;&#24403;&#20182;&#20204;&#19982;&#21457;&#36865;&#32773;&#26377;&#26356;&#24378;&#21644;&#26356;&#20146;&#23494;&#30340;&#32852;&#31995;&#26102;&#65292;&#20063;&#26356;&#26377;&#21487;&#33021;&#20114;&#21160;&#65307;&#20197;&#21450;&#24403;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#22312;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#20013;&#21463;&#27426;&#36814;&#26102;&#65292;&#20063;&#26356;&#26377;&#21487;&#33021;&#20114;&#21160;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#26500;&#24314;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#20139;&#30340;&#38899;&#20048;&#36712;&#36947;&#26159;&#21542;&#20250;&#23548;&#33268;&#25509;&#25910;&#32773;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the social and contextual factors that influence the outcome of person-to-person music recommendations and discovery. Specifically, we use data from Spotify to investigate how a link sent from one user to another results in the receiver engaging with the music of the shared artist. We consider several factors that may influence this process, such as the strength of the sender-receiver relationship, the user's role in the Spotify social network, their music social cohesion, and how similar the new artist is to the receiver's taste. We find that the receiver of a link is more likely to engage with a new artist when (1) they have similar music taste to the sender and the shared track is a good fit for their taste, (2) they have a stronger and more intimate tie with the sender, and (3) the shared artist is popular with the receiver's connections. Finally, we use these findings to build a Random Forest classifier to predict whether a shared music track will result in the receiver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#39118;&#38505;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32771;&#34385;&#36710;&#36742;&#30340;&#37325;&#37327;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#20013;&#65292;&#20197;&#38477;&#20302;&#28508;&#22312;&#39118;&#38505;&#21644;&#20107;&#25925;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.08661</link><description>&lt;p&gt;
&#32771;&#34385;&#36710;&#36742;&#37325;&#37327;&#30340;&#39118;&#38505;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Risk-anticipatory autonomous driving strategies considering vehicles' weights, based on hierarchical deep reinforcement learning. (arXiv:2401.08661v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#39118;&#38505;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32771;&#34385;&#36710;&#36742;&#30340;&#37325;&#37327;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#20013;&#65292;&#20197;&#38477;&#20302;&#28508;&#22312;&#39118;&#38505;&#21644;&#20107;&#25925;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20855;&#26377;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#23548;&#33268;&#30340;&#20107;&#25925;&#21644;&#38477;&#20302;&#36947;&#36335;&#20132;&#36890;&#39118;&#38505;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#22411;&#36710;&#36742;&#30340;&#24615;&#36136;&#65292;&#20854;&#30896;&#25758;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#20107;&#25925;&#65292;&#22240;&#27492;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#32771;&#34385;&#36710;&#36742;&#30340;&#37325;&#37327;&#26469;&#21046;&#23450;&#38477;&#20302;&#28508;&#22312;&#39118;&#38505;&#21644;&#21518;&#26524;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#39118;&#38505;&#39044;&#27979;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32771;&#34385;&#21608;&#22260;&#36710;&#36742;&#30340;&#37325;&#37327;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#39118;&#38505;&#22330;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21608;&#22260;&#36710;&#36742;&#37325;&#37327;&#30340;&#39118;&#38505;&#25351;&#26631;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#20013;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#34892;&#21160;&#31354;&#38388;&#65292;&#20801;&#35768;&#24038;&#36710;&#36947;&#21464;&#36947;&#12289;&#21491;&#36710;&#36947;&#21464;&#36947;&#21644;&#36319;&#36710;&#34892;&#20026;&#65292;&#20351;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#26356;&#33258;&#30001;&#12289;&#26356;&#30495;&#23454;&#22320;&#34892;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#28151;&#21512;&#20915;&#31574;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#20998;&#23618;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;HPPO&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles (AVs) have the potential to prevent accidents caused by drivers' error and reduce road traffic risks. Due to the nature of heavy vehicles, whose collisions cause more serious crashes, the weights of vehicles need to be considered when making driving strategies aimed at reducing the potential risks and their consequences in the context of autonomous driving. This study develops an autonomous driving strategy based on risk anticipation, considering the weights of surrounding vehicles and using hierarchical deep reinforcement learning. A risk indicator integrating surrounding vehicles' weights, based on the risk field theory, is proposed and incorporated into autonomous driving decisions. A hybrid action space is designed to allow for left lane changes, right lane changes and car-following, which enables AVs to act more freely and realistically whenever possible. To solve the above hybrid decision-making problem, a hierarchical proximal policy optimization (HPPO) algor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;</title><link>http://arxiv.org/abs/2401.01404</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23376;&#20108;&#27425;&#26102;&#38388;&#32593;&#32476;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#37325;&#24314;&#26159;&#25351;&#22312;&#21482;&#26377;&#20851;&#20110;&#26465;&#20214;&#20598;&#32852;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#25110;&#22270;&#27169;&#22411;&#30340;&#29420;&#31435;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;N&#20010;&#33410;&#28857;&#20043;&#38388;&#26410;&#35266;&#27979;&#21040;&#30340;&#25104;&#23545;&#32806;&#21512;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20284;&#20046;&#26080;&#27861;&#36991;&#20813;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;O(N^2)&#65292;&#21363;&#35201;&#32771;&#34385;&#27599;&#31181;&#21487;&#33021;&#30340;&#25104;&#23545;&#32806;&#21512;&#33267;&#23569;&#19968;&#27425;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#24863;&#20852;&#36259;&#30340;&#32593;&#32476;&#37117;&#26159;&#31232;&#30095;&#30340;&#65292;&#38750;&#38646;&#32806;&#21512;&#30340;&#25968;&#37327;&#21482;&#26377;O(N)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#37325;&#24314;&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#20854;&#22312;&#23376;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#20854;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24230;&#23485;&#26494;&#19978;&#30028;&#20026;O(N^(3/2)logN)&#65292;&#20294;&#20855;&#26377;&#26356;&#20856;&#22411;&#30340;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;O(Nlog^2 N)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#65292;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.00208</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#22120;&#65306;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#23427;&#20204;&#33021;&#21542;&#35299;&#20915;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#24418;&#24335;&#35821;&#35328;&#12290;&#25506;&#32034;&#36825;&#31867;&#38382;&#39064;&#23558;&#26377;&#21161;&#20110;&#27604;&#36739;transformers&#19982;&#20854;&#20182;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#21464;&#31181;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#36825;&#20010;&#23376;&#39046;&#22495;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#32467;&#26524;&#32972;&#21518;&#30340;&#21508;&#31181;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#35843;&#30475;&#20284;&#30456;&#20114;&#30683;&#30462;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16242</link><description>&lt;p&gt;
ZzzGPT: &#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality. (arXiv:2310.16242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#19990;&#30028;&#20013;&#65292;&#30561;&#30496;&#36136;&#37327;&#23545;&#24635;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25552;&#20379;&#23454;&#26102;&#30417;&#27979;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#35265;&#35299;&#65292;&#23548;&#33268;&#29992;&#25143;&#25918;&#24323;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25216;&#26415;&#22312;&#29702;&#35299;&#30561;&#30496;&#27169;&#24335;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#26088;&#22312;&#25552;&#20379;&#20934;&#30830;&#30340;&#30561;&#30496;&#39044;&#27979;&#21644;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#12290;&#21033;&#29992;GLOBEM&#25968;&#25454;&#38598;&#21644;LLMs&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#30340;&#22686;&#24378;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#30456;&#32467;&#21512;&#65292;&#23558;&#31185;&#23398;&#20934;&#30830;&#24615;&#19982;&#23454;&#29992;&#24615;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's world, sleep quality is pivotal for overall well-being. While wearable sensors offer real-time monitoring, they often lack actionable insights, leading to user abandonment. This paper delves into the role of technology in understanding sleep patterns. We introduce a two-stage framework, utilizing Large Language Models (LLMs), aiming to provide accurate sleep predictions with actionable feedback. Leveraging the GLOBEM dataset and synthetic data from LLMs, we highlight enhanced results with models like XGBoost. Our approach merges advanced machine learning with user-centric design, blending scientific accuracy with practicality.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20026;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;FMs&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#39640;&#21487;&#33021;&#32473;FL-enabled&#30340;&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.04003</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Federated Learning in a Wireless World with Foundation Models. (arXiv:2310.04003v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04003
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20026;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;FMs&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#39640;&#21487;&#33021;&#32473;FL-enabled&#30340;&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26159;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#26368;&#36817;&#20026;&#22810;&#20010;&#20840;&#26032;&#30340;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;FMs&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#30340;&#24895;&#26223;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#65292;&#20854;&#20013;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#20998;&#24067;&#24335;&#32593;&#32476;&#26234;&#33021;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#30446;&#21069;&#65292;FMs&#21644;FL&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;FMs&#21487;&#20197;&#25552;&#39640;FL&#30340;&#24615;&#33021;&#65292;&#32780;FL&#20063;&#21487;&#20197;&#21033;&#29992;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#36741;&#21161;&#35757;&#32451;FMs&#12290;&#28982;&#32780;&#65292;FMs&#23545;&#35745;&#31639;&#36164;&#28304;&#12289;&#23384;&#20648;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#35201;&#27714;&#24322;&#24120;&#39640;&#65292;&#36825;&#32473;FL-enabled&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;FMs&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#26159;&#21542;&#36866;&#29992;&#20110;FL&#65292;&#21253;&#25324;&#23545;&#30740;&#31350;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#20221;FL&#21644;FL&#36164;&#28304;&#30340;&#38656;&#27714;&#30340;&#32852;&#21512;&#35757;&#32451;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are general-purpose artificial intelligence (AI) models that have recently enabled multiple brand-new generative AI applications. The rapid advances in FMs serve as an important contextual backdrop for the vision of next-generation wireless networks, where federated learning (FL) is a key enabler of distributed network intelligence. Currently, the exploration of the interplay between FMs and FL is still in its nascent stage. Naturally, FMs are capable of boosting the performance of FL, and FL could also leverage decentralized data and computing resources to assist in the training of FMs. However, the exceptionally high requirements that FMs have for computing resources, storage, and communication overhead would pose critical challenges to FL-enabled wireless networks. In this article, we explore the extent to which FMs are suitable for FL over wireless networks, including a broad overview of research challenges and opportunities. In particular, we discuss multip
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11905</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25509;&#21463;&#36793;&#30028;&#36827;&#34892;&#21551;&#21457;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21069;&#21521;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#24212;&#35813;&#23398;&#20064;&#30340;&#20869;&#23481;&#12289;&#22914;&#20309;&#35757;&#32451;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;&#30340;&#29702;&#35770;&#35748;&#35782;&#36824;&#24456;&#23569;&#12290;&#36825;&#31181;&#29702;&#35299;&#30340;&#19981;&#36275;&#23548;&#33268;&#25991;&#29486;&#20013;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65288;&#27425;&#20248;&#25104;&#26412;&#23545;&#26368;&#20248;&#25104;&#26412;&#25110;&#21487;&#25509;&#21463;&#23545;&#19981;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#65289;&#21644;&#20248;&#21270;&#25351;&#26631;&#65288;&#20363;&#22914;&#24179;&#26041;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#65289;&#26102;&#36827;&#34892;&#20102;&#20020;&#26102;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#35757;&#32451;&#21551;&#21457;&#24335;&#20989;&#25968;&#32570;&#20047;&#21487;&#25509;&#21463;&#24615;&#65292;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25509;&#21463;&#24615;&#30340;&#37325;&#35201;&#24615;&#20063;&#32570;&#20047;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#30456;&#27604;&#26222;&#36890;&#39640;&#26031;&#20998;&#24067;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25968;&#23398;&#27169;&#22411;&#24544;&#23454;&#22320;&#36981;&#24490;&#20102;&#26368;&#22823;&#29109;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.09764</link><description>&lt;p&gt;
&#21435;&#38500;&#32972;&#26223;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#23578;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#65292;&#22312;&#24066;&#22330;&#19978;&#20855;&#26377;&#24456;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#30001;&#20110;&#26381;&#35013;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#20197;&#21450;&#21508;&#31181;&#22330;&#26223;&#21644;&#32972;&#26223;&#30340;&#23384;&#22312;&#65292;&#26102;&#23578;&#29702;&#35299;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20173;&#28982;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#20013;&#30340;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#33879;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26102;&#23578;&#25968;&#25454;&#36827;&#34892;&#32972;&#26223;&#21435;&#38500;&#12290;&#34987;&#21435;&#38500;&#32972;&#26223;&#30340;&#26102;&#23578;&#22270;&#20687;&#19982;&#26102;&#23578;&#25968;&#25454;&#38598;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#27169;&#22411;&#26550;&#26500;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#12289;&#19982;&#20854;&#20182;&#35757;&#32451;&#25216;&#24039;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20860;&#23481;&#24615;&#20197;&#21450;&#30446;&#26631;&#20219;&#21153;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ZeroGrads&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#21487;&#24494;&#22270;&#24418;&#30340;&#23616;&#37096;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#26080;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#24179;&#28369;&#21644;&#23616;&#37096;&#24615;&#32422;&#26463;&#20248;&#21270;&#26367;&#20195;&#25439;&#22833;&#30340;&#25311;&#21512;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05739</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#32473;&#20219;&#20309;&#38646;&#26799;&#24230;&#65306;&#23398;&#20064;&#38750;&#21487;&#24494;&#22270;&#24418;&#30340;&#23616;&#37096;&#26367;&#20195;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics. (arXiv:2308.05739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ZeroGrads&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#21487;&#24494;&#22270;&#24418;&#30340;&#23616;&#37096;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#26080;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#21160;&#24179;&#28369;&#21644;&#23616;&#37096;&#24615;&#32422;&#26463;&#20248;&#21270;&#26367;&#20195;&#25439;&#22833;&#30340;&#25311;&#21512;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22312;&#22270;&#24418;&#39046;&#22495;&#21464;&#24471;&#26222;&#36941;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26410;&#23450;&#20041;&#25110;&#38646;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#26367;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#20351;&#29992;&#31867;&#20284;&#26497;&#23567;&#20540;&#20294;&#21487;&#24494;&#30340;&#8220;&#26367;&#20195;&#25439;&#22833;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ZeroGrads&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#30340;&#31070;&#32463;&#36924;&#36817;&#65292;&#21363;&#26367;&#20195;&#25439;&#22833;&#65292;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#20219;&#24847;&#40657;&#30418;&#22270;&#24418;&#27969;&#31243;&#36827;&#34892;&#24494;&#20998;&#12290;&#25105;&#20204;&#35757;&#32451;&#26367;&#20195;&#25439;&#22833;&#22312;&#30446;&#26631;&#20989;&#25968;&#30340;&#20027;&#21160;&#24179;&#28369;&#29256;&#26412;&#19978;&#65292;&#24182;&#40723;&#21169;&#23616;&#37096;&#24615;&#65292;&#20351;&#26367;&#20195;&#25439;&#22833;&#30340;&#23481;&#37327;&#38598;&#20013;&#22312;&#24403;&#21069;&#35757;&#32451;&#38454;&#27573;&#30340;&#20851;&#38190;&#20869;&#23481;&#19978;&#12290;&#25311;&#21512;&#26159;&#22312;&#32447;&#25191;&#34892;&#30340;&#65292;&#19982;&#21442;&#25968;&#20248;&#21270;&#21516;&#26102;&#36827;&#34892;&#65292;&#33258;&#30417;&#30563;&#36827;&#34892;&#65292;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#30001;&#20110;&#30446;&#26631;&#30340;&#37319;&#26679;&#26159;&#26114;&#36149;&#30340;&#65288;&#38656;&#35201;&#23436;&#25972;&#30340;&#28210;&#26579;&#25110;&#27169;&#25311;&#36816;&#34892;&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#21487;&#34892;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competit
&lt;/p&gt;</description></item><item><title>PINNsFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#20934;&#30830;&#36924;&#36817;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11833</link><description>&lt;p&gt;
PINNsFormer: &#22522;&#20110;Transformer&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks. (arXiv:2307.11833v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11833
&lt;/p&gt;
&lt;p&gt;
PINNsFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#20934;&#30830;&#36924;&#36817;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#35299;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PINNs&#21644;&#22823;&#22810;&#25968;&#30456;&#20851;&#30740;&#31350;&#37319;&#29992;&#20840;&#36830;&#25509;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20316;&#20026;&#26680;&#24515;&#32467;&#26500;&#65292;&#24573;&#30053;&#20102;PDEs&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#26080;&#27861;&#20934;&#30830;&#36924;&#36817;&#30495;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#21363;PINNsFormer&#65292;&#36890;&#36807;Transformer-based&#27169;&#22411;&#20013;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20934;&#30830;&#36924;&#36817;PDEs&#30340;&#35299;&#12290;PINNsFormer&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#21521;&#37327;&#20197;&#20266;&#24207;&#21015;&#30340;&#24418;&#24335;&#36827;&#34892;&#36817;&#20284;&#39044;&#27979;&#65292;&#36824;&#23558;&#36880;&#28857;&#30340;PINNs&#25439;&#22833;&#25913;&#20026;&#20102;&#39034;&#24207;&#30340;PINNs&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;PINNsFormer&#36824;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21363;&#23567;&#27874;&#20989;&#25968;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#20613;&#37324;&#21494;&#20998;&#35299;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PINNsFormer&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to captu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04661</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#12289;&#20854;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#38543;&#22270;&#36755;&#20837;&#22823;&#23567;&#22686;&#38271;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#28145;&#24230;&#20026;&#20108;&#30340;&#38750;&#21516;&#26500;&#26681;&#26641;&#65292;&#20351;&#24471;GNNs&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#30340;&#26681;&#33410;&#28857;&#12290;&#35777;&#26126;&#20381;&#36182;&#20110;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24050;&#32463;&#30693;&#36947;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#26080;&#30028;GNNs&#65288;&#20854;&#22823;&#23567;&#20801;&#35768;&#38543;&#22270;&#22823;&#23567;&#25913;&#21464;&#65289;&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#36825;&#20123;&#39030;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#26377;&#30028;&#22823;&#23567;&#21644;&#26080;&#30028;&#22823;&#23567;&#30340;GNNs&#20043;&#38388;&#23384;&#22312;&#20005;&#26684;&#30340;&#20998;&#31163;&#65292;&#22238;&#31572;&#20102; [Grohe, 2021] &#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04216</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25439;&#21387;&#32553;&#24050;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#20013;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#36825;&#31181;&#21387;&#32553;&#26041;&#27861;&#23545;&#20110;&#22823;&#23567;&#22312;&#20960;&#20010;PB&#33539;&#22260;&#20869;&#30340;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#21387;&#32553;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20294;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#22312;&#31185;&#23398;&#25968;&#25454;&#39046;&#22495;&#23578;&#26410;&#24191;&#20026;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#30340;&#31185;&#23398;&#22522;&#20934;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#24212;&#29992;&#20110;&#19968;&#31181;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#30340;&#27668;&#20505;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;140&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#39640;&#20998;&#36776;&#29575;&#31038;&#21306;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(CESM) Version 1.3&#30340;&#27169;&#25311;&#25968;&#25454;&#22312;&#21387;&#32553;&#27604;&#36798;&#21040;200&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the recon
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;MLP&#26377;&#28508;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;MLP-Mixer &#21487;&#20197;&#20316;&#20026;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#26377;&#25928;&#22320;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.01470</link><description>&lt;p&gt;
MLP-Mixer&#20316;&#20026;&#23485;&#19988;&#31232;&#30095;&#30340;MLP
&lt;/p&gt;
&lt;p&gt;
MLP-Mixer as a Wide and Sparse MLP. (arXiv:2306.01470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01470
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;MLP&#26377;&#28508;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;MLP-Mixer &#21487;&#20197;&#20316;&#20026;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#26377;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#38382;&#39064;&#30340;&#22522;&#30784;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22522;&#20110;MLP&#30340;&#26550;&#26500;(&#29305;&#21035;&#26159;MLP-Mixer)&#30340;&#23454;&#35777;&#25104;&#21151;&#34920;&#26126;&#65292;&#25552;&#39640;MLP&#30340;&#24615;&#33021;&#20173;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MLP-Mixer&#26377;&#25928;&#22320;&#20316;&#20026;&#20855;&#26377;&#26576;&#20123;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#28548;&#28165;Mixer&#30340;&#28151;&#21512;&#23618;&#21487;&#20197;&#20316;&#20026;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#19988;&#30001;Kronecker&#20056;&#31215;&#34920;&#31034;&#30340;&#26356;&#23485;MLP&#30340;&#26377;&#25928;&#34920;&#36798;&#12290;&#35813;&#34920;&#36798;&#24335;&#33258;&#28982;&#22320;&#23450;&#20041;&#20102;&#19968;&#32452;&#32622;&#25442;-Kronecker(PK)&#23478;&#26063;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#28151;&#21512;&#23618;&#30340;&#19968;&#33324;&#31867;&#65292;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;Monarch&#30697;&#38453;&#30340;&#19968;&#31181;&#36817;&#20284;&#12290;&#38543;&#21518;&#65292;&#30001;&#20110;PK&#23478;&#26063;&#26377;&#25928;&#26500;&#25104;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#65292;&#22240;&#27492;&#65292;&#21487;&#20197;&#24212;&#29992;Golubeva&#12289;Neyshabur&#21644;Gur-Ari(2021)&#25552;&#20986;&#30340;&#20551;&#35774;&#65292;&#21363;&#39044;&#27979;&#24615;&#33021;&#65306;
&lt;/p&gt;
&lt;p&gt;
Multi-layer perceptron (MLP) is a fundamental component of deep learning that has been extensively employed for various problems. However, recent empirical successes in MLP-based architectures, particularly the progress of the MLP-Mixer, have revealed that there is still hidden potential in improving MLPs to achieve better performance. In this study, we reveal that the MLP-Mixer works effectively as a wide MLP with certain sparse weights. Initially, we clarify that the mixing layer of the Mixer has an effective expression as a wider MLP whose weights are sparse and represented by the Kronecker product. This expression naturally defines a permuted-Kronecker (PK) family, which can be regarded as a general class of mixing layers and is also regarded as an approximation of Monarch matrices. Subsequently, because the PK family effectively constitutes a wide MLP with sparse weights, one can apply the hypothesis proposed by Golubeva, Neyshabur and Gur-Ari (2021) that the prediction performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22768;&#28304;&#20998;&#31163;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#23558;&#27969;&#34892;&#30340;&#27169;&#22411;&#21450;&#20854;&#38598;&#25104;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20182;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#20998;&#31163;&#26041;&#27861;&#65292;&#22522;&#20110;&#36866;&#21512;&#29305;&#23450;&#38899;&#36712;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#22312;2023&#24180;&#38899;&#20048;&#20998;&#31163;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#25104;&#32489;&#65292;&#24182;&#24320;&#28304;&#20102;&#20195;&#30721;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07489</link><description>&lt;p&gt;
&#22768;&#38899;&#25286;&#20998;&#20219;&#21153;&#30340;&#22522;&#20934;&#21644;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Benchmarks and leaderboards for sound demixing tasks. (arXiv:2305.07489v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22768;&#28304;&#20998;&#31163;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#23558;&#27969;&#34892;&#30340;&#27169;&#22411;&#21450;&#20854;&#38598;&#25104;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20182;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#20998;&#31163;&#26041;&#27861;&#65292;&#22522;&#20110;&#36866;&#21512;&#29305;&#23450;&#38899;&#36712;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#22312;2023&#24180;&#38899;&#20048;&#20998;&#31163;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#25104;&#32489;&#65292;&#24182;&#24320;&#28304;&#20102;&#20195;&#30721;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#25286;&#20998;&#26159;&#23558;&#32473;&#23450;&#30340;&#21333;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#25104;&#32452;&#25104;&#37096;&#20998;&#65288;&#20363;&#22914;&#40723;&#12289;&#20302;&#38899;&#21644;&#20154;&#22768;&#31561;&#65289;&#19982;&#20854;&#20182;&#20276;&#22863;&#38899;&#20048;&#20998;&#31163;&#30340;&#20219;&#21153;&#12290;&#28304;&#20998;&#31163;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#21313;&#20998;&#26377;&#29992;&#65292;&#21253;&#25324;&#23089;&#20048;&#21644;&#21161;&#21548;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22768;&#28304;&#20998;&#31163;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#27604;&#36739;&#20102;&#27969;&#34892;&#30340;&#22768;&#38899;&#25286;&#20998;&#27169;&#22411;&#21450;&#20854;&#38598;&#25104;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#25490;&#34892;&#27036; https://mvsep.com/quality_checker/&#65292;&#20197;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21487;&#20379;&#19979;&#36733;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#20998;&#31163;&#26041;&#27861;&#65292;&#22522;&#20110;&#36866;&#21512;&#29305;&#23450;&#38899;&#36712;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;2023&#38899;&#20048;&#20998;&#31163;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#25104;&#32489;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#26041;&#27861;&#22312;GitHub&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music demixing is the task of separating different tracks from the given single audio signal into components, such as drums, bass, and vocals from the rest of the accompaniment. Separation of sources is useful for a range of areas, including entertainment and hearing aids. In this paper, we introduce two new benchmarks for the sound source separation tasks and compare popular models for sound demixing, as well as their ensembles, on these benchmarks. For the models' assessments, we provide the leaderboard at https://mvsep.com/quality_checker/, giving a comparison for a range of models. The new benchmark datasets are available for download. We also develop a novel approach for audio separation, based on the ensembling of different models that are suited best for the particular stem. The proposed solution was evaluated in the context of the Music Demixing Challenge 2023 and achieved top results in different tracks of the challenge. The code and the approach are open-sourced on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#65292;&#21487;&#20197;&#21457;&#29616;&#26410;&#30693;&#26448;&#26009;&#24182;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#36136;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#21457;&#29616;&#26356;&#22909;&#30340;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2303.11833</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#22312;&#26497;&#31471;&#29305;&#24615;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Materials Discovery with Extreme Properties via AI-Driven Combinatorial Chemistry. (arXiv:2303.11833v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#65292;&#21487;&#20197;&#21457;&#29616;&#26410;&#30693;&#26448;&#26009;&#24182;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#36136;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#21457;&#29616;&#26356;&#22909;&#30340;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26448;&#26009;&#30340;&#21457;&#29616;&#37117;&#26088;&#22312;&#21457;&#29616;&#27604;&#30446;&#21069;&#24050;&#30693;&#26448;&#26009;&#26356;&#20248;&#36234;&#30340;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#36825;&#24456;&#25509;&#36817;&#20110;&#22806;&#25512;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#23398;&#20064;&#25968;&#25454;&#27010;&#29575;&#20998;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#36825;&#26159;&#19968;&#20010;&#24369;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21453;&#21521;&#20998;&#23376;&#35774;&#35745;&#22120;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21487;&#33021;&#29983;&#25104;&#20174;&#20998;&#23376;&#29255;&#27573;&#32452;&#21512;&#20013;&#33719;&#24471;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#21457;&#29616;&#20855;&#26377;&#26356;&#20248;&#36234;&#24615;&#36136;&#30340;&#26410;&#30693;&#26448;&#26009;&#12290;&#25105;&#20204;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#21457;&#29616;&#26356;&#22909;&#30340;&#26448;&#26009;&#12290;&#22312;&#19968;&#20010;&#26088;&#22312;&#21457;&#29616;&#19971;&#20010;&#30446;&#26631;&#29305;&#24615;&#20998;&#23376;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#19975;&#27425;&#35797;&#39564;&#20013;&#21457;&#29616;&#20102;1315&#20010;&#36798;&#21040;&#20840;&#37096;&#30446;&#26631;&#30340;&#20998;&#23376;&#21644;7629&#20010;&#36798;&#21040;&#20116;&#20010;&#30446;&#26631;&#30340;&#20998;&#23376;&#65292;&#32780;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#27169;&#22411;&#21482;&#26377;&#21457;&#29616;&#20960;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of most materials discovery is to discover materials that are superior to those currently known. Fundamentally, this is close to extrapolation, which is a weak point for most machine learning models that learn the probability distribution of data. Herein, we develop AI-driven combinatorial chemistry, which is a rule-based inverse molecular designer that does not rely on data. Since our model has the potential to generate all possible molecular structures that can be obtained from combinations of molecular fragments, unknown materials with superior properties can be discovered. We theoretically and empirically demonstrate that our model is more suitable for discovering better materials than probability distribution-learning models. In an experiment aimed at discovering molecules that hit seven target properties, our model discovered 1,315 of all target-hitting molecules and 7,629 of five target-hitting molecules out of 100,000 trials, whereas the probability distribution-learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#32447;&#24615;AC&#30005;&#21147;&#27969;&#26041;&#31243;&#30340;ground truth&#65292;&#20197;&#30830;&#23450;&#26368;&#22351;&#30340;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#20351;&#29992;&#39034;&#24207;&#28155;&#21152;&#26377;&#38024;&#23545;&#24615;&#30340;&#20999;&#21106;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#25910;&#32039;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#30452;&#21040;&#35299;&#20915;&#26041;&#26696;&#36275;&#22815;&#32039;&#23494;&#25110;&#36798;&#21040;&#19968;&#20010;&#23433;&#20840;&#24615;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2211.07125</link><description>&lt;p&gt;
AC&#30005;&#21147;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20840;&#23616;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Global Performance Guarantees for Neural Network Models of AC Power Flow. (arXiv:2211.07125v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#32447;&#24615;AC&#30005;&#21147;&#27969;&#26041;&#31243;&#30340;ground truth&#65292;&#20197;&#30830;&#23450;&#26368;&#22351;&#30340;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#20351;&#29992;&#39034;&#24207;&#28155;&#21152;&#26377;&#38024;&#23545;&#24615;&#30340;&#20999;&#21106;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#25910;&#32039;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#30452;&#21040;&#35299;&#20915;&#26041;&#26696;&#36275;&#22815;&#32039;&#23494;&#25110;&#36798;&#21040;&#19968;&#20010;&#23433;&#20840;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#26082;&#24555;&#21448;&#20934;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#12290;&#20294;&#20005;&#26684;&#39564;&#35777;&#40657;&#30418;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26159;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23545;&#20110;&#30005;&#21147;&#31995;&#32479;&#26469;&#35828;&#65292;&#23398;&#20064;AC&#30005;&#21147;&#27969;&#26159;&#20219;&#20309;&#24076;&#26395;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#40657;&#30418;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#26080;&#35770;&#26159;&#20026;&#20102;&#20248;&#21270;&#12289;&#25511;&#21046;&#36824;&#26159;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#19968;&#31181;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#32447;&#24615;AC&#30005;&#21147;&#27969;&#26041;&#31243;&#30340;ground truth&#65292;&#20197;&#30830;&#23450;&#26368;&#22351;&#30340;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;Sequential Targeted Tightening (STT)&#65292;&#23427;&#21033;&#29992;&#26494;&#24347;&#30340;&#20984;&#35268;&#21010;&#37325;&#26500;&#20102;&#21407;&#22987;&#30340;&#39564;&#35777;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#65288;MIQP&#65289;&#12290;&#36890;&#36807;&#39034;&#24207;&#28155;&#21152;&#26377;&#38024;&#23545;&#24615;&#30340;&#20999;&#21106;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#25910;&#32039;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#30452;&#21040;&#35299;&#20915;&#26041;&#26696;&#36275;&#22815;&#32039;&#23494;&#25110;&#36798;&#21040;&#19968;&#20010;&#23433;&#20840;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning can generate black-box surrogate models which are both extremely fast and highly accurate. Rigorously verifying the accuracy of these black-box models, however, is computationally challenging. When it comes to power systems, learning AC power flow is the cornerstone of any machine learning surrogate model wishing to drastically accelerate computations, whether it is for optimization, control, or dynamics. This paper develops for the first time, to our knowledge, a tractable neural network verification procedure which incorporates the ground truth of the non-linear AC power flow equations to determine worst-case neural network performance. Our approach, termed Sequential Targeted Tightening (STT), leverages a loosely convexified reformulation of the original verification problem, which is a mixed integer quadratic program (MIQP). Using the sequential addition of targeted cuts, we iteratively tighten our formulation until either the solution is sufficiently tight or a sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#29616;&#35937;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;11&#31181;&#33080;&#37096;&#21311;&#21517;&#21270;&#26041;&#27861;&#33267;&#23569;&#37096;&#20998;&#21487;&#36870;&#65292;&#24182;&#24378;&#35843;&#20102;&#37325;&#26500;&#21644;&#21453;&#28436;&#23454;&#29616;&#21487;&#36870;&#24615;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.10651</link><description>&lt;p&gt;
&#29702;&#35299;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#65306;Fant\^omas&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fant\^omas: Understanding Face Anonymization Reversibility. (arXiv:2210.10651v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#29616;&#35937;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;11&#31181;&#33080;&#37096;&#21311;&#21517;&#21270;&#26041;&#27861;&#33267;&#23569;&#37096;&#20998;&#21487;&#36870;&#65292;&#24182;&#24378;&#35843;&#20102;&#37325;&#26500;&#21644;&#21453;&#28436;&#23454;&#29616;&#21487;&#36870;&#24615;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33080;&#37096;&#22270;&#20687;&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#20449;&#24687;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35782;&#21035;&#20010;&#20154;&#24182;&#25512;&#26029;&#20182;&#20204;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#38544;&#31169;&#39118;&#38505;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#20351;&#29992;&#23545;&#28165;&#26224;&#22270;&#20687;&#36827;&#34892;&#36716;&#25442;&#20197;&#28151;&#28102;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#23450;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#23427;&#20204;&#20197;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22768;&#26126;&#21457;&#34920;&#65292;&#20294;&#26377;&#26102;&#24182;&#26410;&#32463;&#36807;&#20196;&#20154;&#20449;&#26381;&#30340;&#26041;&#27861;&#35780;&#20272;&#12290;&#23558;&#21311;&#21517;&#21270;&#22270;&#20687;&#36870;&#36716;&#22238;&#20223;&#30495;&#20854;&#30495;&#23454;&#36755;&#20837;&#30340;&#31243;&#24230;&#65292;&#29978;&#33267;&#33021;&#34987;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#35782;&#21035;&#65292;&#36825;&#23545;&#20110;&#21311;&#21517;&#21270;&#30340;&#32570;&#38519;&#26159;&#26368;&#24378;&#26377;&#21147;&#30340;&#25351;&#26631;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#32467;&#26524;&#30340;&#30830;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#26041;&#27861;&#32780;&#35328;&#36825;&#26159;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#21738;&#20123;&#26041;&#27861;&#26159;&#21487;&#36870;&#30340;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#21487;&#36870;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#29616;&#35937;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32463;&#36807;&#27979;&#35797;&#30340;15&#31181;&#26041;&#27861;&#20013;&#65292;&#26377;11&#31181;&#33267;&#23569;&#37096;&#20998;&#21487;&#36870;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#37325;&#26500;&#21644;&#21453;&#28436;&#26159;&#22914;&#20309;&#23454;&#29616;&#21487;&#36870;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face images are a rich source of information that can be used to identify individuals and infer private information about them. To mitigate this privacy risk, anonymizations employ transformations on clear images to obfuscate sensitive information, all while retaining some utility. Albeit published with impressive claims, they sometimes are not evaluated with convincing methodology.  Reversing anonymized images to resemble their real input -- and even be identified by face recognition approaches -- represents the strongest indicator for flawed anonymization. Some recent results indeed indicate that this is possible for some approaches. It is, however, not well understood, which approaches are reversible, and why. In this paper, we provide an exhaustive investigation in the phenomenon of face anonymization reversibility. Among other things, we find that 11 out of 15 tested face anonymizations are at least partially reversible and highlight how both reconstruction and inversion are the u
&lt;/p&gt;</description></item></channel></rss>