<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#24433;&#21160;&#22270;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#65292;&#21363;&#19968;&#23545;&#33402;&#26415;&#22270;&#20687;&#21644;&#19982;&#20043;&#23545;&#40784;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#33402;&#26415;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#35201;&#27714;&#24182;&#31616;&#21270;&#21160;&#20316;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#24182;&#39044;&#27979;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.03190</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#29983;&#25104;&#33402;&#26415;&#24615;&#30340;&#24433;&#21160;&#22270;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#24433;&#21160;&#22270;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#65292;&#21363;&#19968;&#23545;&#33402;&#26415;&#22270;&#20687;&#21644;&#19982;&#20043;&#23545;&#40784;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#33402;&#26415;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#35201;&#27714;&#24182;&#31616;&#21270;&#21160;&#20316;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#24182;&#39044;&#27979;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#30340;&#24433;&#21160;&#22270;&#12290;&#22312;&#22788;&#29702;&#34394;&#26500;&#20803;&#32032;&#21644;&#33402;&#26415;&#39118;&#26684;&#30340;&#25552;&#31034;&#26102;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#35299;&#37322;&#36825;&#20123;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#21160;&#20316;&#30340;&#22797;&#26434;&#24615;&#12290;&#29616;&#26377;&#30340;&#21333;&#22270;&#21160;&#30011;&#26041;&#27861;&#22312;&#33402;&#26415;&#24615;&#36755;&#20837;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#26041;&#27861;&#24120;&#24120;&#24341;&#20837;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#65292;&#38590;&#20197;&#20351;&#26576;&#20123;&#21306;&#22495;&#20445;&#25345;&#38745;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#30340;&#24605;&#24819;&#65292;&#21363;&#33402;&#26415;&#22270;&#20687;&#21644;&#20854;&#19982;&#20687;&#32032;&#23545;&#40784;&#30340;&#33258;&#28982;&#22806;&#35266;&#37197;&#23545;&#12290;&#34429;&#28982;&#33402;&#26415;&#22270;&#20687;&#25551;&#32472;&#20102;&#25105;&#20204;&#22312;&#25991;&#26412;&#25552;&#31034;&#20013;&#35814;&#32454;&#25551;&#36848;&#30340;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20294;&#30495;&#23454;&#30340;&#23545;&#24212;&#22270;&#20687;&#22823;&#22823;&#31616;&#21270;&#20102;&#24067;&#23616;&#21644;&#21160;&#20316;&#20998;&#26512;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#33258;&#28982;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#20986;&#30495;&#23454;&#22270;&#20687;&#24182;&#26681;&#25454;&#35821;&#20041;&#20449;&#24687;&#39044;&#27979;&#20986;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Artistic Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predi
&lt;/p&gt;</description></item><item><title>TGRL&#26159;&#19968;&#31181;&#29992;&#20110;&#25945;&#24072;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#33258;&#21160;&#24179;&#34913;&#20309;&#26102;&#36981;&#24490;&#25945;&#24072;&#25351;&#23548;&#21644;&#20309;&#26102;&#20351;&#29992;&#22870;&#21169;&#65292;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#20250;&#26681;&#25454;&#20195;&#29702;&#30340;&#34920;&#29616;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2307.03186</link><description>&lt;p&gt;
TGRL:&#19968;&#31181;&#29992;&#20110;&#25945;&#24072;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
TGRL: An Algorithm for Teacher Guided Reinforcement Learning. (arXiv:2307.03186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03186
&lt;/p&gt;
&lt;p&gt;
TGRL&#26159;&#19968;&#31181;&#29992;&#20110;&#25945;&#24072;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#33258;&#21160;&#24179;&#34913;&#20309;&#26102;&#36981;&#24490;&#25945;&#24072;&#25351;&#23548;&#21644;&#20309;&#26102;&#20351;&#29992;&#22870;&#21169;&#65292;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#20250;&#26681;&#25454;&#20195;&#29702;&#30340;&#34920;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22870;&#21169;(&#21363;&#24378;&#21270;&#23398;&#20064;&#25110;RL)&#21644;&#23398;&#20064;&#27169;&#20223;&#25945;&#24072;(&#21363;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;)&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#20004;&#31181;&#25104;&#29087;&#26041;&#27861;&#12290;&#20026;&#20102;&#32467;&#21512;&#36825;&#20123;&#19981;&#21516;&#24418;&#24335;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30446;&#26631;&#30340;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#36825;&#20123;&#30446;&#26631;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#38382;&#39064;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#26469;&#24179;&#34913;&#20004;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26377;&#21407;&#21017;"&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#23454;&#29616;"&#21160;&#24577;"&#21644;"&#33258;&#21160;"&#24179;&#34913;&#20309;&#26102;&#36981;&#24490;&#25945;&#24072;&#21644;&#20309;&#26102;&#20351;&#29992;&#22870;&#21169;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#27604;&#36739;&#20195;&#29702;&#30340;&#24615;&#33021;&#19982;&#27809;&#26377;&#25945;&#24072;&#30417;&#30563;&#24182;&#21482;&#20174;&#22870;&#21169;&#20013;&#23398;&#20064;&#30340;&#23545;&#29031;&#24773;&#26223;&#26469;&#35843;&#25972;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#12290;&#22914;&#26524;&#20351;&#29992;&#25945;&#24072;&#30417;&#30563;&#25913;&#21892;&#20102;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#37027;&#20040;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#23601;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\textit{principled}$ approach, along with an approximate implementation for $\textit{dynamically}$ and $\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03176</link><description>&lt;p&gt;
&#24322;&#26500;&#29305;&#24449;&#23376;&#37319;&#26679;&#30340;Ridge Ensemble&#30340;&#23398;&#20064;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21253;&#35013;&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22312;&#38543;&#26426;&#23376;&#26679;&#26412;&#25110;&#29305;&#24449;&#25237;&#24433;&#19978;&#35757;&#32451;&#20272;&#35745;&#22120;&#26469;&#20943;&#23569;&#39044;&#27979;&#26041;&#24046;&#30340;&#25104;&#29087;&#38598;&#25104;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#38598;&#25104;&#36873;&#25321;&#26159;&#21516;&#36136;&#30340;&#65292;&#21363;&#20272;&#35745;&#22120;&#21487;&#29992;&#30340;&#29305;&#24449;&#32500;&#25968;&#22312;&#25972;&#20010;&#38598;&#25104;&#20013;&#26159;&#22343;&#21248;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20272;&#35745;&#22120;&#22522;&#20110;&#21464;&#21160;&#30340;&#29305;&#24449;&#32500;&#25968;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#38598;&#25104;&#65292;&#27599;&#20010;&#39044;&#27979;&#22120;&#20351;&#29992;&#37096;&#20998;&#21487;&#29992;&#29305;&#24449;&#36827;&#34892;&#23725;&#22238;&#24402;&#25311;&#21512;&#12290;&#25105;&#20204;&#20801;&#35768;&#36825;&#20123;&#23376;&#38598;&#20013;&#21253;&#21547;&#30340;&#29305;&#24449;&#25968;&#37327;&#26377;&#25152;&#21464;&#21270;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22797;&#21046;&#25216;&#24039;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#30830;&#23450;&#24615;&#32447;&#24615;&#25513;&#27169;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#23545;&#20110;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#29305;&#24449;&#22122;&#22768;&#30340;&#31561;&#30456;&#30456;&#20851;&#25968;&#25454;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23398;&#20064;&#26354;&#32447;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#21033;&#29992;&#36825;&#20123;&#25512;&#23548;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38598;&#25104;&#22312;&#19981;&#21516;&#29305;&#24449;&#32500;&#25968;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03175</link><description>&lt;p&gt;
&#25512;&#24320;&#32511;&#33394;&#65306;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#26893;&#29289;&#21494;&#29255;&#32972;&#21518;&#30340;&#20869;&#23481;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20892;&#19994;&#24212;&#29992;&#65288;&#20363;&#22914;&#26816;&#26597;&#12289;&#34920;&#22411;&#20998;&#26512;&#12289;&#37319;&#25688;&#27700;&#26524;&#65289;&#38656;&#35201;&#25805;&#20316;&#26893;&#29289;&#21494;&#29255;&#20197;&#26597;&#30475;&#21494;&#23376;&#21644;&#26525;&#24178;&#30340;&#32972;&#21518;&#12290;&#37096;&#20998;&#21487;&#35265;&#24615;&#12289;&#26497;&#31471;&#26434;&#20081;&#12289;&#34180;&#32467;&#26500;&#20197;&#21450;&#26893;&#29289;&#30340;&#26410;&#30693;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#37117;&#20351;&#24471;&#36825;&#31181;&#25805;&#20316;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35757;&#32451;SRPNet&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#39044;&#27979;&#22312;&#32473;&#23450;&#26893;&#29289;&#19978;&#25191;&#34892;&#20505;&#36873;&#21160;&#20316;&#26102;&#20250;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#20132;&#21449;&#29109;&#26041;&#27861;&#30340;SRPNet&#26469;&#39044;&#27979;&#26377;&#25928;&#22320;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#30340;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;SRPNet&#19981;&#20165;&#39044;&#27979;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#65292;&#36824;&#39044;&#27979;&#26174;&#38706;&#20986;&#31354;&#38388;&#30340;&#20301;&#32622;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#30340;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#12290;&#22312;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#30340;&#34276;&#34067;&#21644;&#30495;&#23454;&#26893;&#29289;&#65288;&#40857;&#34880;&#26641;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;5&#20010;&#35774;&#32622;&#65292;&#21253;&#25324;2&#20010;&#27979;&#35797;&#27867;&#21270;&#24615;&#33021;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to
&lt;/p&gt;</description></item><item><title>Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03170</link><description>&lt;p&gt;
Focused Transformer: &#21453;&#24046;&#35757;&#32451;&#23545;&#19978;&#19979;&#25991;&#32553;&#25918;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03170
&lt;/p&gt;
&lt;p&gt;
Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#21270;&#30340;&#26041;&#24335;&#21560;&#32435;&#26032;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#27880;&#24847;&#21147;&#23618;&#25552;&#20379;&#35775;&#38382;&#22806;&#37096;&#23384;&#20648;&#22120;&#30340;&#33021;&#21147;&#65292;&#35813;&#23384;&#20648;&#22120;&#30001;&#65288;&#38190;&#65292;&#20540;&#65289;&#23545;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#38190;&#19982;&#26080;&#20851;&#38190;&#30340;&#27604;&#20363;&#20943;&#23569;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#26080;&#20851;&#38190;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#24515;&#38382;&#39064;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#19982;&#19981;&#21516;&#35821;&#20041;&#20540;&#30456;&#20851;&#32852;&#30340;&#38190;&#21487;&#33021;&#37325;&#21472;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Focused Transformer&#65288;FoT&#65289;&#65292;&#19968;&#31181;&#21463;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#65288;&#38190;&#65292;&#20540;&#65289;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20351;&#19978;&#19979;&#25991;&#38271;&#24230;&#24471;&#20197;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#36866;&#24212;&#22312;&#20943;&#23569;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23545;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#22312;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03157</link><description>&lt;p&gt;
&#22495;&#36866;&#24212;&#33021;&#25552;&#39640;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?. (arXiv:2307.03157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#36866;&#24212;&#22312;&#20943;&#23569;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23545;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#22312;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#35786;&#26029;&#31995;&#32479;&#22312;&#20998;&#31867;&#30382;&#32932;&#30284;&#30151;&#30149;&#21464;&#26102;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#20250;&#24433;&#21709;&#20934;&#30830;&#21487;&#38752;&#30340;&#35786;&#26029;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#20010;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#22495;&#36866;&#24212;&#35757;&#32451;&#26041;&#26696;&#65306;&#21333;&#28304;&#12289;&#32508;&#21512;&#21644;&#22810;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#36866;&#24212;&#22312;&#20108;&#20998;&#31867;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#22312;&#20943;&#23569;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#36824;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#22312;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20854;&#24615;&#33021;&#19981;&#22826;&#26126;&#26174;&#65292;&#38656;&#35201;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#25165;&#33021;&#36798;&#21040;&#22522;&#20934;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#27979;&#35797;&#38169;&#35823;&#19982;&#26631;&#31614;&#20559;&#31227;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based diagnostic system has demonstrated potential in classifying skin cancer conditions when labeled training example are abundant. However, skin lesion analysis often suffers from a scarcity of labeled data, hindering the development of an accurate and reliable diagnostic system. In this work, we leverage multiple skin lesion datasets and investigate the feasibility of various unsupervised domain adaptation (UDA) methods in binary and multi-class skin lesion classification. In particular, we assess three UDA training schemes: single-, combined-, and multi-source. Our experiment results show that UDA is effective in binary classification, with further improvement being observed when imbalance is mitigated. In multi-class task, its performance is less prominent, and imbalance problem again needs to be addressed to achieve above-baseline accuracy. Through our quantitative analysis, we find that the test error of multi-class tasks is strongly correlated with label shift, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.03137</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#32593;&#32476;&#26102;&#65292;&#32593;&#32476;&#24182;&#27809;&#26377;&#26126;&#30830;&#34987;&#35201;&#27714;&#23398;&#20064;&#22270;&#20687;&#30340;&#20840;&#23616;&#19981;&#21464;&#24615;&#65292;&#22914;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#19981;&#21464;&#24615;&#32435;&#20837;&#32593;&#32476;&#35757;&#32451;&#20013;&#21487;&#33021;&#26377;&#21161;&#20110;&#25913;&#21892;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#26159;&#38656;&#35201;&#20998;&#21106;&#30340;&#23545;&#35937;&#30340;&#22266;&#26377;&#29305;&#24615;&#26102;&#12290;&#26412;&#25991;&#20197;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#30340;&#20998;&#21106;&#20026;&#20363;&#65292;&#36825;&#20123;&#34880;&#31649;&#30001;&#20110;&#20154;&#20307;&#35299;&#21078;&#23398;&#65292;&#36890;&#24120;&#22312;&#36523;&#20307;&#20013;&#20197;&#29305;&#23450;&#30340;&#20960;&#20309;&#24418;&#29366;&#20986;&#29616;&#65292;&#24182;&#22312;2D CT&#22270;&#20687;&#19978;&#20027;&#35201;&#21576;&#29616;&#20026;&#22278;&#24418;&#23545;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#24809;&#32602;&#22320;&#38754;&#30495;&#23454;&#20540;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#20998;&#21106;&#32593;&#32476;&#35774;&#35745;&#19981;&#21516;&#65292;&#20808;&#21069;&#30340;&#35774;&#35745;&#26159;&#23558;&#38408;&#20540;&#28388;&#27874;&#24212;&#29992;&#20110;&#39044;&#27979;&#22270;&#20687;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#23545;&#31216;&#38181;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;(SCMWU)&#65292;&#35813;&#31639;&#27861;&#22312;&#20219;&#24847;&#23545;&#31216;&#38181;&#30340;&#36857;&#20026;&#19968;&#22788;&#36827;&#34892;&#22312;&#32447;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26159;&#26080;&#24724;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03136</link><description>&lt;p&gt;
&#22312;&#23545;&#31216;&#38181;&#19978;&#36827;&#34892;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#20056;&#27861;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Multiplicative Updates for Online Convex Optimization over Symmetric Cones. (arXiv:2307.03136v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#23545;&#31216;&#38181;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;(SCMWU)&#65292;&#35813;&#31639;&#27861;&#22312;&#20219;&#24847;&#23545;&#31216;&#38181;&#30340;&#36857;&#20026;&#19968;&#22788;&#36827;&#34892;&#22312;&#32447;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26159;&#26080;&#24724;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21487;&#33021;&#30340;&#25805;&#20316;&#26159;&#23545;&#31216;&#38181;&#20013;&#30340;&#36857;&#20026;&#19968;&#30340;&#20803;&#32032;&#65292;&#36825;&#25193;&#23637;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#19987;&#23478;&#35774;&#32622;&#21450;&#20854;&#37327;&#23376;&#23545;&#24212;&#29289;&#12290;&#23545;&#31216;&#38181;&#20026;&#19968;&#20123;&#26368;&#37325;&#35201;&#30340;&#20248;&#21270;&#27169;&#22411;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#20108;&#38454;&#38181;&#21644;&#21322;&#23450;&#20248;&#21270;&#12290;&#20351;&#29992;&#27431;&#20960;&#37324;&#24503;&#32422;&#26086;&#20195;&#25968;&#39046;&#22495;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#38181;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;(SCMWU)&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20219;&#24847;&#23545;&#31216;&#38181;&#30340;&#36857;&#20026;&#19968;&#22788;&#36827;&#34892;&#22312;&#32447;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SCMWU&#31561;&#20215;&#20110;Follow-the-Regularized-Leader&#21644;Online Mirror Descent&#65292;&#20854;&#27491;&#21017;&#21270;&#22120;&#20026;&#23545;&#31216;&#38181;&#36127;&#29109;&#12290;&#36890;&#36807;&#36825;&#20010;&#32467;&#26500;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SCMWU&#26159;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online convex optimization where the possible actions are trace-one elements in a symmetric cone, generalizing the extensively-studied experts setup and its quantum counterpart. Symmetric cones provide a unifying framework for some of the most important optimization models, including linear, second-order cone, and semidefinite optimization. Using tools from the field of Euclidean Jordan Algebras, we introduce the Symmetric-Cone Multiplicative Weights Update (SCMWU), a projection-free algorithm for online optimization over the trace-one slice of an arbitrary symmetric cone. We show that SCMWU is equivalent to Follow-the-Regularized-Leader and Online Mirror Descent with symmetric-cone negative entropy as regularizer. Using this structural result we show that SCMWU is a no-regret algorithm, and verify our theoretical results with extensive experiments. Our results unify and generalize the analysis for the Multiplicative Weights Update method over the probability simplex and the M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#25239;&#20998;&#24067;&#20559;&#31227;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#20316;&#32773;&#20351;&#29992;13&#20010;&#33879;&#21517;&#30340;TTA&#26041;&#27861;&#21450;&#20854;&#21464;&#20307;&#22312;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#36866;&#24212;&#24615;&#22330;&#26223;&#20013;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03133</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#25239;&#20998;&#24067;&#20559;&#31227;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification. (arXiv:2307.03133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#25239;&#20998;&#24067;&#20559;&#31227;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#20316;&#32773;&#20351;&#29992;13&#20010;&#33879;&#21517;&#30340;TTA&#26041;&#27861;&#21450;&#20854;&#21464;&#20307;&#22312;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#36866;&#24212;&#24615;&#22330;&#26223;&#20013;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#20165;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#22312;&#38754;&#20020;&#20998;&#24067;&#20559;&#31227;&#26102;&#38656;&#35201;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;TTA&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#65292;&#22914;&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#12289;&#20027;&#24178;&#32593;&#32476;&#21644;&#35774;&#35745;&#22330;&#26223;&#65292;&#23548;&#33268;&#32570;&#20047;&#19968;&#33268;&#21644;&#20844;&#24179;&#30340;&#22522;&#20934;&#26469;&#39564;&#35777;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#23545;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65288;CIFAR-10-C&#12289;CIFAR-100-C&#12289;ImageNet-C&#12289;DomainNet&#21644;Office-Home&#65289;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;13&#20010;&#33879;&#21517;&#30340;TTA&#26041;&#27861;&#21450;&#20854;&#21464;&#20307;&#12290;&#36825;&#20123;&#26041;&#27861;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#36866;&#24212;&#24615;&#22330;&#26223;&#65288;&#20363;&#22914;&#22312;&#32447;&#36866;&#24212;&#19982;&#31163;&#32447;&#36866;&#24212;&#12289;&#23454;&#20363;&#36866;&#24212;&#19982;&#25209;&#37327;&#36866;&#24212;&#19982;&#39046;&#22495;&#36866;&#24212;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;TTA&#26041;&#27861;&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#38754;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) is a technique aimed at enhancing the generalization performance of models by leveraging unlabeled samples solely during prediction. Given the need for robustness in neural network systems when faced with distribution shifts, numerous TTA methods have recently been proposed. However, evaluating these methods is often done under different settings, such as varying distribution shifts, backbones, and designing scenarios, leading to a lack of consistent and fair benchmarks to validate their effectiveness. To address this issue, we present a benchmark that systematically evaluates 13 prominent TTA methods and their variants on five widely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C, DomainNet, and Office-Home. These methods encompass a wide range of adaptation scenarios (e.g. online adaptation v.s. offline adaptation, instance adaptation v.s. batch adaptation v.s. domain adaptation). Furthermore, we explore the compatibility of differe
&lt;/p&gt;</description></item><item><title>T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03132</link><description>&lt;p&gt;
T-MARS&#65306;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#26469;&#25913;&#21892;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03132
&lt;/p&gt;
&lt;p&gt;
T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#32593;&#32476;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20026;&#23398;&#20064;&#36890;&#29992;&#35270;&#35273;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#65292;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#24443;&#24213;&#25913;&#21464;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35782;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;&#22914;&#20309;&#31579;&#36873;&#36825;&#20123;&#26085;&#30410;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#20854;&#21160;&#26426;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#36817;40%&#30340;LAION&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#19982;&#35828;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#25991;&#26412;&#12290;&#30452;&#35273;&#19978;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#28010;&#36153;&#36164;&#28304;&#65292;&#22240;&#20026;&#23427;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#32780;&#19981;&#26159;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#21435;&#38500;&#20063;&#21487;&#33021;&#28010;&#36153;&#65292;&#22240;&#20026;&#36825;&#20250;&#20002;&#24323;&#21253;&#21547;&#35270;&#35273;&#29305;&#24449;&#30340;&#22270;&#20687;&#65288;&#38500;&#20102;&#37325;&#21472;&#30340;&#25991;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#27425;&#24494;&#20998;&#20960;&#20309;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#21644;&#26354;&#38754;&#37325;&#24314;&#12290;&#36890;&#36807;&#23558;&#23616;&#37096;&#32447;&#24615;&#36924;&#36817;&#30340;&#28857;&#20113;&#38598;&#21512;&#25104;&#20302;&#32500;&#23376;&#26463;&#65292;&#26681;&#25454;&#27425;&#24494;&#20998;&#24230;&#37327;&#19979;&#30340;&#27425;&#24494;&#20998;&#27979;&#22320;&#32447;&#35299;&#20915;&#20102;&#22810;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03128</link><description>&lt;p&gt;
&#20027;&#35201;&#23376;&#26463;&#29992;&#20110;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Principal subbundles for dimension reduction. (arXiv:2307.03128v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#27425;&#24494;&#20998;&#20960;&#20309;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#21644;&#26354;&#38754;&#37325;&#24314;&#12290;&#36890;&#36807;&#23558;&#23616;&#37096;&#32447;&#24615;&#36924;&#36817;&#30340;&#28857;&#20113;&#38598;&#21512;&#25104;&#20302;&#32500;&#23376;&#26463;&#65292;&#26681;&#25454;&#27425;&#24494;&#20998;&#24230;&#37327;&#19979;&#30340;&#27425;&#24494;&#20998;&#27979;&#22320;&#32447;&#35299;&#20915;&#20102;&#22810;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#27425;&#24494;&#20998;&#20960;&#20309;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#21644;&#26354;&#38754;&#37325;&#24314;&#65292;&#36890;&#36807;&#23558;&#23616;&#37096;&#32447;&#24615;&#36924;&#36817;&#30340;&#28857;&#20113;&#38598;&#21512;&#25104;&#20302;&#32500;&#23376;&#26463;&#12290;&#36890;&#36807;&#23558;&#23616;&#37096;&#20027;&#25104;&#20998;&#20998;&#26512;&#24471;&#21040;&#30340;&#23616;&#37096;&#36924;&#36817;&#25910;&#38598;&#21040;&#19968;&#20010;&#31209;&#20026;$k$&#30340;&#20999;&#35273;&#23376;&#26463;&#19978;&#65292;&#20854;&#20013;$k&lt;d$&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20027;&#35201;&#23376;&#26463;&#12290;&#36825;&#30830;&#23450;&#20102;$\mathbb{R}^d$&#19978;&#30340;&#27425;&#24494;&#20998;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#24230;&#37327;&#19979;&#30340;&#27425;&#24494;&#20998;&#27979;&#22320;&#32447;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#65292;&#20363;&#22914;&#65306;&#26126;&#30830;&#26500;&#36896;&#19968;&#20010;&#36817;&#20284;&#23376;&#27969;&#24418;$M$&#65292;&#22312;$\mathbb{R}^k$&#20013;&#26500;&#36896;&#28857;&#20113;&#30340;&#34920;&#31034;&#65292;&#24182;&#35745;&#31639;&#32771;&#34385;&#23398;&#20064;&#21040;&#30340;&#20960;&#20309;&#30340;&#35266;&#27979;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22312;&#20999;&#31354;&#38388;&#34987;&#20934;&#30830;&#20272;&#35745;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;&#37325;&#24314;&#20445;&#35777;&#19982;&#30495;&#23454;&#23376;&#27969;&#24418;&#30456;&#31561;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#26102;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we demonstrate how sub-Riemannian geometry can be used for manifold learning and surface reconstruction by combining local linear approximations of a point cloud to obtain lower dimensional bundles. Local approximations obtained by local PCAs are collected into a rank $k$ tangent subbundle on $\mathbb{R}^d$, $k&lt;d$, which we call a principal subbundle. This determines a sub-Riemannian metric on $\mathbb{R}^d$. We show that sub-Riemannian geodesics with respect to this metric can successfully be applied to a number of important problems, such as: explicit construction of an approximating submanifold $M$, construction of a representation of the point-cloud in $\mathbb{R}^k$, and computation of distances between observations, taking the learned geometry into account. The reconstruction is guaranteed to equal the true submanifold in the limit case where tangent spaces are estimated exactly. Via simulations, we show that the framework is robust when applied to noisy data. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;WiFi Direct&#32452;&#31649;&#29702;&#21327;&#35758;&#65288;WFD-GM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#23454;&#26102;&#26426;&#20250;&#32593;&#32476;&#20013;&#33258;&#20027;&#36830;&#25509;&#21644;&#32452;&#38388;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.03126</link><description>&lt;p&gt;
&#23454;&#26102;&#26426;&#20250;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;WiFi Direct&#32452;&#37197;&#32622;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Configuration and Management of WiFi Direct Groups for Real Opportunistic Networks. (arXiv:2307.03126v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;WiFi Direct&#32452;&#31649;&#29702;&#21327;&#35758;&#65288;WFD-GM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#23454;&#26102;&#26426;&#20250;&#32593;&#32476;&#20013;&#33258;&#20027;&#36830;&#25509;&#21644;&#32452;&#38388;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi Direct&#26159;&#19968;&#31181;&#22312;&#21830;&#29992;&#31227;&#21160;&#35774;&#22791;&#19978;&#25903;&#25345;&#35774;&#22791;&#23545;&#35774;&#22791;&#36890;&#20449;&#65288;D2D&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26631;&#20934;&#19981;&#36275;&#20197;&#23436;&#20840;&#25903;&#25345;&#22522;&#20110;D2D&#30340;&#26426;&#20250;&#32593;&#32476;&#31561;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#23454;&#38469;&#19978;&#65292;WiFi Direct&#20855;&#26377;&#19968;&#20123;&#29305;&#24449;&#65292;&#21487;&#33021;&#38480;&#21046;&#29992;&#25143;&#20010;&#20154;&#35774;&#22791;&#20043;&#38388;&#33258;&#20027;&#21019;&#24314;D2D&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26631;&#20934;&#26126;&#30830;&#35201;&#27714;&#29992;&#25143;&#25480;&#26435;&#25165;&#33021;&#24314;&#31435;&#20004;&#20010;&#25110;&#22810;&#20010;&#35774;&#22791;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#19988;&#23545;&#32452;&#38388;&#36890;&#20449;&#25552;&#20379;&#26377;&#38480;&#25903;&#25345;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23396;&#31435;&#30340;&#33410;&#28857;&#32452;&#30340;&#21019;&#24314;&#65292;&#33410;&#28857;&#32452;&#20043;&#38388;&#26080;&#27861;&#30456;&#20114;&#36890;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#20214;&#23618;&#21327;&#35758;&#65292;&#29992;&#20110;&#39640;&#25928;&#37197;&#32622;&#21644;&#31649;&#29702;WiFi Direct&#32452;&#65288;WiFi Direct Group Manager&#65292;WFD-GM&#65289;&#65292;&#20197;&#23454;&#29616;&#33258;&#20027;&#36830;&#25509;&#21644;&#32452;&#38388;&#36890;&#20449;&#12290;&#36825;&#20351;&#24471;&#26426;&#20250;&#32593;&#32476;&#24471;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wi-Fi Direct is a promising technology for the support of device-to-device communications (D2D) on commercial mobile devices. However, the standard as-it-is is not sufficient to support the real deployment of networking solutions entirely based on D2D such as opportunistic networks. In fact, WiFi Direct presents some characteristics that could limit the autonomous creation of D2D connections among users' personal devices. Specifically, the standard explicitly requires the user's authorization to establish a connection between two or more devices, and it provides a limited support for inter-group communication. In some cases, this might lead to the creation of isolated groups of nodes which cannot communicate among each other. In this paper, we propose a novel middleware-layer protocol for the efficient configuration and management of WiFi Direct groups (WiFi Direct Group Manager, WFD-GM) to enable autonomous connections and inter-group communication. This enables opportunistic networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#19979;&#30340;&#22810;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#19982;&#21327;&#20316;&#65292;&#26368;&#22823;&#21270;&#25972;&#20307;&#21033;&#28070;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#35746;&#21333;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#21644;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.03119</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#24847;&#22270;&#24863;&#30693;&#36890;&#20449;&#20197;&#23454;&#29616;&#37329;&#34701;&#20013;&#30340;&#26368;&#20248;&#22810;&#35746;&#21333;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance. (arXiv:2307.03119v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#19979;&#30340;&#22810;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#19982;&#21327;&#20316;&#65292;&#26368;&#22823;&#21270;&#25972;&#20307;&#21033;&#28070;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#35746;&#21333;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#21644;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35746;&#21333;&#25191;&#34892;&#26159;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#23436;&#25104;&#29305;&#23450;&#36164;&#20135;&#30340;&#19968;&#31995;&#21015;&#20132;&#26131;&#35746;&#21333;&#30340;&#25910;&#36141;&#25110;&#28165;&#31639;&#12290;&#26368;&#36817;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36827;&#23637;&#20026;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24635;&#26159;&#38024;&#23545;&#21333;&#20010;&#35746;&#21333;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#35270;&#20102;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#35746;&#21333;&#30340;&#23454;&#36341;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#21644;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#26469;&#25191;&#34892;&#22810;&#35746;&#21333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#35270;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#21592;&#26469;&#20132;&#26131;&#19968;&#20010;&#29305;&#23450;&#30340;&#35746;&#21333;&#65292;&#21516;&#26102;&#20445;&#25345;&#24444;&#27492;&#36890;&#20449;&#24182;&#21327;&#20316;&#20197;&#26368;&#22823;&#21270;&#24635;&#20307;&#21033;&#28070;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#20165;&#20132;&#25442;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#26469;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#65292;&#36825;&#22312;&#22797;&#26434;&#30340;&#37329;&#34701;&#29615;&#22659;&#20013;&#26159;&#20302;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#23494;&#30721;&#23398;&#21407;&#29702;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#20813;&#21463;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.03118</link><description>&lt;p&gt;
&#37327;&#23376;&#35299;&#20915;&#38544;&#31169;&#19982;&#25928;&#29992;&#26435;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Quantum Solutions to the Privacy vs. Utility Tradeoff. (arXiv:2307.03118v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#23494;&#30721;&#23398;&#21407;&#29702;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#20813;&#21463;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#23494;&#30721;&#23398;&#21407;&#29702;&#30340;&#26032;&#22411;&#26550;&#26500;&#65288;&#20197;&#21450;&#20854;&#20960;&#20010;&#21464;&#20307;&#65289;&#65292;&#33021;&#22815;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#20219;&#20309;&#29616;&#26377;&#30340;&#32463;&#20856;&#25110;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#20043;&#19978;&#20351;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#22522;&#20110;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20351;&#29992;&#19982;&#21333;&#20301;&#31639;&#23376;&#30456;&#20851;&#30340;&#37327;&#23376;&#38376;&#25552;&#20379;&#20102;&#19982;&#25152;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#23545;&#25163;&#30340;&#26377;&#20445;&#35777;&#30340;&#23433;&#20840;&#24615;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel architecture (and several variants thereof) based on quantum cryptographic primitives with provable privacy and security guarantees regarding membership inference attacks on generative models. Our architecture can be used on top of any existing classical or quantum generative models. We argue that the use of quantum gates associated with unitary operators provides inherent advantages compared to standard Differential Privacy based techniques for establishing guaranteed security from all polynomial-time adversaries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#26893;&#20837;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#65292;&#26469;&#26816;&#27979;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28155;&#21152;&#20102;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#27880;&#20837;&#20869;&#23481;&#30340;&#35760;&#24518;&#26469;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03108</link><description>&lt;p&gt;
&#22914;&#20309;&#26816;&#27979;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#26893;&#20837;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#65292;&#26469;&#26816;&#27979;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28155;&#21152;&#20102;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#27880;&#20837;&#20869;&#23481;&#30340;&#35760;&#24518;&#26469;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#24403;&#27169;&#22411;&#35757;&#32451;&#32773;&#25910;&#38598;&#20102;&#19968;&#20010;&#29305;&#23450;&#33402;&#26415;&#23478;&#21019;&#24314;&#30340;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#24182;&#35797;&#22270;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#33719;&#24471;&#33402;&#26415;&#23478;&#30340;&#35768;&#21487;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#20837;&#30340;&#35760;&#24518;&#21270;&#20869;&#23481;&#26893;&#20837;&#20445;&#25252;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#26469;&#26816;&#27979;&#27492;&#31867;&#26410;&#25480;&#26435;&#25968;&#25454;&#20351;&#29992;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#19978;&#28155;&#21152;&#29420;&#29305;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;&#23545;&#20154;&#31867;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#20294;&#33021;&#22815;&#34987;&#25193;&#25955;&#27169;&#22411;&#25429;&#25417;&#21644;&#35760;&#24518;&#30340;&#38544;&#31192;&#22270;&#20687;&#21253;&#35013;&#20989;&#25968;&#65292;&#26469;&#20462;&#25913;&#21463;&#20445;&#25252;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#26159;&#21542;&#23545;&#27880;&#20837;&#30340;&#20869;&#23481;&#36827;&#34892;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#21028;&#26029;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#36825;&#19968;&#35760;&#24518;&#65288;&#21363;&#26159;&#21542;&#23384;&#22312;&#29983;&#25104;&#31867;&#20284;&#22270;&#20687;&#30340;&#33021;&#21147;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#39640;&#26031;&#36807;&#31243;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#19988;&#26126;&#30830;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#26680;&#20989;&#25968;&#35774;&#35745;&#21644;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#36873;&#39033;&#30340;&#25351;&#23548;&#65292;&#35813;&#26694;&#26550;&#22312;&#20912;&#24029;&#39640;&#31243;&#21464;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03093</link><description>&lt;p&gt;
&#36229;&#36234;&#30452;&#35273;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beyond Intuition, a Framework for Applying GPs to Real-World Data. (arXiv:2307.03093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#39640;&#26031;&#36807;&#31243;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#19988;&#26126;&#30830;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#26680;&#20989;&#25968;&#35774;&#35745;&#21644;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#36873;&#39033;&#30340;&#25351;&#23548;&#65292;&#35813;&#26694;&#26550;&#22312;&#20912;&#24029;&#39640;&#31243;&#21464;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#23567;&#22411;&#12289;&#32467;&#26500;&#21270;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#22238;&#24402;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#21463;&#21040;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#23545;&#20110;&#22914;&#20309;&#23558;GPs&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#25351;&#23548;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;GPs&#22312;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#19988;&#26126;&#30830;&#30340;GP&#27169;&#22411;&#12290;&#25351;&#23548;&#26041;&#38024;&#24418;&#24335;&#21270;&#20102;&#32463;&#39564;&#20016;&#23500;&#30340;GP&#23454;&#36341;&#32773;&#30340;&#20915;&#31574;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#26680;&#20989;&#25968;&#35774;&#35745;&#21644;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#36873;&#39033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20912;&#24029;&#39640;&#31243;&#21464;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#22312;&#27979;&#35797;&#26102;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes (GPs) offer an attractive method for regression over small, structured and correlated datasets. However, their deployment is hindered by computational costs and limited guidelines on how to apply GPs beyond simple low-dimensional datasets. We propose a framework to identify the suitability of GPs to a given problem and how to set up a robust and well-specified GP model. The guidelines formalise the decisions of experienced GP practitioners, with an emphasis on kernel design and options for computational scalability. The framework is then applied to a case study of glacier elevation change yielding more accurate results at test time.
&lt;/p&gt;</description></item><item><title>OpenDelta&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;delta&#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#12290;&#23427;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03084</link><description>&lt;p&gt;
OpenDelta: &#19968;&#31181;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#24211;
&lt;/p&gt;
&lt;p&gt;
OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. (arXiv:2307.03084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03084
&lt;/p&gt;
&lt;p&gt;
OpenDelta&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;delta&#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#12290;&#23427;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411; (PTMs) &#30340;&#35268;&#27169;&#32473;&#35843;&#25972;&#19979;&#28216;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20840;&#21442;&#25968;&#24494;&#35843;&#28041;&#21450;&#39640;&#26114;&#30340;&#20248;&#21270;&#24320;&#38144;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026; "delta &#35843;&#25972;"&#65292;&#21363;&#20165;&#26356;&#26032;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#31216;&#20026; "delta &#27169;&#22359;"&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#24178;&#27169;&#22411;&#30340;&#21442;&#25968;&#22266;&#23450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#23454;&#29616;&#30452;&#25509;&#20462;&#25913;&#20027;&#24178; PTMs &#30340;&#20195;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010; PTM &#30828;&#32534;&#30721;&#29305;&#23450;&#30340; delta &#35843;&#25972;&#26041;&#27861;&#65292;delta &#35843;&#25972;&#30340;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; OpenDelta&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#21508;&#31181; delta &#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26032;&#25216;&#26415;&#28040;&#38500;&#20102;&#20462;&#25913;&#20027;&#24178; PTMs &#20195;&#30721;&#30340;&#38656;&#27714;&#65292;&#20351; OpenDelta &#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#12289;&#29978;&#33267;&#26159;&#26032;&#30340; PTMs &#20860;&#23481;&#12290;OpenDelta &#30340;&#35774;&#35745;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as "delta tuning", which updates only a small subset of parameters, known as "delta modules", while keeping the backbone model's parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs' code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DINES&#65292;&#29992;&#20110;&#22312;&#26080;&#31038;&#20132;&#20551;&#35774;&#30340;&#26377;&#21521;&#24102;&#31526;&#21495;&#22270;&#20013;&#23398;&#20064;&#35299;&#32544;&#33410;&#28857;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#35299;&#32544;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#22270;&#21367;&#31215;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#31526;&#21495;&#20851;&#31995;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.03077</link><description>&lt;p&gt;
&#26080;&#31038;&#20132;&#20551;&#35774;&#19979;&#23398;&#20064;&#26377;&#21521;&#24102;&#31526;&#21495;&#22270;&#20013;&#30340;&#35299;&#32544;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Representations in Signed Directed Graphs without Social Assumptions. (arXiv:2307.03077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DINES&#65292;&#29992;&#20110;&#22312;&#26080;&#31038;&#20132;&#20551;&#35774;&#30340;&#26377;&#21521;&#24102;&#31526;&#21495;&#22270;&#20013;&#23398;&#20064;&#35299;&#32544;&#33410;&#28857;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#35299;&#32544;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#22270;&#21367;&#31215;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#31526;&#21495;&#20851;&#31995;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#31526;&#21495;&#22270;&#26159;&#34920;&#31034;&#20449;&#20219;&#20851;&#31995;&#25110;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20559;&#22909;&#20851;&#31995;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#22312;&#36825;&#26679;&#30340;&#22270;&#20013;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#23545;&#20110;&#35768;&#22810;&#25366;&#25496;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31526;&#21495;&#20851;&#31995;&#21487;&#33021;&#21463;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#31616;&#21270;&#31526;&#21495;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#20381;&#36182;&#31038;&#20132;&#29702;&#35770;&#24182;&#23558;&#20854;&#35270;&#20026;&#31616;&#21333;&#30340;&#22240;&#32032;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25429;&#25417;&#22609;&#36896;&#36825;&#20123;&#20851;&#31995;&#30340;&#22810;&#26679;&#22240;&#32032;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DINES&#65292;&#19968;&#31181;&#22312;&#26080;&#31038;&#20132;&#20551;&#35774;&#30340;&#26377;&#21521;&#24102;&#31526;&#21495;&#22270;&#20013;&#23398;&#20064;&#35299;&#32544;&#33410;&#28857;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#35299;&#32544;&#26694;&#26550;&#65292;&#23558;&#27599;&#20010;&#23884;&#20837;&#20998;&#31163;&#25104;&#19981;&#21516;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#36731;&#37327;&#32423;&#30340;&#22270;&#21367;&#31215;&#65292;&#20165;&#20851;&#27880;&#31526;&#21495;&#21644;&#26041;&#21521;&#65292;&#32780;&#19981;&#20381;&#36182;&#31038;&#20132;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#31526;&#21495;&#20851;&#31995;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed graphs are complex systems that represent trust relationships or preferences in various domains. Learning node representations in such graphs is crucial for many mining tasks. Although real-world signed relationships can be influenced by multiple latent factors, most existing methods often oversimplify the modeling of signed relationships by relying on social theories and treating them as simplistic factors. This limits their expressiveness and their ability to capture the diverse factors that shape these relationships. In this paper, we propose DINES, a novel method for learning disentangled node representations in signed directed graphs without social assumptions. We adopt a disentangled framework that separates each embedding into distinct factors, allowing for capturing multiple latent factors. We also explore lightweight graph convolutions that focus solely on sign and direction, without depending on social theories. Additionally, we propose a decoder that effectively class
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#34701;&#21512;&#26102;&#31354;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20449;&#21495;&#24179;&#28369;&#30340;EEG&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#26550;&#26500;&#21644;&#22270;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.03068</link><description>&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#26102;&#31354;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20449;&#21495;&#24179;&#28369;&#30340;EEG&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hybrid End-to-End Spatio-Temporal Attention Neural Network with Graph-Smooth Signals for EEG Emotion Recognition. (arXiv:2307.03068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03068
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#26102;&#31354;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20449;&#21495;&#24179;&#28369;&#30340;EEG&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#26550;&#26500;&#21644;&#22270;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#29702;&#25968;&#25454;&#22914;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#24773;&#32490;&#29366;&#24577;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24773;&#32490;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#21487;&#20197;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#20449;&#24687;&#30340;&#28145;&#24230;&#26550;&#26500;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26102;&#31354;&#32534;&#30721;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#30340;&#28151;&#21512;&#32467;&#26500;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29983;&#29702;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#20351;&#29992;&#22270;&#20449;&#21495;&#22788;&#29702;&#24037;&#20855;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20197;&#22312;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#22270;&#20449;&#21495;&#24179;&#28369;&#22788;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;DEAP&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#24773;&#32490;&#20998;&#31867;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#32034;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, physiological data such as electroencephalography (EEG) signals have attracted significant attention in affective computing. In this context, the main goal is to design an automated model that can assess emotional states. Lately, deep neural networks have shown promising performance in emotion recognition tasks. However, designing a deep architecture that can extract practical information from raw data is still a challenge. Here, we introduce a deep neural network that acquires interpretable physiological representations by a hybrid structure of spatio-temporal encoding and recurrent attention network blocks. Furthermore, a preprocessing step is applied to the raw data using graph signal processing tools to perform graph smoothing in the spatial domain. We demonstrate that our proposed architecture exceeds state-of-the-art results for emotion classification on the publicly available DEAP dataset. To explore the generality of the learned model, we also evaluate the performance
&lt;/p&gt;</description></item><item><title>DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.03067</link><description>&lt;p&gt;
DeepOnto: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03067
&lt;/p&gt;
&lt;p&gt;
DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;Tensorflow&#20027;&#35201;&#26159;&#20026;Python&#24320;&#21457;&#30340;&#65292;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#26412;&#20307;API&#65288;&#22914;OWL API&#21644;Jena&#65289;&#20027;&#35201;&#26159;&#22522;&#20110;Java&#30340;&#12290;&#20026;&#20102;&#26041;&#20415;&#26080;&#32541;&#38598;&#25104;&#36825;&#20123;&#26694;&#26550;&#21644;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deeponto&#65292;&#19968;&#20010;&#19987;&#20026;&#26412;&#20307;&#24037;&#31243;&#35774;&#35745;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#35748;&#21487;&#21644;&#21487;&#38752;&#30340;OWL API&#30340;&#26680;&#24515;&#26412;&#20307;&#22788;&#29702;&#27169;&#22359;&#65292;&#20197;&#26356;&#8220;Pythonic&#8221;&#30340;&#26041;&#24335;&#23553;&#35013;&#20854;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#21253;&#25324;&#20854;&#20182;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35821;&#35328;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#25237;&#24433;&#31561;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22359;&#65292;Deeponto&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#21508;&#31181;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#20363;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03056</link><description>&lt;p&gt;
&#27867;&#21270;&#21453;&#21521;&#20256;&#25773;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25351;&#31034;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#20869;&#37096;&#24037;&#20316;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#26159;&#20351;&#29992;&#21322;&#29615;&#30340;&#26356;&#19968;&#33324;&#24418;&#24335;&#30340;&#29305;&#20363;&#12290;&#36825;&#31181;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#27867;&#21270;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#22270;&#30340;&#20854;&#20182;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#20363;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#27867;&#21270;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30740;&#31350;BERT&#22312;&#20027;&#35859;&#25968;&#19968;&#33268;&#24615;&#20219;&#21153;&#65288;SVA&#65289;&#19978;&#30340;&#34892;&#20026;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#27169;&#22411;&#32452;&#20214;&#19978;&#36890;&#36807;&#30340;&#26799;&#24230;&#27969;&#37327;&#21453;&#26144;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#26381;&#21153;&#30340;&#36215;&#28857;-&#32456;&#28857;&#20986;&#34892;&#26102;&#38388;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#36712;&#36857;&#20272;&#35745;OD&#23545;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#21382;&#21490;&#36712;&#36857;&#19982;&#24322;&#24120;&#36712;&#36857;&#20043;&#38388;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03048</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#26381;&#21153;&#30340;&#36215;&#28857;-&#32456;&#28857;&#20986;&#34892;&#26102;&#38388;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Origin-Destination Travel Time Oracle for Map-based Services. (arXiv:2307.03048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#26381;&#21153;&#30340;&#36215;&#28857;-&#32456;&#28857;&#20986;&#34892;&#26102;&#38388;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#36712;&#36857;&#20272;&#35745;OD&#23545;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#21382;&#21490;&#36712;&#36857;&#19982;&#24322;&#24120;&#36712;&#36857;&#20043;&#38388;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#36215;&#28857;(O)&#65292;&#19968;&#20010;&#32456;&#28857;(D)&#21644;&#19968;&#20010;&#20986;&#21457;&#26102;&#38388;(T)&#65292;&#36215;&#28857;-&#32456;&#28857;&#20986;&#34892;&#26102;&#38388;&#39044;&#27979;&#31995;&#32479;(ODT-Oracle)&#20250;&#36820;&#22238;&#22312;T&#26102;&#38388;&#20986;&#21457;&#20174;O&#21040;D&#25152;&#38656;&#30340;&#26102;&#38388;&#20272;&#35745;&#12290;ODT-Oracle&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#26381;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#26679;&#30340;&#39044;&#27979;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21382;&#21490;&#36712;&#36857;&#20272;&#35745;OD&#23545;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#20986;&#34892;&#26102;&#38388;&#20272;&#35745;(TTE)&#26041;&#27861;&#12290;&#30001;&#20110;&#36830;&#25509;OD&#23545;&#30340;&#22810;&#20010;&#21382;&#21490;&#36712;&#36857;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#26053;&#34892;&#26102;&#38388;&#65292;&#32780;&#19988;&#36712;&#36857;&#21487;&#33021;&#20114;&#19981;&#30456;&#21516;&#65292;&#22240;&#27492;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26102;&#21435;&#38500;&#24322;&#24120;&#36712;&#36857;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;Diffusion-based Origin-destination Travel Time Estimation (DOT)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;DOT&#20351;&#29992;Pixelated Trajectories (PiT)&#21435;&#22122;&#22120;&#26469;&#24314;&#31435;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#20272;&#35745;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#26597;&#35810;&#30340;&#20986;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an origin (O), a destination (D), and a departure time (T), an Origin-Destination (OD) travel time oracle~(ODT-Oracle) returns an estimate of the time it takes to travel from O to D when departing at T. ODT-Oracles serve important purposes in map-based services. To enable the construction of such oracles, we provide a travel-time estimation (TTE) solution that leverages historical trajectories to estimate time-varying travel times for OD pairs.  The problem is complicated by the fact that multiple historical trajectories with different travel times may connect an OD pair, while trajectories may vary from one another. To solve the problem, it is crucial to remove outlier trajectories when doing travel time estimation for future queries.  We propose a novel, two-stage framework called Diffusion-based Origin-destination Travel Time Estimation (DOT), that solves the problem. First, DOT employs a conditioned Pixelated Trajectories (PiT) denoiser that enables building a diffusion-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;2022&#24180;&#22312;Deezer&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#19978;&#25512;&#20986;&#30340;Track Mix&#20010;&#24615;&#21270;&#27468;&#21333;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#25773;&#25918;&#21015;&#34920;&#30340;&#26354;&#30446;&#24207;&#21015;&#26469;&#29983;&#25104;&#20197;&#21021;&#22987;&#38899;&#20048;&#26354;&#30446;&#20026;&#28789;&#24863;&#30340;&#8220;&#28151;&#21512;&#8221;&#25773;&#25918;&#21015;&#34920;&#65292;&#25552;&#21319;&#29992;&#25143;&#22312;Deezer&#19978;&#30340;&#38899;&#20048;&#21457;&#29616;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.03045</link><description>&lt;p&gt;
&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#20013;&#20351;&#29992;Transformer&#29983;&#25104;&#27468;&#21333;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Track Mix Generation on Music Streaming Services using Transformers. (arXiv:2307.03045v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2022&#24180;&#22312;Deezer&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#19978;&#25512;&#20986;&#30340;Track Mix&#20010;&#24615;&#21270;&#27468;&#21333;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#25773;&#25918;&#21015;&#34920;&#30340;&#26354;&#30446;&#24207;&#21015;&#26469;&#29983;&#25104;&#20197;&#21021;&#22987;&#38899;&#20048;&#26354;&#30446;&#20026;&#28789;&#24863;&#30340;&#8220;&#28151;&#21512;&#8221;&#25773;&#25918;&#21015;&#34920;&#65292;&#25552;&#21319;&#29992;&#25143;&#22312;Deezer&#19978;&#30340;&#38899;&#20048;&#21457;&#29616;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Track Mix&#65292;&#36825;&#26159;&#19968;&#20010;&#20110;2022&#24180;&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;Deezer&#19978;&#25512;&#20986;&#30340;&#20010;&#24615;&#21270;&#27468;&#21333;&#29983;&#25104;&#31995;&#32479;&#12290;Track Mix&#36890;&#36807;&#33258;&#21160;&#20026;&#29992;&#25143;&#29983;&#25104;&#20197;&#21021;&#22987;&#38899;&#20048;&#26354;&#30446;&#20026;&#28789;&#24863;&#30340;&#8220;&#28151;&#21512;&#8221;&#25773;&#25918;&#21015;&#34920;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#19982;&#20182;&#20204;&#21916;&#29233;&#30340;&#20869;&#23481;&#30456;&#20284;&#30340;&#38899;&#20048;&#12290;&#20026;&#20102;&#29983;&#25104;&#36825;&#20123;&#28151;&#21512;&#27468;&#21333;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#22312;&#29992;&#25143;&#25773;&#25918;&#21015;&#34920;&#30340;&#25968;&#30334;&#19975;&#20010;&#26354;&#30446;&#24207;&#21015;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;Transformers&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19982;&#20256;&#32479;&#21512;&#20316;&#36807;&#28388;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26381;&#21153;&#20013;&#20351;&#29992;&#36825;&#31181;&#27169;&#22411;&#36827;&#34892;&#28151;&#21512;&#29983;&#25104;&#25152;&#24102;&#26469;&#30340;&#20248;&#21183;&#12289;&#19981;&#36275;&#21644;&#25216;&#26415;&#25361;&#25112;&#12290;&#33258;&#25512;&#20986;&#20197;&#26469;&#65292;Track Mix&#27599;&#22825;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#29983;&#25104;&#27468;&#21333;&#65292;&#22312;Deezer&#19978;&#25552;&#21319;&#20102;&#20182;&#20204;&#30340;&#38899;&#20048;&#21457;&#29616;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Track Mix, a personalized playlist generation system released in 2022 on the music streaming service Deezer. Track Mix automatically generates "mix" playlists inspired by initial music tracks, allowing users to discover music similar to their favorite content. To generate these mixes, we consider a Transformer model trained on millions of track sequences from user playlists. In light of the growing popularity of Transformers in recent years, we analyze the advantages, drawbacks, and technical challenges of using such a model for mix generation on the service, compared to a more traditional collaborative filtering approach. Since its release, Track Mix has been generating playlists for millions of users daily, enhancing their music discovery experience on Deezer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;Chamfer&#36317;&#31163;&#30340;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;Chamfer&#36317;&#31163;&#30340;$(1+\epsilon)$-&#36817;&#20284;&#20540;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#30340;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03043</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;Chamfer&#36317;&#31163;&#30340;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Near-Linear Time Algorithm for the Chamfer Distance. (arXiv:2307.03043v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;Chamfer&#36317;&#31163;&#30340;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;Chamfer&#36317;&#31163;&#30340;$(1+\epsilon)$-&#36817;&#20284;&#20540;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#30340;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#24847;&#20004;&#20010;&#22823;&#23567;&#19981;&#36229;&#36807;n&#30340;&#28857;&#38598;A&#65292;B&#65292;&#20174;A&#21040;B&#30340;Chamfer&#36317;&#31163;&#23450;&#20041;&#20026;CH(A,B)=&#8721;(a&#8712;A)min(b&#8712;B)dX(a,b)&#65292;&#20854;&#20013;dX&#26159;&#24213;&#23618;&#36317;&#31163;&#24230;&#37327;&#65288;&#20363;&#22914;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#25110;&#26364;&#21704;&#39039;&#36317;&#31163;&#65289;&#12290;Chamfer&#36317;&#31163;&#26159;&#34913;&#37327;&#28857;&#20113;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#30452;&#35266;&#30340;O(dn^2)&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26292;&#21147;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36816;&#34892;&#26102;&#38388;&#23545;n&#30340;&#20108;&#27425;&#20381;&#36182;&#20351;&#24471;&#30452;&#25509;&#31639;&#27861;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#38590;&#20197;&#25215;&#21463;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#36817;&#32447;&#24615;&#36816;&#34892;&#26102;&#38388;&#30340;$(1+\epsilon)$-&#36817;&#20284;&#31639;&#27861;&#26469;&#20272;&#35745;Chamfer&#36317;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;O(ndlog(n)/&#949;^2)&#65292;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For any two point sets $A,B \subset \mathbb{R}^d$ of size up to $n$, the Chamfer distance from $A$ to $B$ is defined as $\text{CH}(A,B)=\sum_{a \in A} \min_{b \in B} d_X(a,b)$, where $d_X$ is the underlying distance measure (e.g., the Euclidean or Manhattan distance). The Chamfer distance is a popular measure of dissimilarity between point clouds, used in many machine learning, computer vision, and graphics applications, and admits a straightforward $O(d n^2)$-time brute force algorithm. Further, the Chamfer distance is often used as a proxy for the more computationally demanding Earth-Mover (Optimal Transport) Distance. However, the \emph{quadratic} dependence on $n$ in the running time makes the naive approach intractable for large datasets.  We overcome this bottleneck and present the first $(1+\epsilon)$-approximate algorithm for estimating the Chamfer distance with a near-linear running time. Specifically, our algorithm runs in time $O(nd \log (n)/\varepsilon^2)$ and is implementa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.03042</link><description>&lt;p&gt;
LLaMA&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#24212;&#29992;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24040;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#36341;&#24615;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#21442;&#25968;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#39046;&#22495;&#36866;&#24212;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;LLaMA-LoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#22312;&#24320;&#28304;LLaMA&#27169;&#22411;&#19978;&#30340;PEFT&#36866;&#37197;&#22120;&#23618;&#12290;&#20020;&#24202;LLaMA-LoRA&#20351;&#29992;&#20174;MIMIC-IV&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#20020;&#24202;&#39046;&#22495;&#35774;&#35745;&#30340;&#19987;&#29992;&#36866;&#37197;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;LLaMA-LoRA&#19982;Downstream LLaMA-LoRA&#36827;&#34892;&#34701;&#21512;&#65292;&#21518;&#32773;&#26159;&#21478;&#19968;&#20010;&#19987;&#20026;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#30340;PEFT&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#19979;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#36807;&#31243;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2307.03034</link><description>&lt;p&gt;
&#24102;&#26377;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models. (arXiv:2307.03034v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#19979;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#36807;&#31243;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#30001;&#20110;&#36164;&#28304;&#32422;&#26463;&#25110;&#29615;&#22659;&#25110;&#22266;&#26377;&#22122;&#22768;&#65292;&#29609;&#23478;&#25805;&#20316;&#38656;&#35201;&#22522;&#20110;&#26576;&#31181;&#26377;&#35823;&#24046;&#30340;&#21453;&#39304;&#26426;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#21453;&#39304;/&#35266;&#27979;&#21160;&#21147;&#23398;&#30340;&#19968;&#33324;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#20174;&#20219;&#24847;&#21021;&#22987;&#20449;&#24565;&#65288;&#20808;&#39564;&#20449;&#24687;&#65289;&#24320;&#22987;&#30340;&#20855;&#26377;&#21487;&#25968;&#20449;&#24565;&#29366;&#24577;&#31354;&#38388;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#37096;&#20998;&#23432;&#24658;&#23450;&#24459;&#65288;PCL&#65289;&#30340;&#21487;&#23454;&#29616;&#21306;&#22495;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#26080;&#38480;&#29366;&#24577;&#38382;&#39064;&#30340;&#21487;&#32034;&#24341;&#24615;&#21644;&#20248;&#20808;&#32423;&#32034;&#24341;&#65288;Whittle&#32034;&#24341;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#36807;&#31243;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#24212;&#29992;Ni&#241;o-Mora&#21644;Bertsimas&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#30340;AG&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#32447;&#24615;&#25193;&#23637;&#31639;&#27861;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03027</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#37325;&#35201;&#24615;&#23398;&#20064;&#25913;&#21892;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Retrieval-Augmented Large Language Models via Data Importance Learning. (arXiv:2307.03027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#32447;&#24615;&#25193;&#23637;&#31639;&#27861;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#25968;&#25454;&#34917;&#20840;&#31561;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20854;&#22522;&#30784;&#26816;&#32034;&#35821;&#26009;&#30340;&#25968;&#25454;&#36136;&#37327;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32447;&#24615;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#22810;&#32447;&#24615;&#25193;&#23637;&#20013;&#23384;&#22312;&#25351;&#25968;&#32423;&#30340;&#39033;&#65292;&#26412;&#25991;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22815;&#31934;&#30830;&#35745;&#31639;&#20855;&#26377;&#21152;&#27861;&#25928;&#29992;&#20989;&#25968;&#21644;&#39564;&#35777;&#38598;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#28857;&#22312;&#26816;&#32034;&#35821;&#26009;&#20013;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#65288;&#949;&#65292;&#948;&#65289;-&#36817;&#20284;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20165;&#20462;&#21098;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\epsilon}, {\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#22312;&#20154;&#31867;&#19987;&#23478;&#20013;&#28155;&#21152;&#20154;&#24037;&#26234;&#33021;&#65292;&#20174;&#20043;&#21069;&#30001;&#20154;&#31867;&#19987;&#23478;&#23457;&#26597;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#23454;&#20363;&#20013;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03003</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#25928;&#29575;: &#22312;&#20154;&#31867;&#19987;&#23478;&#20013;&#28155;&#21152;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts. (arXiv:2307.03003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#22312;&#20154;&#31867;&#19987;&#23478;&#20013;&#28155;&#21152;&#20154;&#24037;&#26234;&#33021;&#65292;&#20174;&#20043;&#21069;&#30001;&#20154;&#31867;&#19987;&#23478;&#23457;&#26597;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#23454;&#20363;&#20013;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#29983;&#25104;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#25193;&#23637;&#20102;ML&#27169;&#22411;&#65292;&#20026;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#28155;&#21152;&#20102;&#20154;&#24037;&#23457;&#26680;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#25345;&#32493;&#20381;&#36182;&#20154;&#31867;&#19987;&#23478;&#22788;&#29702;&#22256;&#38590;&#30340;&#27169;&#22411;&#20998;&#31867;&#20250;&#23548;&#33268;&#20154;&#21147;&#25237;&#20837;&#30340;&#22823;&#24133;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#26377;&#38480;&#36164;&#28304;&#30340;&#21387;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#21019;&#24314;&#20102;&#20154;&#24037;&#19987;&#23478;&#65292;&#20174;&#20043;&#21069;&#30001;&#20154;&#31867;&#19987;&#23478;&#23457;&#26597;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#23454;&#20363;&#20013;&#23398;&#20064;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#31995;&#32479;&#35780;&#20272;&#21738;&#20010;&#20154;&#24037;&#19987;&#23478;&#36866;&#21512;&#20998;&#31867;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#65292;&#24182;&#33258;&#21160;&#20998;&#37197;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#36825;&#20943;&#23569;&#20102;&#20154;&#21147;&#25237;&#20837;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;HITL&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information systems increasingly leverage artificial intelligence (AI) and machine learning (ML) to generate value from vast amounts of data. However, ML models are imperfect and can generate incorrect classifications. Hence, human-in-the-loop (HITL) extensions to ML models add a human review for instances that are difficult to classify. This study argues that continuously relying on human experts to handle difficult model classifications leads to a strong increase in human effort, which strains limited resources. To address this issue, we propose a hybrid system that creates artificial experts that learn to classify data instances from unknown classes previously reviewed by human experts. Our hybrid system assesses which artificial expert is suitable for classifying an instance from an unknown class and automatically assigns it. Over time, this reduces human effort and increases the efficiency of the system. Our experiments demonstrate that our approach outperforms traditional HITL sy
&lt;/p&gt;</description></item><item><title>ContainerGym&#26159;&#19968;&#20010;&#21463;&#21040;&#30495;&#23454;&#19990;&#30028;&#24037;&#19994;&#36164;&#28304;&#20998;&#37197;&#20219;&#21153;&#21551;&#21457;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#23558;&#19968;&#31995;&#21015;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#25361;&#25112;&#32534;&#30721;&#36827;&#21435;&#65292;&#20174;&#32780;&#22312;&#20219;&#20309;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#19978;&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02991</link><description>&lt;p&gt;
ContainerGym: &#19968;&#31181;&#38024;&#23545;&#36164;&#28304;&#20998;&#37197;&#30340;&#29616;&#23454;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation. (arXiv:2307.02991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02991
&lt;/p&gt;
&lt;p&gt;
ContainerGym&#26159;&#19968;&#20010;&#21463;&#21040;&#30495;&#23454;&#19990;&#30028;&#24037;&#19994;&#36164;&#28304;&#20998;&#37197;&#20219;&#21153;&#21551;&#21457;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#23558;&#19968;&#31995;&#21015;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#25361;&#25112;&#32534;&#30721;&#36827;&#21435;&#65292;&#20174;&#32780;&#22312;&#20219;&#20309;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#19978;&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ContainerGym&#65292;&#19968;&#31181;&#21463;&#21040;&#30495;&#23454;&#19990;&#30028;&#24037;&#19994;&#36164;&#28304;&#20998;&#37197;&#20219;&#21153;&#21551;&#21457;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#32534;&#30721;&#20102;&#19968;&#31995;&#21015;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#21487;&#20197;&#37197;&#32622;&#25104;&#19981;&#21516;&#38590;&#24230;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#21464;&#37327;&#32500;&#24230;&#12290;&#19982;&#20854;&#20182;&#38024;&#23545;&#32534;&#30721;&#30495;&#23454;&#19990;&#30028;&#22256;&#38590;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#30452;&#25509;&#26469;&#28304;&#20110;&#19968;&#20010;&#32463;&#36807;&#26368;&#23567;&#21270;&#31616;&#21644;&#31934;&#31616;&#30340;&#30495;&#23454;&#19990;&#30028;&#24037;&#19994;&#38382;&#39064;&#12290;&#23427;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;&#36866;&#29992;&#20110;&#25105;&#20204;&#36164;&#28304;&#20998;&#37197;&#26694;&#26550;&#30340;&#20219;&#20309;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#19978;&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#38500;&#20102;&#36890;&#24120;&#30340;&#35757;&#32451;&#22870;&#21169;&#26354;&#32447;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#29992;&#20110;&#35299;&#37322;&#23427;&#20204;&#30340;&#32479;&#35745;&#24037;&#20855;&#20801;&#35768;&#31361;&#20986;&#26174;&#31034;&#25105;&#20204;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#19968;&#20123;&#26377;&#36259;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ContainerGym, a benchmark for reinforcement learning inspired by a real-world industrial resource allocation task. The proposed benchmark encodes a range of challenges commonly encountered in real-world sequential decision making problems, such as uncertainty. It can be configured to instantiate problems of varying degrees of difficulty, e.g., in terms of variable dimensionality. Our benchmark differs from other reinforcement learning benchmarks, including the ones aiming to encode real-world difficulties, in that it is directly derived from a real-world industrial problem, which underwent minimal simplification and streamlining. It is sufficiently versatile to evaluate reinforcement learning algorithms on any real-world problem that fits our resource allocation framework. We provide results of standard baseline methods. Going beyond the usual training reward curves, our results and the statistical tools used to interpret them allow to highlight interesting limitations of we
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#36523;&#20221;&#20998;&#31867;&#22120;&#20316;&#20026;&#23548;&#21521;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32780;&#23548;&#33268;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02984</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#34892;&#36208;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications. (arXiv:2307.02984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#36523;&#20221;&#20998;&#31867;&#22120;&#20316;&#20026;&#23548;&#21521;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32780;&#23548;&#33268;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#29983;&#25104;&#19982;&#30446;&#26631;&#20998;&#24067;&#21305;&#37197;&#30340;&#21512;&#25104;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#38544;&#31169;&#35282;&#24230;&#26469;&#30475;&#65292;&#20351;&#29992;GAN&#20316;&#20026;&#25968;&#25454;&#20849;&#20139;&#30340;&#20195;&#29702;&#19981;&#26159;&#19968;&#20010;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23884;&#20837;&#25509;&#36817;&#30495;&#23454;&#26679;&#26412;&#30340;&#21103;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21463;k-&#21311;&#21517;&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#32858;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20250;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#23548;&#33322;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36741;&#21161;&#36523;&#20221;&#20998;&#31867;&#22120;&#20316;&#20026;&#23548;&#21521;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#38750;&#32447;&#24615;&#22320;&#22312;&#28857;&#20043;&#38388;&#31227;&#21160;&#65292;&#26368;&#23567;&#21270;&#19982;&#25509;&#36817;&#30495;&#23454;&#26679;&#26412;&#30340;&#21103;&#26412;&#21457;&#29983;&#20914;&#31361;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#38543;&#26426;&#28857;&#23545;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36798;&#21040;&#20102;&#21516;&#26102;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#25928;&#35757;&#32451;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have demonstrated their ability to generate synthetic samples that match a target distribution. However, from a privacy perspective, using GANs as a proxy for data sharing is not a safe solution, as they tend to embed near-duplicates of real samples in the latent space. Recent works, inspired by k-anonymity principles, address this issue through sample aggregation in the latent space, with the drawback of reducing the dataset by a factor of k. Our work aims to mitigate this problem by proposing a latent space navigation strategy able to generate diverse synthetic samples that may support effective training of deep models, while addressing privacy concerns in a principled way. Our approach leverages an auxiliary identity classifier as a guide to non-linearly walk between points in the latent space, minimizing the risk of collision with near-duplicates of real samples. We empirically demonstrate that, given any random pair of points in the latent sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20174;&#26234;&#33021;&#25163;&#26426;&#38899;&#39057;&#25968;&#25454;&#20013;&#36827;&#34892;COVID-19&#30340;&#39640;&#25928;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#31227;&#21160;&#20581;&#24247;&#31995;&#32479;&#20013;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02975</link><description>&lt;p&gt;
&#20351;&#29992;&#25163;&#26426;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;COVID-19&#30340;&#39640;&#25928;&#26816;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data. (arXiv:2307.02975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20174;&#26234;&#33021;&#25163;&#26426;&#38899;&#39057;&#25968;&#25454;&#20013;&#36827;&#34892;COVID-19&#30340;&#39640;&#25928;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#31227;&#21160;&#20581;&#24247;&#31995;&#32479;&#20013;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26234;&#33021;&#25163;&#26426;&#25968;&#25454;&#20013;&#26816;&#27979;&#30142;&#30149;&#26159;&#31227;&#21160;&#20581;&#24247;&#65288;m-health&#65289;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;COVID-19&#21450;&#20854;&#21628;&#21560;&#30151;&#29366;&#26159;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#26089;&#26399;&#26816;&#27979;&#26159;&#23545;&#25239;&#22823;&#27969;&#34892;&#30149;&#30340;&#19968;&#20010;&#28508;&#22312;&#26377;&#25928;&#25163;&#27573;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#20027;&#35201;&#21462;&#20915;&#20110;&#24212;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#30340;AI&#31639;&#27861;&#30340;&#24615;&#33021;&#20197;&#21450;&#20854;&#21487;&#33021;&#30452;&#25509;&#22312;&#29992;&#25143;&#25163;&#26426;&#19978;&#23454;&#26045;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#38382;&#39064;&#21644;&#26377;&#38480;&#30340;&#21487;&#29992;&#25968;&#25454;&#37327;&#65292;&#26412;&#25991;&#36890;&#36807;&#19982;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;VGGish&#65292;YAMNET&#21644;L3-Net&#65289;&#30340;&#23454;&#39564;&#35780;&#20272;&#20197;&#21450;&#22312;&#25152;&#32771;&#34385;&#30340;&#22330;&#26223;&#20013;&#30340;&#20004;&#31181;&#20027;&#35201;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65306;&#29305;&#24449;&#25552;&#21462;&#21644;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;4&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;&#24635;&#20849;13,447&#20010;&#26679;&#26412;&#65289;&#19978;&#36827;&#34892;&#29420;&#31435;&#29992;&#25143;&#23454;&#39564;&#26469;&#35780;&#20272;12&#20010;&#19981;&#21516;&#37197;&#32622;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disease detection from smartphone data represents an open research challenge in mobile health (m-health) systems. COVID-19 and its respiratory symptoms are an important case study in this area and their early detection is a potential real instrument to counteract the pandemic situation. The efficacy of this solution mainly depends on the performances of AI algorithms applied to the collected data and their possible implementation directly on the users' mobile devices. Considering these issues, and the limited amount of available data, in this paper we present the experimental evaluation of 3 different deep learning models, compared also with hand-crafted features, and of two main approaches of transfer learning in the considered scenario: both feature extraction and fine-tuning. Specifically, we considered VGGish, YAMNET, and L\textsuperscript{3}-Net (including 12 different configurations) evaluated through user-independent experiments on 4 different datasets (13,447 samples in total).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#21644;&#20462;&#21098;&#36825;&#20004;&#31181;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#20248;&#20110;&#20462;&#21098;&#12290;</title><link>http://arxiv.org/abs/2307.02973</link><description>&lt;p&gt;
&#20462;&#21098;&#19982;&#37327;&#21270;&#65306;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pruning vs Quantization: Which is Better?. (arXiv:2307.02973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#21644;&#20462;&#21098;&#36825;&#20004;&#31181;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#20248;&#20110;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#21644;&#37327;&#21270;&#25216;&#26415;&#20960;&#20046;&#21644;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#19968;&#26679;&#21476;&#32769;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21482;&#26377;&#20004;&#32773;&#20043;&#38388;&#30340;&#20020;&#26102;&#27604;&#36739;&#21457;&#34920;&#36807;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#21738;&#20010;&#26356;&#22909;&#65306;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36824;&#26159;&#20462;&#21098;&#65311;&#36890;&#36807;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36825;&#20004;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26399;&#26395;&#37327;&#21270;&#21644;&#20462;&#21098;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#20013;&#27599;&#23618;&#20462;&#21098;&#21644;&#37327;&#21270;&#35823;&#24046;&#30340;&#19979;&#30028;&#65292;&#24182;&#23558;&#20854;&#19982;&#20248;&#21270;&#21518;&#30340;&#32463;&#39564;&#35823;&#24046;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;3&#20010;&#20219;&#21153;&#19978;&#30340;8&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#20248;&#20110;&#20462;&#21098;&#12290;&#21482;&#26377;&#22312;&#19968;&#20123;&#26497;&#39640;&#21387;&#32553;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#21098;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.02969</link><description>&lt;p&gt;
DPM: &#36890;&#36807;&#20998;&#31163;&#32858;&#31867;&#25935;&#24863;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DPM: Clustering Sensitive Data through Separation. (arXiv:2307.02969v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#20449;&#24687;&#24471;&#20197;&#20445;&#25252;&#12290;&#20808;&#21069;&#30340;&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20851;&#27880;&#28857;&#22312;&#20110;&#35782;&#21035;&#28857;&#20113;&#30340;&#32858;&#38598;&#12290;&#26412;&#25991;&#21017;&#37319;&#21462;&#21478;&#19968;&#31181;&#26041;&#27861;&#65292;&#20851;&#27880;&#20110;&#35782;&#21035;&#36866;&#24403;&#30340;&#20998;&#31163;&#22120;&#20197;&#20998;&#31163;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#12290;DPM&#35299;&#20915;&#20102;&#23547;&#25214;&#20934;&#30830;&#20998;&#31163;&#22120;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35782;&#21035;&#32858;&#31867;&#38388;&#30340;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#32780;&#19981;&#26159;&#32858;&#31867;&#20869;&#30340;&#23567;&#38388;&#38548;&#20998;&#31163;&#22120;&#65292;&#20197;&#21450;&#22312;&#24320;&#38144;&#38544;&#31169;&#39044;&#31639;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#36739;&#22823;&#23376;&#37096;&#20998;&#30340;&#20998;&#31163;&#22120;&#12290;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25351;&#25968;&#26426;&#21046;&#65292;DPM&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20855;&#26377;&#39640;&#25928;&#29992;&#24615;&#30340;&#32858;&#31867;&#20998;&#31163;&#22120;&#65306;&#23545;&#20110;&#25968;&#25454;&#38598;D&#65292;&#22914;&#26524;&#20013;&#24515;&#30340;60%&#20998;&#20301;&#25968;&#20013;&#23384;&#22312;&#23485;&#30340;&#20302;&#23494;&#24230;&#20998;&#31163;&#22120;&#65292;DPM&#20250;&#21457;&#29616;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving clustering groups data points in an unsupervised manner whilst ensuring that sensitive information remains protected. Previous privacy-preserving clustering focused on identifying concentration of point clouds. In this paper, we take another path and focus on identifying appropriate separators that split a data set. We introduce the novel differentially private clustering algorithm DPM that searches for accurate data point separators in a differentially private manner. DPM addresses two key challenges for finding accurate separators: identifying separators that are large gaps between clusters instead of small gaps within a cluster and, to efficiently spend the privacy budget, prioritising separators that split the data into large subparts. Using the differentially private Exponential Mechanism, DPM randomly chooses cluster separators with provably high utility: For a data set $D$, if there is a wide low-density separator in the central $60\%$ quantile, DPM finds that
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;SegNetr&#65292;&#36890;&#36807;&#24341;&#20837;SegNetr&#22359;&#21644;&#20449;&#24687;&#20445;&#30041;&#36339;&#36291;&#36830;&#25509;&#23454;&#29616;&#20102;&#21160;&#24577;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29305;&#24449;&#30340;&#31934;&#30830;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.02953</link><description>&lt;p&gt;
SegNetr&#65306;&#37325;&#26032;&#24605;&#32771;U&#22411;&#32593;&#32476;&#20013;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#21644;&#36339;&#36291;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks. (arXiv:2307.02953v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;SegNetr&#65292;&#36890;&#36807;&#24341;&#20837;SegNetr&#22359;&#21644;&#20449;&#24687;&#20445;&#30041;&#36339;&#36291;&#36830;&#25509;&#23454;&#29616;&#20102;&#21160;&#24577;&#30340;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29305;&#24449;&#30340;&#31934;&#30830;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#19988;&#26131;&#20110;&#35843;&#25972;&#30340;&#32467;&#26500;&#65292;U&#22411;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;U&#22411;&#20998;&#21106;&#32593;&#32476;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#22797;&#26434;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#24357;&#34917;&#22522;&#20110;&#21367;&#31215;&#25805;&#20316;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#32570;&#22833;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#24635;&#21442;&#25968;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65307;2&#65289;&#31616;&#21333;&#22320;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#31354;&#38388;&#20301;&#32622;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#65292;&#31216;&#20026;SegNetr&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SegNetr&#22359;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#38454;&#27573;&#21160;&#24577;&#22320;&#36827;&#34892;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#65292;&#24182;&#19988;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20449;&#24687;&#20445;&#30041;&#36339;&#36291;&#36830;&#25509;&#65288;IRSC&#65289;&#65292;&#20197;&#20445;&#30041;&#32534;&#30721;&#22120;&#29305;&#24449;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#19982;&#35299;&#30721;&#22120;&#29305;&#24449;&#23454;&#29616;&#31934;&#30830;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, U-shaped networks have dominated the field of medical image segmentation due to their simple and easily tuned structure. However, existing U-shaped segmentation networks: 1) mostly focus on designing complex self-attention modules to compensate for the lack of long-term dependence based on convolution operation, which increases the overall number of parameters and computational complexity of the network; 2) simply fuse the features of encoder and decoder, ignoring the connection between their spatial locations. In this paper, we rethink the above problem and build a lightweight medical image segmentation network, called SegNetr. Specifically, we introduce a novel SegNetr block that can perform local-global interactions dynamically at any stage and with only linear complexity. At the same time, we design a general information retention skip connection (IRSC) to preserve the spatial location information of encoder features and achieve accurate fusion with the decoder features. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02932</link><description>&lt;p&gt;
&#24403;&#25298;&#32477;&#23398;&#20064;&#23545;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#26368;&#20248;&#26102;
&lt;/p&gt;
&lt;p&gt;
When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25298;&#32477;&#23398;&#20064;&#26159;&#30740;&#31350;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30456;&#20114;&#20316;&#29992;&#30340;&#20856;&#22411;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25298;&#32477;&#22120;&#12290;&#22312;&#26679;&#26412;&#21040;&#36798;&#26102;&#65292;&#25298;&#32477;&#22120;&#39318;&#20808;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#23427;&#65307;&#22914;&#26524;&#25509;&#21463;&#65292;&#39044;&#27979;&#22120;&#23436;&#25104;&#39044;&#27979;&#20219;&#21153;&#65307;&#22914;&#26524;&#34987;&#25298;&#32477;&#65292;&#21017;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#12290;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#22120;&#21644;&#25298;&#32477;&#22120;&#12290;&#36825;&#25913;&#21464;&#20102;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#38750;&#20984;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#24102;&#26377;&#25298;&#32477;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#19968;&#20123;&#30740;&#31350;&#24320;&#21457;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#24182;&#30740;&#31350;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#39057;&#24405;&#21046;&#30340;2D&#23039;&#21183;&#20272;&#35745;&#26469;&#30830;&#23450;&#26368;&#20339;&#20256;&#24863;&#22120;&#24067;&#23616;&#65292;&#22312;&#20256;&#24863;&#22120;&#22522;&#30784;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#12290;</title><link>http://arxiv.org/abs/2307.02906</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20256;&#24863;&#22120;&#22522;&#30784;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#20256;&#24863;&#22120;&#20248;&#21270;&#24067;&#23616;&#30340;&#23454;&#26102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition. (arXiv:2307.02906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#39057;&#24405;&#21046;&#30340;2D&#23039;&#21183;&#20272;&#35745;&#26469;&#30830;&#23450;&#26368;&#20339;&#20256;&#24863;&#22120;&#24067;&#23616;&#65292;&#22312;&#20256;&#24863;&#22120;&#22522;&#30784;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#22522;&#30784;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#21487;&#20197;&#26041;&#20415;&#22320;&#30417;&#27979;&#20154;&#20307;&#21160;&#20316;&#65292;&#20294;&#30830;&#23450;&#26368;&#20339;&#20256;&#24863;&#22120;&#24067;&#23616;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#20998;&#31867;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20174;&#30446;&#26631;&#27963;&#21160;&#30340;&#35270;&#39057;&#24405;&#21046;&#20013;&#24471;&#21040;&#30340;&#23454;&#26102;2D&#23039;&#21183;&#20272;&#35745;&#12290;&#36890;&#36807;&#24471;&#21040;&#30340;&#39592;&#39612;&#25968;&#25454;&#65292;&#21487;&#20197;&#30830;&#23450;&#26368;&#20339;&#20256;&#24863;&#22120;&#20301;&#32622;&#30340;&#29420;&#29305;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21487;&#34892;&#24615;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#34987;&#35797;&#32773;&#19978;&#24212;&#29992;&#24815;&#24615;&#20256;&#24863;&#22120;&#30417;&#27979;&#20102;13&#31181;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#20256;&#24863;&#22120;&#24067;&#23616;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#35774;&#22791;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#30830;&#23450;&#26368;&#20339;&#20256;&#24863;&#22120;&#24067;&#23616;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25968;&#25454;&#21311;&#21517;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#25903;&#25345;&#22810;&#29992;&#25143;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor-based Human Activity Recognition facilitates unobtrusive monitoring of human movements. However, determining the most effective sensor placement for optimal classification performance remains challenging. This paper introduces a novel methodology to resolve this issue, using real-time 2D pose estimations derived from video recordings of target activities. The derived skeleton data provides a unique strategy for identifying the optimal sensor location. We validate our approach through a feasibility study, applying inertial sensors to monitor 13 different activities across ten subjects. Our findings indicate that the vision-based method for sensor placement offers comparable results to the conventional deep learning approach, demonstrating its efficacy. This research significantly advances the field of Human Activity Recognition by providing a lightweight, on-device solution for determining the optimal sensor placement, thereby enhancing data anonymization and supporting a multimo
&lt;/p&gt;</description></item><item><title>PUFFIN&#26159;&#19968;&#31181;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#33410;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#21644;&#22270;&#23884;&#20837;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;PUFFIN&#22312;&#39044;&#27979;&#20013;&#32988;&#36807;&#19981;&#20351;&#29992;&#24402;&#32435;&#20559;&#24046;&#25110;&#20351;&#29992;&#36890;&#29992;&#25551;&#36848;&#31526;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02903</link><description>&lt;p&gt;
PUFFIN: &#29992;&#20110;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#30340;&#36335;&#24452;&#32479;&#19968;&#21069;&#21521;&#25509;&#21475;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction. (arXiv:2307.02903v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02903
&lt;/p&gt;
&lt;p&gt;
PUFFIN&#26159;&#19968;&#31181;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#33410;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#21644;&#22270;&#23884;&#20837;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;PUFFIN&#22312;&#39044;&#27979;&#20013;&#32988;&#36807;&#19981;&#20351;&#29992;&#24402;&#32435;&#20559;&#24046;&#25110;&#20351;&#29992;&#36890;&#29992;&#25551;&#36848;&#31526;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#33976;&#27773;&#21387;&#21147;&#23545;&#20110;&#24037;&#19994;&#21644;&#29615;&#22659;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#30340;&#36164;&#28304;&#21644;&#21171;&#21160;&#24378;&#24230;&#65292;&#26080;&#27861;&#33719;&#24471;&#25152;&#26377;&#26377;&#20852;&#36259;&#30340;&#21270;&#21512;&#29289;&#30340;&#20934;&#30830;&#27979;&#37327;&#12290;&#24403;&#24076;&#26395;&#39044;&#27979;&#33976;&#27773;&#21387;&#21147;&#30340;&#28201;&#24230;&#30456;&#20851;&#20851;&#31995;&#26102;&#65292;&#36164;&#28304;&#21644;&#21171;&#21160;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PUFFIN&#65288;&#36335;&#24452;&#32479;&#19968;&#21069;&#21521;&#25509;&#21475;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#36801;&#31227;&#23398;&#20064;&#19982;&#21551;&#21457;&#20110;&#39046;&#22495;&#30693;&#35782;&#65288;&#23433;&#25176;&#19975;&#26041;&#31243;&#65289;&#30340;&#26032;&#24402;&#32435;&#20559;&#24046;&#33410;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#33976;&#27773;&#21387;&#21147;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#21644;&#20351;&#29992;&#22270;&#23884;&#20837;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;PUFFIN&#20248;&#20110;&#19981;&#20351;&#29992;&#24402;&#32435;&#20559;&#24046;&#25110;&#20351;&#29992;&#36890;&#29992;&#25551;&#36848;&#31526;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#34701;&#20837;&#20811;&#26381;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting vapor pressure is vital for various industrial and environmental applications. However, obtaining accurate measurements for all compounds of interest is not possible due to the resource and labor intensity of experiments. The demand for resources and labor further multiplies when a temperature-dependent relationship for predicting vapor pressure is desired. In this paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network), a machine learning framework that combines transfer learning with a new inductive bias node inspired by domain knowledge (the Antoine equation) to improve vapor pressure prediction. By leveraging inductive bias and transfer learning using graph embeddings, PUFFIN outperforms alternative strategies that do not use inductive bias or that use generic descriptors of compounds. The framework's incorporation of domain-specific knowledge to overcome the limitation of poor data availability shows its potential for broader applications in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#30828;&#20214;&#19981;&#21487;&#30693;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#31639;&#27861;&#21644;&#30828;&#20214;&#24863;&#30693;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#65292;&#21487;&#20197;&#20248;&#21270;&#28151;&#21512;&#31934;&#24230;&#37197;&#32622;&#23545;&#29305;&#23450;&#30828;&#20214;&#30446;&#26631;&#30340;&#24310;&#36831;&#12290;&#22312;MobileNetV1&#21644;MobileNetV2&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1000&#31867;ImageNet&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#20110;8&#20301;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#20960;&#20046;&#27809;&#26377;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;28.6&#65285;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38477;&#20302;</title><link>http://arxiv.org/abs/2307.02894</link><description>&lt;p&gt;
&#33258;&#30001;&#20301;&#65306;&#22312;&#36793;&#32536;&#19978;&#20248;&#21270;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge. (arXiv:2307.02894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#30828;&#20214;&#19981;&#21487;&#30693;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#31639;&#27861;&#21644;&#30828;&#20214;&#24863;&#30693;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#65292;&#21487;&#20197;&#20248;&#21270;&#28151;&#21512;&#31934;&#24230;&#37197;&#32622;&#23545;&#29305;&#23450;&#30828;&#20214;&#30446;&#26631;&#30340;&#24310;&#36831;&#12290;&#22312;MobileNetV1&#21644;MobileNetV2&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1000&#31867;ImageNet&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#20110;8&#20301;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#20960;&#20046;&#27809;&#26377;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;28.6&#65285;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#21363;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#37327;&#21270;&#20026;&#19981;&#21516;&#30340;&#31934;&#24230;&#65292;&#20026;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#12289;&#24310;&#36831;&#21644;&#32479;&#35745;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36229;&#36234;&#20102;&#21516;&#36136;&#20301;&#23485;&#37327;&#21270;&#25152;&#33021;&#23454;&#29616;&#30340;&#12290;&#20026;&#20102;&#22312;&#32473;&#23450;&#32593;&#32476;&#30340;&#28151;&#21512;&#31934;&#24230;&#37197;&#32622;&#30340;&#38590;&#20197;&#22788;&#29702;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#12290;&#23427;&#30001;&#19968;&#31181;&#30828;&#20214;&#19981;&#21487;&#30693;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#31639;&#27861;&#21644;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#32452;&#25104;&#65292;&#20197;&#25214;&#21040;&#38024;&#23545;&#29305;&#23450;&#30828;&#20214;&#30446;&#26631;&#20248;&#21270;&#24310;&#36831;&#30340;&#28151;&#21512;&#31934;&#24230;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;MobileNetV1&#21644;MobileNetV2&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#32467;&#26524;&#32593;&#32476;&#37096;&#32626;&#22312;&#19981;&#21516;&#30828;&#20214;&#29305;&#24615;&#30340;&#22810;&#26680;RISC-V&#24494;&#25511;&#21046;&#22120;&#24179;&#21488;&#31995;&#21015;&#19978;&#12290;&#19982;8&#20301;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;1000&#31867;ImageNet&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#20840;&#31934;&#24230;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36798;28.6&#65285;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38477;&#20302;&#65292;&#20934;&#30830;&#24615;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Mixed-precision quantization, where a deep neural network's layers are quantized to different precisions, offers the opportunity to optimize the trade-offs between model size, latency, and statistical accuracy beyond what can be achieved with homogeneous-bit-width quantization. To navigate the intractable search space of mixed-precision configurations for a given network, this paper proposes a hybrid search methodology. It consists of a hardware-agnostic differentiable search algorithm followed by a hardware-aware heuristic optimization to find mixed-precision configurations latency-optimized for a specific hardware target. We evaluate our algorithm on MobileNetV1 and MobileNetV2 and deploy the resulting networks on a family of multi-core RISC-V microcontroller platforms with different hardware characteristics. We achieve up to 28.6% reduction of end-to-end latency compared to an 8-bit model at a negligible accuracy drop from a full-precision baseline on the 1000-class ImageNet dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02891</link><description>&lt;p&gt;
BaBE:&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#22686;&#24378;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#20010;&#32676;&#20307;&#20043;&#38388;&#19981;&#20844;&#24179;&#27495;&#35270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;BaBE (Bayesian Bias Elimination)&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;&#30340;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#20197;&#21450;&#25506;&#32034;&#33719;&#24471;&#20808;&#21069;&#34892;&#20026;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02889</link><description>&lt;p&gt;
&#23398;&#20064;&#25506;&#32034;&#20808;&#21069;&#34892;&#20026;&#26469;&#35299;&#20915;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#20197;&#21450;&#25506;&#32034;&#33719;&#24471;&#20808;&#21069;&#34892;&#20026;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31034;&#33539;&#24120;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20219;&#21153;&#24448;&#24448;&#20855;&#26377;&#19982;&#31034;&#33539;&#19981;&#21516;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#36825;&#23601;&#38656;&#35201;&#39069;&#22806;&#30340;&#20808;&#21069;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#20551;&#35774;&#25105;&#20204;&#24471;&#21040;&#20102;&#8220;&#20174;&#25171;&#24320;&#25277;&#23625;&#20013;&#25343;&#21462;&#29289;&#20307;&#8221;&#30340;&#20219;&#21153;&#30340;&#31034;&#33539;&#65292;&#20294;&#22312;&#35757;&#32451;&#26102;&#25277;&#23625;&#26159;&#20851;&#38381;&#30340;&#12290;&#22914;&#26524;&#27809;&#26377;&#25484;&#25569;&#25171;&#24320;&#25277;&#23625;&#30340;&#20808;&#21069;&#34892;&#20026;&#65292;&#26426;&#22120;&#20154;&#24456;&#38590;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36171;&#20104;&#26234;&#33021;&#20307;&#25506;&#32034;&#21644;&#33719;&#21462;&#25152;&#38656;&#30340;&#20808;&#21069;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#23637;&#31034;&#20808;&#21069;&#34892;&#20026;&#31034;&#33539;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of \emph{picking up an object from an open drawer}, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Rewards Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#23398;&#20064;&#30340;&#26679;&#26412;&#39640;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#21453;&#39304;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#21518;&#22810;&#35266;&#23519;&#25968;&#25454;&#23454;&#29616;&#20102;&#23545;&#20004;&#31181;&#26032;&#30340;POMDP&#23376;&#31867;&#30340;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02884</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#21518;&#22810;&#35266;&#23519;&#25968;&#25454;&#30340;POMDP&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight. (arXiv:2307.02884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#23398;&#20064;&#30340;&#26679;&#26412;&#39640;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#21453;&#39304;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#21518;&#22810;&#35266;&#23519;&#25968;&#25454;&#23454;&#29616;&#20102;&#23545;&#20004;&#31181;&#26032;&#30340;POMDP&#23376;&#31867;&#30340;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#23398;&#20064;&#30340;&#26679;&#26412;&#39640;&#25928;&#24615;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#25351;&#25968;&#32423;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#28216;&#25103;&#20013;&#30340;&#21152;&#36733;&#31561;&#24773;&#26223;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#21453;&#39304;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#20107;&#21518;&#22810;&#35266;&#23519;&#25968;&#25454;&#8221;&#65292;&#20854;&#20013;&#22312;&#19982;POMDP&#36827;&#34892;&#20132;&#20114;&#30340;&#27599;&#20010;&#21608;&#26399;&#20043;&#21518;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#25910;&#38598;&#21040;&#20174;&#36935;&#21040;&#30340;&#28508;&#22312;&#29366;&#24577;&#21457;&#20986;&#30340;&#22810;&#20010;&#38468;&#21152;&#35266;&#27979;&#25968;&#25454;&#65292;&#20294;&#19981;&#33021;&#30452;&#25509;&#35266;&#27979;&#21040;&#28508;&#22312;&#29366;&#24577;&#26412;&#36523;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#21453;&#39304;&#27169;&#22411;&#19979;&#65292;&#23545;&#20110;&#20004;&#31181;&#26032;&#30340;POMDP&#23376;&#31867;&#65288;&#22810;&#35266;&#27979;&#23637;&#31034;POMDP&#21644;&#21487;&#21306;&#20998;POMDP&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#36825;&#20004;&#20010;&#23376;&#31867;&#30456;&#23545;&#20110;&#24191;&#27867;&#30740;&#31350;&#30340;&#23637;&#31034;POMDP&#23376;&#31867;&#26469;&#35828;&#26356;&#21152;&#26222;&#36941;&#21644;&#25918;&#26494;&#65292;&#32780;&#22312;&#26631;&#20934;&#36712;&#36857;&#21453;&#39304;&#19979;&#21487;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21487;&#21306;&#20998;POMDP&#21482;&#38656;&#20351;&#29992;&#26368;&#23569;&#30340;&#35266;&#27979;&#25968;&#25454;&#21644;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called ``multiple observations in hindsight'', where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: \emph{multi-observation revealing POMDPs} and \emph{distinguishable POMDPs}. Both subclasses generalize and substantially relax \emph{revealing POMDPs} -- a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;MLOps&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#25972;&#21512;&#20102;&#31995;&#32479;&#24037;&#31243;&#12289;&#23433;&#20840;&#20445;&#35777;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#65292;&#35299;&#20915;&#20102;&#20877;&#29616;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#12289;&#21327;&#20316;&#24615;&#21644;&#25345;&#32493;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02867</link><description>&lt;p&gt;
&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;MLOps&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain. (arXiv:2307.02867v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;MLOps&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#25972;&#21512;&#20102;&#31995;&#32479;&#24037;&#31243;&#12289;&#23433;&#20840;&#20445;&#35777;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#65292;&#35299;&#20915;&#20102;&#20877;&#29616;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#12289;&#21327;&#20316;&#24615;&#21644;&#25345;&#32493;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#21333;&#29420;&#24182;&#19981;&#36275;&#20197;&#23454;&#29616;&#38750;&#21463;&#38480;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#26080;&#20154;&#39550;&#39542;&#21015;&#36710;&#36816;&#34892;&#65288;&#31216;&#20026;GoA 4&#65289;&#12290;&#29616;&#20170;&#65292;&#25152;&#38656;&#30340;&#24863;&#30693;&#20219;&#21153;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23454;&#29616;&#65292;&#22240;&#27492;&#38656;&#35201;&#21487;&#38752;&#39640;&#25928;&#22320;&#24320;&#21457;&#21644;&#37096;&#32626;&#12290;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#20351;&#29992;&#19968;&#20010;&#23433;&#20840;&#30340;MLOps&#27969;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#25913;&#36827;&#20877;&#29616;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#12289;&#21327;&#20316;&#24615;&#21644;&#26080;&#20154;&#39550;&#39542;&#36816;&#33829;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#26465;&#20214;&#30340;&#25345;&#32493;&#36866;&#24212;&#24615;&#12290;MLOps&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24320;&#21457;&#21644;&#25805;&#20316;&#65288;Ops&#65289;&#65292;&#22522;&#20110;&#36816;&#33829;&#21453;&#39304;&#23454;&#29616;&#39640;&#39057;&#36719;&#20214;&#21457;&#24067;&#21644;&#25345;&#32493;&#21019;&#26032;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;MLOps&#27969;&#31243;&#12290;&#23427;&#23558;&#31995;&#32479;&#24037;&#31243;&#12289;&#23433;&#20840;&#20445;&#35777;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#34701;&#20837;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#27969;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional automation technologies alone are not sufficient to enable driverless operation of trains (called Grade of Automation (GoA) 4) on non-restricted infrastructure. The required perception tasks are nowadays realized using Machine Learning (ML) and thus need to be developed and deployed reliably and efficiently. One important aspect to achieve this is to use an MLOps process for tackling improved reproducibility, traceability, collaboration, and continuous adaptation of a driverless operation to changing conditions. MLOps mixes ML application development and operation (Ops) and enables high frequency software releases and continuous innovation based on the feedback from operations. In this paper, we outline a safe MLOps process for the continuous development and safety assurance of ML-based systems in the railway domain. It integrates system engineering, safety assurance, and the ML life-cycle in a comprehensive workflow. We present the individual stages of the process and thei
&lt;/p&gt;</description></item><item><title>PLIERS&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#20256;&#25773;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#25512;&#33616;&#29289;&#21697;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#12289;&#30456;&#20851;&#24615;&#21644;&#25512;&#33616;&#30340;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02865</link><description>&lt;p&gt;
PLIERS: &#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#20256;&#25773;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PLIERS: a Popularity-Based Recommender System for Content Dissemination in Online Social Networks. (arXiv:2307.02865v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02865
&lt;/p&gt;
&lt;p&gt;
PLIERS&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#20256;&#25773;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#25512;&#33616;&#29289;&#21697;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#12289;&#30456;&#20851;&#24615;&#21644;&#25512;&#33616;&#30340;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26631;&#31614;&#30340;&#25512;&#33616;&#31995;&#32479;PLIERS&#65292;&#35813;&#31995;&#32479;&#20551;&#35774;&#29992;&#25143;&#20027;&#35201;&#23545;&#19982;&#20182;&#20204;&#24050;&#25317;&#26377;&#30340;&#29289;&#21697;&#21644;&#26631;&#31614;&#20855;&#26377;&#30456;&#20284;&#27969;&#34892;&#24230;&#30340;&#29289;&#21697;&#21644;&#26631;&#31614;&#24863;&#20852;&#36259;&#12290;PLIERS&#26088;&#22312;&#22312;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#25512;&#33616;&#29289;&#21697;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;PLIERS&#22312;&#20010;&#24615;&#21270;&#12289;&#30456;&#20851;&#24615;&#21644;&#25512;&#33616;&#30340;&#26032;&#39062;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel tag-based recommender system called PLIERS, which relies on the assumption that users are mainly interested in items and tags with similar popularity to those they already own. PLIERS is aimed at reaching a good tradeoff between algorithmic complexity and the level of personalization of recommended items. To evaluate PLIERS, we performed a set of experiments on real OSN datasets, demonstrating that it outperforms state-of-the-art solutions in terms of personalization, relevance, and novelty of recommendations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#30446;&#26631;&#20197;&#21450;&#32447;&#24615;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#20110;&#19981;&#21516;&#36924;&#36817;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02842</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#36845;&#20195;CVaR&#24378;&#21270;&#23398;&#20064;&#19982;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02842
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#30446;&#26631;&#20197;&#21450;&#32447;&#24615;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#20110;&#19981;&#21516;&#36924;&#36817;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#24179;&#34913;&#26399;&#26395;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24418;&#24335;&#65292;&#37319;&#29992;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#30446;&#26631;&#20197;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#21517;&#20026;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#30340;&#26032;&#24418;&#24335;&#65292;&#20026;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#23433;&#20840;&#20445;&#35777;&#26041;&#24335;&#12290;&#23545;&#20110;&#37319;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;ICVaR-L&#65292;&#35813;&#31639;&#27861;&#30340;&#21518;&#24724;&#24230;&#20026;$\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$&#65292;&#20854;&#20013;$\alpha$&#26159;&#39118;&#38505;&#27700;&#24179;&#65292;$d$&#26159;&#29366;&#24577;&#34892;&#21160;&#29305;&#24449;&#30340;&#32500;&#24230;&#65292;$H$&#26159;&#27599;&#20010;episode&#30340;&#38271;&#24230;&#65292;$K$&#26159;episode&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;$\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$&#65292;&#20197;&#39564;&#35777;ICVaR-L&#22312;$d$&#21644;$K$&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;&#23545;&#20110;&#37319;&#29992;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;ICVaR-G&#65292;&#23427;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev
&lt;/p&gt;</description></item><item><title>&#25919;&#31574;&#23545;&#27604;&#20223;&#30495;&#23398;&#20064;(PCIL)&#26159;&#19968;&#31181;&#35299;&#20915;&#25932;&#23545;&#20223;&#30495;&#23398;&#20064;(AIL)&#24615;&#33021;&#19981;&#20339;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;PCIL&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#29983;&#25104;&#24179;&#28369;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#22870;&#21169;&#65292;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#30340;&#20195;&#29702;&#19982;&#25919;&#31574;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.02829</link><description>&lt;p&gt;
&#25919;&#31574;&#23545;&#27604;&#20223;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Policy Contrastive Imitation Learning. (arXiv:2307.02829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02829
&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#23545;&#27604;&#20223;&#30495;&#23398;&#20064;(PCIL)&#26159;&#19968;&#31181;&#35299;&#20915;&#25932;&#23545;&#20223;&#30495;&#23398;&#20064;(AIL)&#24615;&#33021;&#19981;&#20339;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;PCIL&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#29983;&#25104;&#24179;&#28369;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#22870;&#21169;&#65292;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#30340;&#20195;&#29702;&#19982;&#25919;&#31574;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25932;&#23545;&#20223;&#30495;&#23398;&#20064;&#65288;AIL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;AIL&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#30001;&#20110;AIL&#37492;&#21035;&#22120;&#34920;&#31034;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#30001;&#20110;AIL&#37492;&#21035;&#22120;&#36890;&#36807;&#20108;&#20803;&#20998;&#31867;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19981;&#19968;&#23450;&#20197;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#21306;&#20998;&#25919;&#31574;&#21644;&#19987;&#23478;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#22870;&#21169;&#21487;&#33021;&#20063;&#26159;&#27809;&#26377;&#24847;&#20041;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25919;&#31574;&#23545;&#27604;&#20223;&#30495;&#23398;&#20064;&#65288;PCIL&#65289;&#12290;PCIL&#36890;&#36807;&#38170;&#23450;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#23398;&#20064;&#23545;&#27604;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#29983;&#25104;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#24179;&#28369;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;AIL&#30446;&#26631;&#30340;&#26356;&#24378;&#29256;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#20195;&#29702;&#19982;&#25919;&#31574;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#23398;&#24466;&#25216;&#33402;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial imitation learning (AIL) is a popular method that has recently achieved much success. However, the performance of AIL is still unsatisfactory on the more challenging tasks. We find that one of the major reasons is due to the low quality of AIL discriminator representation. Since the AIL discriminator is trained via binary classification that does not necessarily discriminate the policy from the expert in a meaningful way, the resulting reward might not be meaningful either. We propose a new method called Policy Contrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns a contrastive representation space by anchoring on different policies and generates a smooth cosine-similarity-based reward. Our proposed representation learning objective can be viewed as a stronger version of the AIL objective and provide a more meaningful comparison between the agent and the policy. From a theoretical perspective, we show the validity of our method using the apprenticeship le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37319;&#26679;-based&#24555;&#36895;&#26799;&#24230;&#37325;&#26631;&#23450;&#26041;&#27861;&#65288;S-FGRM&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#37325;&#26631;&#23450;&#26469;&#26367;&#20195;&#26799;&#24230;&#26356;&#26032;&#20013;&#30340;&#31526;&#21495;&#20989;&#25968;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26799;&#24230;&#26356;&#26032;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#23545;&#25239;&#36716;&#31227;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02828</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#24555;&#36895;&#26799;&#24230;&#37325;&#26631;&#23450;&#26041;&#27861;&#29992;&#20110;&#39640;&#24230;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2307.02828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37319;&#26679;-based&#24555;&#36895;&#26799;&#24230;&#37325;&#26631;&#23450;&#26041;&#27861;&#65288;S-FGRM&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#37325;&#26631;&#23450;&#26469;&#26367;&#20195;&#26799;&#24230;&#26356;&#26032;&#20013;&#30340;&#31526;&#21495;&#20989;&#25968;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26799;&#24230;&#26356;&#26032;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#23545;&#25239;&#36716;&#31227;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#36890;&#36807;&#21521;&#26080;&#23475;&#36755;&#20837;&#28155;&#21152;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#26469;&#21046;&#36896;&#30340;&#23545;&#25239;&#26679;&#26412;&#26159;&#33030;&#24369;&#30340;&#12290;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#20960;&#20046;&#23454;&#29616;&#20102;100&#65285;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#21518;&#65292;&#26356;&#22810;&#30340;&#20851;&#27880;&#28857;&#36716;&#21521;&#20102;&#40657;&#30418;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26080;&#35770;&#21738;&#31181;&#24773;&#20917;&#65292;&#24120;&#35265;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#31526;&#21495;&#20989;&#25968;&#26469;&#29983;&#25104;&#26799;&#24230;&#26356;&#26032;&#30340;&#25200;&#21160;&#65292;&#36825;&#25552;&#20379;&#20102;&#22823;&#33268;&#27491;&#30830;&#30340;&#26041;&#21521;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#23427;&#21487;&#33021;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21407;&#22987;&#26799;&#24230;&#21644;&#29983;&#25104;&#22122;&#22768;&#20043;&#38388;&#30340;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#26799;&#24230;&#26356;&#26032;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#23545;&#25239;&#36716;&#31227;&#30340;&#27425;&#20248;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#24555;&#36895;&#26799;&#24230;&#37325;&#26631;&#23450;&#26041;&#27861;&#65288;S-FGRM&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#37325;&#26631;&#23450;&#26469;&#26367;&#20195;&#31526;&#21495;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input. After achieving nearly 100% attack success rates in white-box setting, more focus is shifted to black-box attacks, of which the transferability of adversarial examples has gained significant attention. In either case, the common gradient-based methods generally use the sign function to generate perturbations on the gradient update, that offers a roughly correct direction and has gained great success. But little work pays attention to its possible limitation. In this work, we observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability. To this end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM). Specifically, we use data rescaling to substitute the sign function without extra computational cost. We
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#33041;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30740;&#31350;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#24182;&#26088;&#22312;&#20026;&#26412;&#31185;&#30740;&#31350;&#29983;&#25552;&#20379;&#20102;&#35299;BCI&#39046;&#22495;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32508;&#21512;&#26368;&#26032;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#20379;&#23545;BCI&#30740;&#31350;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#25214;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.02819</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#21457;&#23637;&#36235;&#21183;&#65306;&#26412;&#31185;&#30740;&#31350;&#29983;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Trends in Machine Learning and Electroencephalogram (EEG): A Review for Undergraduate Researchers. (arXiv:2307.02819v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02819
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#33041;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30740;&#31350;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#24182;&#26088;&#22312;&#20026;&#26412;&#31185;&#30740;&#31350;&#29983;&#25552;&#20379;&#20102;&#35299;BCI&#39046;&#22495;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32508;&#21512;&#26368;&#26032;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#20379;&#23545;BCI&#30740;&#31350;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#25214;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#33041;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#20851;&#27880;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30740;&#31350;&#65292;&#24378;&#35843;&#26368;&#26032;&#30340;&#36235;&#21183;&#65288;&#25130;&#33267;2023&#24180;&#65289;&#12290;&#30446;&#26631;&#26159;&#20026;&#26412;&#31185;&#30740;&#31350;&#29983;&#25552;&#20379;&#23545;BCI&#39046;&#22495;&#30340;&#21487;&#33719;&#21462;&#27010;&#36848;&#65292;&#28085;&#30422;&#20219;&#21153;&#12289;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#32508;&#21512;&#26368;&#26032;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;BCI&#30740;&#31350;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#35782;&#21035;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a systematic literature review on Brain-Computer Interfaces (BCIs) in the context of Machine Learning. Our focus is on Electroencephalography (EEG) research, highlighting the latest trends as of 2023. The objective is to provide undergraduate researchers with an accessible overview of the BCI field, covering tasks, algorithms, and datasets. By synthesizing recent findings, our aim is to offer a fundamental understanding of BCI research, identifying promising avenues for future investigations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02813</link><description>&lt;p&gt;
CPDG: &#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#21160;&#24577;&#22270;&#20013;&#34164;&#21547;&#20016;&#23500;&#20449;&#24687;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21160;&#24577;&#22270;&#25968;&#25454;&#25366;&#25496;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#23613;&#31649;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#32473;&#22312;&#24037;&#19994;&#24773;&#26223;&#20013;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;CPDG&#36890;&#36807;&#19968;&#31181;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#22120;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#21644;&#24037;&#19994;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transf
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#22238;&#24402;&#26041;&#27861;OLR-WA&#36890;&#36807;&#23558;&#26032;&#21040;&#36798;&#30340;&#25968;&#25454;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#29992;&#25143;&#23450;&#20041;&#30340;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#22312;&#36935;&#21040;&#25968;&#25454;&#21464;&#21270;&#26102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#26082;&#33021;&#36866;&#24212;&#19968;&#33268;&#25968;&#25454;&#21448;&#33021;&#36866;&#24212;&#21464;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.02804</link><description>&lt;p&gt;
OLR-WA &#24102;&#21152;&#26435;&#24179;&#22343;&#30340;&#22312;&#32447;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
OLR-WA Online Regression with Weighted Average. (arXiv:2307.02804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02804
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22238;&#24402;&#26041;&#27861;OLR-WA&#36890;&#36807;&#23558;&#26032;&#21040;&#36798;&#30340;&#25968;&#25454;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#29992;&#25143;&#23450;&#20041;&#30340;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#22312;&#36935;&#21040;&#25968;&#25454;&#21464;&#21270;&#26102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#26082;&#33021;&#36866;&#24212;&#19968;&#33268;&#25968;&#25454;&#21448;&#33021;&#36866;&#24212;&#21464;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26377;&#26102;&#20505;&#25968;&#25454;&#26159;&#36880;&#27493;&#21040;&#36798;&#30340;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#23384;&#20648;&#31354;&#38388;&#65292;&#24182;&#19988;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#12290;&#22312;&#32447;&#23398;&#20064;&#36890;&#36807;&#22312;&#36935;&#21040;&#25968;&#25454;&#26102;&#36880;&#27493;&#20462;&#25913;&#27169;&#22411;&#65292;&#28982;&#21518;&#20002;&#24323;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#26032;&#21040;&#36798;&#30340;&#25968;&#25454;&#19982;&#20808;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#27169;&#22411;&#12290;&#24341;&#20837;&#30340;&#27169;&#22411;&#21517;&#20026; OLR-WA&#65288;&#24102;&#21152;&#26435;&#24179;&#22343;&#30340;&#22312;&#32447;&#22238;&#24402;&#65289;&#65292;&#20351;&#29992;&#29992;&#25143;&#23450;&#20041;&#30340;&#26435;&#37325;&#26469;&#22312;&#25968;&#25454;&#21464;&#21270;&#26102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#20197;&#20559;&#21521;&#26087;&#25968;&#25454;&#25110;&#26032;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;2D&#21644;3D&#23454;&#39564;&#65292;&#23558; OLR-WA &#19982;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#38745;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25968;&#25454;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;OLR-WA &#21644;&#38745;&#24577;&#25209;&#22788;&#29702;&#27169;&#22411;&#30340;&#34920;&#29616;&#31867;&#20284;&#65292;&#32780;&#22312;&#25968;&#25454;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#35774;&#32622; OLR-WA &#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#25968;&#25454;&#25110;&#26087;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning requires a large amount of training data in order to build accurate models. Sometimes the data arrives over time, requiring significant storage space and recalculating the model to account for the new data. On-line learning addresses these issues by incrementally modifying the model as data is encountered, and then discarding the data. In this study we introduce a new online linear regression approach. Our approach combines newly arriving data with a previously existing model to create a new model. The introduced model, named OLR-WA (OnLine Regression with Weighted Average) uses user-defined weights to provide flexibility in the face of changing data to bias the results in favor of old or new data. We have conducted 2-D and 3-D experiments comparing OLR-WA to a static model using the entire data set. The results show that for consistent data, OLR-WA and the static batch model perform similarly and for varying data, the user can set the OLR-WA to adapt more quickly or t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.02799</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#65292;&#20445;&#30041;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information. (arXiv:2307.02799v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#21040;&#30697;&#38453;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#65288;PSM&#65289;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;&#19982;&#19968;&#33324;&#30340;&#26174;&#33879;&#24615;&#22270;&#30456;&#27604;&#65292;PSM&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#30340;&#26144;&#23556;&#25351;&#31034;&#20102;&#20010;&#20307;&#29305;&#23450;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#23545;&#20110;&#20174;&#20957;&#35270;&#21306;&#22495;&#30340;&#24322;&#36136;&#24615;&#20013;&#33719;&#21462;&#20010;&#20307;&#35270;&#35273;&#20559;&#22909;&#38750;&#24120;&#26377;&#29992;&#12290;PSM&#30340;&#39044;&#27979;&#26159;&#20026;&#20102;&#33719;&#21462;&#26410;&#35265;&#22270;&#20687;&#30340;PSM&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20957;&#35270;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20174;&#26377;&#38480;&#30340;&#30524;&#21160;&#25968;&#25454;&#20013;&#35782;&#21035;&#20010;&#20307;&#20957;&#35270;&#27169;&#24335;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#20010;&#20307;&#20043;&#38388;&#20957;&#35270;&#36235;&#21183;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#26041;&#27861;&#20013;&#65292;PSMs&#34987;&#21521;&#37327;&#21270;&#20197;&#36866;&#24212;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#32780;&#24573;&#35270;&#20102;&#19982;&#22270;&#20687;&#23545;&#24212;&#30340;PSMs&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#33258;&#21160;&#25581;&#31034;PSMs&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the
&lt;/p&gt;</description></item><item><title>&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02796</link><description>&lt;p&gt;
VerifAI&#65306;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02796
&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#20854;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#20173;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#38169;&#35823;&#20915;&#31574;&#65292;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#20405;&#29359;&#38544;&#31169;&#65292;&#27861;&#24459;&#36131;&#20219;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#36827;&#34892;&#24212;&#23545;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#65292;&#22914;&#36879;&#26126;&#24230;&#65292;&#38544;&#31169;&#20445;&#25252;&#65292;&#20559;&#35265;&#32531;&#35299;&#20197;&#21450;&#31038;&#20250;&#21644;&#29615;&#22659;&#36131;&#20219;&#31561;&#65292;&#20294;&#30001;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#38169;&#35823;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#12290;&#36825;&#21253;&#25324;&#20998;&#26512;&#26469;&#33258;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#21253;&#25324;&#25991;&#26412;&#25991;&#20214;&#65292;&#34920;&#26684;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36755;&#20986;&#22880;&#23450;&#26356;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#39564;&#35777;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#21457;&#29616;&#20998;&#31867;&#22120;&#23558;&#20010;&#20307;&#20998;&#20026;&#23376;&#32676;&#30340;&#33021;&#21147;&#22312;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#23545;&#31639;&#27861;&#20559;&#35265;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23376;&#32676;&#21487;&#20998;&#24615;&#12289;&#23376;&#32676;&#24046;&#24322;&#21644;&#27169;&#22411;&#22312;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38477;&#32423;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#36825;&#20026;&#20844;&#24179;&#21307;&#23398;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02791</link><description>&lt;p&gt;
&#23376;&#32676;&#21487;&#20998;&#24615;&#22312;&#32452;&#20844;&#24179;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Subgroup Separability in Group-Fair Medical Image Classification. (arXiv:2307.02791v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#21457;&#29616;&#20998;&#31867;&#22120;&#23558;&#20010;&#20307;&#20998;&#20026;&#23376;&#32676;&#30340;&#33021;&#21147;&#22312;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#23545;&#31639;&#27861;&#20559;&#35265;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23376;&#32676;&#21487;&#20998;&#24615;&#12289;&#23376;&#32676;&#24046;&#24322;&#21644;&#27169;&#22411;&#22312;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38477;&#32423;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#36825;&#20026;&#20844;&#24179;&#21307;&#23398;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20998;&#31867;&#22120;&#23558;&#20010;&#20307;&#20998;&#20026;&#23376;&#32676;&#30340;&#33021;&#21147;&#22312;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65307;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#23545;&#31639;&#27861;&#20559;&#35265;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23376;&#32676;&#21487;&#20998;&#24615;&#12289;&#23376;&#32676;&#24046;&#24322;&#21644;&#27169;&#22411;&#22312;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#25968;&#25454;&#65288;&#22914;&#27424;&#35786;&#26029;&#65289;&#26102;&#30340;&#24615;&#33021;&#38477;&#32423;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#20026;&#20844;&#24179;&#21307;&#23398;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation, we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#21644;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#33021;&#21147;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#65292;&#28385;&#36275;&#29992;&#25143;&#20010;&#20154;&#38656;&#27714;&#24182;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02779</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI
&lt;/p&gt;
&lt;p&gt;
Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02779
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#21644;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#33021;&#21147;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#65292;&#28385;&#36275;&#29992;&#25143;&#20010;&#20154;&#38656;&#27714;&#24182;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#30340;&#21457;&#23637;&#26397;&#30528;&#36830;&#25509;&#26234;&#33021;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#19968;&#27010;&#24565;&#35774;&#24819;&#20102;&#22312;&#36229;&#36830;&#25509;&#30340;&#32593;&#32476;&#29289;&#29702;&#19990;&#30028;&#20013;&#65292;&#20154;&#31867;&#12289;&#29289;&#20307;&#21644;&#26234;&#33021;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#20114;&#32852;&#12290;&#36793;&#32536;AI&#20316;&#20026;&#23454;&#29616;&#36830;&#25509;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#32452;&#32455;&#12289;&#36866;&#24212;&#21644;&#20248;&#21270;&#33258;&#24049;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#23384;&#25918;&#22312;&#20113;&#31471;&#65292;&#20854;&#20182;AI&#27169;&#22411;&#34987;&#20849;&#21516;&#37096;&#32626;&#22312;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;GPT&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#21327;&#35843;&#36793;&#32536;AI&#27169;&#22411;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20010;&#20154;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#36339;&#36291;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#24046;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#31283;&#20581;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#21644;&#24378;&#24230;&#36339;&#36291;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02766</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#36339;&#36291;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#24046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning for High-Dimensional PIDEs with Jumps. (arXiv:2307.02766v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#36339;&#36291;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#24046;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#31283;&#20581;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#21644;&#24378;&#24230;&#36339;&#36291;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#24046;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65288;PIDEs&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;Levy&#36807;&#31243;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#27169;&#25311;&#25972;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#26041;&#31243;&#30340;&#35299;&#21644;&#38750;&#23616;&#37096;&#39033;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#24046;&#35823;&#24046;&#12289;&#32456;&#27490;&#26465;&#20214;&#21644;&#38750;&#23616;&#37096;&#39033;&#30340;&#23646;&#24615;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22312;100&#32500;&#30340;&#23454;&#39564;&#20013;&#30456;&#23545;&#35823;&#24046;&#36798;&#21040;&#20102;O(10^{-3})&#65292;&#22312;&#19968;&#32500;&#32431;&#36339;&#36291;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;O(10^{-4})&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#31283;&#20581;&#24615;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#21644;&#24378;&#24230;&#36339;&#36291;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a deep learning framework for solving high-dimensional partial integro-differential equations (PIDEs) based on the temporal difference learning. We introduce a set of Levy processes and construct a corresponding reinforcement learning model. To simulate the entire process, we use deep neural networks to represent the solutions and non-local terms of the equations. Subsequently, we train the networks using the temporal difference error, termination condition, and properties of the non-local terms as the loss function. The relative error of the method reaches O(10^{-3}) in 100-dimensional experiments and O(10^{-4}) in one-dimensional pure jump problems. Additionally, our method demonstrates the advantages of low computational cost and robustness, making it well-suited for addressing problems with different forms and intensities of jumps.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20309;&#26102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#32423;&#32852;&#24310;&#36831;&#21487;&#33021;&#22833;&#36133;&#65292;&#20197;&#21450;&#20309;&#26102;&#22791;&#36873;&#30340;&#24310;&#36831;&#31574;&#30053;&#21487;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20107;&#21518;&#24310;&#36831;&#26426;&#21046;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02764</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#32423;&#32852;&#24310;&#36831;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Does Confidence-Based Cascade Deferral Suffice?. (arXiv:2307.02764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20309;&#26102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#32423;&#32852;&#24310;&#36831;&#21487;&#33021;&#22833;&#36133;&#65292;&#20197;&#21450;&#20309;&#26102;&#22791;&#36873;&#30340;&#24310;&#36831;&#31574;&#30053;&#21487;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20107;&#21518;&#24310;&#36831;&#26426;&#21046;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#23454;&#29616;&#36866;&#24212;&#24615;&#22320;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#20854;&#20013;&#25353;&#39034;&#24207;&#35843;&#29992;&#19968;&#31995;&#21015;&#20998;&#31867;&#22120;&#12290;&#24310;&#36831;&#35268;&#21017;&#30830;&#23450;&#26159;&#21542;&#35843;&#29992;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#25110;&#32773;&#32456;&#27490;&#39044;&#27979;&#12290;&#19968;&#31181;&#31616;&#21333;&#30340;&#24310;&#36831;&#35268;&#21017;&#21033;&#29992;&#24403;&#21069;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#65292;&#20363;&#22914;&#22522;&#20110;&#26368;&#22823;&#39044;&#27979;&#30340;softmax&#27010;&#29575;&#12290;&#23613;&#31649;&#23545;&#32423;&#32852;&#32467;&#26500;&#19981;&#25935;&#24863;&#8212;&#8212;&#20363;&#22914;&#19981;&#24314;&#27169;&#19979;&#28216;&#27169;&#22411;&#30340;&#38169;&#35823;&#8212;&#8212;&#20294;&#36825;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24310;&#36831;&#32463;&#24120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24310;&#36831;&#21487;&#33021;&#22833;&#36133;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450;&#20309;&#26102;&#22791;&#36873;&#30340;&#24310;&#36831;&#31574;&#30053;&#21487;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26368;&#20248;&#24310;&#36831;&#35268;&#21017;&#36827;&#34892;&#20102;&#29702;&#35770;&#34920;&#24449;&#65292;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24310;&#36831;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#35774;&#32622;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#20107;&#21518;&#24310;&#36831;&#26426;&#21046;&#65292;&#24182;&#39564;&#35777;&#23427;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02752</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#23545;&#22522;&#20934;&#30340;&#26222;&#36941;&#20351;&#29992;&#23548;&#33268;&#20102;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24573;&#35270;&#12290;&#30001;&#20110;&#25506;&#32034;&#25110;&#23433;&#20840;&#32771;&#34385;&#30340;&#25361;&#25112;&#65292;&#23454;&#38469;&#31163;&#32447;RL&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#31354;&#38388;&#19978;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20855;&#20307;&#35828;&#26126;&#20102;&#31163;&#32447;RL&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#35206;&#30422;&#29575;&#36981;&#24490;&#19968;&#20010;&#30001;&#20559;&#24577;&#31574;&#30053;&#25152;&#29305;&#24449;&#21270;&#30340;&#24130;&#24459;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20998;&#24067;&#32422;&#26463;&#30340;&#20856;&#22411;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;&#20445;&#23432;Q&#23398;&#20064;&#65288;CQL&#65289;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#25552;&#21462;&#31574;&#30053;&#26159;&#26080;&#25928;&#30340;&#12290;&#21463;&#33258;&#28982;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CQL&#30340;&#22686;&#24378;&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#22238;&#24518;&#20197;&#24448;&#30456;&#20851;&#32463;&#39564;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#20219;&#21153;&#32423;&#21035;&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#24182;&#19981;&#33021;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#35774;&#35745;&#65292;&#26080;&#27861;&#33719;&#21462;&#20851;&#20110;&#22914;&#20309;&#35780;&#20272;&#21644;&#36873;&#25321;&#27169;&#22411;&#30340;&#21487;&#38752;&#24773;&#20917;&#30340;&#23436;&#25972;&#30011;&#38754;&#12290;</title><link>http://arxiv.org/abs/2307.02732</link><description>&lt;p&gt;
&#35780;&#20272;&#35780;&#20272;&#22120;&#65306;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#36866;&#29992;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?. (arXiv:2307.02732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#20219;&#21153;&#32423;&#21035;&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#24182;&#19981;&#33021;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#35774;&#35745;&#65292;&#26080;&#27861;&#33719;&#21462;&#20851;&#20110;&#22914;&#20309;&#35780;&#20272;&#21644;&#36873;&#25321;&#27169;&#22411;&#30340;&#21487;&#38752;&#24773;&#20917;&#30340;&#23436;&#25972;&#30011;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#22522;&#20934;&#37117;&#38598;&#20013;&#22312;&#23545;&#35768;&#22810;&#20219;&#21153;&#24179;&#22343;&#24615;&#33021;&#30340;&#35780;&#20272;&#19978;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22914;&#20309;&#21487;&#38752;&#22320;&#35780;&#20272;&#21644;&#35843;&#25972;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#20219;&#21153;&#32423;&#21035;&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#22312;&#37096;&#32626;&#27169;&#22411;&#26102;&#26159;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#24615;&#33021;&#20272;&#35745;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#31574;&#30053;&#65292;&#24182;&#26816;&#26597;&#20102;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#40065;&#26834;&#30340;&#35780;&#20272;&#22120;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29992;&#36739;&#23569;&#30340;&#25240;&#21472;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#26159;&#30452;&#25509;&#20272;&#35745;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#29992;&#33258;&#21161;&#27861;&#25110;&#22823;&#37327;&#25240;&#21472;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#26356;&#36866;&#21512;&#20110;&#27169;&#22411;&#36873;&#25321;&#30340;&#30446;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#24182;&#19981;&#33021;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#35774;&#35745;&#65292;&#26080;&#27861;&#33719;&#21462;&#20851;&#20110;&#22914;&#20309;&#35780;&#20272;&#21644;&#36873;&#25321;&#27169;&#22411;&#30340;&#21487;&#38752;&#24773;&#20917;&#30340;&#23436;&#25972;&#30011;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. However all of these benchmarks focus on performance averaged over many tasks, and the question of how to reliably evaluate and tune models trained for individual tasks in this regime has not been addressed. This paper presents the first investigation into task-level evaluation -- a fundamental step when deploying a model. We measure the accuracy of performance estimators in the few-shot setting, consider strategies for model selection, and examine the reasons for the failure of evaluators usually thought of as being robust. We conclude that cross-validation with a low number of folds is the best choice for directly estimating the performance of a model, whereas using bootstrapping or cross validation with a large number of folds is better for model selection purposes. Overall, we find that existing benchmarks for few-shot learning are not designed in such a way that one can get a reliable picture of how e
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.02728</link><description>&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#65306;&#26397;&#30528;&#21487;&#34892;&#30340;&#22522;&#20110;&#25480;&#26435;&#30340;&#25216;&#33021;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02728
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#38656;&#35201;&#22823;&#37327;&#30340;&#25216;&#33021;&#12290; &#25480;&#26435; - &#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#22823;&#20114;&#20449;&#24687; - &#20026;&#23398;&#20064;&#22823;&#37327;&#19981;&#21516;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#65292;&#20294;&#20114;&#20449;&#24687;&#24456;&#38590;&#20248;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20998;&#23618;&#25480;&#26435;&#65292;&#36890;&#36807;&#38598;&#25104;&#30446;&#26631;&#26465;&#20214;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#35745;&#31639;&#25480;&#26435;&#26356;&#21152;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#21487;&#29992;&#20110;&#35745;&#31639;&#30701;&#26399;&#35270;&#35282;&#19979;&#30340;&#25480;&#26435;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#25351;&#25968;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#25480;&#26435;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#36129;&#29486;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#34434;&#34433;&#23548;&#33322;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#22235;&#32423;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#35206;&#30422;&#38754;&#31215;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#23545;&#22810;&#31181;&#23454;&#20307;&#21305;&#37197;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23454;&#20307;&#21305;&#37197;&#23384;&#22312;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#36807;&#24230;&#20195;&#34920;&#25110;&#26576;&#20123;&#32676;&#20307;&#20013;&#30340;&#22995;&#21517;&#26356;&#30456;&#20284;&#26102;&#65292;&#23454;&#20307;&#21305;&#37197;&#21487;&#33021;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02726</link><description>&lt;p&gt;
&#32463;&#36807;&#20844;&#24179;&#24615;&#35282;&#24230;&#30340;&#23454;&#39564;&#20998;&#26512;&#21644;&#35780;&#20272;&#23454;&#20307;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching. (arXiv:2307.02726v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#23545;&#22810;&#31181;&#23454;&#20307;&#21305;&#37197;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23454;&#20307;&#21305;&#37197;&#23384;&#22312;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#36807;&#24230;&#20195;&#34920;&#25110;&#26576;&#20123;&#32676;&#20307;&#20013;&#30340;&#22995;&#21517;&#26356;&#30456;&#20284;&#26102;&#65292;&#23454;&#20307;&#21305;&#37197;&#21487;&#33021;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22810;&#20010;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#21322;&#20010;&#22810;&#19990;&#32426;&#12290;&#31639;&#27861;&#20844;&#24179;&#24615;&#20063;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#20559;&#35265;&#21450;&#20854;&#31038;&#20250;&#24433;&#21709;&#30340;&#19968;&#20010;&#21450;&#26102;&#35805;&#39064;&#12290;&#23613;&#31649;&#23545;&#36825;&#20004;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23454;&#20307;&#21305;&#37197;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#23545;&#22810;&#31181;&#23454;&#20307;&#21305;&#37197;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#20174;&#20844;&#24320;&#21487;&#24471;&#30340;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20102;&#20004;&#20010;&#31038;&#20132;&#25968;&#25454;&#38598;&#65292;&#20197;&#36890;&#36807;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;&#23457;&#26597;&#23454;&#20307;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#29616;&#23454;&#31038;&#20250;&#20013;&#20004;&#31181;&#24120;&#35265;&#24773;&#20917;&#19979;&#30340;&#28508;&#22312;&#19981;&#20844;&#24179;&#24615;&#65306;&#65288;i&#65289;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#30340;&#36807;&#24230;&#20195;&#34920;&#21644;&#65288;ii&#65289;&#26576;&#20123;&#32676;&#20307;&#20013;&#30340;&#22995;&#21517;&#30456;&#27604;&#20854;&#20182;&#32676;&#20307;&#26356;&#30456;&#20284;&#12290;&#22312;&#25105;&#20204;&#20247;&#22810;&#30340;&#30740;&#31350;&#21457;&#29616;&#20013;&#65292;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#34429;&#28982;&#21508;&#31181;&#20844;&#24179;&#24615;&#23450;&#20041;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#37117;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20294;&#30001;&#20110;&#23454;&#20307;&#21305;&#37197;&#20855;&#26377;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#27491;&#39044;&#27979;&#29575;&#31561;&#25351;&#26631;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26356;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity matching (EM) is a challenging problem studied by different communities for over half a century. Algorithmic fairness has also become a timely topic to address machine bias and its societal impacts. Despite extensive research on these two topics, little attention has been paid to the fairness of entity matching.  Towards addressing this gap, we perform an extensive experimental evaluation of a variety of EM techniques in this paper. We generated two social datasets from publicly available datasets for the purpose of auditing EM through the lens of fairness. Our findings underscore potential unfairness under two common conditions in real-world societies: (i) when some demographic groups are overrepresented, and (ii) when names are more similar in some groups compared to others. Among our many findings, it is noteworthy to mention that while various fairness definitions are valuable for different settings, due to EM's class imbalance nature, measures such as positive predictive va
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#23454;&#36136;&#19978;&#26159;&#38024;&#23545;&#35813;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02719</link><description>&lt;p&gt;
&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#23454;&#36136;&#19978;&#26159;&#38024;&#23545;&#35813;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#39034;&#24207;&#22320;&#26597;&#35810;&#24403;&#21069;&#39044;&#27979;&#27169;&#22411;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#20351;&#29992;&#24448;&#24448;&#26159;&#21551;&#21457;&#24335;&#30340;&#65306;&#65288;i&#65289;&#20851;&#20110;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#19979;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#20934;&#30830;&#23450;&#20041;&#27809;&#26377;&#20849;&#35782;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#33021;&#22815;&#32473;&#20986;&#19968;&#20010;&#26631;&#20934;&#21327;&#35758;&#26469;&#23454;&#26045;&#35813;&#31639;&#27861;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#20248;&#21270;&#31639;&#27861;&#26694;&#26550;&#19979;&#22914;&#20309;&#22788;&#29702;&#39034;&#24207;&#21040;&#36798;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#21644;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#30830;&#31435;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#38024;&#23545;&#36825;&#31181;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#19968;&#35266;&#28857;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#36866;&#24403;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30456;&#20284;&#24230;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65288;MSCon&#65289;&#65292;&#36890;&#36807;&#32852;&#21512;&#21033;&#29992;&#22810;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#30417;&#30563;&#26469;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#26356;&#22909;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02712</link><description>&lt;p&gt;
&#22810;&#30456;&#20284;&#24230;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Similarity Contrastive Learning. (arXiv:2307.02712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30456;&#20284;&#24230;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65288;MSCon&#65289;&#65292;&#36890;&#36807;&#32852;&#21512;&#21033;&#29992;&#22810;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#30417;&#30563;&#26469;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#26356;&#22909;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#19968;&#31181;&#34920;&#31034;&#65292;&#20854;&#20013;&#30456;&#20284;&#30340;&#26679;&#26412;&#34987;&#25512;&#21040;&#19968;&#36215;&#65292;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#34987;&#25289;&#24320;&#12290;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21040;&#23383;&#24149;&#29983;&#25104;&#31561;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#22312;&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#19981;&#21516;&#30456;&#20284;&#24615;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30456;&#20284;&#24230;&#23545;&#27604;&#25439;&#22833;&#65288;MSCon&#65289;&#65292;&#36890;&#36807;&#32852;&#21512;&#21033;&#29992;&#22810;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#30417;&#30563;&#26469;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#30456;&#24212;&#30456;&#20284;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#21160;&#23398;&#20064;&#23545;&#27604;&#30456;&#20284;&#24230;&#30340;&#26435;&#37325;&#65292;&#38477;&#20302;&#19981;&#30830;&#23450;&#20219;&#21153;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#26356;&#22909;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;MSCon&#35757;&#32451;&#30340;&#32593;&#32476;&#22312;&#39046;&#22495;&#20869;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a similarity metric, contrastive methods learn a representation in which examples that are similar are pushed together and examples that are dissimilar are pulled apart. Contrastive learning techniques have been utilized extensively to learn representations for tasks ranging from image classification to caption generation. However, existing contrastive learning approaches can fail to generalize because they do not take into account the possibility of different similarity relations. In this paper, we propose a novel multi-similarity contrastive loss (MSCon), that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity. Our method automatically learns contrastive similarity weightings based on the uncertainty in the corresponding similarity, down-weighting uncertain tasks and leading to better out-of-domain generalization to new tasks. We show empirically that networks trained with MSCon outperform state-of-the-art baselines on in-domain
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyMat&#30340;&#26032;&#30340;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#23545;&#31216;&#24615;&#36827;&#34892;&#24863;&#30693;&#65292;&#24182;&#33021;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02707</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#23545;&#31216;&#24863;&#30693;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Symmetry-Aware Generation of Periodic Materials. (arXiv:2307.02707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyMat&#30340;&#26032;&#30340;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#23545;&#31216;&#24615;&#36827;&#34892;&#24863;&#30693;&#65292;&#24182;&#33021;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#28145;&#24230;&#27169;&#22411;&#29983;&#25104;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23545;&#20110;&#23545;&#31216;&#24863;&#30693;&#30340;&#20998;&#23376;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#21608;&#26399;&#24615;&#26448;&#26009;&#20855;&#26377;&#19981;&#21516;&#30340;&#23545;&#31216;&#24615;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#21040;&#36825;&#20123;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;SyMat&#65292;&#33021;&#22815;&#25429;&#25417;&#21608;&#26399;&#24615;&#26448;&#26009;&#32467;&#26500;&#30340;&#29289;&#29702;&#23545;&#31216;&#24615;&#12290;SyMat&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29983;&#25104;&#26448;&#26009;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#26230;&#26684;&#65292;&#36890;&#36807;&#29983;&#25104;&#21407;&#23376;&#31867;&#22411;&#38598;&#12289;&#26230;&#26684;&#38271;&#24230;&#21644;&#26230;&#26684;&#35282;&#24230;&#12290;&#27492;&#22806;&#65292;SyMat&#37319;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26448;&#26009;&#30340;&#21407;&#23376;&#22352;&#26631;&#65292;&#22312;&#22352;&#26631;&#25193;&#25955;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#24863;&#30693;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;SyMat&#22312;&#25152;&#26377;&#26448;&#26009;&#30340;&#23545;&#31216;&#21464;&#25442;&#19978;&#29702;&#35770;&#19978;&#26159;&#19981;&#21464;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;SyMat&#22312;&#38543;&#26426;&#29983;&#25104;&#21644;&#24615;&#33021;&#20248;&#21270;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;NTK&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25214;&#21040;&#21487;&#22788;&#29702;&#30340;&#20869;&#26680;&#34920;&#36798;&#24418;&#24335;&#26469;&#35299;&#20915;&#19968;&#33324;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#25968;&#25454;&#31934;&#28860;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.02693</link><description>&lt;p&gt;
&#20869;&#26680;&#65292;&#25968;&#25454;&#21644;&#29289;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernels, Data &amp; Physics. (arXiv:2307.02693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;NTK&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25214;&#21040;&#21487;&#22788;&#29702;&#30340;&#20869;&#26680;&#34920;&#36798;&#24418;&#24335;&#26469;&#35299;&#20915;&#19968;&#33324;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#25968;&#25454;&#31934;&#28860;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;Julia Kempe&#25945;&#25480;&#22312;Les Houches&#20030;&#21150;&#30340;&#22799;&#23395;&#23398;&#26657;&#8220;&#26426;&#22120;&#23398;&#20064;&#30340;&#32479;&#35745;&#29289;&#29702;&#8221;&#20013;&#25152;&#35762;&#25480;&#30340;&#35838;&#31243;&#31508;&#35760;&#12290;&#31508;&#35760;&#35752;&#35770;&#20102;&#25152;&#35859;&#30340;NTK&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25214;&#21040;&#21487;&#22788;&#29702;&#30340;&#20869;&#26680;&#34920;&#36798;&#24418;&#24335;&#26469;&#29702;&#35299;&#19968;&#33324;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#31508;&#35760;&#20027;&#35201;&#20851;&#27880;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#25968;&#25454;&#31934;&#28860;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20063;&#35752;&#35770;&#20102;&#24402;&#32435;&#20559;&#24046;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lecture notes from the course given by Professor Julia Kempe at the summer school "Statistical physics of Machine Learning" in Les Houches. The notes discuss the so-called NTK approach to problems in machine learning, which consists of gaining an understanding of generally unsolvable problems by finding a tractable kernel formulation. The notes are mainly focused on practical applications such as data distillation and adversarial robustness, examples of inductive bias are also discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#28436;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#38480;&#21046;&#19982;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02690</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#25193;&#23637;&#19978;&#19979;&#25991;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Scaling In-Context Demonstrations with Structured Attention. (arXiv:2307.02690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#28436;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#38480;&#21046;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#20174;&#23569;&#25968;&#28436;&#31034;&#20013;&#8220;&#23398;&#20064;&#8221;&#25191;&#34892;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#21040;&#27169;&#22411;&#26550;&#26500;&#30340;&#38480;&#21046;&#65306;1&#65289;&#30001;&#20110;&#20301;&#32622;&#23884;&#20837;&#65292;&#28436;&#31034;&#30340;&#20351;&#29992;&#21463;&#21040;&#26368;&#22823;&#21477;&#23376;&#38271;&#24230;&#30340;&#38480;&#21046;&#65307;2&#65289;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#29992;&#25143;&#26377;&#25928;&#20351;&#29992;&#26356;&#22810;&#30340;&#28436;&#31034;&#65307;3&#65289;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23545;&#28436;&#31034;&#30340;&#39034;&#24207;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26356;&#22909;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAICL&#65288;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#65289;&#65292;&#23427;&#36890;&#36807;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26367;&#25442;&#20840;&#27880;&#24847;&#21147;&#65292;&#24182;&#28040;&#38500;&#20102;&#20010;&#21035;&#28436;&#31034;&#20043;&#38388;&#19981;&#24517;&#35201;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#23545;&#28436;&#31034;&#30340;&#25490;&#21015;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of large language models (LLMs) highlights their ability to perform in-context learning, i.e., "learning" to perform a task from a few demonstrations in the context without any parameter updates. However, their capabilities of in-context learning are limited by the model architecture: 1) the use of demonstrations is constrained by a maximum sentence length due to positional embeddings; 2) the quadratic complexity of attention hinders users from using more demonstrations efficiently; 3) LLMs are shown to be sensitive to the order of the demonstrations. In this work, we tackle these challenges by proposing a better architectural design for in-context learning. We propose SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations. We e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GIT&#65292;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#38169;&#35823;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26799;&#24230;&#20449;&#24687;&#21644;&#19981;&#21464;&#24615;&#21464;&#25442;&#12290;&#36890;&#36807;&#23558;&#38169;&#20998;&#26679;&#26412;&#36716;&#22238;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21306;&#22495;&#65292;&#24182;&#27979;&#37327;&#21021;&#22987;&#39044;&#27979;&#19982;&#20351;&#29992;&#36716;&#25442;&#26679;&#26412;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#24212;&#35745;&#31639;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;GIT&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02672</link><description>&lt;p&gt;
GIT: &#20351;&#29992;&#26799;&#24230;&#21644;&#19981;&#21464;&#24615;&#21464;&#25442;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#12289;&#36229;&#20986;&#20998;&#24067;&#21644;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations. (arXiv:2307.02672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GIT&#65292;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#38169;&#35823;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26799;&#24230;&#20449;&#24687;&#21644;&#19981;&#21464;&#24615;&#21464;&#25442;&#12290;&#36890;&#36807;&#23558;&#38169;&#20998;&#26679;&#26412;&#36716;&#22238;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21306;&#22495;&#65292;&#24182;&#27979;&#37327;&#21021;&#22987;&#39044;&#27979;&#19982;&#20351;&#29992;&#36716;&#25442;&#26679;&#26412;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#24212;&#35745;&#31639;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;GIT&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20250;&#20570;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#26816;&#27979;&#22120;&#26469;&#24212;&#23545;&#38169;&#35823;&#20998;&#31867;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#23545;&#25239;&#25915;&#20987;&#25110;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#20316;&#20026;&#38169;&#35823;&#39044;&#27979;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20986;&#29616;&#30340;&#27867;&#21270;&#35823;&#24046;&#24448;&#24448;&#19982;&#23398;&#20064;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIT&#65292;&#19968;&#31181;&#26816;&#27979;&#27867;&#21270;&#38169;&#35823;&#30340;&#25972;&#20307;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26799;&#24230;&#20449;&#24687;&#21644;&#19981;&#21464;&#24615;&#21464;&#25442;&#30340;&#20351;&#29992;&#12290;&#19981;&#21464;&#24615;&#21464;&#25442;&#30340;&#35774;&#35745;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#36716;&#22238;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21306;&#22495;&#65292;&#32780;&#26799;&#24230;&#20449;&#24687;&#21017;&#36890;&#36807;&#27979;&#37327;&#21021;&#22987;&#39044;&#27979;&#19982;&#20351;&#29992;&#36716;&#25442;&#26679;&#26412;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#24212;&#22266;&#26377;&#35745;&#31639;&#20043;&#38388;&#30340;&#30683;&#30462;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GIT&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks tend to make overconfident predictions and often require additional detectors for misclassifications, particularly for safety-critical applications. Existing detection methods usually only focus on adversarial attacks or out-of-distribution samples as reasons for false predictions. However, generalization errors occur due to diverse reasons often related to poorly learning relevant invariances. We therefore propose GIT, a holistic approach for the detection of generalization errors that combines the usage of gradient information and invariance transformations. The invariance transformations are designed to shift misclassified samples back into the generalization area of the neural network, while the gradient information measures the contradiction between the initial prediction and the corresponding inherent computations of the neural network using the transformed sample. Our experiments demonstrate the superior performance of GIT compared to the state-of-the-art on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#20027;&#21160;&#31867;&#21035;&#36873;&#25321;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;(FIASco)&#65292;&#20351;&#33258;&#20027;&#20195;&#29702;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#20165;&#23545;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23569;&#25968;&#23545;&#35937;&#36827;&#34892;&#26631;&#27880;&#26469;&#19981;&#26029;&#23398;&#20064;&#26032;&#23545;&#35937;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#28508;&#21147;&#22330;&#30340;&#23548;&#33322;&#25216;&#26415;&#38598;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.02641</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20027;&#21160;&#31867;&#21035;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Active Class Selection for Few-Shot Class-Incremental Learning. (arXiv:2307.02641v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#20027;&#21160;&#31867;&#21035;&#36873;&#25321;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;(FIASco)&#65292;&#20351;&#33258;&#20027;&#20195;&#29702;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#20165;&#23545;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23569;&#25968;&#23545;&#35937;&#36827;&#34892;&#26631;&#27880;&#26469;&#19981;&#26029;&#23398;&#20064;&#26032;&#23545;&#35937;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#28508;&#21147;&#22330;&#30340;&#23548;&#33322;&#25216;&#26415;&#38598;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#36890;&#36807;&#19982;&#29992;&#25143;&#26377;&#38480;&#30340;&#20132;&#20114;&#19981;&#26029;&#22320;&#22312;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#21644;&#20027;&#21160;&#31867;&#21035;&#36873;&#25321;&#65288;ACS&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#27979;&#35797;&#37117;&#22312;&#32422;&#26463;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;FSCIL&#21644;ACS&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#33258;&#20027;&#20195;&#29702;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#20165;&#23545;&#29615;&#22659;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23569;&#25968;&#23545;&#35937;&#36827;&#34892;&#26631;&#27880;&#26469;&#19981;&#26029;&#23398;&#20064;&#26032;&#23545;&#35937;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#26368;&#26032;&#25216;&#26415;&#30340;FSCIL&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#21152;&#20837;&#20102;ACS&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#25216;&#26415;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#27169;&#22411;&#21629;&#21517;&#20026;Few-shot Incremental Active class SeleCtiOn&#65288;FIASco&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#28508;&#21147;&#22330;&#30340;&#23548;&#33322;&#25216;&#26415;&#19982;&#25105;&#20204;&#30340;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;FIASco&#27169;&#22411;&#23545;&#20854;&#24863;&#30693;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#25512;&#29702;&#65292;&#24182;&#26397;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23545;&#35937;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
For real-world applications, robots will need to continually learn in their environments through limited interactions with their users. Toward this, previous works in few-shot class incremental learning (FSCIL) and active class selection (ACS) have achieved promising results but were tested in constrained setups. Therefore, in this paper, we combine ideas from FSCIL and ACS to develop a novel framework that can allow an autonomous agent to continually learn new objects by asking its users to label only a few of the most informative objects in the environment. To this end, we build on a state-of-the-art (SOTA) FSCIL model and extend it with techniques from ACS literature. We term this model Few-shot Incremental Active class SeleCtiOn (FIASco). We further integrate a potential field-based navigation technique with our model to develop a complete framework that can allow an agent to process and reason on its sensory data through the FIASco model, navigate towards the most informative obje
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32416;&#32544;&#38203;&#36896;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#24577;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#22522;&#24577;&#20301;&#20018;&#65292;&#28040;&#38500;&#20102;&#25351;&#25968;&#32423;&#27714;&#21644;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31995;&#32479;&#19978;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02633</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#34203;&#23450;&#35860;&#38203;&#36896;&#30340;&#28151;&#21512;&#22522;&#24577;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger Forging. (arXiv:2307.02633v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02633
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32416;&#32544;&#38203;&#36896;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#24577;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#22522;&#24577;&#20301;&#20018;&#65292;&#28040;&#38500;&#20102;&#25351;&#25968;&#32423;&#27714;&#21644;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31995;&#32479;&#19978;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32416;&#32544;&#38203;&#36896;&#30340;&#21464;&#20998;&#31639;&#27861;&#21033;&#29992;&#37327;&#23376;&#31995;&#32479;&#30340;&#21452;&#20998;&#21106;&#26469;&#35299;&#20915;&#22522;&#24577;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;Schmidt&#20998;&#35299;&#26102;&#38656;&#35201;&#23545;&#26080;&#25968;&#28508;&#22312;&#22522;&#24577;&#36827;&#34892;&#25351;&#25968;&#32423;&#27714;&#21644;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32416;&#32544;&#38203;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#22522;&#24577;&#20301;&#20018;&#65292;&#28040;&#38500;&#20102;&#25351;&#25968;&#32423;&#27714;&#21644;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#24230;&#36882;&#22686;&#30340;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#35777;&#28436;&#31034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#32416;&#32544;&#38203;&#36896;&#26631;&#20934;&#23454;&#29616;&#30456;&#27604;&#21487;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25511;&#21046;&#25152;&#38656;&#36164;&#28304;&#30340;&#25968;&#37327;&#65292;&#35813;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#22823;&#30340;&#38750;&#32622;&#25442;&#19981;&#21464;&#31995;&#32479;&#65292;&#21518;&#32773;&#38480;&#21046;&#19982;&#28023;&#26862;&#20271;&#38203;&#36896;&#36807;&#31243;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement forging based variational algorithms leverage the bi-partition of quantum systems for addressing ground state problems. The primary limitation of these approaches lies in the exponential summation required over the numerous potential basis states, or bitstrings, when performing the Schmidt decomposition of the whole system. To overcome this challenge, we propose a new method for entanglement forging employing generative neural networks to identify the most pertinent bitstrings, eliminating the need for the exponential sum. Through empirical demonstrations on systems of increasing complexity, we show that the proposed algorithm achieves comparable or superior performance compared to the existing standard implementation of entanglement forging. Moreover, by controlling the amount of required resources, this scheme can be applied to larger, as well as non permutation invariant systems, where the latter constraint is associated with the Heisenberg forging procedure. We substan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;Q-learning&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#20048;&#35266;&#24615;&#35757;&#32451;&#21644;&#20462;&#25913;&#21518;&#30340;&#31574;&#30053;&#35299;&#20915;Q-learning&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#21644;&#31639;&#27861;&#25910;&#25947;&#21152;&#36895;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02632</link><description>&lt;p&gt;
Q-Learning &#30340;&#31283;&#23450;&#24615;&#36890;&#36807;&#35774;&#35745;&#21644;&#20048;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of Q-Learning Through Design and Optimism. (arXiv:2307.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;Q-learning&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#20048;&#35266;&#24615;&#35757;&#32451;&#21644;&#20462;&#25913;&#21518;&#30340;&#31574;&#30053;&#35299;&#20915;Q-learning&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#21644;&#31639;&#27861;&#25910;&#25947;&#21152;&#36895;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;20&#19990;&#32426;80&#24180;&#20195;Chris Watkins&#30340;&#35770;&#25991;&#20013;&#20171;&#32461;&#20197;&#26469;&#65292;Q-learning&#24050;&#25104;&#20026;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#37096;&#20998;&#26159;&#20851;&#20110;&#38543;&#26426;&#36924;&#36817;&#21644;Q-learning&#30340;&#25945;&#31243;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;INFORMS APS&#21457;&#24067;&#30340;&#31532;&#19968;&#23626;&#24212;&#29992;&#27010;&#29575;&#20449;&#25176;&#20840;&#20307;&#22823;&#20250;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#33021;&#21152;&#36895;&#25910;&#25947;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#20854;&#20182;&#35774;&#32622;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#12290;&#20004;&#20010;&#36129;&#29486;&#26159;&#20840;&#26032;&#30340;&#65306;1. Q-learning&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31283;&#23450;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#24453;&#30740;&#31350;&#30340;&#35805;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20048;&#35266;&#35757;&#32451;&#21644;&#20462;&#25913;&#21518;&#30340;Gibbs&#31574;&#30053;&#65292;&#21487;&#20197;&#23384;&#22312;&#28385;&#36275;&#25237;&#24433;Bellman&#26041;&#31243;&#30340;&#35299;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#26159;&#31283;&#23450;&#30340;&#65288;&#21442;&#25968;&#20272;&#35745;&#26377;&#30028;&#65289;&#12290;&#25910;&#25947;&#24615;&#20173;&#28982;&#26159;&#20247;&#22810;&#24453;&#30740;&#31350;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;2. &#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#22312;&#23567;&#25209;&#37327;&#25191;&#34892;&#20013;&#25913;&#21892;&#20102;&#36924;&#36817;&#31639;&#27861;&#30340;&#36845;&#20195;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.  The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:  1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.  2. The ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.02631</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20197;&#25903;&#25345;AML&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#26159;&#19968;&#31181;&#26368;&#20855;&#20405;&#30053;&#24615;&#30340;&#34880;&#28082;&#32959;&#30244;&#12290;&#20026;&#20102;&#25903;&#25345;&#19987;&#23478;&#20851;&#20110;&#21512;&#36866;&#27835;&#30103;&#30340;&#20915;&#31574;&#65292;AML&#24739;&#32773;&#26681;&#25454;&#20854;&#32454;&#32990;&#36951;&#20256;&#21644;&#20998;&#23376;&#29305;&#24449;&#33719;&#24471;&#39044;&#21518;&#20449;&#24687;&#65292;&#36890;&#24120;&#20998;&#20026;&#26377;&#21033;&#12289;&#20013;&#31561;&#21644;&#19981;&#21033;&#19977;&#20010;&#39118;&#38505;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#24050;&#30693;&#38382;&#39064;&#65292;&#22914;&#21516;&#19968;&#39118;&#38505;&#32452;&#20013;&#24739;&#32773;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#20013;&#39118;&#38505;&#31867;&#21035;&#30340;&#28165;&#26224;&#23450;&#20041;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;AML&#24739;&#32773;&#34987;&#24402;&#20026;&#20013;&#39118;&#38505;&#20998;&#31867;&#65292;&#19987;&#23478;&#24120;&#38656;&#36827;&#34892;&#20854;&#20182;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#23548;&#33268;&#27835;&#30103;&#24310;&#36831;&#21644;&#24739;&#32773;&#20020;&#24202;&#29366;&#20917;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#25454;&#20998;&#26512;&#21644;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#26681;&#25454;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable
&lt;/p&gt;</description></item><item><title>FLuID&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#36739;&#20302;&#35774;&#22791;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#65292;FLuID&#33021;&#26377;&#25928;&#22320;&#20943;&#36731;&#38459;&#22622;&#35774;&#22791;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.02623</link><description>&lt;p&gt;
FLuID: &#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38459;&#22622;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02623
&lt;/p&gt;
&lt;p&gt;
FLuID&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#36739;&#20302;&#35774;&#22791;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#65292;FLuID&#33021;&#26377;&#25928;&#22320;&#20943;&#36731;&#38459;&#22622;&#35774;&#22791;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20010;&#20307;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#21516;&#27493;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#20294;&#20063;&#30001;&#20110;&#19981;&#21516;&#35774;&#22791;&#30340;&#24615;&#33021;&#24046;&#24322;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#24322;&#26500;&#30340;&#35757;&#32451;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#22312;FL&#20013;&#65292;&#24615;&#33021;&#36739;&#20302;&#30340;&#38459;&#22622;&#35774;&#22791;&#32463;&#24120;&#20915;&#23450;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#26469;&#20943;&#36731;&#30001;&#20110;&#38459;&#22622;&#22120;&#20135;&#29983;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21464;&#24615;&#20002;&#22833;&#65292;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#26356;&#26032;&#38408;&#20540;&#25552;&#21462;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22312;&#27492;&#20002;&#22833;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;FLuID&#12290;FLuID&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#23376;&#27169;&#22411;&#25552;&#21462;&#26041;&#27861;&#26469;&#35843;&#33410;&#35745;&#31639;&#24378;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#38459;&#22622;&#35774;&#22791;&#30340;&#36127;&#36733;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02620</link><description>&lt;p&gt;
&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#35266;&#27979;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#12289;&#20379;&#26262;&#19982;&#21046;&#20919;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;-&#24863;&#30693;&#24490;&#29615;&#36890;&#24120;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#37117;&#21487;&#20197;&#33719;&#24471;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#27979;&#37327;&#65292;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#28023;&#21644;&#34892;&#26143;&#26426;&#22120;&#20154;&#25506;&#32034;&#12289;&#26448;&#26009;&#35774;&#35745;&#21644;&#21307;&#23398;&#31561;&#24212;&#29992;&#20013;&#65292;&#27979;&#37327;&#25110;&#32773;&#36817;&#20284;&#29615;&#22659;&#29366;&#24577;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36817;&#26469;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#37319;&#21462;&#20102;RL&#20195;&#29702;&#21487;&#33021;&#19981;&#38656;&#35201;&#25110;&#32773;&#19981;&#24819;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36827;&#34892;&#26114;&#36149;&#27979;&#37327;&#30340;&#35266;&#28857;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Dynamic Multi-Step Observationless Agent (DMSOA)&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#22312;OpenAI gym&#21644;Atari Pong&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#21644;&#28176;&#36827;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#36807;&#31243;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.02615</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#21551;&#21457;&#30340;&#28176;&#36827;&#23545;&#40784;&#21644;&#27604;&#36739;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition. (arXiv:2307.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#21644;&#28176;&#36827;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#36807;&#31243;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#21463;&#30417;&#30563;&#21644;&#25345;&#32493;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#20511;&#37492;&#20102;&#20154;&#31867;&#23156;&#20799;&#20064;&#24471;&#31532;&#19968;&#38376;&#35821;&#35328;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#21463;&#35748;&#30693;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#20351;&#35745;&#31639;&#27169;&#22411;&#33021;&#22815;&#27604;&#36739;&#21508;&#31181;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#23398;&#20064;&#36807;&#28388;&#20986;&#24182;&#25552;&#21462;&#20849;&#21516;&#30340;&#20449;&#24687;&#29992;&#20110;&#27599;&#20010;&#20849;&#20139;&#30340;&#35821;&#35328;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#35789;&#27719;&#33719;&#21462;&#26694;&#26550;&#23450;&#20041;&#20026;&#26082;&#21253;&#25324;&#20449;&#24687;&#36807;&#28388;&#36807;&#31243;&#65292;&#20063;&#21253;&#25324;&#34920;&#24449;-&#31526;&#21495;&#26144;&#23556;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25345;&#32493;&#39640;&#25928;&#22320;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label. We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping. This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently. Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;"&#25903;&#25345;&#22806;"&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#21152;&#27861;&#35299;&#30721;&#22120;&#33021;&#22815;&#23545;&#28508;&#21464;&#37327;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02598</link><description>&lt;p&gt;
&#28155;&#21152;&#35299;&#30721;&#22120;&#29992;&#20110;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;&#31515;&#21345;&#23572;&#31215;&#25512;&#31639;
&lt;/p&gt;
&lt;p&gt;
Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. (arXiv:2307.02598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02598
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;"&#25903;&#25345;&#22806;"&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#21152;&#27861;&#35299;&#30721;&#22120;&#33021;&#22815;&#23545;&#28508;&#21464;&#37327;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;&#8220;&#25903;&#25345;&#22806;&#8221;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31867;&#25105;&#20204;&#31216;&#20026;&#8220;&#21152;&#27861;&#8221;&#30340;&#35299;&#30721;&#22120;&#20013;&#65292;&#36825;&#20004;&#32773;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#20123;&#35299;&#30721;&#22120;&#31867;&#20284;&#20110;&#29992;&#20110;&#38754;&#21521;&#23545;&#35937;&#34920;&#31034;&#23398;&#20064;&#65288;OCRL&#65289;&#30340;&#35299;&#30721;&#22120;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#35299;&#20026;&#22810;&#20010;&#29305;&#23450;&#23545;&#35937;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20351;&#29992;&#21152;&#27861;&#35299;&#30721;&#22120;&#23436;&#20840;&#35299;&#20915;&#37325;&#26500;&#38382;&#39064;&#26102;&#65292;&#23545;&#28508;&#21464;&#37327;&#22359;&#36827;&#34892;&#20102;&#32622;&#25442;&#21644;&#22359;&#29366;&#36870;&#21464;&#25442;&#30340;&#35782;&#21035;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#20445;&#35777;&#20165;&#22522;&#20110;&#20851;&#20110;&#28508;&#22240;&#23376;&#20998;&#24067;&#30340;&#38750;&#24120;&#24369;&#30340;&#20551;&#35774;&#65292;&#28508;&#22240;&#23376;&#21487;&#33021;&#23384;&#22312;&#32479;&#35745;&#20381;&#36182;&#24182;&#19988;&#20855;&#26377;&#20960;&#20046;&#20219;&#24847;&#24418;&#29366;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21487;&#33021;&#24615;&#30340;&#26032;&#35774;&#32622;&#65292;&#24182;&#19988;&#22686;&#21152;&#20102;&#25105;&#20204;&#23545;OCRL&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21152;&#27861;&#35299;&#30721;&#22120;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can 
&lt;/p&gt;</description></item><item><title>TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.02588</link><description>&lt;p&gt;
TransformerG2G&#65306;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#23398;&#20064;&#26102;&#24577;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02588
&lt;/p&gt;
&lt;p&gt;
TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#24050;&#25104;&#20026;&#22788;&#29702;&#19981;&#21516;&#26102;&#38388;&#22270;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#38142;&#36335;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#22270;&#29983;&#25104;&#65289;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20123;&#26102;&#24577;&#22270;&#23637;&#29616;&#20102;&#24322;&#36136;&#30340;&#30636;&#26102;&#21160;&#24577;&#12289;&#19981;&#21516;&#30340;&#26102;&#38388;&#38388;&#38548;&#20197;&#21450;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#39640;&#24230;&#21464;&#21270;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23558;&#21382;&#21490;&#22270;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#34701;&#20837;&#21040;&#23398;&#20064;&#26102;&#24577;&#21160;&#24577;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;TransformerG2G&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#20174;&#24403;&#21069;&#29366;&#24577;&#65288;$t$&#65289;&#21644;&#20043;&#21069;&#30340;&#19978;&#19979;&#25991;&#65288;&#26102;&#38388;&#25139;[$t-1, t-l$]&#65292;$l$&#26159;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#65289;&#20013;&#39318;&#20808;&#23398;&#20064;&#20013;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#25237;&#24433;&#23618;&#26469;&#29983;&#25104;&#27599;&#20010;&#33410;&#28857;&#30340;&#20302;&#32500;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#65292;&#20316;&#20026;&#20854;&#28508;&#22312;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#20135;&#21697;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#31561;&#22810;&#31181;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#22312;&#22823;&#22411;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02578</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#26159;&#33391;&#22909;&#30340;&#20135;&#21697;&#38656;&#27714;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters. (arXiv:2307.02578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#20135;&#21697;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#31561;&#22810;&#31181;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#22312;&#22823;&#22411;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#38656;&#27714;&#39044;&#27979;&#26088;&#22312;&#21033;&#29992;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#20135;&#21697;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#12289;&#22522;&#20110;&#22270;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#22810;&#27169;&#24577;&#20135;&#21697;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#38656;&#27714;&#39044;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#21382;&#21490;&#38656;&#27714;&#12289;&#20135;&#21697;&#31867;&#21035;&#21644;&#20854;&#20182;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#23395;&#33410;&#24615;&#21644;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20026;&#29305;&#23450;&#20135;&#21697;&#25552;&#20379;&#36275;&#22815;&#30340;&#21382;&#21490;&#25968;&#25454;&#20043;&#21069;&#24456;&#38590;&#39044;&#27979;&#20135;&#21697;&#38656;&#27714;&#65292;&#20197;&#21450;&#23427;&#20204;&#26080;&#27861;&#22949;&#21892;&#22788;&#29702;&#31867;&#21035;&#21160;&#24577;&#24615;&#12290;&#36890;&#36807;&#34701;&#21512;&#20135;&#21697;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#24182;&#36229;&#36234;&#23427;&#20204;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#39044;&#27979;&#20102;&#21508;&#31181;&#20135;&#21697;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal demand forecasting aims at predicting product demand utilizing visual, textual, and contextual information. This paper proposes a method for multimodal product demand forecasting using convolutional, graph-based, and transformer-based architectures. Traditional approaches to demand forecasting rely on historical demand, product categories, and additional contextual information such as seasonality and events. However, these approaches have several shortcomings, such as the cold start problem making it difficult to predict product demand until sufficient historical data is available for a particular product, and their inability to properly deal with category dynamics. By incorporating multimodal information, such as product images and textual descriptions, our architecture aims to address the shortcomings of traditional approaches and outperform them. The experiments conducted on a large real-world dataset show that the proposed approach effectively predicts demand for a wide 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;11&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#22312;&#38750;&#27954;&#20892;&#30000;&#20998;&#31867;&#21644;&#22522;&#20110;&#21355;&#26143;&#22320;&#29699;&#35266;&#27979;&#30340;&#20892;&#19994;&#30417;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#20854;&#38656;&#27714;&#30340;&#22320;&#22270;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#24037;&#20316;&#25913;&#36827;&#22320;&#22270;&#30340;&#19968;&#33268;&#24615;&#21644;&#20302;&#31934;&#24230;&#21306;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02575</link><description>&lt;p&gt;
&#29616;&#26377;&#30340;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#20892;&#19994;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#30340;&#20934;&#30830;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?. (arXiv:2307.02575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;11&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#22312;&#38750;&#27954;&#20892;&#30000;&#20998;&#31867;&#21644;&#22522;&#20110;&#21355;&#26143;&#22320;&#29699;&#35266;&#27979;&#30340;&#20892;&#19994;&#30417;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#20854;&#38656;&#27714;&#30340;&#22320;&#22270;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#24037;&#20316;&#25913;&#36827;&#22320;&#22270;&#30340;&#19968;&#33268;&#24615;&#21644;&#20302;&#31934;&#24230;&#21306;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21487;&#20197;&#25552;&#20379;&#32463;&#27982;&#23454;&#24800;&#21644;&#21450;&#26102;&#30340;&#20449;&#24687;&#26469;&#35780;&#20272;&#20316;&#29289;&#29366;&#20917;&#21644;&#31918;&#39135;&#29983;&#20135;&#12290;&#22312;&#38750;&#27954;&#65292;&#36825;&#26679;&#30340;&#30417;&#27979;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#37027;&#37324;&#23384;&#22312;&#31918;&#39135;&#19981;&#23433;&#20840;&#21644;&#32570;&#20047;&#20892;&#19994;&#32479;&#35745;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;EO&#30340;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#20934;&#30830;&#30340;&#20892;&#30000;&#22320;&#22270;&#26469;&#25552;&#20379;&#26377;&#20851;&#20892;&#30000;&#30340;&#20449;&#24687;&#65292;&#20294;&#26159;&#32570;&#20047;&#25968;&#25454;&#26469;&#30830;&#23450;&#21738;&#20123;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#26368;&#20934;&#30830;&#22320;&#35782;&#21035;&#38750;&#27954;&#22269;&#23478;&#30340;&#20892;&#30000;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;8&#20010;&#22269;&#23478;&#30340;&#32479;&#35745;&#20005;&#35880;&#30340;&#21442;&#32771;&#25968;&#25454;&#65292;&#23545;11&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22303;&#22320;&#35206;&#30422;&#22320;&#22270;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#20892;&#30000;&#20998;&#31867;&#21644;&#22522;&#20110;EO&#30340;&#38750;&#27954;&#20892;&#19994;&#30417;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#30830;&#23450;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#22320;&#22270;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#30340;&#24037;&#20316;&#38598;&#20013;&#35299;&#20915;&#22320;&#22270;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#24182;&#25552;&#39640;&#20302;&#31934;&#24230;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite Earth observations (EO) can provide affordable and timely information for assessing crop conditions and food production. Such monitoring systems are essential in Africa, where there is high food insecurity and sparse agricultural statistics. EO-based monitoring systems require accurate cropland maps to provide information about croplands, but there is a lack of data to determine which of the many available land cover maps most accurately identify cropland in African countries. This study provides a quantitative evaluation and intercomparison of 11 publicly available land cover maps to assess their suitability for cropland classification and EO-based agriculture monitoring in Africa using statistically rigorous reference datasets from 8 countries. We hope the results of this study will help users determine the most suitable map for their needs and encourage future work to focus on resolving inconsistencies between maps and improving accuracy in low-accuracy regions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34903;&#26223;&#22270;&#20687;&#21644;OpenStreetMap&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20272;&#35745;&#24314;&#31569;&#29289;&#30340;&#39640;&#24230;&#65292;&#24182;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;3D&#22478;&#24066;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.02574</link><description>&lt;p&gt;
&#22522;&#20110;&#34903;&#26223;&#22270;&#20687;&#21644;OpenStreetMap&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#24314;&#31569;&#39640;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation. (arXiv:2307.02574v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34903;&#26223;&#22270;&#20687;&#21644;OpenStreetMap&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20272;&#35745;&#24314;&#31569;&#29289;&#30340;&#39640;&#24230;&#65292;&#24182;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;3D&#22478;&#24066;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24314;&#31569;&#39640;&#24230;&#20272;&#35745;&#26159;&#20174;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#33258;&#21160;&#25512;&#23548;&#20986;3D&#22478;&#24066;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#24535;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#22522;&#20110;&#20302;&#25104;&#26412;VGI&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#24314;&#31569;&#39640;&#24230;&#33258;&#21160;&#20272;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;VGI&#25968;&#25454;&#24179;&#21488;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;OpenStreetMap&#65288;OSM&#65289;&#21644;&#20247;&#21253;&#34903;&#26223;&#22270;&#20687;&#65288;SVI&#65289;&#65292;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mapillary SVI&#21644;OSM&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20302;&#25104;&#26412;&#21644;&#24320;&#28304;&#30340;LoD1 3D&#22478;&#24066;&#27169;&#22411;&#12290;&#25152;&#25552;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SSL&#27169;&#24335;&#65292;&#21487;&#20197;&#22312;&#30417;&#30563;&#22238;&#24402;&#36807;&#31243;&#20013;&#35774;&#32622;&#19981;&#21516;&#27604;&#20363;&#30340;&#8220;&#20266;&#26631;&#31614;&#8221;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#20174;OSM&#25968;&#25454;&#65288;&#21363;&#24314;&#31569;&#29289;&#21644;&#34903;&#36947;&#65289;&#20013;&#25552;&#21462;&#22810;&#32423;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#29992;&#20110;&#25512;&#26029;&#24314;&#31569;&#29289;&#39640;&#24230;&#65307;
&lt;/p&gt;
&lt;p&gt;
Accurate building height estimation is key to the automatic derivation of 3D city models from emerging big geospatial data, including Volunteered Geographical Information (VGI). However, an automatic solution for large-scale building height estimation based on low-cost VGI data is currently missing. The fast development of VGI data platforms, especially OpenStreetMap (OSM) and crowdsourced street-view images (SVI), offers a stimulating opportunity to fill this research gap. In this work, we propose a semi-supervised learning (SSL) method of automatically estimating building height from Mapillary SVI and OSM data to generate low-cost and open-source 3D city modeling in LoD1. The proposed method consists of three parts: first, we propose an SSL schema with the option of setting a different ratio of "pseudo label" during the supervised regression; second, we extract multi-level morphometric features from OSM data (i.e., buildings and streets) for the purposed of inferring building height;
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#39640;&#32500;&#38382;&#39064;&#20013;&#20195;&#29702;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;Korhunen-Lo\'{e}ve&#23637;&#24320;&#26469;&#34920;&#31034;&#21442;&#25968;&#22330;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23454;&#29616;&#23545;&#30452;&#25509;&#27979;&#37327;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2307.02572</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;Korhunen-Lo\'{e}ve&#22238;&#24402;&#27169;&#22411;&#30340;&#22522;&#20934;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#65306;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#36870;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Conditional Korhunen-Lo\'{e}ve regression model with Basis Adaptation for high-dimensional problems: uncertainty quantification and inverse modeling. (arXiv:2307.02572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#39640;&#32500;&#38382;&#39064;&#20013;&#20195;&#29702;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;Korhunen-Lo\'{e}ve&#23637;&#24320;&#26469;&#34920;&#31034;&#21442;&#25968;&#22330;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23454;&#29616;&#23545;&#30452;&#25509;&#27979;&#37327;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#29289;&#29702;&#31995;&#32479;&#30340;&#21487;&#35266;&#23519;&#21709;&#24212;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#29289;&#29702;&#31995;&#32479;&#30340;&#21709;&#24212;&#26159;&#20854;&#31354;&#38388;&#24322;&#36136;&#21442;&#25968;&#22330;&#30340;&#20989;&#25968;&#65292;&#24212;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21442;&#25968;&#20272;&#35745;&#12290;&#24403;&#21442;&#25968;&#22330;&#30340;&#30452;&#25509;&#27979;&#37327;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;Korhunen-Lo\'{e}ve&#23637;&#24320;&#65288;CKLEs&#65289;&#26469;&#34920;&#31034;&#21442;&#25968;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;CKLEs&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#36890;&#36807;&#26465;&#20214;&#23637;&#24320;&#30340;&#21327;&#26041;&#24046;&#26680;&#23545;&#30452;&#25509;&#27979;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for improving the accuracy of surrogate models of the observable response of physical systems as a function of the systems' spatially heterogeneous parameter fields with applications to uncertainty quantification and parameter estimation in high-dimensional problems. Practitioners often formulate finite-dimensional representations of spatially heterogeneous parameter fields using truncated unconditional Karhunen-Lo\'{e}ve expansions (KLEs) for a certain choice of unconditional covariance kernel and construct surrogate models of the observable response with respect to the random variables in the KLE. When direct measurements of the parameter fields are available, we propose improving the accuracy of these surrogate models by representing the parameter fields via conditional Karhunen-Lo\'{e}ve expansions (CKLEs). CKLEs are constructed by conditioning the covariance kernel of the unconditional expansion on the direct measurements via Gaussian process regression an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#26032;&#30340;&#36817;&#20284;&#25110;&#19978;&#30028;&#26469;&#34913;&#37327;&#22522;&#20110;&#22238;&#24402;&#30340;&#27979;&#35797;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#26041;&#27861;RBPT&#65292;&#23545;&#27169;&#22411;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02520</link><description>&lt;p&gt;
&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Conditional independence testing under model misspecification. (arXiv:2307.02520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#26032;&#30340;&#36817;&#20284;&#25110;&#19978;&#30028;&#26469;&#34913;&#37327;&#22522;&#20110;&#22238;&#24402;&#30340;&#27979;&#35797;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#26041;&#27861;RBPT&#65292;&#23545;&#27169;&#22411;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#26816;&#39564;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#29616;&#20195;&#30340;CI&#26816;&#39564;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#22238;&#24402;&#20989;&#25968;&#25110;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20934;&#30830;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#25110;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#26102;&#20445;&#35777;&#20102;&#25511;&#21046;&#31532;&#19968;&#31867;&#38169;&#35823;&#65292;&#20294;&#23427;&#20204;&#22312;&#27169;&#22411;&#38169;&#35823;&#23548;&#33268;&#22833;&#36133;&#26102;&#30340;&#34892;&#20026;&#23578;&#19981;&#28165;&#26970;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35762;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#36890;&#29992;&#36924;&#36817;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#27169;&#22411;&#38169;&#35823;&#20063;&#21487;&#33021;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#36817;&#20284;&#25110;&#19978;&#30028;&#26469;&#34913;&#37327;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#19977;&#20010;&#22522;&#20110;&#22238;&#24402;&#30340;&#27979;&#35797;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rao-Blackwellized Predictor Test&#65288;RBPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#65292;&#23545;&#27169;&#22411;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional independence (CI) testing is fundamental and challenging in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step. Although the methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors, their behavior is less understood when they fail due to model misspecification. In a broader sense, model misspecification can arise even when universal approximators (e.g., deep neural nets) are employed. Then, we study the performance of regression-based CI tests under model misspecification. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a novel regression-based CI test robust against model mis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#34920;&#24449;&#24046;&#24322;&#24615;&#26469;&#38477;&#20302;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#24120;&#35265;&#22833;&#36133;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#26550;&#26500;&#20043;&#38388;&#19981;&#21516;&#28145;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#24046;&#24322;&#24615;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#22833;&#36133;&#27169;&#24335;&#30340;&#24378;&#22823;&#38598;&#21512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38598;&#21512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02516</link><description>&lt;p&gt;
&#25506;&#32034;&#26032;&#30340;&#26041;&#27861;&#65306;&#24378;&#21270;&#34920;&#24449;&#24046;&#24322;&#20197;&#23398;&#20064;&#26032;&#29305;&#24449;&#24182;&#38477;&#20302;&#38169;&#35823;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency. (arXiv:2307.02516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#34920;&#24449;&#24046;&#24322;&#24615;&#26469;&#38477;&#20302;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#24120;&#35265;&#22833;&#36133;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#26550;&#26500;&#20043;&#38388;&#19981;&#21516;&#28145;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#24046;&#24322;&#24615;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#22833;&#36133;&#27169;&#24335;&#30340;&#24378;&#22823;&#38598;&#21512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38598;&#21512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#22312;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#30456;&#20851;&#24615;&#21644;&#24120;&#35265;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#20197;&#24448;&#30340;&#23581;&#35797;&#30528;&#37325;&#20110;&#20943;&#23567;&#36755;&#20986;&#39044;&#27979;&#25110;logits&#30340;&#30456;&#20851;&#24615;&#65292;&#32467;&#26524;&#20135;&#29983;&#20102;&#19981;&#19968;&#33268;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65292;&#21363;&#21033;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#24046;&#24322;&#24615;&#65292;&#32780;&#19981;&#26159;&#34913;&#37327;&#35757;&#32451;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#26550;&#26500;&#20043;&#38388;&#19981;&#21516;&#28145;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#30340;&#24046;&#24322;&#24615;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#22833;&#36133;&#27169;&#24335;&#30340;&#24378;&#22823;&#38598;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#39640;&#24230;&#24046;&#24322;&#30340;&#20013;&#38388;&#34920;&#31034;&#23548;&#33268;&#26356;&#23569;&#30456;&#20851;&#30340;&#36755;&#20986;&#39044;&#27979;&#21644;&#31245;&#24494;&#36739;&#20302;&#30340;&#38169;&#35823;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#20102;&#36830;&#25509;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independently trained machine learning models tend to learn similar features. Given an ensemble of independently trained models, this results in correlated predictions and common failure modes. Previous attempts focusing on decorrelation of output predictions or logits yielded mixed results, particularly due to their reduction in model accuracy caused by conflicting optimization objectives. In this paper, we propose the novel idea of utilizing methods of the representational similarity field to promote dissimilarity during training instead of measuring similarity of trained models. To this end, we promote intermediate representations to be dissimilar at different depths between architectures, with the goal of learning robust ensembles with disjoint failure modes. We show that highly dissimilar intermediate representations result in less correlated output predictions and slightly lower error consistency, resulting in higher ensemble accuracy. With this, we shine first light on the conne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02511</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27004;&#23618;&#24179;&#38754;&#31034;&#20363;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Computational Design at the Example of Floor Plans. (arXiv:2307.02511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#29983;&#25104;&#22120;&#22240;&#20854;&#33021;&#22815;&#26681;&#25454;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#20294;&#26159;&#65292;&#22312;&#22303;&#26408;&#24037;&#31243;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#38656;&#35201;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#21019;&#24314;&#29305;&#23450;&#30340;&#24314;&#31569;&#35774;&#35745;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#27004;&#23618;&#24179;&#38754;&#20316;&#20026;&#31034;&#20363;&#65292;&#25506;&#32034;&#22522;&#20110;&#25193;&#25955;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30446;&#21069;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#20174;6%&#25552;&#39640;&#21040;90%&#65292;&#24182;&#25913;&#36827;&#20102;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#20026;&#22303;&#26408;&#24037;&#31243;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#21040;&#21512;&#24182;&#26641;&#30340;Wasserstein&#24230;&#37327;&#31354;&#38388;&#12290;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;</title><link>http://arxiv.org/abs/2307.02509</link><description>&lt;p&gt;
Wasserstein&#33258;&#32534;&#30721;&#21512;&#24182;&#26641;&#65288;&#21644;&#25345;&#32493;&#22270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#21040;&#21512;&#24182;&#26641;&#30340;Wasserstein&#24230;&#37327;&#31354;&#38388;&#12290;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#26641;&#30340;Wasserstein&#33258;&#32534;&#30721;(MT-WAE)&#65292;&#36825;&#26159;&#23558;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25193;&#23637;&#21040;&#21512;&#24182;&#26641;&#30340;Wasserstein&#24230;&#37327;&#31354;&#38388;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;&#19982;&#25805;&#20316;&#21521;&#37327;&#21270;&#25968;&#25454;&#30340;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#26126;&#30830;&#22320;&#25805;&#20316;&#21512;&#24182;&#26641;&#30340;&#20851;&#32852;&#24230;&#37327;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#21069;&#26399;&#32447;&#24615;&#23581;&#35797;[65]&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#23427;&#20063;&#24456;&#23481;&#26131;&#25193;&#23637;&#21040;&#25345;&#32493;&#22270;&#12290;&#23545;&#20844;&#20849;&#36830;&#38145;&#21453;&#24212;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;MT-WAE&#30340;&#35745;&#31639;&#24179;&#22343;&#26102;&#38388;&#20165;&#20026;&#20960;&#20998;&#38047;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#26469;&#23637;&#31034;&#65292;&#36825;&#20123;&#24212;&#29992;&#26159;&#22522;&#20110;&#20197;&#21069;&#20851;&#20110;&#21512;&#24182;&#26641;&#32534;&#30721;&#30340;&#24037;&#20316;[65]&#36827;&#34892;&#35843;&#25972;&#24471;&#21040;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;MT-WAE&#24212;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#21487;&#38752;&#22320;&#21387;&#32553;&#21512;&#24182;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02507</link><description>&lt;p&gt;
STS-CCL&#65306;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#37319;&#29992;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#29992;&#20110;&#26102;&#31354;&#22270;&#25968;&#25454;&#30340;&#22522;&#26412;&#21644;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25200;&#21160;&#20102;&#22270;&#32467;&#26500;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#36827;&#34892;&#33258;&#36866;&#24212;&#22686;&#24378;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#31354;&#21516;&#27493;&#23545;&#27604;&#27169;&#22359;&#65288;STS-CM&#65289;&#65292;&#20197;&#21516;&#26102;&#25429;&#25417;&#33391;&#22909;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#24182;&#23454;&#29616;&#22270;&#32423;&#23545;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21306;&#20998;&#36127;&#31579;&#36873;&#20013;&#30340;&#33410;&#28857;&#20010;&#20307;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#29305;&#24449;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#20551;&#35774;&#31867;&#30340;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#26469;&#25511;&#21046;&#27867;&#21270;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26377;&#38480;&#20998;&#24418;&#32500;&#24230;&#33719;&#24471;&#20102;&#26032;&#30340;&#30028;&#38480;&#65292;&#24182;&#31616;&#21270;&#20102;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26080;&#32500;&#24230;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2307.02501</link><description>&lt;p&gt;
&#36890;&#36807;&#31639;&#27861;&#30456;&#20851;&#30340;Rademacher&#22797;&#26434;&#24230;&#23454;&#29616;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generalization Guarantees via Algorithm-dependent Rademacher Complexity. (arXiv:2307.02501v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#20551;&#35774;&#31867;&#30340;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#26469;&#25511;&#21046;&#27867;&#21270;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26377;&#38480;&#20998;&#24418;&#32500;&#24230;&#33719;&#24471;&#20102;&#26032;&#30340;&#30028;&#38480;&#65292;&#24182;&#31616;&#21270;&#20102;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26080;&#32500;&#24230;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#34892;&#20026;&#38656;&#35201;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#30028;&#38480;&#26469;&#35299;&#37322;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#23384;&#22312;&#30528;&#28041;&#21450;(&#21508;&#31181;&#24418;&#24335;&#30340;)&#20114;&#20449;&#24687;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#20197;&#21450;&#22522;&#20110;&#20551;&#35774;&#38598;&#31283;&#23450;&#24615;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#25216;&#26415;&#19978;&#19981;&#21516;&#20294;&#22312;&#27010;&#24565;&#19978;&#30456;&#20851;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#26469;&#25511;&#21046;&#27867;&#21270;&#38169;&#35823;&#65292;&#21363;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#20551;&#35774;&#31867;&#30340;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#12290;&#32467;&#21512;Rademacher&#22797;&#26434;&#24230;&#30340;&#26631;&#20934;&#23646;&#24615;&#21644;&#35813;&#31867;&#30340;&#20415;&#25463;&#32467;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#65306;(i)&#22522;&#20110;&#26377;&#38480;&#20998;&#24418;&#32500;&#24230;&#33719;&#24471;&#26032;&#30340;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#23558;&#21069;&#20154;&#24037;&#20316;&#20013;&#30340;&#20998;&#24418;&#32500;&#24230;&#30028;&#38480;&#20174;&#36830;&#32493;&#30340;&#20551;&#35774;&#31867;&#25512;&#24191;&#21040;&#26377;&#38480;&#20551;&#35774;&#31867;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#38656;&#35201;&#30340;&#20114;&#20449;&#24687;&#39033;&#65307;(ii)&#22823;&#22823;&#31616;&#21270;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#38024;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26080;&#32500;&#24230;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#25991;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#36125;&#21494;&#26031;&#24341;&#23548;&#30340;&#22810;&#20803;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#21306;&#22495;&#21270;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#31354;&#38388;&#20998;&#24067;&#30340;&#27700;&#25991;&#21442;&#25968;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#21487;&#34892;&#35299;&#30340;&#31561;&#20284;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02497</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27700;&#25991;&#21464;&#20998;&#25968;&#25454;&#21516;&#21270;&#65306;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#36125;&#21494;&#26031;&#24341;&#23548;&#30340;&#22810;&#20803;&#22238;&#24402;&#36827;&#34892;&#21306;&#22495;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression. (arXiv:2307.02497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#25991;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#36125;&#21494;&#26031;&#24341;&#23548;&#30340;&#22810;&#20803;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#21306;&#22495;&#21270;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#31354;&#38388;&#20998;&#24067;&#30340;&#27700;&#25991;&#21442;&#25968;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#21487;&#34892;&#35299;&#30340;&#31561;&#20284;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20272;&#35745;&#31354;&#38388;&#20998;&#24067;&#30340;&#27700;&#25991;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#26410;&#27979;&#37327;&#27700;&#36947;&#19978;&#30340;&#27946;&#27700;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32541;&#21306;&#22495;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#30340;&#22797;&#26434;&#21306;&#22495;&#36716;&#31227;&#20989;&#25968;&#12290;&#35813;&#36716;&#31227;&#20989;&#25968;&#20381;&#36182;&#20110;&#65306;&#65288;i&#65289;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#36890;&#36807;&#26080;&#32541;&#30340;&#26799;&#24230;&#35745;&#31639;&#27969;&#21160;&#26469;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#65292;&#25110;&#32773;&#65288;ii&#65289;&#19968;&#20010;&#22810;&#20803;&#22238;&#24402;&#65292;&#30001;&#21464;&#20998;&#25968;&#25454;&#21516;&#21270;&#31639;&#27861;&#20248;&#21270;&#24182;&#30001;&#36125;&#21494;&#26031;&#20272;&#35745;&#24341;&#23548;&#65292;&#35299;&#20915;&#21487;&#34892;&#35299;&#30340;&#31561;&#20284;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#21487;&#25512;&#26029;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#32435;&#20837;&#21040;&#21487;&#24494;&#20998;&#30340;&#27700;&#25991;&#27169;&#22411;&#20013;&#65292;&#24182;&#21033;&#29992;&#31934;&#30830;&#30340;&#22522;&#20110;&#20276;&#38543;&#30340;&#31354;&#38388;&#20998;&#24067;&#26799;&#24230;&#35745;&#31639;&#30340;&#22810;&#27979;&#37327;&#25968;&#25454;&#19978;&#35745;&#31639;&#30340;&#20195;&#20215;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tackling the difficult problem of estimating spatially distributed hydrological parameters, especially for floods on ungauged watercourses, this contribution presents a novel seamless regionalization technique for learning complex regional transfer functions designed for high-resolution hydrological models. The transfer functions rely on: (i) a multilayer perceptron enabling a seamless flow of gradient computation to employ machine learning optimization algorithms, or (ii) a multivariate regression mapping optimized by variational data assimilation algorithms and guided by Bayesian estimation, addressing the equifinality issue of feasible solutions. The approach involves incorporating the inferable regionalization mappings into a differentiable hydrological model and optimizing a cost function computed on multi-gauge data with accurate adjoint-based spatially distributed gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#37325;&#24314;&#30005;&#35299;&#36807;&#31243;&#20013;&#30340;&#27668;&#27873;&#20998;&#24067;&#21644;&#30005;&#23548;&#29575;&#22270;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02496</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#23398;&#20064;&#21033;&#29992;&#23548;&#30005;&#22270;&#37325;&#24314;&#27668;&#27873;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#37325;&#24314;&#30005;&#35299;&#36807;&#31243;&#20013;&#30340;&#27668;&#27873;&#20998;&#24067;&#21644;&#30005;&#23548;&#29575;&#22270;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#35299;&#26159;&#29615;&#20445;&#30340;&#27682;&#27668;&#29983;&#20135;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#65292;&#20294;&#26159;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#27668;&#27873;&#20250;&#38459;&#30861;&#21453;&#24212;&#65292;&#38477;&#20302;&#30005;&#27744;&#25928;&#29575;&#65292;&#22686;&#21152;&#33021;&#37327;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27668;&#27873;&#20250;&#23548;&#33268;&#30005;&#27744;&#20869;&#37096;&#30340;&#30005;&#23548;&#29575;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#21608;&#22260;&#20135;&#29983;&#24863;&#24212;&#30913;&#22330;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#22806;&#37096;&#30913;&#20256;&#24863;&#22120;&#27979;&#37327;&#36825;&#20123;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#24182;&#27714;&#35299;Biot-Savart&#23450;&#24459;&#30340;&#21453;&#38382;&#39064;&#65292;&#21487;&#20197;&#20272;&#35745;&#30005;&#27744;&#20869;&#30340;&#30005;&#23548;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#27668;&#27873;&#30340;&#22823;&#23567;&#21644;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#20960;&#20010;&#24863;&#24212;&#30913;&#22330;&#27979;&#37327;&#20540;&#30830;&#23450;&#39640;&#20998;&#36776;&#29575;&#30005;&#23548;&#29575;&#22270;&#26159;&#19968;&#20010;&#30149;&#24577;&#21453;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#37325;&#24314;&#30005;&#23548;&#29575;&#22330;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#32467;&#26524;&#21644;&#20351;&#29992;&#38543;&#26426;&#35823;&#24046;&#25193;&#25955;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;Tikhonov&#27491;&#21017;&#21270;&#30456;&#27604;&#65292;INN&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#22330;&#26223;TFDA&#65292;&#21363;&#19977;&#26080;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30446;&#26631;&#26631;&#31614;&#12289;&#28304;&#25968;&#25454;&#21644;&#39046;&#22495;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#23454;&#29992;&#65292;&#36991;&#20813;&#20102;&#23545;&#20808;&#21069;&#39046;&#22495;&#20449;&#24687;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02493</link><description>&lt;p&gt;
FREEDOM: &#26080;&#30446;&#26631;&#26631;&#31614;&#12289;&#26080;&#28304;&#25968;&#25454;&#21644;&#26080;&#39046;&#22495;&#20449;&#24687;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization. (arXiv:2307.02493v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#22330;&#26223;TFDA&#65292;&#21363;&#19977;&#26080;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30446;&#26631;&#26631;&#31614;&#12289;&#28304;&#25968;&#25454;&#21644;&#39046;&#22495;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#23454;&#29992;&#65292;&#36991;&#20813;&#20102;&#23545;&#20808;&#21069;&#39046;&#22495;&#20449;&#24687;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26381;&#21153;&#35282;&#24230;&#26469;&#30475;&#65292;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#26159;&#19968;&#31181;&#36866;&#24212;&#37096;&#32626;&#27169;&#22411;&#21040;&#23458;&#25143;&#25968;&#25454;&#38598;&#30340;&#26377;&#24076;&#26395;&#30340;&#22330;&#26223;&#12290;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#30446;&#26631;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#36866;&#24212;&#65292;&#24182;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#26500;&#24314;&#30340;&#28304;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#39046;&#22495;&#20449;&#24687;&#65292;&#21363;&#26377;&#22810;&#23569;&#20010;&#39046;&#22495;&#23384;&#22312;&#20197;&#21450;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;MSDA&#38656;&#35201;&#21516;&#26102;&#25317;&#26377;&#28304;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#65288;&#29289;&#29702;&#19978;&#65289;&#65292;&#36825;&#20250;&#23548;&#33268;&#23458;&#25143;&#35774;&#22791;&#23384;&#20648;&#38480;&#21046;&#25110;&#36890;&#36807;&#23558;&#23458;&#25143;&#25968;&#25454;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#24341;&#36215;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#23454;&#38469;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#22330;&#26223;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#22330;&#26223;&#65292;&#21363;&#19977;&#26080;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;TFDA&#65289;&#65292;&#20854;&#20013;1&#65289;&#30446;&#26631;&#26631;&#31614;&#65292;2&#65289;&#28304;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#22823;&#37096;&#20998;3&#65289;&#28304;&#39046;&#22495;&#20449;&#24687;&#65288;&#39046;&#22495;&#26631;&#31614;+&#39046;&#22495;&#25968;&#37327;&#65289;&#37117;&#19981;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;TablEye&#65292;&#36890;&#36807;&#29983;&#25104;&#34920;&#26684;&#22270;&#20687;&#26469;&#23454;&#29616;&#39046;&#22495;&#36716;&#25442;&#65292;&#20811;&#26381;&#20102;&#24418;&#25104;&#34920;&#26684;&#25968;&#25454;&#20808;&#39564;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02491</link><description>&lt;p&gt;
TablEye: &#36890;&#36807;&#22270;&#20687;&#35270;&#35282;&#30475;&#23567;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
TablEye: Seeing small Tables through the Lens of Images. (arXiv:2307.02491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;TablEye&#65292;&#36890;&#36807;&#29983;&#25104;&#34920;&#26684;&#22270;&#20687;&#26469;&#23454;&#29616;&#39046;&#22495;&#36716;&#25442;&#65292;&#20811;&#26381;&#20102;&#24418;&#25104;&#34920;&#26684;&#25968;&#25454;&#20808;&#39564;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#30340;&#25506;&#32034;&#21464;&#24471;&#36843;&#20999;&#12290;&#34920;&#26684;&#25968;&#25454;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#26679;&#30340;&#20449;&#24687;&#65292;&#20294;&#20063;&#23384;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#26631;&#35760;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#21487;&#34892;&#25429;&#25417;&#27599;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#29420;&#31435;&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#23450;&#20041;&#34920;&#26684;&#25968;&#25454;&#36793;&#30028;&#30340;&#20869;&#22312;&#27169;&#31946;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26410;&#24320;&#21457;&#20986;&#26080;&#38480;&#21046;&#26465;&#20214;&#30340;&#26377;&#24847;&#20041;&#19988;&#21487;&#34892;&#30340;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;TablEye&#65292;&#36890;&#36807;&#37319;&#29992;&#39046;&#22495;&#36716;&#25442;&#26469;&#20811;&#26381;&#24418;&#25104;&#34920;&#26684;&#25968;&#25454;&#20808;&#39564;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#34920;&#26684;&#22270;&#20687;&#26469;&#23454;&#29616;&#39046;&#22495;&#36716;&#25442;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#31561;&#33394;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#31283;&#24577;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#21644;&#20351;&#29992;&#33391;&#22909;&#26465;&#20214;&#30340;Jacobian&#26469;&#20445;&#35777;&#26435;&#37325;&#20849;&#20139;&#30340;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02263</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#31561;&#33394;&#24615;&#30340;&#20005;&#26684;&#20844;&#24179;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dynamical Isometry based Rigorous Fair Neural Architecture Search. (arXiv:2307.02263v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#31561;&#33394;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#31283;&#24577;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#21644;&#20351;&#29992;&#33391;&#22909;&#26465;&#20214;&#30340;Jacobian&#26469;&#20445;&#35777;&#26435;&#37325;&#20849;&#20139;&#30340;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26435;&#37325;&#20849;&#20139;&#25216;&#26415;&#26174;&#33879;&#21152;&#24555;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26435;&#37325;&#20849;&#20139;&#31574;&#30053;&#20165;&#22522;&#20110;&#32463;&#39564;&#25110;&#35266;&#23519;&#65292;&#23548;&#33268;&#25628;&#32034;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23545;&#20844;&#24179;&#24615;&#30340;&#24573;&#35270;&#65292;&#24403;&#21069;&#26041;&#27861;&#22312;&#27169;&#22359;&#35780;&#20272;&#20013;&#23481;&#26131;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#31561;&#33394;&#24615;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22343;&#22330;&#29702;&#35770;&#20013;&#30340;&#22266;&#23450;&#28857;&#20998;&#26512;&#26041;&#27861;&#26469;&#20998;&#26512;&#31283;&#24577;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#20197;&#21450;&#21160;&#24577;&#31561;&#33394;&#24615;&#22914;&#20309;&#20445;&#35777;&#22522;&#20110;&#26435;&#37325;&#20849;&#20139;&#30340;NAS&#30340;&#20844;&#24179;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#25152;&#26377;&#27169;&#22359;&#30340;&#33391;&#22909;&#26465;&#20214;&#30340;Jacobian&#30340;&#27867;&#21270;&#35823;&#24046;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22359;&#36873;&#25321;&#31574;&#30053;&#26159;&#20005;&#26684;&#20844;&#24179;&#30340;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26550;&#26500;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the weight-sharing technique has significantly speeded up the training and evaluation procedure of neural architecture search. However, most existing weight-sharing strategies are solely based on experience or observation, which makes the searching results lack interpretability and rationality. In addition, due to the negligence of fairness, current methods are prone to make misjudgments in module evaluation. To address these problems, we propose a novel neural architecture search algorithm based on dynamical isometry. We use the fix point analysis method in the mean field theory to analyze the dynamics behavior in the steady state random neural network, and how dynamic isometry guarantees the fairness of weight-sharing based NAS. Meanwhile, we prove that our module selection strategy is rigorous fair by estimating the generalization error of all modules with well-conditioned Jacobian. Extensive experiments show that, with the same size, the architecture searched by the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01715</link><description>&lt;p&gt;
&#31526;&#21512;&#30446;&#26631;&#65306;&#20351;&#29992;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#22312;CTC&#27169;&#22411;&#20013;&#20248;&#21270;&#25152;&#38656;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#26159;&#35757;&#32451;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#20934;&#21017;&#12290;&#23427;&#36890;&#36807;&#23558;&#23436;&#32654;&#23545;&#40784;&#65288;&#20135;&#29983;&#22522;&#26412;&#20107;&#23454;&#65289;&#30340;&#36793;&#38469;&#21270;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31216;&#20026;&#23545;&#20854;&#65292;&#20197;&#20195;&#20215;&#19981;&#23436;&#32654;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#23545;&#40784;&#30340;&#20108;&#20803;&#21306;&#20998;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20854;&#20182;&#20851;&#38190;&#23545;&#40784;&#23646;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Align With Purpose}$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;CTC&#26465;&#20214;&#19979;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#38656;&#23646;&#24615;&#30340;$\textbf{&#36890;&#29992;&#25554;&#20837;&#24335;&#26694;&#26550;}$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#34917;&#20805;CTC&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24178;&#39044;CTC&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#36731;&#26494;&#20248;&#21270;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21306;&#20998;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#35843;&#24230;&#39044;&#27979;&#31639;&#27861;&#65292;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20303;&#23429;&#38656;&#27714;&#25511;&#21046;&#65292;&#24182;&#21516;&#26102;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#24212;&#23545;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.01622</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network. (arXiv:2307.01622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#35843;&#24230;&#39044;&#27979;&#31639;&#27861;&#65292;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20303;&#23429;&#38656;&#27714;&#25511;&#21046;&#65292;&#24182;&#21516;&#26102;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#24212;&#23545;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#33021;&#22815;&#24110;&#21161;&#37197;&#30005;&#32593;&#32476;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#38752;&#22320;&#36816;&#34892;&#65292;&#24182;&#26377;&#25928;&#22320;&#25972;&#21512;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39044;&#27979;&#12289;&#20248;&#21270;&#21644;&#25511;&#21046;/&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#38656;&#27714;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#35843;&#24230;&#39044;&#27979;&#65288;rTPNN-FES&#65289;&#65292;&#29992;&#20110;&#25552;&#20379;&#39640;&#25928;&#30340;&#20303;&#23429;&#38656;&#27714;&#25511;&#21046;&#12290;rTPNN-FES&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#24182;&#23433;&#25490;&#23478;&#30005;&#30340;&#20351;&#29992;&#26102;&#38388;&#12290;&#36890;&#36807;&#20854;&#23884;&#20837;&#24335;&#32467;&#26500;&#65292;rTPNN-FES&#28040;&#38500;&#20102;&#20351;&#29992;&#21333;&#29420;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#21644;&#35843;&#24230;&#30340;&#38656;&#35201;&#65292;&#24182;&#29983;&#25104;&#23545;&#39044;&#27979;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#35843;&#24230;&#23433;&#25490;&#12290;&#26412;&#25991;&#36824;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#22312;&#29289;&#32852;&#32593;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;rTPNN-FES&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart home energy management systems help the distribution grid operate more efficiently and reliably, and enable effective penetration of distributed renewable energy sources. These systems rely on robust forecasting, optimization, and control/scheduling algorithms that can handle the uncertain nature of demand and renewable generation. This paper proposes an advanced ML algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), to provide efficient residential demand control. rTPNN-FES is a novel neural network architecture that simultaneously forecasts renewable energy generation and schedules household appliances. By its embedded structure, rTPNN-FES eliminates the utilization of separate algorithms for forecasting and scheduling and generates a schedule that is robust against forecasting errors. This paper also evaluates the performance of the proposed algorithm for an IoT-enabled smart home. The evaluation results reveal that rTPNN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#33258;&#30465;&#24335;&#26426;&#22120;&#20154;&#35013;&#37197;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#34892;&#30340;&#31034;&#20363;&#26469;&#35757;&#32451;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#19981;&#21487;&#34892;&#30340;&#35013;&#37197;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.01317</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#19982;&#24402;&#19968;&#21270;&#27969;&#22312;&#33258;&#30465;&#24335;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly. (arXiv:2307.01317v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#33258;&#30465;&#24335;&#26426;&#22120;&#20154;&#35013;&#37197;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#34892;&#30340;&#31034;&#20363;&#26469;&#35757;&#32451;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#19981;&#21487;&#34892;&#30340;&#35013;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#20013;&#38656;&#35201;&#23545;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#33258;&#30465;&#65292;&#21363;&#21028;&#26029;&#20854;&#21487;&#34892;&#24615;&#65292;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#25928;&#29575;&#38477;&#20302;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22312;&#35757;&#32451;&#26102;&#38656;&#35201;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20135;&#21697;&#21464;&#20307;&#26102;&#65292;&#25910;&#38598;&#36275;&#22815;&#30340;&#19981;&#21487;&#34892;&#31034;&#20363;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;&#21487;&#34892;&#31034;&#20363;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#21487;&#34892;&#24615;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#20998;&#24067;&#21306;&#20998;&#65292;&#24402;&#19968;&#21270;&#27969;&#26159;&#29992;&#20110;&#20272;&#35745;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#24378;&#22823;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#35013;&#37197;&#26696;&#20363;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#21333;&#20998;&#31867;&#22522;&#32447;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#20986;&#19981;&#21487;&#34892;&#30340;&#35013;&#37197;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP) need to be introspective on the predicted solutions, i.e. whether they are feasible or not, to circumvent potential efficiency degradation. Previous works need both feasible and infeasible examples during training. However, the infeasible ones are hard to collect sufficiently when re-training is required for swift adaptation to new product variants. In this work, we propose a density-based feasibility learning method that requires only feasible examples. Concretely, we formulate the feasibility learning problem as Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are powerful generative models for estimating complex probability distributions. Empirically, the proposed method is demonstrated on robotic assembly use cases and outperforms other single-class baselines in detecting infeasible assemblies. We further investigate the internal working mechanism of our method and show that a large memo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAL&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#20195;&#34920;&#24615;&#38169;&#35823;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#25913;&#36827;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#32771;&#34385;&#38169;&#35823;&#23454;&#20363;&#21450;&#20854;&#37051;&#22495;&#20013;&#30340;&#38169;&#35823;&#23494;&#24230;&#65292;REAL&#22312;&#20934;&#30830;&#29575;&#21644;F1-macro&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.00968</link><description>&lt;p&gt;
REAL:&#19968;&#31181;&#22522;&#20110;&#20195;&#34920;&#24615;&#38169;&#35823;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
REAL: A Representative Error-Driven Approach for Active Learning. (arXiv:2307.00968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAL&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#20195;&#34920;&#24615;&#38169;&#35823;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#25913;&#36827;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#32771;&#34385;&#38169;&#35823;&#23454;&#20363;&#21450;&#20854;&#37051;&#22495;&#20013;&#30340;&#38169;&#35823;&#23494;&#24230;&#65292;REAL&#22312;&#20934;&#30830;&#29575;&#21644;F1-macro&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#26377;&#26377;&#38480;&#26631;&#35760;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#20197;&#33719;&#21462;&#26631;&#31614;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20027;&#21160;&#23398;&#20064;&#36890;&#24120;&#26681;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#34913;&#37327;&#26410;&#26631;&#35760;&#23454;&#20363;&#30340;&#20449;&#24687;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#24182;&#19981;&#32771;&#34385;&#20855;&#26377;&#37051;&#22495;&#38169;&#35823;&#23494;&#24230;&#30340;&#38169;&#35823;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAL&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#20195;&#34920;&#24615;&#38169;&#35823;&#30340;&#25968;&#25454;&#23454;&#20363;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#12290;&#23427;&#23558;&#23569;&#25968;&#39044;&#27979;&#20316;&#20026;&#32858;&#31867;&#20013;&#30340;&#8220;&#20266;&#38169;&#35823;&#8221;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#38169;&#35823;&#23494;&#24230;&#20026;&#35813;&#32858;&#31867;&#20998;&#37197;&#33258;&#36866;&#24212;&#30340;&#37319;&#26679;&#39044;&#31639;&#12290;&#22312;&#20116;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;REAL&#22312;&#20934;&#30830;&#29575;&#21644;F1-macro&#24471;&#20998;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#34920;&#29616;&#26368;&#20339;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive $\underline{L}$earning. It identifies minority predictions as \emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.15782</link><description>&lt;p&gt;
UTRNet: &#21360;&#21047;&#25991;&#26723;&#20013;&#39640;&#20998;&#36776;&#29575;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#23610;&#24230;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;UTRNet&#26550;&#26500;&#65292;&#19968;&#20010;&#28151;&#21512;CNN-RNN&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#24037;&#20316;&#24456;&#38590;&#25512;&#24191;&#21040;&#20044;&#23572;&#37117;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UTRSet-Real&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;11,000&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;UTRSet-Synth&#65292;&#19968;&#20010;&#19982;&#23454;&#38469;&#19990;&#30028;&#38750;&#24120;&#30456;&#20284;&#30340;&#21547;&#26377;20,000&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;IIITH&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#30495;&#23454;&#24615;&#36827;&#34892;&#20102;&#20462;&#27491;&#65292;&#20351;&#20854;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#26356;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;UrduDoc&#65292;&#19968;&#31181;&#29992;&#20110;&#25195;&#25551;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#24037;&#20855;&#65292;&#36890;&#36807;&#23558;UTRNet&#19982;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#20044;&#23572;&#37117;OCR&#38598;&#25104;&#22312;&#21360;&#21047;&#25991;&#26723;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#22914;&#20309;&#20197;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#21644;&#25277;&#26679;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#34987;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2306.15083</link><description>&lt;p&gt;
&#24179;&#34913;&#36807;&#28388;&#20351;&#29992;&#38750;&#27844;&#38706;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Balanced Filtering via Non-Disclosive Proxies. (arXiv:2306.15083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#22914;&#20309;&#20197;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#21644;&#25277;&#26679;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#34987;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#22312;&#25910;&#38598;&#26102;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#19982;&#25935;&#24863;&#32676;&#32452;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25910;&#38598;&#26426;&#21046;&#19981;&#20250;&#27604;&#22522;&#26412;&#27604;&#29575;&#33021;&#22815;&#30830;&#23450;&#30340;&#20219;&#20309;&#20010;&#20307;&#26679;&#26412;&#30340;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#26356;&#22810;&#22320;&#36879;&#38706;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20844;&#24179;&#27969;&#31243;&#30340;&#35266;&#28857;&#65292;&#21363;&#23398;&#20064;&#32773;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20195;&#29702;&#20989;&#25968;&#65292;&#36825;&#20010;&#20195;&#29702;&#20989;&#25968;&#20197;&#21518;&#21487;&#20197;&#29992;&#20110;&#36825;&#20010;&#36807;&#28388;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#20989;&#25968;&#30340;&#33539;&#22260;&#19982;&#25277;&#26679;&#27010;&#29575;&#30456;&#20851;&#32852;&#65307;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#20505;&#36873;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#26681;&#25454;&#19982;&#20854;&#20195;&#29702;&#20998;&#31867;&#23545;&#24212;&#30340;&#25277;&#26679;&#27010;&#29575;&#36873;&#25321;&#23427;&#20316;&#20026;&#25105;&#20204;&#30340;&#26679;&#26412;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35201;&#27714;&#20195;&#29702;&#20998;&#31867;&#26412;&#36523;&#19981;&#20250;&#36879;&#38706;&#20219;&#20309;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#30340;&#37325;&#35201;&#20449;&#24687;&#65288;&#21363;&#65292;&#23427;&#24212;&#35813;&#26159;&#38750;&#27844;&#38706;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of non-disclosively collecting a sample of data that is balanced with respect to sensitive groups when group membership is unavailable or prohibited from use at collection time. Specifically, our collection mechanism does not reveal significantly more about group membership of any individual sample than can be ascertained from base rates alone. To do this, we adopt a fairness pipeline perspective, in which a learner can use a small set of labeled data to train a proxy function that can later be used for this filtering task. We then associate the range of the proxy function with sampling probabilities; given a new candidate, we classify it using our proxy function, and then select it for our sample with probability proportional to the sampling probability corresponding to its proxy classification. Importantly, we require that the proxy classification itself not reveal significant information about the sensitive group membership of any individual sample (i.e., it sho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;FPGA&#19978;&#30340;&#21521;&#37327;&#25628;&#32034;&#26694;&#26550;FANNS&#65292;&#23454;&#29616;&#20102;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21644;&#30828;&#20214;&#39044;&#31639;&#29983;&#25104;&#30456;&#24212;&#30340;&#21152;&#36895;&#22120;&#12290;&#19982;FPGA&#21644;CPU&#22522;&#20934;&#30456;&#27604;&#65292;FANNS&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11182</link><description>&lt;p&gt;
&#20026;&#21521;&#37327;&#25628;&#32034;&#36827;&#34892;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Co-design Hardware and Algorithm for Vector Search. (arXiv:2306.11182v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;FPGA&#19978;&#30340;&#21521;&#37327;&#25628;&#32034;&#26694;&#26550;FANNS&#65292;&#23454;&#29616;&#20102;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21644;&#30828;&#20214;&#39044;&#31639;&#29983;&#25104;&#30456;&#24212;&#30340;&#21152;&#36895;&#22120;&#12290;&#19982;FPGA&#21644;CPU&#22522;&#20934;&#30456;&#27604;&#65292;FANNS&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25628;&#32034;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#20687;Google&#21644;Bing&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#35780;&#20272;&#32534;&#30721;&#26597;&#35810;&#25991;&#26412;&#21644;&#32593;&#32476;&#25991;&#26723;&#20043;&#38388;&#30340;&#21521;&#37327;&#30456;&#20284;&#24230;&#65292;&#27599;&#31186;&#22788;&#29702;&#25968;&#19975;&#20010;&#26597;&#35810;&#65292;&#22312;&#25317;&#26377;PB&#32423;&#25991;&#26723;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;&#38543;&#30528;&#23545;&#21521;&#37327;&#25628;&#32034;&#31995;&#32479;&#24615;&#33021;&#30340;&#38656;&#27714;&#28608;&#22686;&#65292;&#22312;&#25705;&#23572;&#23450;&#24459;&#26102;&#20195;&#21518;&#65292;&#21152;&#36895;&#30828;&#20214;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;FPGA&#19978;&#30340;&#31471;&#21040;&#31471;&#21487;&#25193;&#23637;&#21521;&#37327;&#25628;&#32034;&#26694;&#26550;FANNS&#12290;&#32473;&#23450;&#29992;&#25143;&#25552;&#20379;&#30340;&#23545;&#25968;&#25454;&#38598;&#30340;&#21484;&#22238;&#35201;&#27714;&#21644;&#30828;&#20214;&#36164;&#28304;&#39044;&#31639;&#65292;FANNS&#33258;&#21160;&#36827;&#34892;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#38543;&#21518;&#29983;&#25104;&#30456;&#24212;&#30340;&#21152;&#36895;&#22120;&#12290;&#35813;&#26694;&#26550;&#36824;&#36890;&#36807;&#22312;&#21152;&#36895;&#22120;&#20013;&#24341;&#20837;&#30828;&#20214;TCP/IP&#22534;&#26632;&#26469;&#25903;&#25345;&#35268;&#27169;&#25193;&#23637;&#12290;&#19982;FPGA&#21644;CPU&#22522;&#20934;&#30456;&#27604;&#65292;FANNS&#20998;&#21035;&#23454;&#29616;&#20102;23.0&#20493;&#21644;37.2&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware offers a promising solution in the post-Moore's Law era. We introduce \textit{FANNS}, an end-to-end and scalable vector search framework on FPGAs. Given a user-provided recall requirement on a dataset and a hardware resource budget, \textit{FANNS} automatically co-designs hardware and algorithm, subsequently generating the corresponding accelerator. The framework also supports scale-out by incorporating a hardware TCP/IP stack in the accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup compared to FPGA and CPU baselines, respectively, and demonstrates superior scalabil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10259</link><description>&lt;p&gt;
&#22810;&#40657;&#30418;&#39044;&#35328;&#19979;&#30340;&#20027;&#21160;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26377;&#25928;&#31574;&#30053;&#24448;&#24448;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#19979;&#65292;&#20154;&#20204;&#36890;&#24120;&#21482;&#33021;&#25509;&#35302;&#21040;&#22810;&#20010;&#27425;&#20248;&#30340;&#40657;&#30418;&#39044;&#35328;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26368;&#20248;&#30340;&#39044;&#35328;&#65292;&#36825;&#20123;&#39044;&#35328;&#19981;&#33021;&#22312;&#25152;&#26377;&#29366;&#24577;&#19979;&#26222;&#36941;&#20248;&#20110;&#24444;&#27492;&#65292;&#36825;&#32473;&#20027;&#21160;&#20915;&#23450;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#20351;&#29992;&#21738;&#31181;&#39044;&#35328;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#21508;&#33258;&#20272;&#35745;&#20540;&#20989;&#25968;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;MAPS&#21644;MAPS-SE&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;Wasserstein barycenters&#25193;&#23637;`Strong Demographic Parity`&#30340;&#23450;&#20041;&#65292;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#22238;&#24402;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10155</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein Barycenters&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Multi-Task Learning via Wasserstein Barycenters. (arXiv:2306.10155v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;Wasserstein barycenters&#25193;&#23637;`Strong Demographic Parity`&#30340;&#23450;&#20041;&#65292;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#22238;&#24402;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24050;&#32463;&#25104;&#29087;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20943;&#23569;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#30830;&#20445;&#21333;&#21464;&#37327;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24615;&#65292;&#21363;&#30446;&#26631;&#26159;&#21435;&#38500;&#21333;&#20010;&#20219;&#21153;&#30340;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#23558;&#20844;&#24179;&#24615;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#29615;&#22659;&#65292;&#20854;&#20013;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#26469;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#20803;Wasserstein barycenters&#23558;\textit{Strong Demographic Parity}&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26368;&#20248;&#30340;&#20844;&#24179;&#22810;&#20219;&#21153;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#23553;&#38381;&#24335;&#35299;&#65292;&#21253;&#25324;&#22238;&#24402;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#20197;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#25968;&#23383;&#23454;&#39564;&#12290;&#32463;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#22312;&#20419;&#36827;&#20844;&#24179;&#20915;&#31574;&#26041;&#38754;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Fairness is an established field in machine learning that aims to reduce biases in data. Recent advances have proposed various methods to ensure fairness in a univariate environment, where the goal is to de-bias a single task. However, extending fairness to a multi-task setting, where more than one objective is optimised using a shared representation, remains underexplored. To bridge this gap, we develop a method that extends the definition of \textit{Strong Demographic Parity} to multi-task learning using multi-marginal Wasserstein barycenters. Our approach provides a closed form solution for the optimal fair multi-task predictor including both regression and binary classification tasks. We develop a data-driven estimation procedure for the solution and run numerical experiments on both synthetic and real datasets. The empirical results highlight the practical value of our post-processing methodology in promoting fair decision-making.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22686;&#24378;&#27169;&#22411;&#22534;&#21472;&#26041;&#27861;&#65292;&#22312;Codeforces&#21644;Leetcode&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;77.8%&#30340;&#20934;&#30830;&#29575;&#21644;0.815&#30340;PR-AUC&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.10077</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#27169;&#22411;&#30340;&#22534;&#21472;&#26041;&#27861;&#35299;&#20915;&#32534;&#30721;&#38382;&#39064;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Stacking of Hyperparameter Tuned Models for Tagging Coding Problems. (arXiv:2306.10077v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22686;&#24378;&#27169;&#22411;&#22534;&#21472;&#26041;&#27861;&#65292;&#22312;Codeforces&#21644;Leetcode&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;77.8%&#30340;&#20934;&#30830;&#29575;&#21644;0.815&#30340;PR-AUC&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;&#38382;&#39064;&#26159;&#38656;&#35201;&#20197;&#35745;&#31639;&#26426;&#31243;&#24207;&#24418;&#24335;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;&#32534;&#30721;&#38382;&#39064;&#22312;&#23398;&#29983;&#21644;&#19987;&#19994;&#20154;&#22763;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#21319;&#20182;&#20204;&#30340;&#25216;&#33021;&#21644;&#32844;&#19994;&#26426;&#20250;&#12290;&#19968;&#20010;&#33021;&#22815;&#24110;&#21161;&#32451;&#20064;&#32534;&#30721;&#38382;&#39064;&#30340;AI&#31995;&#32479;&#23558;&#38750;&#24120;&#26377;&#29992;&#65292;&#24182;&#19988;&#23384;&#22312;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22686;&#24378;&#27169;&#22411;&#22534;&#21472;&#26041;&#27861;&#65292;&#20197;&#22312;&#20174;Codeforces&#21644;Leetcode&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;77.8&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;0.815&#30340;PR-AUC&#20998;&#25968;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#20026;&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coding problems are problems that require a solution in the form of a computer program. Coding problems are popular among students and professionals as it enhances their skills and career opportunities. An AI system that would help those who practice coding problems would be highly useful and there is a huge potential for such a system. In this work, we propose a model which uses stacking of hyperparameter tuned boosting models to achieve impressive metric scores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped from Codeforces and Leetcode. We open source the dataset and the models developed for this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;LLMs&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#23427;&#20204;&#27491;&#22312;&#22686;&#24378;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35299;&#37322;&#33021;&#21147;&#12289;&#25552;&#21319;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20197;&#21450;&#31616;&#21270;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.06767</link><description>&lt;p&gt;
ChatGPT&#21644;LLMs&#23545;&#21307;&#23398;&#24433;&#20687;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#24433;&#21709;&#65306;&#35266;&#28857;&#21644;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases. (arXiv:2306.06767v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;LLMs&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#23427;&#20204;&#27491;&#22312;&#22686;&#24378;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35299;&#37322;&#33021;&#21147;&#12289;&#25552;&#21319;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20197;&#21450;&#31616;&#21270;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;OpenAI ChatGPT&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#20511;&#21161;&#20844;&#20849;&#25968;&#25454;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#27491;&#22312;&#22686;&#24378;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#21319;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#24182;&#31616;&#21270;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23637;&#31034;LLMs&#19982;&#21307;&#23398;&#24433;&#20687;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#21253;&#25324;&#20225;&#19994;&#12289;&#20445;&#38505;&#26426;&#26500;&#12289;&#25919;&#24220;&#12289;&#30740;&#31350;&#26426;&#26500;&#21644;&#21307;&#38498;&#65288;&#34987;&#31216;&#20026;BIGR-H&#65289;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#12289;&#31034;&#20363;&#24212;&#29992;&#26696;&#20363;&#20197;&#21450;&#23545;&#24191;&#27867;&#24433;&#21709;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#22312;AI&#39537;&#21160;&#21307;&#30103;&#20445;&#20581;&#26102;&#20195;&#30340;&#25112;&#30053;&#35268;&#21010;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#35752;&#35770;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#30340;MetNet-3&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#23545;20&#23567;&#26102;&#20869;&#30340;&#22825;&#27668;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;MetNet-3&#30340;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#21487;&#23398;&#20064;&#21367;&#31215;&#12289;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#25110;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#26356;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06079</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#26085;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Day Forecasts from Sparse Observations. (arXiv:2306.06079v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#30340;MetNet-3&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#23545;20&#23567;&#26102;&#20869;&#30340;&#22825;&#27668;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;MetNet-3&#30340;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#21487;&#23398;&#20064;&#21367;&#31215;&#12289;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#25110;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#26356;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#24314;&#27169;&#22825;&#27668;&#26465;&#20214;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#31070;&#32463;&#27169;&#22411;&#22312;&#25968;&#25454;&#21487;&#29992;&#26102;&#20197;&#23569;&#20110;1&#31186;&#30340;&#36895;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20197;&#38750;&#24120;&#39640;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#20174;&#22823;&#27668;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#26159;&#36825;&#20123;&#27169;&#22411;&#29420;&#29305;&#30340;&#20248;&#21183;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20165;&#20165;&#39044;&#27979;&#38477;&#27700;&#36825;&#19968;&#21807;&#19968;&#21464;&#37327;&#26102;&#65292;&#20165;&#33021;&#20351;&#29992;&#22823;&#27668;&#35266;&#27979;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#25165;&#33021;&#36798;&#21040;&#19982;&#29616;&#26377;&#27010;&#29575;&#24615;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30456;&#24403;&#30340;&#33391;&#22909;&#34920;&#29616;&#21040;12&#20010;&#23567;&#26102;&#30340;&#25552;&#21069;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MetNet-3&#65292;&#23427;&#26174;&#33879;&#25193;&#23637;&#20102;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#33391;&#22909;&#39044;&#27979;&#30340;&#24341;&#23548;&#26102;&#38388;&#33539;&#22260;&#21644;&#21464;&#37327;&#12290;MetNet-3&#20174;&#23494;&#38598;&#21644;&#31232;&#30095;&#30340;&#25968;&#25454;&#20256;&#24863;&#22120;&#20013;&#23398;&#20064;&#65292;&#24182;&#20026;&#38477;&#27700;&#12289;&#39118;&#12289;&#28201;&#24230;&#21644;&#38706;&#28857;&#36827;&#34892;24&#23567;&#26102;&#30340;&#39044;&#27979;&#12290;MetNet-3&#22312;&#20307;&#31995;&#32467;&#26500;&#23618;&#38754;&#24341;&#20837;&#20102;&#35768;&#22810;&#25216;&#26415;&#21019;&#26032;&#65292;&#36825;&#34987;&#35777;&#26126;&#23545;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26102;&#31354;&#21367;&#31215;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#31070;&#32463;&#27169;&#22411;&#27867;&#21270;&#20026;&#20165;&#25509;&#21463;&#31232;&#30095;&#30340;&#27668;&#21387;&#35745;&#35266;&#27979;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#65292;&#25110;&#36890;&#36807;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#25968;&#20540;&#27169;&#22411;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#26041;&#27861;&#26159;&#26377;&#30410;&#30340;&#12290;MetNet-3&#22312;&#38477;&#27700;&#12289;&#28201;&#24230;&#21644;&#38706;&#28857;&#39044;&#27979;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22823;&#27668;&#27169;&#22411;&#22312;&#25552;&#21069;&#33267;24&#23567;&#26102;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks offer an alternative paradigm for modeling weather conditions. The ability of neural models to make a prediction in less than a second once the data is available and to do so with very high temporal and spatial resolution, and the ability to learn directly from atmospheric observations, are just some of these models' unique advantages. Neural models trained using atmospheric observations, the highest fidelity and lowest latency data, have to date achieved good performance only up to twelve hours of lead time when compared with state-of-the-art probabilistic Numerical Weather Prediction models and only for the sole variable of precipitation. In this paper, we present MetNet-3 that extends significantly both the lead time range and the variables that an observation based neural model can predict well. MetNet-3 learns from both dense and sparse data sensors and makes predictions up to 24 hours ahead for precipitation, wind, temperature and dew point. MetNet-3 introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04504</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;ChatGPT&#65306;&#19982;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#30340;&#38646;&#26679;&#20363;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#21508;&#31181;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22914;BioGPT&#21644;BioBART&#12290;&#36825;&#34920;&#26126;ChatGPT&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#20351;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#19987;&#19994;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#22810;&#35282;&#24230;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#21644;&#23545;&#20598;&#20844;&#24335;&#30340;&#23436;&#20840;&#31561;&#20215;&#24615;&#65292;&#24182;&#26368;&#32456;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#31561;&#20215;&#24615;&#21644;&#25552;&#20379;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.17251</link><description>&lt;p&gt;
&#22810;&#35282;&#24230;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#30340;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
Duality in Multi-View Restricted Kernel Machines. (arXiv:2305.17251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#22810;&#35282;&#24230;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#21644;&#23545;&#20598;&#20844;&#24335;&#30340;&#23436;&#20840;&#31561;&#20215;&#24615;&#65292;&#24182;&#26368;&#32456;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#31561;&#20215;&#24615;&#21644;&#25552;&#20379;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#21463;&#38480;&#21046;&#20869;&#26680;&#26426;&#22120;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#22810;&#35282;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#21487;&#20197;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#35813;&#26694;&#26550;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#34920;&#31034;&#65292;&#24182;&#20174;&#29702;&#35770;&#35282;&#24230;&#20851;&#32852;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21407;&#22987;&#21464;&#37327;&#26469;&#23454;&#29616;&#21407;&#22987;&#21644;&#23545;&#20598;&#20844;&#24335;&#30340;&#23436;&#20840;&#31561;&#20215;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#36882;&#24402;&#39044;&#27979;&#26410;&#30475;&#21040;&#27979;&#35797;&#25968;&#25454;&#24182;&#21487;&#35270;&#21270;&#25152;&#23398;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#22312;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#39564;&#35777;&#20102;&#31561;&#20215;&#24615;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#20851;&#31995;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a unifying setting that combines existing restricted kernel machine methods into a single primal-dual multi-view framework for kernel principal component analysis in both supervised and unsupervised settings. We derive the primal and dual representations of the framework and relate different training and inference algorithms from a theoretical perspective. We show how to achieve full equivalence in primal and dual formulations by rescaling primal variables. Finally, we experimentally validate the equivalence and provide insight into the relationships between different methods on a number of time series data sets by recursively forecasting unseen test data and visualizing the learned features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27973;&#23618;&#30340;&#38543;&#26426;&#26354;&#29575;&#20449;&#24687;&#20026;&#27599;&#19968;&#23618;&#35745;&#31639;&#33258;&#36866;&#24212;&#27493;&#38271;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#35813;&#31574;&#30053;&#30340;&#31639;&#27861;&#22312;DNN&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#31934;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#29256;&#26412;&#20197;&#21450;&#27969;&#34892;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13664</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning. (arXiv:2305.13664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27973;&#23618;&#30340;&#38543;&#26426;&#26354;&#29575;&#20449;&#24687;&#20026;&#27599;&#19968;&#23618;&#35745;&#31639;&#33258;&#36866;&#24212;&#27493;&#38271;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#35813;&#31574;&#30053;&#30340;&#31639;&#27861;&#22312;DNN&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#31934;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#29256;&#26412;&#20197;&#21450;&#27969;&#34892;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#27493;&#38271;&#31574;&#30053;&#65292;&#29992;&#20110;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289; &#27973;&#23618;&#20013;&#21253;&#21547;&#30340;&#23545;&#35282;&#32447;&#22359;&#30340;&#23618;&#38543;&#26426;&#26354;&#29575;&#20449;&#24687;&#26469;&#35745;&#31639;&#27599;&#19968;&#23618;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#65288;&#21363;&#23398;&#20064;&#29575;&#65289;&#12290;&#35813;&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#24403;&#65292;&#32780;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20165;&#22686;&#21152;&#20102;&#32422;&#31561;&#20110;&#21478;&#19968;&#20010;&#26799;&#24230;&#35745;&#31639;&#37327;&#30340;&#37327;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#32467;&#21512;&#25152;&#25552;&#20986;&#30340;&#20998;&#23618;&#27493;&#24133;&#22823;&#23567;&#30340;SGD&#21160;&#37327;&#27861;&#21644;AdamW&#33021;&#22815;&#36873;&#25321;&#26377;&#25928;&#30340;&#23398;&#20064;&#29575;&#36827;&#24230;&#65292;&#24182;&#22312;Autoencoder&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20219;&#21153;&#30340;DNN&#35757;&#32451;&#20013;&#20248;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#31934;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#29256;&#26412;&#20197;&#21450;&#27969;&#34892;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new per-layer adaptive step-size procedure for stochastic first-order optimization methods for minimizing empirical loss functions in deep learning, eliminating the need for the user to tune the learning rate (LR). The proposed approach exploits the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian in deep neural networks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer. The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation. Numerical experiments show that SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods as well as popular first-order and second-order algorithms for training DNNs on Autoencoder, Convolutional Neural Network (CN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#32467;&#26500;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22686;&#24378;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20877;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10457</link><description>&lt;p&gt;
&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Time Series Clustering With Random Convolutional Kernels. (arXiv:2305.10457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#32467;&#26500;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22686;&#24378;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20877;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#21487;&#25551;&#36848;&#24191;&#27867;&#30340;&#33258;&#28982;&#21644;&#31038;&#20250;&#29616;&#35937;&#65292;&#22914;&#27668;&#20505;&#12289;&#22320;&#38663;&#12289;&#32929;&#31080;&#20215;&#26684;&#25110;&#32593;&#31449;&#35775;&#38382;&#36235;&#21183;&#12290;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26377;&#21161;&#20110;&#25214;&#21040;&#24322;&#24120;&#20540;&#65292;&#36825;&#20123;&#24322;&#24120;&#20540;&#21487;&#33021;&#20195;&#34920;&#28201;&#24230;&#24322;&#24120;&#12289;&#28779;&#23665;&#29190;&#21457;&#12289;&#24066;&#22330;&#24178;&#25200;&#25110;&#27450;&#35784;&#24615;&#32593;&#31449;&#27969;&#37327;&#12290;&#22522;&#20110;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#38543;&#26426;&#26680;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#38543;&#26426;&#21367;&#31215;&#32467;&#26500;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22686;&#24378;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#32858;&#31867;&#31639;&#27861;&#23545;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#22522;&#20934;&#19978;&#25913;&#21892;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series can describe a wide range of natural and social phenomena. A few samples are climate and seismic measures trends, stock prices, or website visits. Time-series clustering helps to find outliers that, related to these instances, could represent temperature anomalies, imminent volcanic eruptions, market disturbances, or fraudulent web traffic. Founded on the success of automatic feature extraction techniques, specifically employing random kernels, we develop a new method for time series clustering consisting of two steps. First, a random convolutional structure transforms the data into an enhanced feature representation. Afterwards, a clustering algorithm classifies the transformed data. The method improves state-of-the-art results on time series clustering benchmarks.
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;14&#20010;&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#26837;&#30702;&#26143;&#30340;&#26377;&#25928;&#28201;&#24230;&#21487;&#20197;&#34987;&#39044;&#27979;&#65292;&#20294;&#25512;&#26029;&#34920;&#38754;&#37325;&#21147;&#21152;&#36895;&#24230;&#21644;&#37329;&#23646;&#20016;&#24230;&#19982;&#27169;&#22411;&#32593;&#26684;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.07719</link><description>&lt;p&gt;
&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#19982;&#26426;&#22120;&#23398;&#20064;&#22823;&#27668;&#21453;&#28436;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning. (arXiv:2305.07719v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;14&#20010;&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#26837;&#30702;&#26143;&#30340;&#26377;&#25928;&#28201;&#24230;&#21487;&#20197;&#34987;&#39044;&#27979;&#65292;&#20294;&#25512;&#26029;&#34920;&#38754;&#37325;&#21147;&#21152;&#36895;&#24230;&#21644;&#37329;&#23646;&#20016;&#24230;&#19982;&#27169;&#22411;&#32593;&#26684;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27425;&#24658;&#26143;&#20809;&#35889;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#23545;&#20110;&#33258;&#27965;&#27169;&#22411;&#32593;&#26684;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;14&#20010;&#20197;&#21069;&#21457;&#34920;&#30340;&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#65288;&#20174;1997&#21040;2021&#24180;&#65289;&#12290;&#38543;&#26426;&#26862;&#26519;&#26041;&#27861;&#35753;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#32593;&#26684;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#22312;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#26694;&#26550;&#19979;&#35299;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;3&#20010;&#22522;&#20934;&#26837;&#30702;&#26143;&#65288;Gl 570D&#65292;&#949; Indi Ba&#21644;Bb&#65289;&#20197;&#21450;19&#20010;L&#22411;&#21644;T&#22411;&#30702;&#26143;&#30340;&#26679;&#26412;&#65307;&#36825;&#20010;&#26679;&#26412;&#20043;&#21069;&#26366;&#20351;&#29992;&#20256;&#32479;&#36125;&#21494;&#26031;&#26041;&#27861;&#65288;&#23884;&#22871;&#21462;&#26679;&#65289;&#22312;Lueber&#31561;&#20154;&#65288;2022&#65289;&#20013;&#36827;&#34892;&#36807;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#29420;&#31435;&#20110;&#25152;&#36873;&#25321;&#30340;&#27169;&#22411;&#32593;&#26684;&#65292;&#24378;&#26377;&#21147;&#22320;&#39044;&#27979;&#26837;&#30702;&#26143;&#30340;&#26377;&#25928;&#28201;&#24230;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#26837;&#30702;&#26143;&#34920;&#38754;&#37325;&#21147;&#21152;&#36895;&#24230;&#21644;&#37329;&#23646;&#20016;&#24230;&#22240;&#25152;&#36873;&#25321;&#30340;&#27169;&#22411;&#32593;&#26684;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding differences between sub-stellar spectral data and models has proven to be a major challenge, especially for self-consistent model grids that are necessary for a thorough investigation of brown dwarf atmospheres. Using the supervised machine learning method of the random forest, we study the information content of 14 previously published model grids of brown dwarfs (from 1997 to 2021). The random forest method allows us to analyze the predictive power of these model grids, as well as interpret data within the framework of Approximate Bayesian Computation (ABC). Our curated dataset includes 3 benchmark brown dwarfs (Gl 570D, {\epsilon} Indi Ba and Bb) as well as a sample of 19 L and T dwarfs; this sample was previously analyzed in Lueber et al. (2022) using traditional Bayesian methods (nested sampling). We find that the effective temperature of a brown dwarf can be robustly predicted independent of the model grid chosen for the interpretation. However, inference of the sur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#23545;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#22312;&#29482;&#32905;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#26085;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.05677</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#23545;&#29482;&#32905;&#20215;&#26684;&#39044;&#27979;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Effects of data time lag in a decision-making system using machine learning for pork price prediction. (arXiv:2305.05677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#23545;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#22312;&#29482;&#32905;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#26085;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35199;&#29677;&#29273;&#26159;&#19990;&#30028;&#19978;&#31532;&#19977;&#22823;&#29482;&#32905;&#29983;&#20135;&#22269;&#65292;&#35768;&#22810;&#20892;&#22330;&#20381;&#36182;&#20110;&#36825;&#20010;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23450;&#20215;&#20307;&#31995;&#26159;&#19981;&#20844;&#24179;&#30340;&#65292;&#22240;&#20026;&#19968;&#20123;&#20154;&#27604;&#20854;&#20182;&#20154;&#26377;&#26356;&#22909;&#30340;&#24066;&#22330;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21382;&#21490;&#20215;&#26684;&#26159;&#26131;&#20110;&#33719;&#21462;&#21644;&#32463;&#27982;&#23454;&#24800;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#24110;&#21161;&#25152;&#26377;&#20195;&#29702;&#21830;&#26356;&#22909;&#22320;&#20102;&#35299;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#21487;&#33021;&#20250;&#24433;&#21709;&#20182;&#20204;&#30340;&#23450;&#20215;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#39044;&#27979;&#31639;&#27861;&#22312;&#20215;&#26684;&#39044;&#27979;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#37319;&#38598;&#24310;&#36831;&#23545;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#26368;&#20339;&#25552;&#26696;&#30340;&#38598;&#25104;&#21040;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21407;&#22411;&#20013;&#65292;&#24182;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#35199;&#29677;&#29273;&#26368;&#37325;&#35201;&#30340;&#22320;&#21306;&#29482;&#32905;&#24066;&#22330;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30001;&#20892;&#19994;&#37096;&#21457;&#24067;&#65292;&#24310;&#36831;&#20004;&#21608;&#65292;&#21644;&#35746;&#38405;&#26041;&#24335;&#33719;&#21462;&#30340;&#21516;&#19968;&#26085;&#30340;&#21516;&#19968;&#24066;&#22330;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#39044;&#27979;&#31639;&#27861;&#21644;&#20004;&#21608;&#24310;&#36831;&#30340;&#25968;&#25454;&#30456;&#27604;&#65292;&#35746;&#38405;&#25968;&#25454;&#21487;&#20197;&#20943;&#23569;&#38169;&#35823;&#24046;&#24322;&#65292;&#36825;&#35777;&#26126;&#20102;&#38598;&#25104;&#22810;&#20010;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#26102;&#38388;&#28382;&#21518;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spain is the third-largest producer of pork meat in the world, and many farms in several regions depend on the evolution of this market. However, the current pricing system is unfair, as some actors have better market information than others. In this context, historical pricing is an easy-to-find and affordable data source that can help all agents to be better informed. However, the time lag in data acquisition can affect their pricing decisions. In this paper, we study the effect that data acquisition delay has on a price prediction system using multiple prediction algorithms. We describe the integration of the best proposal into a decision support system prototype and test it in a real-case scenario. Specifically, we use public data from the most important regional pork meat markets in Spain published by the Ministry of Agriculture with a two-week delay and subscription-based data of the same markets obtained on the same day. The results show that the error difference between the bes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;FedVS&#65292;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28382;&#21518;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;</title><link>http://arxiv.org/abs/2304.13407</link><description>&lt;p&gt;
FedVS: &#38754;&#21521;&#20998;&#21106;&#27169;&#22411;&#30340;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;FedVS&#65292;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28382;&#21518;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#35768;&#22810;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#32452;&#25104;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#34987;&#22402;&#30452;&#20998;&#21106;&#65292;&#19981;&#21516;&#30340;&#29305;&#24449;&#23384;&#20648;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#19978;&#12290;&#20998;&#21106;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#35757;&#32451;&#19968;&#20010;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#21010;&#20998;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20998;&#21106;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#36831;&#28382;&#30340;&#23458;&#25143;&#31471;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#65307;2&#65289;&#23458;&#25143;&#31471;&#19978;&#20256;&#25968;&#25454;&#23884;&#20837;&#23548;&#33268;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedVS&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;FedVS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#26412;&#22320;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#31192;&#23494;&#20849;&#20139;&#26041;&#26696;&#65292;&#20174;&#32780;&#20445;&#35777;&#38024;&#23545;&#21246;&#32467;&#23458;&#25143;&#21644;&#22909;&#22855;&#26381;&#21153;&#22120;&#30340;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#65292;&#24182;&#19988;&#36890;&#36807;&#35299;&#23494;&#35745;&#31639;&#32929;&#20221;&#65292;&#26080;&#25439;&#37325;&#26500;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#30340;&#27719;&#24635;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;VFL&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#34920;&#26684;&#65292;CV&#65292;&#22270;&#20687;&#65292;NLP&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedVS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#36890;&#36807;&#23616;&#37096;&#35266;&#27979;&#23398;&#20064;&#39044;&#27979;&#30495;&#23454;&#29615;&#22659;&#20013;&#23548;&#33322;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#32988;&#36807;&#20004;&#20010;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26080;&#20559;&#26412;&#22320;&#26041;&#21521;&#36719;&#36710;&#36947;&#27010;&#29575;&#22330;&#12290;</title><link>http://arxiv.org/abs/2304.13242</link><description>&lt;p&gt;
&#20174;&#23616;&#37096;&#35266;&#27979;&#23398;&#20064;&#39044;&#27979;&#23548;&#33322;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning to Predict Navigational Patterns from Partial Observations. (arXiv:2304.13242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#36890;&#36807;&#23616;&#37096;&#35266;&#27979;&#23398;&#20064;&#39044;&#27979;&#30495;&#23454;&#29615;&#22659;&#20013;&#23548;&#33322;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#32988;&#36807;&#20004;&#20010;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26080;&#20559;&#26412;&#22320;&#26041;&#21521;&#36719;&#36710;&#36947;&#27010;&#29575;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#36981;&#23432;&#30456;&#20114;&#30693;&#26195;&#30340;&#23548;&#33322;&#27169;&#24335;&#22312;&#36981;&#24490;&#35268;&#21017;&#30340;&#29615;&#22659;&#19979;&#36827;&#34892;&#21512;&#20316;&#23548;&#33322;&#65292;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#34920;&#31034;&#20026;&#26041;&#21521;&#36335;&#24452;&#25110;&#36947;&#36335;&#36710;&#36947;&#12290;&#20174;&#19981;&#23436;&#20840;&#35266;&#27979;&#21040;&#30340;&#29615;&#22659;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23548;&#33322;&#27169;&#24335;&#26159;&#26234;&#33021;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#26410;&#26144;&#23556;&#20301;&#32622;&#25805;&#20316;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#23450;&#20041;&#36825;&#20123;&#23548;&#33322;&#27169;&#24335;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#23398;&#20064;&#25512;&#26029;&#30495;&#23454;&#29615;&#22659;&#20013;&#23548;&#33322;&#27169;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#20960;&#20309;&#25968;&#25454;&#22686;&#24378;&#65292;&#39044;&#27979;&#19990;&#30028;&#24314;&#27169;&#21644;&#20449;&#24687;&#35770;&#27491;&#21017;&#21270;&#22120;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#39044;&#27979;&#26080;&#20559;&#26412;&#22320;&#26041;&#21521;&#36719;&#36710;&#36947;&#27010;&#29575;&#65288;DSLP&#65289;&#22330;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#26368;&#22823;&#20284;&#28982;&#22270;&#25311;&#21512;&#21040;DSLP&#22330;&#20013;&#26469;&#25512;&#26029;&#20840;&#23616;&#23548;&#33322;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#23398;&#20064;&#39044;&#27979;&#23548;&#33322;&#27169;&#24335;&#30340;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#20004;&#20010;SOTA&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human beings cooperatively navigate rule-constrained environments by adhering to mutually known navigational patterns, which may be represented as directional pathways or road lanes. Inferring these navigational patterns from incompletely observed environments is required for intelligent mobile robots operating in unmapped locations. However, algorithmically defining these navigational patterns is nontrivial. This paper presents the first self-supervised learning (SSL) method for learning to infer navigational patterns in real-world environments from partial observations only. We explain how geometric data augmentation, predictive world modeling, and an information-theoretic regularizer enables our model to predict an unbiased local directional soft lane probability (DSLP) field in the limit of infinite data. We demonstrate how to infer global navigational patterns by fitting a maximum likelihood graph to the DSLP field. Experiments show that our SSL model outperforms two SOTA supervis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#23548;&#21521;&#21644;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340; Faster R-CNN &#31639;&#27861;&#26469;&#24314;&#35758;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#27969;&#38376;&#30340;&#20301;&#32622;&#65292;&#24182;&#35780;&#20272;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#65292;&#26377;&#25928;&#22320;&#22635;&#34917;&#20102;&#32463;&#39564;&#19981;&#36275;&#30340;&#36229;&#22768;&#21307;&#29983;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2304.05463</link><description>&lt;p&gt;
&#33258;&#21160;&#23548;&#21521;&#21644;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#29992;&#20110;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery. (arXiv:2304.05463v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05463
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#23548;&#21521;&#21644;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340; Faster R-CNN &#31639;&#27861;&#26469;&#24314;&#35758;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#27969;&#38376;&#30340;&#20301;&#32622;&#65292;&#24182;&#35780;&#20272;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#65292;&#26377;&#25928;&#22320;&#22635;&#34917;&#20102;&#32463;&#39564;&#19981;&#36275;&#30340;&#36229;&#22768;&#21307;&#29983;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32974;&#20799;&#36229;&#22768;&#31579;&#26597;&#20013;&#65292;&#36890;&#36807;&#33040;&#24102;&#36827;&#34892;&#30417;&#27979;&#30340;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#22270;&#20687;&#23545;&#20110;&#30417;&#27979;&#32974;&#20799;&#30340;&#34880;&#28082;&#20379;&#24212;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25429;&#25417;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#22270;&#20687;&#65292;&#38656;&#35201;&#27491;&#30830;&#22320;&#25191;&#34892;&#22810;&#20010;&#27493;&#39588;&#65306;&#22312;&#36229;&#22768;&#22270;&#20687;&#20013;&#25918;&#32622;&#38376;&#65292;&#20197;&#33719;&#21462;&#34880;&#27969;&#27874;&#24418;&#65292;&#24182;&#21028;&#26029;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#12290;&#36825;&#20123;&#27493;&#39588;&#37117;&#20381;&#36182;&#20110;&#25805;&#20316;&#32773;&#30340;&#32463;&#39564;&#12290;&#32463;&#39564;&#19981;&#36275;&#30340;&#36229;&#22768;&#21307;&#29983;&#30340;&#30701;&#32570;&#22240;&#27492;&#20135;&#29983;&#20102;&#26426;&#22120;&#36741;&#21161;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#26469;&#22635;&#34917;&#36825;&#20010;&#32570;&#21475;&#12290;&#20351;&#29992;&#25913;&#36827;&#30340; Faster R-CNN &#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#24314;&#35758;&#22810;&#26222;&#21202;&#27969;&#38376;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#38543;&#21518;&#35780;&#20272;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#22269;&#23478;&#36229;&#22768;&#31579;&#26597;&#25968;&#25454;&#24211;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#28085;&#30422;&#20102;657&#20010;&#25195;&#25551;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25351;&#23548;&#25805;&#20316;&#32773;&#25429;&#25417;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#22270;&#20687;&#21644;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fetal ultrasound screening, Doppler images on the umbilical artery (UA) are important for monitoring blood supply through the umbilical cord. However, to capture UA Doppler images, a number of steps need to be done correctly: placing the gate at a proper location in the ultrasound image to obtain blood flow waveforms, and judging the Doppler waveform quality. Both of these rely on the operator's experience. The shortage of experienced sonographers thus creates a demand for machine assistance. We propose an automatic system to fill this gap. Using a modified Faster R-CNN we obtain an algorithm that suggests Doppler flow gate locations. We subsequently assess the Doppler waveform quality. We validate the proposed system on 657 scans from a national ultrasound screening database. The experimental results demonstrate that our system is useful in guiding operators for UA Doppler image capture and quality assessment.
&lt;/p&gt;</description></item><item><title>SLPerf&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#21644;&#24320;&#25918;&#24335;&#30740;&#31350;&#24211;&#65292;&#29992;&#20110;&#20849;&#20139;&#23398;&#20064;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#24773;&#20917;&#19979;&#19981;&#21516;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01502</link><description>&lt;p&gt;
SLPerf&#65306;&#22522;&#20934;&#27979;&#35797;&#20849;&#20139;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SLPerf: a Unified Framework for Benchmarking Split Learning. (arXiv:2304.01502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01502
&lt;/p&gt;
&lt;p&gt;
SLPerf&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#21644;&#24320;&#25918;&#24335;&#30740;&#31350;&#24211;&#65292;&#29992;&#20110;&#20849;&#20139;&#23398;&#20064;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#24773;&#20917;&#19979;&#19981;&#21516;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#20849;&#20139;&#23398;&#20064;&#33539;&#24335;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#26085;&#30410;&#20005;&#37325;&#65292;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#20013;&#22830;&#21270;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#38656;&#35201;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#26694;&#26550;&#65306;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#12290;&#34429;&#28982;FL&#24050;&#32463;&#24314;&#31435;&#20102;&#21508;&#31181;&#22522;&#20934;&#26694;&#26550;&#21644;&#30740;&#31350;&#24211;&#65292;&#20294;SL&#30446;&#21069;&#23578;&#32570;&#20047;&#32479;&#19968;&#30340;&#24211;&#65292;&#23588;&#20854;&#26159;&#22312;&#26631;&#31614;&#20849;&#20139;&#12289;&#27169;&#22411;&#32858;&#21512;&#21644;&#20999;&#21106;&#23618;&#36873;&#25321;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#31181;&#26631;&#20934;&#21270;&#32570;&#20047;&#20351;&#24471;&#27604;&#36739;SL&#33539;&#24335;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLPerf&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20849;&#20139;&#23398;&#20064;&#30740;&#31350;&#26694;&#26550;&#21644;&#24320;&#25918;&#24335;&#30740;&#31350;&#24211;&#65292;&#24182;&#22312;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;IID&#21644;&#38750;IID&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;SL&#33539;&#24335;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#19981;&#21516;SL&#33539;&#24335;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#36827;&#34892;&#35814;&#32454;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#20197;&#21450;&#29992;&#20110;&#25913;&#36827;SL&#33539;&#24335;&#30340;&#20016;&#23500;&#24037;&#31243;&#24102;&#36208;&#20449;&#24687;&#21644;&#30740;&#31350;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy concerns has made centralized training of data, which is scattered across silos, infeasible, leading to the need for collaborative learning frameworks. To address that, two prominent frameworks emerged, i.e., federated learning (FL) and split learning (SL). While FL has established various benchmark frameworks and research libraries, SL currently lacks a unified library despite its diversity in terms of label sharing, model aggregation, and cut layer choice. This lack of standardization makes comparing SL paradigms difficult. To address this, we propose SLPerf, a unified research framework and open research library for SL, and conduct extensive experiments on four widely-used datasets under both IID and Non-IID data settings. Our contributions include a comprehensive survey of recently proposed SL paradigms, a detailed benchmark comparison of different SL paradigms in different situations, and rich engineering take-away messages and research insights for improving SL parad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#35013;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21387;&#32553;&#24615;&#33021;&#65292;&#22312;&#39640;&#28165;&#35270;&#39057;&#20256;&#36755;&#21644;&#35821;&#38899;&#35782;&#21035;&#35270;&#39057;&#21387;&#32553;&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.11473</link><description>&lt;p&gt;
&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;&#65306;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23553;&#35013;&#26469;&#39640;&#25928;&#25193;&#23637;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers. (arXiv:2303.11473v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#35013;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21387;&#32553;&#24615;&#33021;&#65292;&#22312;&#39640;&#28165;&#35270;&#39057;&#20256;&#36755;&#21644;&#35821;&#38899;&#35782;&#21035;&#35270;&#39057;&#21387;&#32553;&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;--&#19968;&#31181;&#22312;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#21608;&#22260;&#21253;&#35013;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#39057;&#21387;&#32553;&#31995;&#32479;&#12290;&#35813;&#22841;&#24515;&#26694;&#26550;&#30001;&#31070;&#32463;&#21069;&#22788;&#29702;&#22120;&#12289;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#21644;&#31070;&#32463;&#21518;&#22788;&#29702;&#22120;&#32452;&#25104;&#12290;&#36825;&#20123;&#32593;&#32476;&#34987;&#32852;&#21512;&#35757;&#32451;&#20197;&#20248;&#21270;&#30721;&#29575;-&#22833;&#30495;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#21387;&#32553;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#21892;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#38656;&#35201;&#19968;&#20010;&#21487;&#24494;&#30340;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#20195;&#29702;&#65292;&#23427;&#21253;&#25324;&#26102;&#38388;&#22788;&#29702;&#12289;&#36816;&#21160;&#34917;&#20607;&#12289;&#20869;/&#38388;&#27169;&#24335;&#20915;&#31574;&#21644;&#24490;&#29615;&#28388;&#27874;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20851;&#38190;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#32452;&#20214;&#30340;&#21487;&#24494;&#36924;&#36817;&#65292;&#24182;&#35777;&#26126;&#20102;&#22841;&#24515;&#30340;&#31070;&#32463;&#32534;&#30721;&#30456;&#23545;&#20110;&#22312;&#20004;&#20010;&#37325;&#35201;&#22330;&#26223;&#20013;&#21387;&#32553;&#36755;&#20837;&#35270;&#39057;&#30340;&#21407;&#22987;&#24103;&#32780;&#35328;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#22909;&#30340;&#30721;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;&#22312;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;HEVC&#20256;&#36755;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#30340;&#24773;&#20917;&#19979;&#65292;&#22841;&#24515;&#31995;&#32479;&#33719;&#24471;&#20102;6.5 dB&#30340;PSNR&#25913;&#21892;&#65307;&#22312;&#21478;&#19968;&#31181;&#22330;&#26223;&#20013;&#65292;&#21387;&#32553;&#22823;&#35789;&#27719;&#35821;&#38899;&#35782;&#21035;&#35270;&#39057;&#22312;0.02 bpp&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22841;&#24515;&#31995;&#32479;&#65292;&#33719;&#24471;&#20102;30%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose sandwiched video compression -- a video compression system that wraps neural networks around a standard video codec. The sandwich framework consists of a neural pre- and post-processor with a standard video codec between them. The networks are trained jointly to optimize a rate-distortion loss function with the goal of significantly improving over the standard codec in various compression scenarios. End-to-end training in this setting requires a differentiable proxy for the standard video codec, which incorporates temporal processing with motion compensation, inter/intra mode decisions, and in-loop filtering. We propose differentiable approximations to key video codec components and demonstrate that the neural codes of the sandwich lead to significantly better rate-distortion performance compared to compressing the original frames of the input video in two important scenarios. When transporting high-resolution video via low-resolution HEVC, the sandwich system obtains 6.5 dB
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2303.10135</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#25928;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#65288;RASP&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#20195;&#21046;&#36896;&#19994;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#24212;&#21147;&#65292;&#38543;&#30528;&#23545;&#26356;&#22823;&#37327;&#21270;&#29983;&#20135;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#31181;&#33258;&#21160;&#21270;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20174;&#19981;&#26029;&#22686;&#21152;&#30340;&#28508;&#22312;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#36827;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#35013;&#37197;&#36824;&#38656;&#35201;&#25104;&#26412;&#26114;&#36149;&#30340;&#21487;&#34892;&#24615;&#26816;&#26597;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#25324;&#20135;&#21697;&#35013;&#37197;&#22270;&#30340;&#22270;&#24418;&#26041;&#27861;&#21644;&#19968;&#20010;&#21517;&#20026;GRACE&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;GRACE&#20174;&#22270;&#24418;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#36880;&#27493;&#39044;&#27979;&#35013;&#37197;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22312;&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#39044;&#27979;&#38109;&#22411;&#26448;&#20135;&#21697;&#21464;&#20307;&#30340;&#21487;&#34892;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CubeScope&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#22810;&#26041;&#38754;&#22320;&#25366;&#25496;&#22797;&#26434;&#30340;&#26102;&#38388;&#25139;&#20107;&#20214;&#27969;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#31361;&#28982;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#19981;&#21516;&#30340;&#21160;&#24577;&#27169;&#24335;&#65292;&#24182;&#23545;&#25152;&#26377;&#23646;&#24615;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#30340;&#32676;&#20307;&#21644;&#20854;&#20851;&#31995;&#12290;CubeScope&#36824;&#33021;&#26816;&#27979;&#21040;&#24322;&#24120;&#30340;&#31361;&#28982;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03789</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#22810;&#26041;&#38754;&#25366;&#25496;&#22797;&#26434;&#30340;&#26102;&#38388;&#25139;&#20107;&#20214;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast and Multi-aspect Mining of Complex Time-stamped Event Streams. (arXiv:2303.03789v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CubeScope&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#22810;&#26041;&#38754;&#22320;&#25366;&#25496;&#22797;&#26434;&#30340;&#26102;&#38388;&#25139;&#20107;&#20214;&#27969;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#31361;&#28982;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#19981;&#21516;&#30340;&#21160;&#24577;&#27169;&#24335;&#65292;&#24182;&#23545;&#25152;&#26377;&#23646;&#24615;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#30340;&#32676;&#20307;&#21644;&#20854;&#20851;&#31995;&#12290;CubeScope&#36824;&#33021;&#26816;&#27979;&#21040;&#24322;&#24120;&#30340;&#31361;&#28982;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#26102;&#38388;&#28436;&#21464;&#20107;&#20214;&#27969;&#65288;&#22914;&#22312;&#32447;&#36141;&#29289;&#26085;&#24535;&#65306;&#39033;&#30446;&#12289;&#20215;&#26684;&#12289;&#21697;&#29260;&#12289;&#26102;&#38388;&#21644;&#22320;&#29702;&#20301;&#32622;&#27963;&#21160;&#65306;&#19978;&#36710;&#21644;&#19979;&#36710;&#22320;&#28857;&#12289;&#26102;&#38388;&#65289;&#65292;&#25105;&#20204;&#22914;&#20309;&#23545;&#22823;&#35268;&#27169;&#12289;&#21160;&#24577;&#12289;&#39640;&#38454;&#24352;&#37327;&#27969;&#36827;&#34892;&#25688;&#35201;&#65311;&#25105;&#20204;&#30340;&#22238;&#31572;&#26159;&#19987;&#27880;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#24335;&#65292;&#21363;&#8220;&#21046;&#24230;&#8221;&#21644;&#8220;&#32452;&#20214;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;CubeScope&#26469;&#22788;&#29702;&#39640;&#38454;&#24352;&#37327;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35782;&#21035;&#20219;&#20309;&#31361;&#28982;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#24182;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#21160;&#24577;&#27169;&#24335;&#8220;&#21046;&#24230;&#8221;&#65288;&#20363;&#22914;&#24037;&#20316;&#26085;/&#21608;&#26411;/&#20551;&#26399;&#27169;&#24335;&#65289;&#12290;&#22312;&#27599;&#20010;&#21046;&#24230;&#20013;&#65292;&#23427;&#36824;&#23545;&#25152;&#26377;&#23646;&#24615;&#65288;&#20363;&#22914;&#39033;&#30446;&#12289;&#20215;&#26684;&#12289;&#21697;&#29260;&#21644;&#26102;&#38388;&#65289;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#28508;&#22312;&#32676;&#20307;&#65288;&#20363;&#22914;&#39033;&#30446;/&#21697;&#29260;&#32676;&#65289;&#21450;&#20854;&#20851;&#31995;&#30340;&#38544;&#34255;&#30340;&#8220;&#32452;&#20214;&#8221;&#12290;&#30001;&#20110;&#20854;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#25688;&#35201;&#65292;CubeScope&#36824;&#33021;&#26816;&#27979;&#21040;&#24322;&#24120;&#30340;&#31361;&#28982;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a huge, online stream of time-evolving events with multiple attributes, such as online shopping logs: (item, price, brand, time), and local mobility activities: (pick-up and drop-off locations, time), how can we summarize large, dynamic high-order tensor streams? How can we see any hidden patterns, rules, and anomalies? Our answer is to focus on two types of patterns, i.e., ''regimes'' and ''components'', for which we present CubeScope, an efficient and effective method over high-order tensor streams. Specifically, it identifies any sudden discontinuity and recognizes distinct dynamical patterns, ''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also performs multi-way summarization for all attributes (e.g., item, price, brand, and time) and discovers hidden ''components'' representing latent groups (e.g., item/brand groups) and their relationship. Thanks to its concise but effective summarization, CubeScope can also detect the sudden appearance of anomalie
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#38454;&#24179;&#22374;&#24230;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26799;&#24230;&#33539;&#25968;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#23547;&#25214;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#20855;&#26377;&#22343;&#21248;&#23567;&#26354;&#29575;&#30340;&#26497;&#23567;&#20540;&#65292;&#25552;&#39640;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;&#21644;&#27979;&#35797;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.03108</link><description>&lt;p&gt;
&#26799;&#24230;&#33539;&#25968;&#24863;&#30693;&#26368;&#23567;&#21270;&#22312;&#23547;&#25214;&#19968;&#38454;&#24179;&#22374;&#24230;&#20013;&#25913;&#36827;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03108
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#38454;&#24179;&#22374;&#24230;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26799;&#24230;&#33539;&#25968;&#24863;&#30693;&#26368;&#23567;&#21270;&#31639;&#27861;&#23547;&#25214;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#20855;&#26377;&#22343;&#21248;&#23567;&#26354;&#29575;&#30340;&#26497;&#23567;&#20540;&#65292;&#25552;&#39640;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;&#21644;&#27979;&#35797;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270; (SAM) &#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM &#21450;&#20854;&#21518;&#32493;&#35752;&#35770;&#20013;&#24403;&#21069;&#20851;&#20110;&#24179;&#22374;&#24615;&#30340;&#23450;&#20041;&#20165;&#38480;&#20110;&#38646;&#38454;&#24179;&#22374;&#24615; (&#21363;&#25200;&#21160;&#21322;&#24452;&#20869;&#26368;&#22351;&#25439;&#22833;)&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#23384;&#22312;&#21333;&#19968;&#26368;&#23567;&#20540;&#25110;&#32473;&#23450;&#25200;&#21160;&#21322;&#24452;&#20869;&#30340;&#22810;&#20010;&#26368;&#23567;&#20540;&#26102;&#65292;&#38646;&#38454;&#24179;&#22374;&#24230;&#21487;&#33021;&#19981;&#36275;&#20197;&#21306;&#20998;&#20855;&#26377;&#20302;&#27867;&#21270;&#35823;&#24046;&#21644;&#39640;&#27867;&#21270;&#35823;&#24046;&#30340;&#26497;&#23567;&#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#38454;&#24179;&#22374;&#24230;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24378;&#30340;&#24179;&#22374;&#24230;&#27979;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#25200;&#21160;&#21322;&#24452;&#20869;&#30340;&#26368;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#20854;&#38480;&#21046;&#20102;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340; Hessian &#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#21644; SAM &#30340;&#27491;&#21017;&#21270;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Gradient norm Aware Minimization (GAM) &#30340;&#26032;&#22411;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23547;&#25214;&#25152;&#26377;&#26041;&#21521;&#19978;&#26354;&#29575;&#22343;&#21248;&#23567;&#30340;&#26368;&#23567;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GAM &#26174;&#30528;&#25913;&#36827;&#20102;&#24191;&#20041;&#21270;&#33021;&#21147;&#21644;&#27979;&#35797;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#31070;&#32463;&#21183;&#30340;&#38750;&#24179;&#34913;&#20998;&#23376;&#21435;&#22122;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#39640;&#21183;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02216</link><description>&lt;p&gt;
&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#31070;&#32463;&#21183;&#30340;&#38750;&#24179;&#34913;&#20998;&#23376;&#21435;&#22122;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials. (arXiv:2303.02216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#31070;&#32463;&#21183;&#30340;&#38750;&#24179;&#34913;&#20998;&#23376;&#21435;&#22122;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#39640;&#21183;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36827;&#23637;&#20351;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#24320;&#21457;&#24555;&#36895;&#30340;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21147;&#23398;&#65288;QM&#65289;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#20998;&#23376;&#21183;&#33021;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;GNN&#24314;&#31435;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#30340;&#21183;&#33021;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#21463;&#21040;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;QM&#26041;&#27861;&#30340;&#29702;&#35770;&#23618;&#27425;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#20998;&#23376;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#38750;&#24179;&#34913;&#20998;&#23376;&#26500;&#22411;&#36827;&#34892;&#21435;&#22122;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#30340;GNN&#21183;&#33021;&#39044;&#27979;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#37319;&#26679;&#30340;&#38750;&#24179;&#34913;&#26500;&#22411;&#30340;&#21407;&#23376;&#22352;&#26631;&#36827;&#34892;&#38543;&#26426;&#22122;&#22768;&#25200;&#21160;&#65292;&#24182;&#39044;&#35757;&#32451;GNN&#23545;&#25200;&#21160;&#30340;&#20998;&#23376;&#26500;&#22411;&#36827;&#34892;&#21435;&#22122;&#65292;&#20174;&#32780;&#24674;&#22797;&#21407;&#22987;&#22352;&#26631;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#21183;&#33021;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#21487;&#36801;&#31227;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#19981;&#21516;&#31995;&#32479;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in equivariant graph neural networks (GNNs) have made deep learning amenable to developing fast surrogate models to expensive ab initio quantum mechanics (QM) approaches for molecular potential predictions. However, building accurate and transferable potential models using GNNs remains challenging, as the data is greatly limited by the expensive computational costs and level of theory of QM methods, especially for large and complex molecular systems. In this work, we propose denoise pretraining on nonequilibrium molecular conformations to achieve more accurate and transferable GNN potential predictions. Specifically, atomic coordinates of sampled nonequilibrium conformations are perturbed by random noises and GNNs are pretrained to denoise the perturbed molecular conformations which recovers the original coordinates. Rigorous experiments on multiple benchmarks reveal that pretraining significantly improves the accuracy of neural potentials. Furthermore, we show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28151;&#21512;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#29616;&#20102;&#32479;&#35745;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#30830;&#23450;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#20043;&#38388;&#30340;&#24179;&#28369;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.02118</link><description>&lt;p&gt;
&#28151;&#21512;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#32479;&#35745;&#19982;&#35745;&#31639;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression. (arXiv:2303.02118v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28151;&#21512;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#29616;&#20102;&#32479;&#35745;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#30830;&#23450;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#20043;&#38388;&#30340;&#24179;&#28369;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20004;&#20010;&#37096;&#20998;&#30340;&#28151;&#21512;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#38656;&#35201;&#20174;n&#20010;&#26080;&#26631;&#31614;&#30340;&#22122;&#22768;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;&#20004;&#20010;&#23454;&#25968;k&#31232;&#30095;&#20449;&#21495;&#946;1&#12289;&#946;2&#12290;&#31232;&#30095;&#24230;&#20801;&#35768;&#22312;&#32500;&#24230;&#19978;&#26159;&#20122;&#32447;&#24615;&#30340;&#65292;&#19988;&#20551;&#35774;&#28155;&#21152;&#30340;&#22122;&#22768;&#26159;&#29420;&#31435;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#26041;&#24046;&#20026;&#963;&#178;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#38382;&#39064;&#23384;&#22312;&#19968;&#20010;k/SNR&#178;&#21040;k&#178;/SNR&#178;&#30340;&#32479;&#35745;&#21040;&#35745;&#31639;&#38388;&#38553;&#65292;&#31867;&#20284;&#20110;&#20854;&#20182;&#20855;&#26377;&#35745;&#31639;&#25361;&#25112;&#24615;&#30340;&#39640;&#32500;&#25512;&#26029;&#38382;&#39064;&#65292;&#22914;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#40065;&#26834;&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;&#65307;&#36825;&#37324;&#30340;SNR&#26159;&#20449;&#22122;&#27604;&#12290;&#36890;&#36807;&#20302;&#27425;&#22810;&#39033;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#23384;&#22312;&#26356;&#24191;&#27867;&#30340;&#35745;&#31639;&#38556;&#30861;&#65292;&#20294;&#21482;&#22312;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#31216;&#21442;&#25968;&#33539;&#22260;&#20869;&#25165;&#34920;&#29616;&#20986;&#35745;&#31639;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;n&#21644;&#36816;&#34892;&#26102;&#38388;&#20043;&#38388;&#30340;&#24179;&#28369;&#20449;&#24687;-&#35745;&#31639;&#26435;&#34913;&#20851;&#31995;&#65292;&#23545;&#20110;&#20219;&#20309;&#38543;&#26426;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of mixed sparse linear regression with two components, where two real $k$-sparse signals $\beta_1, \beta_2$ are to be recovered from $n$ unlabelled noisy linear measurements. The sparsity is allowed to be sublinear in the dimension, and additive noise is assumed to be independent Gaussian with variance $\sigma^2$. Prior work has shown that the problem suffers from a $\frac{k}{SNR^2}$-to-$\frac{k^2}{SNR^2}$ statistical-to-computational gap, resembling other computationally challenging high-dimensional inference problems such as Sparse PCA and Robust Sparse Mean Estimation; here $SNR$ is the signal-to-noise ratio. We establish the existence of a more extensive computational barrier for this problem through the method of low-degree polynomials, but show that the problem is computationally hard only in a very narrow symmetric parameter regime. We identify a smooth information-computation tradeoff between the sample complexity $n$ and runtime for any randomized algor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07729</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#37117;&#20197;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#21069;&#35328;&#65292;&#20197;&#24635;&#32467;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#20142;&#28857;&#19981;&#20165;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#35770;&#25991;&#30340;&#36129;&#29486;&#65292;&#36824;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#22686;&#21152;&#20102;&#25991;&#31456;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#30740;&#31350;&#35770;&#25991;&#30340;&#29305;&#23450;&#27573;&#33853;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#26500;&#24314;&#30740;&#31350;&#20142;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#35206;&#30422;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#23618;&#30340;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65292;&#23558;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20026;SciBERT&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;CSPubSum&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36824;&#25552;&#20986;&#20102;MixSub&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#26032;&#30340;&#36328;&#23398;&#31185;&#35770;&#25991;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;CSPubSum&#21644;MixSub&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#30456;&#20851;&#21464;&#20307;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;CSPubSum&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#35770;&#25991;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25552;&#21462;&#30340;&#24515;&#30005;&#22270;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#23545;&#29992;&#25143;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06529</link><description>&lt;p&gt;
&#20805;&#20998;&#21457;&#25381;&#24515;&#30005;&#22270;&#30340;&#33021;&#21147;&#65306;&#19968;&#31181;&#22312;&#20855;&#26377;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#21307;&#30103;&#31995;&#32479;&#20013;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Electrocardiograms: A novel approach for Patient Identification in Healthcare Systems with ECG Signals. (arXiv:2302.06529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25552;&#21462;&#30340;&#24515;&#30005;&#22270;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#23545;&#29992;&#25143;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#21033;&#29992;&#24515;&#33039;&#20449;&#21495;&#20316;&#20026;&#29983;&#29289;&#35782;&#21035;&#27169;&#24335;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;&#23545;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#35782;&#21035;&#31995;&#32479;&#22312;&#22810;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20840;&#38754;&#20102;&#35299;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#26222;&#36890;&#29992;&#25143;&#35782;&#21035;&#20013;&#24448;&#24448;&#24573;&#30053;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#24739;&#32773;&#30340;&#24515;&#34880;&#31649;&#29366;&#20917;&#65292;&#30830;&#20445;&#25152;&#24471;&#32467;&#26524;&#19981;&#20855;&#26377;&#20559;&#35265;&#25110;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25152;&#24471;&#32467;&#26524;&#20855;&#26377;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the course of the past two decades, a substantial body of research has substantiated the viability of utilising cardiac signals as a biometric modality. This paper presents a novel approach for patient identification in healthcare systems using electrocardiogram signals. A convolutional neural network is used to classify users based on images extracted from ECG signals. The proposed identification system is evaluated in multiple databases, providing a comprehensive understanding of its potential in real-world scenarios. The impact of Cardiovascular Diseases on generic user identification has been largely overlooked in previous studies. The presented method takes into account the cardiovascular condition of the patients, ensuring that the results obtained are not biased or limited. Furthermore, the results obtained are consistent and reliable, with lower error rates and higher accuracy metrics, as demonstrated through extensive experimentation. All these features make the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2302.00736</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#36793;&#38469;&#36129;&#29486;&#36817;&#20284;&#35745;&#31639;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#20026;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#29609;&#23478;&#20998;&#37197;&#26377;&#24847;&#20041;&#30340;&#36129;&#29486;&#20540;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#65292;&#26368;&#36817;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;Shapley&#20540;&#30340;&#26377;&#24847;&#20041;&#24615;&#28304;&#20110;&#20165;&#26377;Shapley&#20540;&#28385;&#36275;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#30830;&#20999;&#35745;&#31639;&#30340;&#20195;&#20215;&#26159;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#39640;&#25928;&#36817;&#20284;Shapley&#20540;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22260;&#32469;&#30528;&#29609;&#23478;&#30340;&#36793;&#38469;&#36129;&#29486;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#30340;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#25324;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#29992;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#30340;&#23454;&#35777;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.13166</link><description>&lt;p&gt;
ESC&#65306;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#23548;&#33322;&#21040;&#29305;&#23450;&#29289;&#20307;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#24182;&#19982;&#29289;&#20307;&#20132;&#20114;&#20197;&#23436;&#25104;&#20219;&#21153;&#30340;&#23454;&#20307;&#20195;&#29702;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#26631;&#35760;&#29289;&#20307;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#36825;&#31181;&#35757;&#32451;&#25928;&#26524;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26032;&#39062;&#29289;&#20307;&#19978;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861;&#8212;&#8212;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#25506;&#32034;&#65288;ESC&#65289;&#65292;&#23427;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#36716;&#31227;&#21040;&#22312;&#35270;&#35273;&#29615;&#22659;&#19978;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#23548;&#33322;&#26102;&#19981;&#38656;&#35201;&#36827;&#34892;&#23548;&#33322;&#25110;&#20854;&#20182;&#35270;&#35273;&#29615;&#22659;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;ESC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#22320;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24120;&#35782;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25151;&#38388;&#21644;&#29289;&#20307;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;ESC&#36890;&#36807;&#23558;&#24120;&#35782;&#30693;&#35782;&#24314;&#27169;&#20026;&#36719;&#36923;&#36753;&#35859;&#35789;&#26469;&#20351;&#20854;&#36716;&#21270;&#20026;&#23548;&#33322;&#21160;&#20316;&#65292;&#20174;&#32780;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#22312;MP3D&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;......
&lt;/p&gt;
&lt;p&gt;
The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12313</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#32570;&#22833;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#12290;&#26368;&#36817;&#65292;Arakelyan&#31561;&#20154;&#65288;2021&#65289;&#65307;Minervini&#31561;&#20154;&#65288;2022&#65289;&#34920;&#26126;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#20063;&#21487;&#20197;&#29992;&#20110;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65306;&#20182;&#20204;&#30340;&#36830;&#32493;&#26597;&#35810;&#20998;&#35299;&#65288;CQD&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#26597;&#35810;&#65292;&#20351;&#29992;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22238;&#31572;&#24182;&#36890;&#36807;t-&#33539;&#25968;&#26469;&#32858;&#21512;&#20854;&#20998;&#25968;&#65292;&#20197;&#23545;&#27599;&#20010;&#22797;&#26434;&#26597;&#35810;&#30340;&#31572;&#26696;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;CQD&#19981;&#22788;&#29702;&#21542;&#23450;&#24182;&#19988;&#20165;&#20351;&#29992;&#21407;&#23376;&#35757;&#32451;&#26597;&#35810;&#30340;&#35757;&#32451;&#20449;&#21495;&#65306;&#22312;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26399;&#38388;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#27809;&#26377;&#36890;&#36807;&#27169;&#31946;&#36923;&#36753;t-&#33539;&#25968;&#36827;&#34892;&#26657;&#20934;&#20197;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#36825;&#20010;&#26032;&#32452;&#20214;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#27861;&#22312;&#22797;&#26434;&#26597;&#35810;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24120;&#29992;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#24471;&#20986;&#26041;&#27861;&#30340;&#38598;&#21512;&#36890;&#24120;&#20250;&#26377;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.01054</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#24120;&#35265;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise. (arXiv:2301.01054v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24120;&#29992;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#24471;&#20986;&#26041;&#27861;&#30340;&#38598;&#21512;&#36890;&#24120;&#20250;&#26377;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#26377;&#25152;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#33021;&#22815;&#21028;&#26029;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#23384;&#22312;&#36739;&#22823;&#30340;&#38169;&#35823;&#20998;&#31867;&#39118;&#38505;&#26102;&#33021;&#22815;&#25298;&#32477;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#20998;&#31867;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20215;&#65292;&#37325;&#28857;&#20851;&#27880;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#24212;&#22312;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#25298;&#32477;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#26631;&#31614;&#22122;&#22768;&#26041;&#38754;&#23545;&#20999;&#29255;&#32423;&#21035;&#21644;&#29926;&#29255;&#32423;&#21035;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#28145;&#24230;&#38598;&#25104;&#12289;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#12289;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#12289;&#27979;&#35797;&#26102;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#21518;&#20004;&#31181;&#26041;&#27861;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26041;&#27861;&#30340;&#38598;&#21512;&#36890;&#24120;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, deep learning has seen an increase in usage in the domain of histopathological applications. However, while these approaches have shown great potential, in high-risk environments deep learning models need to be able to judge their uncertainty and be able to reject inputs when there is a significant chance of misclassification. In this work, we conduct a rigorous evaluation of the most commonly used uncertainty and robustness methods for the classification of Whole Slide Images, with a focus on the task of selective classification, where the model should reject the classification in situations in which it is uncertain. We conduct our experiments on tile-level under the aspects of domain shift and label noise, as well as on slide-level. In our experiments, we compare Deep Ensembles, Monte-Carlo Dropout, Stochastic Variational Inference, Test-Time Data Augmentation as well as ensembles of the latter approaches. We observe that ensembles of methods generally lead to bett
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.09811</link><description>&lt;p&gt;
&#39640;&#25928;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#21024;&#20943;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#21452;&#35821;&#32763;&#35793;&#31995;&#32479;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#20174;&#30693;&#35782;&#36716;&#31227;&#20013;&#33719;&#30410;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#22810;&#35821;&#35328;&#24615;&#30340;&#38480;&#21046;&#65292;&#38500;&#38750;&#36827;&#34892;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#21542;&#21017;&#20250;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22823;&#24133;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;NLLB-200&#26159;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#20363;&#23376;&#12290;&#23427;&#28085;&#30422;&#20102;202&#31181;&#35821;&#35328;&#65292;&#20294;&#20165;&#25512;&#29702;&#23601;&#38656;&#35201;&#33267;&#23569;&#22235;&#20010;32GB&#30340;GPU&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#20801;&#35768;&#21024;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20294;&#32763;&#35793;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#35813;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#21098;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#35782;&#21035;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; R2N2 &#30340;&#36882;&#24402;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#23450;&#21046;&#30340;&#36845;&#20195;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;R2N2 &#23558;&#29983;&#25104;&#20449;&#24687;&#21644;&#32452;&#35013;&#20449;&#24687;&#30340;&#36807;&#31243;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35780;&#20272;&#20989;&#25968;&#26469;&#29983;&#25104;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#35780;&#20272;&#30340;&#32447;&#24615;&#32452;&#21512;&#29992;&#20110;&#26356;&#26032;&#19979;&#19968;&#27425;&#36845;&#20195;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#38477;&#20302;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.12386</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#36845;&#20195;&#31639;&#27861;&#30340;&#36882;&#24402;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476; (R2N2) &#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms. (arXiv:2211.12386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; R2N2 &#30340;&#36882;&#24402;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#23450;&#21046;&#30340;&#36845;&#20195;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;R2N2 &#23558;&#29983;&#25104;&#20449;&#24687;&#21644;&#32452;&#35013;&#20449;&#24687;&#30340;&#36807;&#31243;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35780;&#20272;&#20989;&#25968;&#26469;&#29983;&#25104;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#35780;&#20272;&#30340;&#32447;&#24615;&#32452;&#21512;&#29992;&#20110;&#26356;&#26032;&#19979;&#19968;&#27425;&#36845;&#20195;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#38477;&#20302;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#31639;&#27861;&#30340;&#20803;&#23398;&#20064;&#21253;&#25324;&#23545;&#31639;&#27861;&#32467;&#26500;&#21644;&#30456;&#20851;&#36229;&#21442;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#35782;&#21035;&#21644;&#36866;&#24212;&#12290;&#20026;&#20102;&#38480;&#21046;&#20803;&#23398;&#20064;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;&#20855;&#26377;&#23545;&#26377;&#21033;&#31639;&#27861;&#32467;&#26500;&#30340;&#26576;&#31181;&#24402;&#32435;&#20559;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#20043;&#21069;&#20171;&#32461;&#30340;&#40857;&#26684;&#65293;&#24211;&#22612;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#20026;&#19968;&#31181;&#36882;&#24402;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476; (R2N2) &#36229;&#32467;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#23450;&#21046;&#30340;&#36845;&#20195;&#31639;&#27861;&#12290;&#19982;&#29616;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#27169;&#22359;&#21010;&#20998;&#65292;&#29992;&#20110;&#29983;&#25104;&#20449;&#24687;&#21644;&#23558;&#35813;&#20449;&#24687;&#32452;&#35013;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20174;&#24403;&#21069;&#22806;&#37096;&#36845;&#20195;&#24320;&#22987;&#36827;&#34892;&#20174;&#23646;&#20869;&#37096;&#36845;&#20195;&#30340;&#24490;&#29615;&#20989;&#25968;&#35780;&#20272;&#65292;&#21487;&#20197;&#29983;&#25104;&#23616;&#37096;&#20449;&#24687;&#65292;&#20197;&#23376;&#31354;&#38388;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#19979;&#19968;&#27425;&#22806;&#37096;&#36845;&#20195;&#30340;&#26356;&#26032;&#26159;&#36890;&#36807;&#36825;&#20123;&#35780;&#20272;&#30340;&#32447;&#24615;&#32452;&#21512;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning of numerical algorithms for a given task consists of the data-driven identification and adaptation of an algorithmic structure and the associated hyperparameters. To limit the complexity of the meta-learning problem, neural architectures with a certain inductive bias towards favorable algorithmic structures can, and should, be used. We generalize our previously introduced Runge-Kutta neural network to a recursively recurrent neural network (R2N2) superstructure for the design of customized iterative algorithms. In contrast to off-the-shelf deep learning approaches, it features a distinct division into modules for generation of information and for the subsequent assembly of this information towards a solution. Local information in the form of a subspace is generated by subordinate, inner, iterations of recurrent function evaluations starting at the current outer iterate. The update to the next outer iterate is computed as a linear combination of these evaluations, reducing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#27425;&#39640;&#26031;&#19988;&#20855;&#26377;Lipschitz&#31215;&#20998;&#26680;&#26102;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#27493;&#38271;&#24207;&#21015;&#21644;&#31890;&#23376;&#25968;&#37327;&#65292;&#21487;&#20197;&#20197;1/&#8730;(log log n)&#30340;&#36895;&#24230;&#23558;&#26680;Stein&#24046;&#24322;&#36924;&#36817;&#38646;&#12290;</title><link>http://arxiv.org/abs/2211.09721</link><description>&lt;p&gt;
&#12298;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Finite-Particle Convergence Rate for Stein Variational Gradient Descent. (arXiv:2211.09721v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#27425;&#39640;&#26031;&#19988;&#20855;&#26377;Lipschitz&#31215;&#20998;&#26680;&#26102;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#27493;&#38271;&#24207;&#21015;&#21644;&#31890;&#23376;&#25968;&#37327;&#65292;&#21487;&#20197;&#20197;1/&#8730;(log log n)&#30340;&#36895;&#24230;&#23558;&#26680;Stein&#24046;&#24322;&#36924;&#36817;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#19968;&#32452;&#31890;&#23376;&#36924;&#36817;&#27010;&#29575;&#20998;&#24067;&#30340;&#27969;&#34892;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21482;&#35201;&#30446;&#26631;&#20998;&#24067;&#26159;&#27425;&#39640;&#26031;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;Lipschitz&#31215;&#20998;&#26680;&#65292;&#20351;&#29992;n&#20010;&#31890;&#23376;&#21644;&#36866;&#24403;&#30340;&#27493;&#38271;&#24207;&#21015;&#36827;&#34892;SVGD&#65292;&#26680;Stein&#24046;&#24322;&#23558;&#20197;1/&#8730;(log log n)&#30340;&#36895;&#24230;&#36235;&#20110;&#38646;&#12290;&#25105;&#20204;&#24576;&#30097;n&#30340;&#20381;&#36182;&#24615;&#21487;&#20197;&#25913;&#36827;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#35777;&#26126;&#31574;&#30053;&#33021;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with n particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order 1/sqrt(log log n) rate. We suspect that the dependence on n can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2210.14896</link><description>&lt;p&gt;
DiffusionDB: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#30011;&#24266;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14896
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#32454;&#33410;&#30340;&#22270;&#20687;&#38656;&#35201;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#32780;&#19988;&#24448;&#24448;&#19981;&#28165;&#26970;&#27169;&#22411;&#23545;&#19981;&#21516;&#25552;&#31034;&#30340;&#21453;&#24212;&#25110;&#26368;&#20339;&#25552;&#31034;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffusionDB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;6.5TB&#65292;&#21253;&#21547;&#20351;&#29992;Stable Diffusion&#29983;&#25104;&#30340;1400&#19975;&#24352;&#22270;&#20687;&#65292;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#21644;&#30001;&#30495;&#23454;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#25351;&#20986;&#20102;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#20540;&#21644;&#25552;&#31034;&#26679;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#26377;&#23475;&#27169;&#22411;&#20351;&#29992;&#30340;&#35777;&#25454;&#65292;&#22914;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#20026;&#39537;&#21160;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#21069;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20026;&#20102;&#35299;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28151;&#27788;&#29702;&#35770;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#37327;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#8220;&#26131;&#21463;&#24615;&#27604;&#8221;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#28145;&#24230;&#22686;&#21152;&#20250;&#26174;&#33879;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20855;&#26377;&#23433;&#20840;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.13235</link><description>&lt;p&gt;
&#28151;&#27788;&#29702;&#35770;&#19982;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chaos Theory and Adversarial Robustness. (arXiv:2210.13235v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28151;&#27788;&#29702;&#35770;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#37327;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#8220;&#26131;&#21463;&#24615;&#27604;&#8221;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#28145;&#24230;&#22686;&#21152;&#20250;&#26174;&#33879;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20855;&#26377;&#23433;&#20840;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#20851;&#38190;&#39046;&#22495;&#25110;&#23545;&#25239;&#24615;&#24212;&#29992;&#20013;&#20351;&#29992;&#21069;&#24212;&#36827;&#34892;&#20005;&#26684;&#30340;&#23457;&#26597;&#12290;&#26412;&#25991;&#21033;&#29992;&#28151;&#27788;&#29702;&#35770;&#30340;&#24605;&#24819;&#65292;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#21644;&#40065;&#26834;&#24615;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#8220;&#26131;&#21463;&#24615;&#27604;&#8221;&#65292;&#30001;$\hat \Psi(h, \theta)$&#32473;&#20986;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#36755;&#20986;&#23545;&#20110;&#32473;&#23450;&#36755;&#20837;&#25200;&#21160;&#30340;&#21464;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#26174;&#33879;&#22686;&#38271;&#65292;&#36825;&#23545;&#20110;&#35774;&#35745;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23433;&#20840;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;$\hat \Psi$&#19982;&#20998;&#31867;&#27169;&#22411;&#25915;&#20987;&#21518;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#30340;&#23454;&#39564;&#35777;&#25454;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#32570;&#20047;&#30828;&#20915;&#31574;&#36793;&#30028;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#22914;&#20309;&#24555;&#36895;&#31616;&#20415;&#22320;&#36817;&#20284;&#35745;&#31639;&#35748;&#35777;&#30340;r&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks, being susceptible to adversarial attacks, should face a strict level of scrutiny before being deployed in critical or adversarial applications. This paper uses ideas from Chaos Theory to explain, analyze, and quantify the degree to which neural networks are susceptible to or robust against adversarial attacks. To this end, we present a new metric, the "susceptibility ratio," given by $\hat \Psi(h, \theta)$, which captures how greatly a model's output will be changed by perturbations to a given input.  Our results show that susceptibility to attack grows significantly with the depth of the model, which has safety implications for the design of neural networks for production environments. We provide experimental evidence of the relationship between $\hat \Psi$ and the post-attack accuracy of classification models, as well as a discussion of its application to tasks lacking hard decision boundaries. We also demonstrate how to quickly and easily approximate the certified r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35299;&#20915;&#19968;&#31867;&#21442;&#25968;&#21270;PDEs&#25152;&#38656;&#30340;&#36866;&#24403;&#25509;&#21475;&#26465;&#20214;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#22810;&#39046;&#22495;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12669</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#25509;&#21475;&#26465;&#20214;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta Learning of Interface Conditions for Multi-Domain Physics-Informed Neural Networks. (arXiv:2210.12669v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35299;&#20915;&#19968;&#31867;&#21442;&#25968;&#21270;PDEs&#25152;&#38656;&#30340;&#36866;&#24403;&#25509;&#21475;&#26465;&#20214;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#22810;&#39046;&#22495;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20316;&#20026;&#19968;&#31181;&#26080;&#32593;&#26684;&#27714;&#35299;&#22120;&#65292;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#26368;&#36817;&#30340;&#25193;&#23637;&#23558;&#39046;&#22495;&#20998;&#35299;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;PINNs&#26469;&#35299;&#20915;&#27599;&#20010;&#23376;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25509;&#21475;&#22788;&#25340;&#25509;&#23376;&#22495;&#12290;&#36825;&#26679;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#36731;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20801;&#35768;&#24182;&#34892;&#21270;&#12290;&#28982;&#32780;&#65292;&#22810;&#39046;&#22495;PINNs&#30340;&#24615;&#33021;&#23545;&#25509;&#21475;&#26465;&#20214;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#26465;&#20214;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#22914;&#20309;&#26681;&#25454;&#20855;&#20307;&#38382;&#39064;&#36873;&#25321;&#26465;&#20214;&#30340;&#24314;&#35758;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;META Learning of Interface Conditions (METALIC)&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#20294;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#30830;&#23450;&#35299;&#20915;&#19968;&#31867;&#21442;&#25968;&#21270;PDEs&#25152;&#38656;&#30340;&#36866;&#24403;&#25509;&#21475;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#65288;MAB&#65289;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#19968;&#20010;G+-
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) are emerging as popular mesh-free solvers for partial differential equations (PDEs). Recent extensions decompose the domain, apply different PINNs to solve the problem in each subdomain, and stitch the subdomains at the interface. Thereby, they can further alleviate the problem complexity, reduce the computational cost, and allow parallelization. However, the performance of multi-domain PINNs is sensitive to the choice of the interface conditions. While quite a few conditions have been proposed, there is no suggestion about how to select the conditions according to specific problems. To address this gap, we propose META Learning of Interface Conditions (METALIC), a simple, efficient yet powerful approach to dynamically determine appropriate interface conditions for solving a family of parametric PDEs. Specifically, we develop two contextual multi-arm bandit (MAB) models. The first one applies to the entire training course, and online updates a G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;Top Tagger&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#22238;&#39038;&#29616;&#26377;&#27169;&#22411;&#24182;&#25506;&#32034;&#19981;&#21516;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#35782;&#21035;&#39030;&#22840;&#20811;&#30340;&#21943;&#27880;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04371</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;Top Tagger&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#30340;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Detailed Study of Interpretability of Deep Neural Network based Top Taggers. (arXiv:2210.04371v4 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;Top Tagger&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#22238;&#39038;&#29616;&#26377;&#27169;&#22411;&#24182;&#25506;&#32034;&#19981;&#21516;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#35782;&#21035;&#39030;&#22840;&#20811;&#30340;&#21943;&#27880;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021; (XAI) &#26041;&#27861;&#30340;&#21457;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNNs) &#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#25581;&#31034;&#26377;&#20851;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#20102;&#35299;&#25968;&#25454;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36830;&#25509;&#26041;&#24335;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35774;&#35745;&#29992;&#20110;&#22312;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426; (LHC) &#19978;&#35782;&#21035;&#26469;&#33258;&#39030;&#22840;&#20811;&#34928;&#21464;&#30340;&#21943;&#27880;&#30340;DNN&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#37096;&#20998;&#29616;&#26377;&#30340;&#39030;&#22840;&#20811;&#26631;&#35760;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#23450;&#37327;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#29305;&#24449;&#22312;&#35782;&#21035;&#39030;&#22840;&#20811;&#30340;&#21943;&#27880;&#20013;&#36215;&#30528;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;XAI&#25351;&#26631;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#21464;&#21270;&#26041;&#24335;&#21450;&#20854;&#35299;&#37322;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22914;&#20309;&#32534;&#30721;&#20449;&#24687;&#20197;&#21450;&#19982;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35828;&#26126;&#20102;&#22914;&#20309;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in the methods of explainable AI (XAI) allow researchers to explore the inner workings of deep neural networks (DNNs), revealing crucial information about input-output relationships and realizing how data connects with machine learning models. In this paper we explore interpretability of DNN models designed to identify jets coming from top quark decay in high energy proton-proton collisions at the Large Hadron Collider (LHC). We review a subset of existing top tagger models and explore different quantitative methods to identify which features play the most important roles in identifying the top jets. We also investigate how and why feature importance varies across different XAI metrics, how correlations among features impact their explainability, and how latent space representations encode information as well as correlate with physically meaningful quantities. Our studies uncover some major pitfalls of existing XAI methods and illustrate how they can be overcome to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861; PathProx&#65292;&#23427;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26631;&#20934;&#26435;&#20540;&#34928;&#20943;&#35757;&#32451;&#25152;&#20849;&#20139;&#30340;&#31232;&#30095;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.03069</link><description>&lt;p&gt;
PathProx: &#19968;&#31181;&#29992;&#20110;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks. (arXiv:2210.03069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861; PathProx&#65292;&#23427;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26631;&#20934;&#26435;&#20540;&#34928;&#20943;&#35757;&#32451;&#25152;&#20849;&#20139;&#30340;&#31232;&#30095;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#20540;&#34928;&#20943;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20043;&#19968;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#39537;&#21160;&#26435;&#20540;&#34928;&#20943;&#30340;&#20248;&#21270;&#30446;&#26631;&#26159;&#25439;&#22833;&#20043;&#21644;&#21152;&#19978;&#19982;&#26435;&#20540;&#24179;&#26041;&#21644;&#25104;&#27604;&#20363;&#30340;&#39033;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21487;&#33021;&#26159;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#31181;&#20302;&#25928;&#31639;&#27861;&#12290;&#23545;&#20110;&#24102;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26435;&#37325;&#34928;&#20943;&#30446;&#26631;&#30340;&#35299;&#19982;&#21478;&#19968;&#20010;&#30446;&#26631;&#30340;&#35299;&#26159;&#31561;&#20215;&#30340;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#25913;&#20026;&#19982;&#27599;&#20010;ReLU&#31070;&#32463;&#20803;&#20851;&#32852;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#26435;&#37325;&#30340;$\ell_2$&#65288;&#19981;&#26159;&#24179;&#26041;&#65289;&#33539;&#25968;&#20056;&#31215;&#20043;&#21644;&#12290;&#36825;&#31181;&#26367;&#20195;&#65288;&#24182;&#19988;&#26377;&#25928;&#31561;&#20215;&#65289;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#31034;&#23427;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26631;&#20934;&#26435;&#20540;&#34928;&#20943;&#35757;&#32451;&#25152;&#20849;&#20139;&#30340;&#31232;&#30095;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of $\ell_2$ (not squared) norms of the input and output weights associated with each ReLU neuron. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#26469;&#39044;&#27979;&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;&#30340;&#24490;&#29615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#32858;&#21512;&#37051;&#23621;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2209.05251</link><description>&lt;p&gt;
&#20174;&#21355;&#26143;&#20013;&#21457;&#29616;&#30149;&#27602;&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;&#30340;&#24490;&#29615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks. (arXiv:2209.05251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#26469;&#39044;&#27979;&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;&#30340;&#24490;&#29615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#32858;&#21512;&#37051;&#23621;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;(WNV)&#30340;&#21457;&#29983;&#20195;&#34920;&#20102;&#26368;&#24120;&#35265;&#30340;&#34442;&#23186;&#24615;&#21160;&#29289;&#20256;&#25773;&#30149;&#27602;&#24863;&#26579;&#20043;&#19968;&#12290;&#23427;&#30340;&#24490;&#29615;&#36890;&#24120;&#19982;&#36866;&#21512;&#20110;&#23186;&#20171;&#34442;&#23376;&#32321;&#27542;&#21644;&#30149;&#27602;&#22797;&#21046;&#30340;&#27668;&#20505;&#21644;&#29615;&#22659;&#26465;&#20214;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24050;&#24320;&#21457;&#20102;&#20960;&#31181;&#32479;&#35745;&#27169;&#22411;&#26469;&#22609;&#36896;&#21644;&#39044;&#27979;WNV&#30340;&#24490;&#29615;&#65306;&#29305;&#21035;&#26159;&#65292;&#22320;&#29699;&#35266;&#27979;(EO)&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21487;&#29992;&#24615;&#65292;&#21152;&#19978;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#23558;&#21355;&#26143;&#22270;&#20687;&#36755;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26469;&#39044;&#27979;WNV&#30340;&#24490;&#29615;&#65292;&#36825;&#20123;&#21355;&#26143;&#22270;&#20687;&#24050;&#32463;&#24191;&#27867;&#26174;&#31034;&#20855;&#26377;&#29615;&#22659;&#21644;&#27668;&#20505;&#29305;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;&#20043;&#21069;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#20998;&#26512;&#27599;&#20010;&#22320;&#29702;&#28857;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#38468;&#36817;&#28857;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20381;&#38752;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#32858;&#21512;&#37051;&#23621;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The occurrence of West Nile Virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth Observation (EO) data, coupled with the continuous advances in the field of Artificial Intelligence, offer valuable opportunities.  In this paper, we seek to predict WNV circulation by feeding Deep Neural Networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon Graph Neural Networks (GNN) to aggregate features from neighbour
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11543</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30005;&#21147;&#37197;&#30005;&#31995;&#32479;&#24191;&#27867;&#37319;&#29992;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;DER&#65289;&#20197;&#28385;&#36275;&#33021;&#28304;&#38656;&#27714;&#65292;&#26222;&#36941;&#35748;&#20026;&#36825;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#22914;&#38388;&#27463;&#24615;&#21487;&#29992;&#24615;&#12289;&#22825;&#27668;&#26465;&#20214;&#30340;&#21160;&#24577;&#21464;&#21270;&#12289;&#38750;&#32447;&#24615;&#31561;&#65289;&#21487;&#33021;&#23545;&#30005;&#32593;&#36816;&#33829;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#24102;&#26377;&#22826;&#38451;&#33021;&#20809;&#20239;&#21457;&#30005;&#30340;&#37197;&#30005;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#26465;&#20214;&#33719;&#24471;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#22797;&#26434;&#30456;&#20851;&#32593;&#32476;&#65292;&#24182;&#35745;&#31639;&#20102;&#21508;&#31181;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#32593;&#32476;&#30340;&#24377;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#19981;&#33391;&#26465;&#20214;&#19979;&#20445;&#25345;&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Electrical Distribution Systems are extensively penetrated with the Distributed Energy Resources (DERs) to cater the energy demands with general perception that it enhances the system resiliency. However, it may be adverse for the grid operation due to various factors like its intermittent availability, dynamics in weather condition, introduction of nonlinearity, complexity etc. This needs a detailed understanding of system resiliency that our method proposes here. We introduce a methodology using complex network theory to identify the resiliency of distribution system when incorporated with Solar PV generation under various undesirable configurations. Complex correlated networks for different conditions were obtained and various network parameters were computed for identifying the resiliency of those networks. The proposed methodology identifies the hosting capacity of solar panels in the system while maintaining the resiliency under different unwanted conditions hence helps
&lt;/p&gt;</description></item><item><title>DataPerf&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2207.10062</link><description>&lt;p&gt;
DataPerf&#65306;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DataPerf: Benchmarks for Data-Centric AI Development. (arXiv:2207.10062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10062
&lt;/p&gt;
&lt;p&gt;
DataPerf&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#19968;&#30452;&#27880;&#37325;&#27169;&#22411;&#32780;&#19981;&#26159;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#33879;&#21517;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#38382;&#39064;&#30340;&#24191;&#24230;&#12289;&#38590;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#24573;&#35270;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#23548;&#33268;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#12289;&#20559;&#35265;&#21644;&#33030;&#24369;&#24615;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#36798;&#21040;&#20102;&#39281;&#21644;&#29366;&#24577;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DataPerf&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#20013;&#24515;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;&#25105;&#20204;&#20351;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#33021;&#22815;&#36845;&#20195;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#24320;&#25918;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#20197;&#25903;&#25345;&#36825;&#31181;&#36845;&#20195;&#24320;&#21457;&#12290;DataPerf&#30340;&#31532;&#19968;&#20010;&#36845;&#20195;&#21253;&#21547;&#20102;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33719;&#21462;&#31561;&#22810;&#31181;&#25968;&#25454;&#20013;&#24515;&#25216;&#26415;&#12289;&#20219;&#21153;&#21644;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#32447;&#24179;&#21488;&#19978;&#20869;&#23481;&#21019;&#20316;&#32773;&#28608;&#21169;&#26426;&#21046;&#30340;&#24314;&#27169;&#65292;&#36890;&#36807;&#20998;&#26512;&#31639;&#27861;&#36873;&#25321;&#23545;&#26333;&#20809;&#28216;&#25103;&#65288;&#21253;&#25324;&#29616;&#20195;&#20998;&#35299;&#21644;&#20004;&#22612;&#26550;&#26500;&#65289;&#20013;&#65288;&#32435;&#20160;&#65289;&#22343;&#34913;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26333;&#20809;&#28216;&#25103;&#27169;&#22411;&#36827;&#34892;&#39044;&#37096;&#32626;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26399;&#26395;&#21644;&#28608;&#21169;&#20869;&#23481;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2206.13102</link><description>&lt;p&gt;
&#22312;&#31639;&#27861;&#31574;&#21010;&#24179;&#21488;&#19978;&#24314;&#27169;&#20869;&#23481;&#21019;&#20316;&#32773;&#30340;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Modeling Content Creator Incentives on Algorithm-Curated Platforms. (arXiv:2206.13102v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#32447;&#24179;&#21488;&#19978;&#20869;&#23481;&#21019;&#20316;&#32773;&#28608;&#21169;&#26426;&#21046;&#30340;&#24314;&#27169;&#65292;&#36890;&#36807;&#20998;&#26512;&#31639;&#27861;&#36873;&#25321;&#23545;&#26333;&#20809;&#28216;&#25103;&#65288;&#21253;&#25324;&#29616;&#20195;&#20998;&#35299;&#21644;&#20004;&#22612;&#26550;&#26500;&#65289;&#20013;&#65288;&#32435;&#20160;&#65289;&#22343;&#34913;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26333;&#20809;&#28216;&#25103;&#27169;&#22411;&#36827;&#34892;&#39044;&#37096;&#32626;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26399;&#26395;&#21644;&#28608;&#21169;&#20869;&#23481;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;&#20105;&#22842;&#29992;&#25143;&#27880;&#24847;&#21147;&#12290;&#20182;&#20204;&#30340;&#24433;&#21709;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#22312;&#32447;&#24179;&#21488;&#24320;&#21457;&#32773;&#25152;&#20570;&#30340;&#31639;&#27861;&#36873;&#25321;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#26333;&#20809;&#29575;&#65292;&#35768;&#22810;&#21019;&#20316;&#32773;&#37319;&#21462;&#25112;&#30053;&#24615;&#30340;&#35843;&#25972;&#65292;&#22914;&#25628;&#32034;&#24341;&#25806;&#20248;&#21270;&#34892;&#19994;&#30340;&#20363;&#23376;&#25152;&#35777;&#26126;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#26377;&#38480;&#29992;&#25143;&#27880;&#24847;&#21147;&#27744;&#30340;&#31454;&#20105;&#12290;&#25105;&#20204;&#22312;&#25152;&#35859;&#30340;&#26333;&#20809;&#28216;&#25103;&#20013;&#24418;&#24335;&#21270;&#20102;&#36825;&#20123;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#31639;&#27861;&#24341;&#36215;&#30340;&#28608;&#21169;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#29616;&#20195;&#20998;&#35299;&#21644;&#65288;&#28145;&#23618;&#65289;&#20004;&#22612;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30475;&#20284;&#26080;&#23475;&#30340;&#31639;&#27861;&#36873;&#25321;&#65292;&#20363;&#22914;&#38750;&#36127;&#19982;&#26080;&#32422;&#26463;&#20998;&#35299;&#65292;&#22312;&#26333;&#20809;&#28216;&#25103;&#20013;&#26174;&#33879;&#24433;&#21709;&#65288;&#32435;&#20160;&#65289;&#22343;&#34913;&#30340;&#23384;&#22312;&#21644;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21019;&#20316;&#32773;&#34892;&#20026;&#27169;&#22411;&#65292;&#22914;&#26333;&#20809;&#28216;&#25103;&#65292;&#36827;&#34892;&#65288;ex-ante&#65289;&#39044;&#37096;&#32626;&#23457;&#35745;&#12290;&#36825;&#26679;&#30340;&#23457;&#35745;&#21487;&#20197;&#35782;&#21035;&#26399;&#26395;&#21644;&#28608;&#21169;&#20869;&#23481;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20869;&#23481;&#36807;&#28388;&#21644;&#31649;&#29702;&#31561;&#20107;&#21518;&#25514;&#26045;&#19978;&#36827;&#34892;&#34917;&#20805;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by algorithms, including modern factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices, e.g., non-negative vs. unconstrained factorization, significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models, like exposure games, for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#20998;&#26512;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#24515;&#33039;&#30913;&#20849;&#25391;&#25968;&#25454;&#24211;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#20174;&#30701;&#36724;&#24515;&#33039;&#30913;&#20849;&#25391;&#24433;&#20687;&#20013;&#33258;&#21160;&#37327;&#21270;&#24515;&#33039;&#21151;&#33021;&#65292;&#24182;&#20855;&#22791;&#33258;&#21160;&#21270;&#30340;&#36136;&#37327;&#25511;&#21046;&#21151;&#33021;&#65292;&#21487;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#12290;&#35813;&#31639;&#27861;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#20855;&#22791;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25512;&#24191;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.08137</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#20998;&#26512;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#24515;&#33039;&#30913;&#20849;&#25391;&#25968;&#25454;&#24211;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
An AI tool for automated analysis of large-scale unstructured clinical cine CMR databases. (arXiv:2206.08137v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#20998;&#26512;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#24515;&#33039;&#30913;&#20849;&#25391;&#25968;&#25454;&#24211;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#20174;&#30701;&#36724;&#24515;&#33039;&#30913;&#20849;&#25391;&#24433;&#20687;&#20013;&#33258;&#21160;&#37327;&#21270;&#24515;&#33039;&#21151;&#33021;&#65292;&#24182;&#20855;&#22791;&#33258;&#21160;&#21270;&#30340;&#36136;&#37327;&#25511;&#21046;&#21151;&#33021;&#65292;&#21487;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#12290;&#35813;&#31639;&#27861;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#20855;&#22791;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25512;&#24191;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#20998;&#26512;&#30701;&#36724;&#24515;&#33039;&#30913;&#20849;&#25391;&#24433;&#20687;&#65288;CMR&#65289;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#33258;&#21160;&#20998;&#26512;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;CMR&#25968;&#25454;&#24211;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#39564;&#35777;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#20020;&#24202;&#25968;&#25454;&#24211;&#30340;&#30701;&#36724;&#24515;&#33039;CMR&#20013;&#20840;&#33258;&#21160;&#37327;&#21270;&#24515;&#33039;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#33258;&#21160;&#35782;&#21035;&#27491;&#30830;&#25968;&#25454;&#12289;&#31283;&#20581;&#30340;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#30701;&#36724;CMR&#30340;&#21452;&#24515;&#23460;&#20998;&#21106;&#21644;&#21151;&#33021;&#29983;&#29289;&#26631;&#24535;&#20272;&#35745;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#21450;&#33258;&#21160;&#21518;&#20998;&#26512;&#36136;&#37327;&#25511;&#21046;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#12290;&#20998;&#21106;&#31639;&#27861;&#26159;&#22312;&#20004;&#23478;&#33521;&#22269;&#22269;&#23478;&#21307;&#30103;&#26381;&#21153;&#20307;&#31995;&#65288;NHS&#65289;&#21307;&#38498;&#30340;2793&#20010;CMR&#25195;&#25551;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#30340;&#20854;&#20182;&#26696;&#20363;&#65288;n=414&#65289;&#21644;&#20116;&#20010;&#22806;&#37096;&#25968;&#25454;&#38598;&#65288;n=6888&#65289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#26469;&#33258;12&#20010;&#19981;&#21516;&#20013;&#24515;&#20351;&#29992;CMR&#25195;&#25551;&#20202;&#33719;&#21462;&#30340;&#21508;&#31181;&#30142;&#30149;&#24739;&#32773;&#30340;&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) techniques have been proposed for automating analysis of short axis (SAX) cine cardiac magnetic resonance (CMR), but no CMR analysis tool exists to automatically analyse large (unstructured) clinical CMR datasets. We develop and validate a robust AI tool for start-to-end automatic quantification of cardiac function from SAX cine CMR in large clinical databases. Our pipeline for processing and analysing CMR databases includes automated steps to identify the correct data, robust image pre-processing, an AI algorithm for biventricular segmentation of SAX CMR and estimation of functional biomarkers, and automated post-analysis quality control to detect and correct errors. The segmentation algorithm was trained on 2793 CMR scans from two NHS hospitals and validated on additional cases from this dataset (n=414) and five external datasets (n=6888), including scans of patients with a range of diseases acquired at 12 different centres using CMR scanners from all maj
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#38024;&#23545;&#36825;&#19968;&#22797;&#26434;&#39046;&#22495;&#24314;&#31435;&#20102;&#35270;&#35273;&#39046;&#22495;&#20013;&#36830;&#32493;&#25511;&#21046;&#30340;&#31616;&#21333;&#22522;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#31163;&#32447;RL&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2206.04779</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#38024;&#23545;&#36825;&#19968;&#22797;&#26434;&#39046;&#22495;&#24314;&#31435;&#20102;&#35270;&#35273;&#39046;&#22495;&#20013;&#36830;&#32493;&#25511;&#21046;&#30340;&#31616;&#21333;&#22522;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#31163;&#32447;RL&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20351;&#24471;Agent&#21487;&#20197;&#36991;&#20813;&#36890;&#24120;&#36153;&#26102;&#26114;&#36149;&#30340;&#22312;&#32447;&#25968;&#25454;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#65292;&#22522;&#20110;&#35270;&#35273;&#35266;&#23519;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#22312;&#36825;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#20013;&#23545;&#20851;&#38190;&#25361;&#25112;&#30340;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24314;&#31435;&#31616;&#21333;&#30340;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#26088;&#22312;&#26356;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#31163;&#32447;RL&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#21463;&#31163;&#32447;RL&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#19968;&#32452;&#26399;&#26395;&#25152;&#25351;&#23548;&#65292;&#21253;&#25324;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#31283;&#20581;&#24615;&#21644;&#21160;&#21147;&#23398;&#20013;&#21487;&#35270;&#21270;&#21464;&#21270;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#22871;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DreamerV2&#21644;DrQ-v2&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suf
&lt;/p&gt;</description></item><item><title>&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23376;&#31354;&#38388;&#35757;&#32451;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#26524;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2205.13104</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#65306;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trainable Weight Averaging: A General Approach for Subspace Training. (arXiv:2205.13104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13104
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23376;&#31354;&#38388;&#35757;&#32451;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#26524;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26159;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25110;&#22312;&#35757;&#32451;&#36712;&#36857;&#19978;&#25191;&#34892;&#38477;&#32500;&#26041;&#27861;&#26469;&#25552;&#21462;&#23376;&#31354;&#38388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#32500;&#24230;&#21644;&#25968;&#20540;&#36816;&#31639;&#26041;&#38754;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#25110;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23376;&#31354;&#38388;&#35757;&#32451;&#19982;&#26435;&#37325;&#24179;&#22343;&#20540;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35757;&#32451;&#26435;&#37325;&#24179;&#22343;&#20540;(TWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#27867;&#21270;&#20197;&#21069;&#21162;&#21147;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;TWA&#22312;&#32500;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#26696;&#26469;&#24212;&#23545;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#24182;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#20869;&#23384;&#21644;&#35745;&#31639;&#36127;&#25285;&#22343;&#21248;&#20998;&#37197;&#32473;&#27599;&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#23558;TWA&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) in low-dimensional subspaces is a promising direction for achieving efficient training and better generalization performance. Previous works extract the subspaces by using random projection or performing dimensionality reduction method on the training trajectory, but these methods can be inefficient or unstable in terms of dimensionality and numerical operations. In this paper, we connect subspace training to weight averaging and propose Trainable Weight Averaging (TWA), a general approach for subspace training that generalizes the previous efforts. TWA is efficient in terms of dimensionality and also easy to use, making it a promising new method for subspace training. We further design an efficient scheme for subspace training to cope with large-scale problems, which allows parallel training across multiple nodes and evenly distributing the memory and computation burden to each node. We apply TWA to efficient neural network training and improving f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.03447</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21451;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#29992;&#20110;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#35821;&#20041;&#32593;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20854;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#21305;&#37197;&#35780;&#20272;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21253;&#21547;&#20851;&#31995;&#26144;&#23556;&#30340;&#26377;&#38480;&#35780;&#20272;&#12289;&#21442;&#32771;&#26144;&#23556;&#30340;&#20122;&#20248;&#35299;&#20197;&#21450;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;Mondo&#21644;UMLS&#20013;&#25552;&#21462;&#30340;&#26412;&#20307;&#12290;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#12289;&#26412;&#20307;&#20462;&#21098;&#31561;&#26041;&#24335;&#30830;&#20445;&#21442;&#32771;&#26144;&#23556;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#38750;&#26426;&#22120;&#23398;&#20064;&#30340;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#21487;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#30456;&#27604;&#23384;&#22312;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#21644;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#32553;&#23567;&#36825;&#19968;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2204.09269</link><description>&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21450;&#20854;&#25193;&#23637;&#30340;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond. (arXiv:2204.09269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#21487;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#30456;&#27604;&#23384;&#22312;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#21644;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#32553;&#23567;&#36825;&#19968;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#29983;&#25104;&#39318;&#27425;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#25552;&#20986;&#65292;&#26088;&#22312;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;NAR&#29983;&#25104;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#26426;&#22120;&#32763;&#35793;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#19982;&#20854;&#23545;&#24212;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#29983;&#25104;&#30456;&#27604;&#65292;&#20854;&#32763;&#35793;&#20934;&#30830;&#24615;&#26377;&#25152;&#38477;&#20302;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26032;&#27169;&#22411;&#21644;&#31639;&#27861;&#24050;&#34987;&#35774;&#35745;/&#25552;&#20986;&#20197;&#24357;&#34917;NAR&#29983;&#25104;&#19982;AR&#29983;&#25104;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#26041;&#38754;&#30340;&#21508;&#31181;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;NAT&#30340;&#21162;&#21147;&#20998;&#25104;&#20102;&#20960;&#20010;&#26041;&#21521;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#24314;&#27169;&#26041;&#27861;&#12289;&#35757;&#32451;&#26631;&#20934;&#12289;&#35299;&#30721;&#31639;&#27861;&#20197;&#21450;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30410;&#22788;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31616;&#35201;&#22238;&#39038;&#20102;NAR&#27169;&#22411;&#30340;&#20854;&#20182;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#39640;&#32500;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#20302;&#32500;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#24102;&#23485;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#33719;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;&#31639;&#27861;&#30340;&#20302;&#32500;&#23884;&#20837;&#32467;&#26524;&#21487;&#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2203.00126</link><description>&lt;p&gt;
&#20174;&#39640;&#32500;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#20302;&#32500;&#38750;&#32447;&#24615;&#32467;&#26500;&#65306;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach. (arXiv:2203.00126v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#39640;&#32500;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#20302;&#32500;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#24102;&#23485;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#33719;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;&#31639;&#27861;&#30340;&#20302;&#32500;&#23884;&#20837;&#32467;&#26524;&#21487;&#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26680;&#35889;&#23884;&#20837;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#39640;&#32500;&#22122;&#22768;&#35266;&#27979;&#20013;&#23398;&#20064;&#20302;&#32500;&#38750;&#32447;&#24615;&#32467;&#26500;&#65292;&#20854;&#20013;&#20551;&#35774;&#25968;&#25454;&#38598;&#20174;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#21463;&#21040;&#39640;&#32500;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24102;&#23485;&#36873;&#25321;&#36807;&#31243;&#65292;&#19981;&#20381;&#36182;&#20110;&#23545;&#24213;&#23618;&#27969;&#24418;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25152;&#33719;&#24471;&#30340;&#20302;&#32500;&#23884;&#20837;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#24182;&#19988;&#20855;&#26377;&#23454;&#38469;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#30340;&#32500;&#24230;&#21644;&#22823;&#23567;&#30456;&#23545;&#36739;&#22823;&#26102;&#65292;&#24314;&#31435;&#20102;&#26368;&#32456;&#23884;&#20837;&#21040;&#26080;&#22122;&#22768;&#23545;&#24212;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#21051;&#30011;&#20102;&#20449;&#22122;&#27604;&#23545;&#25910;&#25947;&#36895;&#24230;&#21644;&#30456;&#21464;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23884;&#20837;&#21040;&#30001;&#26680;&#23450;&#20041;&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a kernel-spectral embedding algorithm for learning low-dimensional nonlinear structures from high-dimensional and noisy observations, where the datasets are assumed to be sampled from an intrinsically low-dimensional manifold and corrupted by high-dimensional noise. The algorithm employs an adaptive bandwidth selection procedure which does not rely on prior knowledge of the underlying manifold. The obtained low-dimensional embeddings can be further utilized for downstream purposes such as data visualization, clustering and prediction. Our method is theoretically justified and practically interpretable. Specifically, we establish the convergence of the final embeddings to their noiseless counterparts when the dimension and size of the samples are comparably large, and characterize the effect of the signal-to-noise ratio on the rate of convergence and phase transition. We also prove convergence of the embeddings to the eigenfunctions of an integral operator defined by the kern
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20250;&#22312;&#26410;&#24471;&#21040;&#36275;&#22815;&#26381;&#21153;&#30340;&#20154;&#32676;&#20013;&#20135;&#29983;&#39640;&#34394;&#25253;&#29575;&#65292;&#21487;&#33021;&#25918;&#22823;&#20102;&#31995;&#32479;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#19981;&#36275;&#20197;&#20840;&#38754;&#30740;&#31350;&#31639;&#27861;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#65292;&#32780;&#19988;&#20351;&#29992;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20559;&#20506;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#32467;&#26524;&#30340;&#35299;&#37322;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2201.07856</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20559;&#20506;&#30340;&#28508;&#22312;&#26469;&#28304;&#20351;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26410;&#35786;&#26029;&#38382;&#39064;&#30340;&#30740;&#31350;&#22797;&#26434;&#21270;
&lt;/p&gt;
&lt;p&gt;
Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. (arXiv:2201.07856v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20250;&#22312;&#26410;&#24471;&#21040;&#36275;&#22815;&#26381;&#21153;&#30340;&#20154;&#32676;&#20013;&#20135;&#29983;&#39640;&#34394;&#25253;&#29575;&#65292;&#21487;&#33021;&#25918;&#22823;&#20102;&#31995;&#32479;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#19981;&#36275;&#20197;&#20840;&#38754;&#30740;&#31350;&#31639;&#27861;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#65292;&#32780;&#19988;&#20351;&#29992;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20559;&#20506;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#32467;&#26524;&#30340;&#35299;&#37322;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#25253;&#21578;&#24341;&#36215;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#25918;&#22823;&#30001;&#35757;&#32451;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#20559;&#35265;&#23548;&#33268;&#30340;&#20581;&#24247;&#24046;&#24322;&#30340;&#25285;&#24551;&#12290; Seyyed-Kalantari&#31561;&#20154;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#8220;&#26410;&#21457;&#29616;&#8221;&#26631;&#31614;&#65288;&#34920;&#31034;&#27809;&#26377;&#30142;&#30149;&#65289;&#30340;&#20122;&#32452;&#20043;&#38388;&#20135;&#29983;&#20102;&#34394;&#25253;&#29575;&#65288;FPR&#65289;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#24050;&#30693;&#21382;&#21490;&#19978;&#26410;&#24471;&#21040;&#36275;&#22815;&#26381;&#21153;&#30340;&#20122;&#32452;&#20013;&#19968;&#30452;&#20135;&#29983;&#26356;&#39640;&#30340;FPR&#65292;&#24182;&#19988;&#35813;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#20123;&#27169;&#22411;&#26174;&#31034;&#24182;&#19988;&#21487;&#33021;&#20250;&#25918;&#22823;&#31995;&#32479;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#30740;&#31350;&#20013;&#30340;&#23454;&#39564;&#35774;&#32622;&#19981;&#36275;&#20197;&#30740;&#31350;&#31639;&#27861;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#22312;&#32570;&#20047;&#20851;&#20110;&#25968;&#25454;&#38598;&#20559;&#20506;&#31243;&#24230;&#21644;&#24615;&#36136;&#30340;&#20855;&#20307;&#30693;&#35782;&#65288;&#25110;&#20551;&#35774;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#35843;&#26597;&#27169;&#22411;&#20559;&#20506;&#38382;&#39064;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#20182;&#20204;&#20351;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#23637;&#31034;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#30340;&#20559;&#20506;&#65288;&#30001;&#20110;&#38543;&#26426;&#20998;&#21106;&#65289;&#65292;&#20005;&#37325;&#22797;&#26434;&#21270;&#20102;&#25152;&#25253;&#36947;&#30340;&#24046;&#24322;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of reports raise concerns about the risk that machine learning algorithms could amplify health disparities due to biases embedded in the training data. Seyyed-Kalantari et al. find that models trained on three chest X-ray datasets yield disparities in false-positive rates (FPR) across subgroups on the 'no-finding' label (indicating the absence of disease). The models consistently yield higher FPR on subgroups known to be historically underserved, and the study concludes that the models exhibit and potentially even amplify systematic underdiagnosis. We argue that the experimental setup in the study is insufficient to study algorithmic underdiagnosis. In the absence of specific knowledge (or assumptions) about the extent and nature of the dataset bias, it is difficult to investigate model bias. Importantly, their use of test data exhibiting the same bias as the training data (due to random splitting) severely complicates the interpretation of the reported disparities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#20271;&#24681;&#26031;&#22374;&#22312;&#32447;&#32858;&#21512;&#65288;BOA&#65289;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20010;&#32929;&#25910;&#30410;&#39044;&#27979;&#32467;&#21512;&#25104;&#22810;&#31354;&#31574;&#30053;&#30340;&#26500;&#24314;&#12290;&#19987;&#23478;&#22312;&#32447;&#32858;&#21512;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26377;&#21560;&#24341;&#21147;&#30340;&#25237;&#36164;&#32452;&#21512;&#34920;&#29616;&#65292;&#32988;&#36807;&#21333;&#29420;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#25913;&#21892;&#25972;&#20307;&#28151;&#21512;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.15365</link><description>&lt;p&gt;
&#37329;&#34701;&#39044;&#27979;&#30340;&#19987;&#23478;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Expert Aggregation for Financial Forecasting. (arXiv:2111.15365v4 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.15365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#20271;&#24681;&#26031;&#22374;&#22312;&#32447;&#32858;&#21512;&#65288;BOA&#65289;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20010;&#32929;&#25910;&#30410;&#39044;&#27979;&#32467;&#21512;&#25104;&#22810;&#31354;&#31574;&#30053;&#30340;&#26500;&#24314;&#12290;&#19987;&#23478;&#22312;&#32447;&#32858;&#21512;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26377;&#21560;&#24341;&#21147;&#30340;&#25237;&#36164;&#32452;&#21512;&#34920;&#29616;&#65292;&#32988;&#36807;&#21333;&#29420;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#25913;&#21892;&#25972;&#20307;&#28151;&#21512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#27880;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#22312;&#22810;&#20010;&#31639;&#27861;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#21487;&#33021;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#38543;&#26102;&#38388;&#32780;&#19981;&#31283;&#23450;&#12290;&#19987;&#23478;&#22312;&#32447;&#32858;&#21512;&#23558;&#26377;&#38480;&#38598;&#21512;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#21512;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#26041;&#27861;&#20013;&#65292;&#32780;&#19981;&#23545;&#27169;&#22411;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#26412;&#25991;&#23558;&#20271;&#24681;&#26031;&#22374;&#22312;&#32447;&#32858;&#21512;&#65288;BOA&#65289;&#31243;&#24207;&#24212;&#29992;&#20110;&#30001;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#32929;&#25910;&#30410;&#39044;&#27979;&#26500;&#24314;&#30340;&#22810;&#31354;&#31574;&#30053;&#30340;&#26500;&#24314;&#20013;&#12290;&#19987;&#23478;&#22312;&#32447;&#32858;&#21512;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#20063;&#33021;&#20135;&#29983;&#26377;&#21560;&#24341;&#21147;&#30340;&#25237;&#36164;&#32452;&#21512;&#34920;&#29616;&#12290;&#32858;&#21512;&#26041;&#27861;&#32988;&#36807;&#21333;&#29420;&#30340;&#31639;&#27861;&#65292;&#22312;&#25237;&#36164;&#32452;&#21512;&#22799;&#26222;&#27604;&#29575;&#36739;&#39640;&#12289;&#20111;&#25439;&#36739;&#20302;&#65292;&#24182;&#19988;&#25442;&#25163;&#29575;&#30456;&#20284;&#12290;&#36824;&#25552;&#20986;&#20102;&#19987;&#23478;&#21644;&#32858;&#21512;&#29305;&#21270;&#30340;&#25193;&#23637;&#65292;&#20197;&#25913;&#21892;&#25972;&#20307;&#28151;&#21512;&#22312;&#19968;&#26063;&#25237;&#36164;&#32452;&#21512;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms dedicated to financial time series forecasting have gained a lot of interest. But choosing between several algorithms can be challenging, as their estimation accuracy may be unstable over time. Online aggregation of experts combine the forecasts of a finite set of models in a single approach without making any assumption about the models. In this paper, a Bernstein Online Aggregation (BOA) procedure is applied to the construction of long-short strategies built from individual stock return forecasts coming from different machine learning models. The online mixture of experts leads to attractive portfolio performances even in environments characterised by non-stationarity. The aggregation outperforms individual algorithms, offering a higher portfolio Sharpe Ratio, lower shortfall, with a similar turnover. Extensions to expert and aggregation specialisations are also proposed to improve the overall mixture on a family of portfolio evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#19968;&#38454;&#26041;&#27861;&#26469;&#27714;&#35299;&#19968;&#31867;&#22823;&#35268;&#27169;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.05192</link><description>&lt;p&gt;
&#20984;&#20985; &#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Convex-Concave Min-Max Stackelberg Games. (arXiv:2110.05192v8 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#19968;&#38454;&#26041;&#27861;&#26469;&#27714;&#35299;&#19968;&#31867;&#22823;&#35268;&#27169;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65288;&#21363;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65289;&#22240;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#32780;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#20855;&#26377;&#29420;&#31435;&#31574;&#30053;&#38598;&#30340;&#21338;&#24328;&#65307;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#31574;&#30053;&#38598;&#30340;&#21338;&#24328;&#27714;&#35299;&#30693;&#20043;&#29978;&#23569;&#65292;&#36825;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#19968;&#38454;&#26041;&#27861;&#26469;&#27714;&#35299;&#19968;&#31867;&#22823;&#35268;&#27169;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25910;&#25947;&#12290;&#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;&#39318;&#27425;&#30001; Wald &#30740;&#31350;&#65292;&#20197; Wald &#30340;&#26497;&#23567;&#26497;&#22823;&#27169;&#22411;&#30340;&#36951;&#20070;&#21517;&#23383;&#36827;&#34892;&#21629;&#21517;&#65292;&#36825;&#19968;&#21464;&#31181;&#26159;&#40065;&#26834;&#20248;&#21270;&#20013;&#20027;&#35201;&#33539;&#20363;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20984;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312; Fisher &#24066;&#22330;&#20013;&#35745;&#31639;&#31454;&#20105;&#22343;&#34913;&#20063;&#21253;&#21547;&#20102;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823; Stackelberg &#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. We introduce two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald's maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. We observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game. Further, we demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38750;&#20132;&#25442;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#25968;&#25512;&#24191;&#65292;&#21033;&#29992;&#20195;&#25968;&#20449;&#21495;&#22788;&#29702;&#29702;&#35770;&#24314;&#27169;&#38750;&#20132;&#25442;&#21367;&#31215;&#26550;&#26500;&#65292;&#24182;&#25512;&#23548;&#20986;&#31283;&#23450;&#24615;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#38750;&#20132;&#25442;&#21367;&#31215;&#26550;&#26500;&#21487;&#20197;&#22312;&#31639;&#23376;&#31354;&#38388;&#19978;&#20445;&#25345;&#31283;&#23450;&#24615;&#65292;&#19988;&#38750;&#20132;&#25442;&#28388;&#27874;&#22120;&#29420;&#31435;&#22320;&#22788;&#29702;&#20613;&#37324;&#21494;&#20998;&#37327;&#12290;&#31283;&#23450;&#24615;&#21644;&#36873;&#25321;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30001;&#20302;&#32500;&#30697;&#38453;&#31354;&#38388;&#20013;&#30340;&#30697;&#38453;&#22810;&#39033;&#24335;&#20989;&#25968;&#25152;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2108.09923</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20132;&#25442;&#20195;&#25968;&#30340;&#21367;&#31215;&#28388;&#27874;&#21644;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Filtering and Neural Networks with Non Commutative Algebras. (arXiv:2108.09923v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38750;&#20132;&#25442;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#25968;&#25512;&#24191;&#65292;&#21033;&#29992;&#20195;&#25968;&#20449;&#21495;&#22788;&#29702;&#29702;&#35770;&#24314;&#27169;&#38750;&#20132;&#25442;&#21367;&#31215;&#26550;&#26500;&#65292;&#24182;&#25512;&#23548;&#20986;&#31283;&#23450;&#24615;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#38750;&#20132;&#25442;&#21367;&#31215;&#26550;&#26500;&#21487;&#20197;&#22312;&#31639;&#23376;&#31354;&#38388;&#19978;&#20445;&#25345;&#31283;&#23450;&#24615;&#65292;&#19988;&#38750;&#20132;&#25442;&#28388;&#27874;&#22120;&#29420;&#31435;&#22320;&#22788;&#29702;&#20613;&#37324;&#21494;&#20998;&#37327;&#12290;&#31283;&#23450;&#24615;&#21644;&#36873;&#25321;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30001;&#20302;&#32500;&#30697;&#38453;&#31354;&#38388;&#20013;&#30340;&#30697;&#38453;&#22810;&#39033;&#24335;&#20989;&#25968;&#25152;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#38750;&#20132;&#25442;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#25968;&#25512;&#24191;&#12290;&#25105;&#20204;&#21033;&#29992;&#20195;&#25968;&#20449;&#21495;&#22788;&#29702;&#29702;&#35770;&#26469;&#24314;&#27169;&#38750;&#20132;&#25442;&#21367;&#31215;&#26550;&#26500;&#65292;&#24182;&#25512;&#23548;&#20986;&#20855;&#20307;&#30340;&#31283;&#23450;&#24615;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#29992;&#20110;&#20132;&#25442;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#20132;&#25442;&#21367;&#31215;&#26550;&#26500;&#21487;&#20197;&#23545;&#31639;&#23376;&#31354;&#38388;&#19978;&#30340;&#21464;&#24418;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#38750;&#20132;&#25442;&#20449;&#21495;&#27169;&#22411;&#30340;&#35889;&#34920;&#31034;&#65292;&#20197;&#23637;&#31034;&#38750;&#20132;&#25442;&#28388;&#27874;&#22120;&#29420;&#31435;&#22320;&#22788;&#29702;&#20613;&#37324;&#21494;&#20998;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34429;&#28982;&#38750;&#20132;&#25442;&#27169;&#22411;&#20013;&#20449;&#21495;&#30340;&#35889;&#20998;&#35299;&#19982;&#32500;&#24230;&#22823;&#20110;1&#30340;&#29305;&#24449;&#31354;&#38388;&#30456;&#20851;&#32852;&#65292;&#20294;&#23384;&#22312;&#30528;&#31283;&#23450;&#24615;&#21644;&#36873;&#25321;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36825;&#30001;&#20302;&#32500;&#30697;&#38453;&#31354;&#38388;&#20013;&#30340;&#30697;&#38453;&#22810;&#39033;&#24335;&#20989;&#25968;&#25152;&#25511;&#21046;&#12290;&#36825;&#19968;&#26435;&#34913;&#23637;&#31034;&#20102;&#24403;&#20195;&#25968;&#30340;&#28388;&#27874;&#22120;&#30456;&#20114;&#20316;&#29992;&#22312;&#38750;&#20132;&#25442;&#20195;&#25968;&#32467;&#26500;&#19978;&#21457;&#29983;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce and study the algebraic generalization of non commutative convolutional neural networks. We leverage the theory of algebraic signal processing to model convolutional non commutative architectures, and we derive concrete stability bounds that extend those obtained in the literature for commutative convolutional neural networks. We show that non commutative convolutional architectures can be stable to deformations on the space of operators. We develop the spectral representation of non commutative signal models to show that non commutative filters process Fourier components independently of each other. In particular we prove that although the spectral decompositions of signals in non commutative models are associated to eigenspaces of dimension larger than one, there exists a trade-off between stability and selectivity, which is controlled by matrix polynomial functions in spaces of matrices of low dimension. This tradeoff shows how when the filters in the alge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#29983;&#25104;&#29616;&#26377;&#32764;&#22411;&#65292;&#36827;&#32780;&#20248;&#21270;&#21512;&#25104;&#32764;&#22411;&#30340;&#27668;&#21160;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;VAEGAN&#27169;&#22411;&#23558;&#32764;&#22411;&#32534;&#30721;&#20026;&#28508;&#22312;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#26144;&#23556;&#28508;&#22312;&#21521;&#37327;&#21040;&#32764;&#22411;&#22352;&#26631;&#31354;&#38388;&#29983;&#25104;&#26032;&#30340;&#32764;&#22411;&#12290;</title><link>http://arxiv.org/abs/2101.04757</link><description>&lt;p&gt;
Airfoil GAN: &#38024;&#23545;&#27668;&#21160;&#22806;&#24418;&#20248;&#21270;&#36827;&#34892;&#31354;&#27668;&#21160;&#21147;&#23398;&#36229;&#22768;&#27874;&#25104;&#20687;&#32534;&#30721;&#19982;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Airfoil GAN: Encoding and Synthesizing Airfoils for Aerodynamic Shape Optimization. (arXiv:2101.04757v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.04757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#29983;&#25104;&#29616;&#26377;&#32764;&#22411;&#65292;&#36827;&#32780;&#20248;&#21270;&#21512;&#25104;&#32764;&#22411;&#30340;&#27668;&#21160;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;VAEGAN&#27169;&#22411;&#23558;&#32764;&#22411;&#32534;&#30721;&#20026;&#28508;&#22312;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#26144;&#23556;&#28508;&#22312;&#21521;&#37327;&#21040;&#32764;&#22411;&#22352;&#26631;&#31354;&#38388;&#29983;&#25104;&#26032;&#30340;&#32764;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27668;&#21160;&#22806;&#24418;&#35774;&#35745;&#65292;&#22914;&#32764;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#27169;&#25311;&#65292;&#20197;&#25506;&#32034;&#21487;&#33021;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#36890;&#24120;&#65292;&#36825;&#26679;&#30340;&#35774;&#35745;&#20381;&#36182;&#20110;&#39044;&#20808;&#23450;&#20041;&#30340;&#35774;&#35745;&#21442;&#25968;&#65292;&#24182;&#23545;&#21512;&#25104;&#26032;&#24418;&#29366;&#26045;&#21152;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24418;&#29366;&#32534;&#30721;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#20174;&#29616;&#26377;&#30340;&#32764;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#26032;&#30340;&#32764;&#22411;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#23545;&#21512;&#25104;&#32764;&#22411;&#30340;&#27668;&#21160;&#24615;&#33021;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;VAEGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#65288;1&#65289;&#23558;&#29616;&#26377;&#30340;&#32764;&#22411;&#32534;&#30721;&#20026;&#28508;&#22312;&#21521;&#37327;&#65292;&#24182;&#20174;&#20013;&#37325;&#26500;&#20986;&#32764;&#22411;&#65292;&#65288;2&#65289;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#28508;&#22312;&#21521;&#37327;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#32764;&#22411;&#22352;&#26631;&#31354;&#38388;&#65292;&#29983;&#25104;&#26032;&#30340;&#32764;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current design of aerodynamic shapes, like airfoils, involves computationally intensive simulations to explore the possible design space. Usually, such design relies on the prior definition of design parameters and places restrictions on synthesizing novel shapes. In this work, we propose a data-driven shape encoding and generating method, which automatically learns representations from existing airfoils and uses the learned representations to generate new airfoils. The representations are then used in the optimization of synthesized airfoil shapes based on their aerodynamic performance. Our model is built upon VAEGAN, a neural network that combines Variational Autoencoder with Generative Adversarial Network and is trained by the gradient-based technique. Our model can (1) encode the existing airfoil into a latent vector and reconstruct the airfoil from that, (2) generate novel airfoils by randomly sampling the latent vectors and mapping the vectors to the airfoil coordinate domain
&lt;/p&gt;</description></item><item><title>ShadowNet&#26159;&#19968;&#31181;&#23433;&#20840;&#39640;&#25928;&#30340;&#35774;&#22791;&#20869;&#25512;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#65292;&#21516;&#26102;&#23558;&#27169;&#22411;&#30340;&#37325;&#22411;&#32447;&#24615;&#23618;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#38544;&#31169;&#30340;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2011.05905</link><description>&lt;p&gt;
ShadowNet&#65306;&#19968;&#31181;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#39640;&#25928;&#30340;&#35774;&#22791;&#20869;&#25512;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ShadowNet: A Secure and Efficient On-device Model Inference System for Convolutional Neural Networks. (arXiv:2011.05905v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.05905
&lt;/p&gt;
&lt;p&gt;
ShadowNet&#26159;&#19968;&#31181;&#23433;&#20840;&#39640;&#25928;&#30340;&#35774;&#22791;&#20869;&#25512;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#65292;&#21516;&#26102;&#23558;&#27169;&#22411;&#30340;&#37325;&#22411;&#32447;&#24615;&#23618;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#38544;&#31169;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#20351;&#29992;AI&#21152;&#36895;&#22120;&#30340;&#22686;&#21152;&#65292;&#35774;&#22791;&#20869;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20170;&#22825;&#65292;&#22312;&#25968;&#21313;&#20159;&#20010;&#19981;&#21463;&#20449;&#20219;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#20102;&#25968;&#21315;&#20010;&#19987;&#26377;&#30340;ML&#27169;&#22411;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#27169;&#22411;&#38544;&#31169;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#20002;&#22833;&#23545;&#19981;&#21463;&#20449;&#20219;&#30340;AI&#21152;&#36895;&#22120;&#30340;&#35775;&#38382;&#26435;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#22791;&#20869;&#27169;&#22411;&#25512;&#26029;&#31995;&#32479;ShadowNet&#12290;ShadowNet&#36890;&#36807;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#65292;&#21516;&#26102;&#23433;&#20840;&#22320;&#23558;&#27169;&#22411;&#30340;&#37325;&#22411;&#32447;&#24615;&#23618;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;ShadowNet&#36890;&#36807;&#22312;&#22806;&#21253;&#20043;&#21069;&#36716;&#25442;&#32447;&#24615;&#23618;&#30340;&#26435;&#37325;&#24182;&#22312;TEE&#20869;&#24674;&#22797;&#32467;&#26524;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#38750;&#32447;&#24615;&#23618;&#20063;&#34987;&#23433;&#20840;&#22320;&#20445;&#30041;&#22312;TEE&#20869;&#12290;ShadowNet&#30340;&#35774;&#35745;&#30830;&#20445;&#20102;&#26435;&#37325;&#30340;&#39640;&#25928;&#36716;&#25442;&#21644;&#32467;&#26524;&#30340;&#21518;&#32493;&#24674;&#22797;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;ShadowNet&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
With the increased usage of AI accelerators on mobile and edge devices, on-device machine learning (ML) is gaining popularity. Thousands of proprietary ML models are being deployed today on billions of untrusted devices. This raises serious security concerns about model privacy. However, protecting model privacy without losing access to the untrusted AI accelerators is a challenging problem. In this paper, we present a novel on-device model inference system, ShadowNet. ShadowNet protects the model privacy with Trusted Execution Environment (TEE) while securely outsourcing the heavy linear layers of the model to the untrusted hardware accelerators. ShadowNet achieves this by transforming the weights of the linear layers before outsourcing them and restoring the results inside the TEE. The non-linear layers are also kept secure inside the TEE. ShadowNet's design ensures efficient transformation of the weights and the subsequent restoration of the results. We build a ShadowNet prototype b
&lt;/p&gt;</description></item></channel></rss>