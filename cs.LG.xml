<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21040;&#26368;&#21518;&#23618;&#30340;&#28608;&#27963;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20351;&#29992;HCR&#30028;&#38480;&#21487;&#37327;&#21270;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2404.02866</link><description>&lt;p&gt;
&#36890;&#36807;Hammersley-Chapman-Robbins&#30028;&#38480;&#20445;&#35777;&#26426;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21040;&#26368;&#21518;&#23618;&#30340;&#28608;&#27963;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20351;&#29992;HCR&#30028;&#38480;&#21487;&#37327;&#21270;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#36807;&#31243;&#20013;&#36890;&#36807;&#21521;&#26368;&#21518;&#20960;&#23618;&#30340;&#28608;&#27963;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#25252;&#38544;&#31169;&#26159;&#21487;&#33021;&#30340;&#12290;&#36825;&#20123;&#23618;&#20013;&#30340;&#28608;&#27963;&#34987;&#31216;&#20026;&#8220;&#29305;&#24449;&#8221;&#65288;&#23569;&#35265;&#30340;&#31216;&#20026;&#8220;&#23884;&#20837;&#8221;&#25110;&#8220;&#29305;&#24449;&#23884;&#20837;&#8221;&#65289;&#12290;&#28155;&#21152;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#38450;&#27490;&#20174;&#22024;&#26434;&#30340;&#29305;&#24449;&#20013;&#37325;&#24314;&#36755;&#20837;&#12290;&#36890;&#36807;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#26080;&#20559;&#20272;&#35745;&#37327;&#30340;&#26041;&#24046;&#36827;&#34892;&#19979;&#38480;&#20272;&#35745;&#65292;&#37327;&#21270;&#20102;&#30001;&#27492;&#28155;&#21152;&#30340;&#22122;&#22768;&#20135;&#29983;&#30340;&#26426;&#23494;&#24615;&#12290;&#32463;&#20856;&#19981;&#31561;&#24335;Hammersley&#21644;Chapman&#20197;&#21450;Robbins&#25552;&#20379;&#20415;&#21033;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#30028;&#38480;-- HCR&#30028;&#38480;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;&#21253;&#21547;10&#20010;&#31867;&#21035;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#8220;MNIST&#8221;&#21644;&#8220;CIFAR-10&#8221;&#65292;HCR&#30028;&#38480;&#22312;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;HCR&#30028;&#38480;&#20284;&#20046;&#21333;&#29420;&#26080;&#27861;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25237;&#24433;&#26041;&#24046;&#23545;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#38598;&#25104;&#20102;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#21644;&#21487;&#24494;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26680;&#36924;&#36817;&#26469;&#35299;&#20915;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#20174;&#32780;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01697</link><description>&lt;p&gt;
&#38450;&#27490;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Preventing Model Collapse in Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25237;&#24433;&#26041;&#24046;&#23545;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#38598;&#25104;&#20102;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#21644;&#21487;&#24494;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26680;&#36924;&#36817;&#26469;&#35299;&#20915;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#20174;&#32780;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gaussian process latent variable models (GPLVMs)&#26159;&#19968;&#31867;&#22810;&#25165;&#22810;&#33402;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#24120;&#29992;&#20110;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#29992;GPLVMs&#23545;&#25968;&#25454;&#24314;&#27169;&#26102;&#24120;&#35265;&#30340;&#25361;&#25112;&#21253;&#25324;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#21644;&#25237;&#24433;&#22122;&#22768;&#36873;&#25321;&#19981;&#24403;&#65292;&#23548;&#33268;&#20102;&#19968;&#31181;&#20197;&#27169;&#31946;&#28508;&#21464;&#37327;&#34920;&#31034;&#20026;&#20027;&#35201;&#29305;&#24449;&#30340;&#27169;&#22411;&#23849;&#28291;&#65292;&#36825;&#31181;&#34920;&#31034;&#19981;&#21453;&#26144;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#36890;&#36807;&#32447;&#24615;GPLVM&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#25237;&#24433;&#26041;&#24046;&#23545;&#27169;&#22411;&#23849;&#28291;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#38598;&#25104;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#21644;&#21487;&#24494;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26680;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#23548;&#33268;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#36890;&#36807;&#29616;&#25104;&#30340;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#23454;&#29616;&#23398;&#20064;&#26680;&#21442;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01697v1 Announce Type: cross  Abstract: Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hype
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2404.01291</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Visual Generation with Image-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01291
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#32508;&#21512;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VQAScore&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#23545;&#31616;&#21333;&#30340;&#8220;&#36825;&#24133;&#22270;&#34920;&#29616;&#20986;&#20102;'{&#25991;&#26412;}'&#21527;&#65311;&#8221;&#38382;&#39064;&#30340;&#8220;&#26159;&#8221;&#31572;&#26696;&#30340;&#27010;&#29575;&#26469;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#12290;&#23613;&#31649;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#65292;&#20294;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#35745;&#31639;&#30340;VQAScore&#22312;&#35768;&#22810;&#65288;8&#20010;&#65289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.17993</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#28982;&#26234;&#33021;&#30456;&#34701;&#21512;&#65306;&#20174;&#32479;&#35745;&#21147;&#23398;&#21040;&#20154;&#24037;&#26234;&#33021;&#20877;&#21040;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17993
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21453;&#24605;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#26410;&#26469;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#28237;&#27969;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#26681;&#26893;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#27169;&#22411;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#23457;&#26597;&#20102;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#21508;&#31181;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#27969;&#20307;&#21147;&#23398;&#30340;&#21516;&#26102;&#21457;&#23637;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
&lt;/p&gt;</description></item><item><title>Foundation Models&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24102;&#26469;&#21019;&#26032;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#20855;&#20307;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14735</link><description>&lt;p&gt;
&#22522;&#20110;Foundation Models&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65306;&#25945;&#31243;&#19982;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Foundation Models for Time Series Analysis: A Tutorial and Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14735
&lt;/p&gt;
&lt;p&gt;
Foundation Models&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24102;&#26469;&#21019;&#26032;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#20855;&#20307;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20316;&#20026;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#30340;&#28966;&#28857;&#65292;&#26159;&#25552;&#21462;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#22522;&#30707;&#65292;&#23545;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;Foundation Models&#65288;FMs&#65289;&#30340;&#21457;&#23637;&#26681;&#26412;&#24615;&#22320;&#25913;&#21464;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#35774;&#35745;&#30340;&#33539;&#24335;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;FMs&#65292;&#20197;&#33719;&#21462;&#19987;&#38376;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#37327;&#36523;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;Foundation Models&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#27010;&#36848;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;Foundation Models&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25110;&#31649;&#36947;&#26041;&#38754;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#28145;&#20837;&#20102;&#35299;&#38416;&#26126;Foundation Models&#22914;&#20309;&#21463;&#30410;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#37319;&#29992;&#20102;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14735v1 Announce Type: new  Abstract: Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric class
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Lie&#32676;&#30340;&#21160;&#21147;&#23398;Langevin Monte Carlo&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21644;&#31934;&#32454;&#31163;&#25955;&#21270;&#23454;&#29616;&#20102;Lie&#32676;&#32467;&#26500;&#30340;&#20445;&#25345;&#65292;&#24182;&#22312;W2&#36317;&#31163;&#19979;&#35777;&#26126;&#20102;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#31163;&#25955;&#37319;&#26679;&#22120;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12012</link><description>&lt;p&gt;
&#22522;&#20110;Lie&#32676;&#30340;&#21160;&#21147;&#23398;Langevin Monte Carlo&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of Kinetic Langevin Monte Carlo on Lie groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Lie&#32676;&#30340;&#21160;&#21147;&#23398;Langevin Monte Carlo&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21644;&#31934;&#32454;&#31163;&#25955;&#21270;&#23454;&#29616;&#20102;Lie&#32676;&#32467;&#26500;&#30340;&#20445;&#25345;&#65292;&#24182;&#22312;W2&#36317;&#31163;&#19979;&#35777;&#26126;&#20102;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#31163;&#25955;&#37319;&#26679;&#22120;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#21464;&#20998;&#20248;&#21270;&#21644;&#24038;&#24179;&#20961;&#21270;&#31561;&#25216;&#26415;&#26500;&#24314;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#12289;&#22522;&#20110;&#21160;&#37327;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#29992;&#20110;&#20248;&#21270;&#23450;&#20041;&#22312;Lie&#32676;&#19978;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36866;&#24403;&#22320;&#20026;&#20248;&#21270;&#21160;&#21147;&#23398;&#28155;&#21152;&#21487;&#22788;&#29702;&#30340;&#22122;&#22768;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#37319;&#26679;&#21160;&#21147;&#23398;&#65292;&#21033;&#29992;&#21160;&#37327;&#21464;&#37327;&#26159;&#27431;&#20960;&#37324;&#24471;&#30340;&#36825;&#19968;&#26377;&#21033;&#29305;&#24615;&#65292;&#23613;&#31649;&#28508;&#22312;&#20989;&#25968;&#23384;&#22312;&#20110;&#27969;&#24418;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#31163;&#25955;&#21270;&#23548;&#33268;&#30340;&#21160;&#21147;&#23398;&#37319;&#26679;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#19968;&#20010;Lie&#32676;MCMC&#37319;&#26679;&#22120;&#12290;&#36825;&#31181;&#31163;&#25955;&#21270;&#23436;&#20840;&#20445;&#25345;&#20102;Lie&#32676;&#32467;&#26500;&#12290;&#22312;W2&#36317;&#31163;&#19979;&#65292;&#20998;&#21035;&#23545;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#31163;&#25955;&#37319;&#26679;&#22120;&#35777;&#26126;&#20102;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#20854;&#20013;&#21482;&#38656;&#35201;Lie&#32676;&#30340;&#32039;&#33268;&#24615;&#21644;&#28508;&#22312;&#20989;&#25968;&#30340;&#27979;&#22320;L-&#20809;&#28369;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#21160;&#21147;&#23398;Langevin&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12012v1 Announce Type: cross  Abstract: Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under W2 distance. Only compactness of the Lie group and geodesically L-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26426;&#26800;&#20027;&#20041;&#21644;&#21151;&#33021;&#20027;&#20041;&#20004;&#31181;&#26041;&#27861;&#20197;&#23398;&#20064;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#26435;&#37325;&#30340;&#26377;&#29992;&#34920;&#31034;&#65292;&#24182;&#21457;&#23637;&#20102;&#26694;&#26550;&#26469;&#29983;&#25104;&#26377;&#21161;&#20110;&#30830;&#23450;RNN&#34892;&#20026;&#30340;&#20016;&#23500;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.11998</link><description>&lt;p&gt;
&#23398;&#20064;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30697;&#38453;&#30340;&#26377;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Useful Representations of Recurrent Neural Network Weight Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26426;&#26800;&#20027;&#20041;&#21644;&#21151;&#33021;&#20027;&#20041;&#20004;&#31181;&#26041;&#27861;&#20197;&#23398;&#20064;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#26435;&#37325;&#30340;&#26377;&#29992;&#34920;&#31034;&#65292;&#24182;&#21457;&#23637;&#20102;&#26694;&#26550;&#26469;&#29983;&#25104;&#26377;&#21161;&#20110;&#30830;&#23450;RNN&#34892;&#20026;&#30340;&#20016;&#23500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNNs)&#26159;&#36890;&#29992;&#30340;&#24182;&#34892;&#20018;&#34892;&#35745;&#31639;&#26426;&#12290; RNN&#30340;&#31243;&#24207;&#26159;&#20854;&#26435;&#37325;&#30697;&#38453;&#12290; &#22914;&#20309;&#23398;&#20064;&#26377;&#21161;&#20110;RNN&#20998;&#26512;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#30340;RNN&#26435;&#37325;&#30340;&#26377;&#29992;&#34920;&#31034;&#65311; &#23613;&#31649;&#26426;&#26800;&#20027;&#20041;&#26041;&#27861;&#30452;&#25509;&#26597;&#30475;&#19968;&#20123;RNN&#30340;&#26435;&#37325;&#26469;&#39044;&#27979;&#20854;&#34892;&#20026;&#65292;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#20998;&#26512;&#20854;&#25972;&#20307;&#21151;&#33021;--&#20855;&#20307;&#26469;&#35828;&#26159;&#20854;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#12290; &#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;RNN&#26435;&#37325;&#30340;&#26426;&#26800;&#20027;&#20041;&#26041;&#27861;&#65292;&#24182;&#20026;RNN&#24341;&#20837;&#20102;&#32622;&#25442;&#31561;&#21464;&#30340;&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#23618;&#12290;&#25105;&#20204;&#30340;&#20004;&#31181;&#26032;&#39062;&#30340;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#36890;&#36807;&#8220;&#35810;&#38382;&#8221;&#36755;&#20837;&#32780;&#20174;RNN&#26435;&#37325;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#21161;&#20110;&#30830;&#23450;RNN&#34892;&#20026;&#30340;&#20016;&#23500;&#34920;&#31034;&#30340;&#26465;&#20214;&#12290; &#25105;&#20204;&#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#20004;&#20010;&#8220;&#27169;&#22411;&#21160;&#29289;&#22253;&#8221;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;RNN&#26435;&#37325;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11998v1 Announce Type: new  Abstract: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation 
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#38024;&#23545;&#36719;Q-learning&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65292;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.06366</link><description>&lt;p&gt;
&#36719;Q-learning&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65306;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#38024;&#23545;&#36719;Q-learning&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65292;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Soft Q-learning&#26159;Q-learning&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#29109;&#27491;&#21017;&#21270;&#20540;&#20989;&#25968;&#12290;&#23613;&#31649;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23545;&#36719;Q-learning&#30340;&#29702;&#35770;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#36719;Q-learning&#31639;&#27861;&#30340;&#26032;&#39062;&#21644;&#32479;&#19968;&#30340;&#26377;&#38480;&#26102;&#38388;&#12289;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#36719;Q-learning&#31639;&#27861;&#65306;&#19968;&#31181;&#21033;&#29992;&#23545;&#25968;&#21644;&#25351;&#25968;&#36816;&#31639;&#23376;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#29627;&#23572;&#20857;&#26364;&#36816;&#31639;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#25512;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20998;&#26512;&#33021;&#22815;&#36890;&#36807;&#19982;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#24314;&#31435;&#32852;&#31995;&#26469;&#21152;&#28145;&#23545;&#36719;Q-learning&#30340;&#24403;&#21069;&#29702;&#35299;&#65292;&#29978;&#33267;&#20026;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#28385;&#36275;&#24191;&#27867;&#35201;&#27714;&#30340;&#21512;&#25104;&#12289;&#21322;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#22270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06017</link><description>&lt;p&gt;
&#24357;&#34917;&#20844;&#24179;&#22270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65306;&#36208;&#21521;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06017
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#28385;&#36275;&#24191;&#27867;&#35201;&#27714;&#30340;&#21512;&#25104;&#12289;&#21322;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#22270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#22270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#20844;&#24179;&#22270;&#23398;&#20064;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#24448;&#24448;&#20381;&#36182;&#20110;&#26500;&#36896;&#19981;&#20339;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#25110;&#20302;&#26631;&#20934;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20063;&#21487;&#20197;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#32988;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#35768;&#22810;&#25968;&#25454;&#38598;&#26410;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#36793;&#32536;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#22270;&#32467;&#26500;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20805;&#20998;&#28385;&#36275;&#24191;&#27867;&#35201;&#27714;&#30340;&#21512;&#25104;&#12289;&#21322;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#38598;&#21512;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#21253;&#25324;&#23545;&#20110;&#27169;&#22411;&#20844;&#24179;&#35780;&#20215;&#33267;&#20851;&#37325;&#35201;&#30340;&#30456;&#20851;&#22270;&#32467;&#26500;&#21644;&#20559;&#24046;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06017v1 Announce Type: new  Abstract: Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to
&lt;/p&gt;</description></item><item><title>HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05396</link><description>&lt;p&gt;
HistGen&#65306;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05396
&lt;/p&gt;
&lt;p&gt;
HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#30284;&#30151;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20020;&#24202;&#25253;&#21578;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#19968;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25351;&#23548;&#30284;&#30151;&#27835;&#30103;&#21644;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#23558;&#26497;&#22823;&#25552;&#21319;&#20020;&#24202;&#25928;&#29575;&#65292;&#24182;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#22312;&#25253;&#21578;&#25776;&#20889;&#26041;&#38754;&#30340;&#21171;&#21160;&#24378;&#24230;&#21644;&#32791;&#26102;&#36127;&#25285;&#12290;&#20026;&#36861;&#27714;&#36825;&#19968;&#36827;&#27493;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;HistGen&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#22686;&#24378;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;HistGen&#21463;&#35786;&#26029;&#21644;&#25253;&#21578;&#25776;&#20889;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#25253;&#21578;&#65292;&#20174;&#26412;&#22320;&#21644;&#20840;&#23616;&#31890;&#24230;&#25552;&#21319;&#25253;&#21578;&#29983;&#25104;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#20998;&#23618;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#21306;&#22495;&#20013;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;</title><link>https://arxiv.org/abs/2403.04082</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#36827;&#34892;&#25512;&#26029;&#65306;&#23545;&#27604;&#34920;&#31034;&#21487;&#35777;&#26126;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25105;&#20204;&#22914;&#20309;&#22238;&#31572;&#35832;&#22914;&#8220;&#26410;&#26469;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#25105;&#20204;&#26159;&#22914;&#20309;&#21040;&#36798;&#36825;&#37324;&#30340;&#65311;&#8221;&#36825;&#31867;&#27010;&#29575;&#25512;&#26029;&#38382;&#39064;&#22312;&#35266;&#27979;&#20540;&#20026;&#39640;&#32500;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#30340;&#32039;&#20945;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#23545;&#27604;&#23398;&#20064;&#30340;&#21464;&#20307;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#27010;&#29575;&#27604;&#12290;&#36890;&#36807;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25193;&#23637;&#20197;&#34920;&#26126;&#34920;&#31034;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#25105;&#20204;&#38543;&#21518;&#35777;&#26126;&#34920;&#31034;&#30340;&#32852;&#21512;&#20998;&#24067;&#20063;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#20123;&#32467;&#26524;&#20849;&#21516;&#34920;&#26126;&#65292;&#36890;&#36807;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#19968;&#31181;&#22270;&#24418;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#34920;&#31034;&#36827;&#34892;&#30340;&#25512;&#26029;&#65288;&#20363;&#22914;&#39044;&#27979;&#12289;&#35268;&#21010;&#65289;&#23545;&#24212;&#20110;&#21453;&#28436;&#20302;&#32500;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#36890;&#36807;&#20998;&#22359;&#39044;&#35013;&#22635;&#25216;&#26415;&#24179;&#34913;&#20102;GPU&#35745;&#31639;&#39281;&#21644;&#21644;&#21333;&#20010;&#26631;&#35760;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.02310</link><description>&lt;p&gt;
&#22312;LLM&#25512;&#29702;&#20013;&#24179;&#34913;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#26435;&#34913;&#30340;&#30740;&#31350;&#65306;Sarathi-Serve&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02310
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#36890;&#36807;&#20998;&#22359;&#39044;&#35013;&#22635;&#25216;&#26415;&#24179;&#34913;&#20102;GPU&#35745;&#31639;&#39281;&#21644;&#21644;&#21333;&#20010;&#26631;&#35760;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#20010;LLM&#26381;&#21153;&#35831;&#27714;&#32463;&#21382;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#26159;prefill&#38454;&#27573;&#65292;&#22788;&#29702;&#25972;&#20010;&#36755;&#20837;&#25552;&#31034;&#20197;&#29983;&#25104;&#19968;&#20010;&#36755;&#20986;&#26631;&#35760;&#65307;&#31532;&#20108;&#20010;&#26159;decode&#38454;&#27573;&#65292;&#36880;&#20010;&#29983;&#25104;&#20854;&#20313;&#30340;&#36755;&#20986;&#26631;&#35760;&#12290;Prefill&#36845;&#20195;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#24182;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#20351;GPU&#35745;&#31639;&#39281;&#21644;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;decode&#36845;&#20195;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#65292;&#20294;&#20063;&#20165;&#20351;&#29992;&#36739;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#27599;&#20010;&#35831;&#27714;&#21482;&#22788;&#29702;&#19968;&#20010;&#26631;&#35760;&#12290;&#36825;&#20351;&#24471;&#23545;&#35299;&#30721;&#26469;&#35828;&#25209;&#22788;&#29702;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#23545;&#25972;&#20307;&#21534;&#21520;&#37327;&#20063;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25209;&#37327;&#22788;&#29702;&#22810;&#20010;&#35831;&#27714;&#20250;&#23548;&#33268;prefill&#21644;decode&#36845;&#20195;&#20132;&#38169;&#36827;&#34892;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#30340;&#24179;&#34913;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#28789;&#24863;&#26469;&#33258;&#25105;&#20204;&#26368;&#21021;&#20026;&#20248;&#21270;Sarathi&#30340;&#21534;&#21520;&#37327;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;Sarathi-Serve&#21033;&#29992;&#20102;&#20174;Sarathi&#20013;&#24341;&#20837;&#30340;&#20998;&#22359;prefill&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi 
&lt;/p&gt;</description></item><item><title>EEG2Rep&#36890;&#36807;&#22312;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#21644;&#20351;&#29992;&#26032;&#30340;&#35821;&#20041;&#23376;...</title><link>https://arxiv.org/abs/2402.17772</link><description>&lt;p&gt;
EEG2Rep&#65306;&#36890;&#36807;&#20449;&#24687;&#21270;&#36974;&#34109;&#36755;&#20837;&#22686;&#24378;&#33258;&#30417;&#30563;&#33041;&#30005;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17772
&lt;/p&gt;
&lt;p&gt;
EEG2Rep&#36890;&#36807;&#22312;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#21644;&#20351;&#29992;&#26032;&#30340;&#35821;&#20041;&#23376;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#34920;&#31034;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#38754;&#20020;EEG&#25968;&#25454;&#22266;&#26377;&#30340;&#19977;&#20010;&#29305;&#23450;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20302;&#20449;&#22122;&#27604;&#25361;&#25112;&#23398;&#21040;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#65288;2&#65289;&#25391;&#24133;&#33539;&#22260;&#24191;&#65292;&#20174;&#38750;&#24120;&#23567;&#21040;&#30456;&#23545;&#36739;&#22823;&#65292;&#30001;&#20110;&#35832;&#22914;&#21463;&#35797;&#32773;&#38388;&#21464;&#24322;&#24615;&#31561;&#22240;&#32032;&#65292;&#39118;&#38505;&#23548;&#33268;&#27169;&#22411;&#34987;&#39640;&#25391;&#24133;&#33539;&#22260;&#20027;&#23548;&#65292;&#21644;&#65288;3&#65289;&#36830;&#32493;&#20540;&#24207;&#21015;&#20013;&#32570;&#20047;&#26126;&#30830;&#20998;&#21106;&#65292;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#36739;&#23569;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EEG2Rep&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;EEG&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#33258;&#39044;&#27979;&#26041;&#27861;&#12290;EEG2Rep&#30340;&#20004;&#20010;&#26680;&#24515;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#22914;&#19979;&#65306;1&#65289;EEG2Rep&#19981;&#26159;&#23398;&#20064;&#20174;&#21407;&#22987;EEG&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#65292;&#32780;&#26159;&#23398;&#20064;&#22312;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#65292;2&#65289;EEG2Rep&#19981;&#20351;&#29992;&#20256;&#32479;&#30340;&#36974;&#34109;&#26041;&#27861;&#65292;&#32780;&#26159;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#35821;&#20041;&#23376;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17772v1 Announce Type: cross  Abstract: Self-supervised approaches for electroencephalography (EEG) representation learning face three specific challenges inherent to EEG data: (1) The low signal-to-noise ratio which challenges the quality of the representation learned, (2) The wide range of amplitudes from very small to relatively large due to factors such as the inter-subject variability, risks the models to be dominated by higher amplitude ranges, and (3) The absence of explicit segmentation in the continuous-valued sequences which can result in less informative representations. To address these challenges, we introduce EEG2Rep, a self-prediction approach for self-supervised representation learning from EEG. Two core novel components of EEG2Rep are as follows: 1) Instead of learning to predict the masked input from raw EEG, EEG2Rep learns to predict masked input in latent representation space, and 2) Instead of conventional masking methods, EEG2Rep uses a new semantic sub
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GPHT&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#25968;&#25454;&#38598;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#24573;&#35270;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16516</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Hierarchical Transformer for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16516
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GPHT&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#25968;&#25454;&#38598;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#24573;&#35270;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#19968;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#24191;&#27867;&#37319;&#29992;&#19968;&#27493;&#29983;&#25104;&#27169;&#24335;&#65292;&#36825;&#38656;&#35201;&#23450;&#21046;&#21270;&#30340;&#39044;&#27979;&#22836;&#37096;&#65292;&#24573;&#30053;&#20102;&#36755;&#20986;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#36328;&#24230;&#35774;&#32622;&#19979;&#20250;&#23548;&#33268;&#22686;&#21152;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#39044;&#27979;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer&#26550;&#26500;&#65292;&#21629;&#21517;&#20026;GPHT&#12290;GPHT&#20013;&#26377;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16516v1 Announce Type: new  Abstract: Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various dataset
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#24120;&#29992;&#30340;&#32676;&#65292;&#30740;&#31350;&#25581;&#31034;&#20986;&#27809;&#26377;&#26377;&#25928;&#30340;&#21487;&#35745;&#31639;&#30340;&#26694;&#26550;&#36873;&#25321;&#33021;&#22815;&#20445;&#25345;&#34987;&#24179;&#22343;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#65292;&#20294;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21152;&#26435;&#26694;&#26550;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.16077</link><description>&lt;p&gt;
&#31561;&#21464;&#26694;&#26550;&#19982;&#36830;&#32493;&#35268;&#33539;&#21270;&#30340;&#19981;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivariant Frames and the Impossibility of Continuous Canonicalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16077
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24120;&#29992;&#30340;&#32676;&#65292;&#30740;&#31350;&#25581;&#31034;&#20986;&#27809;&#26377;&#26377;&#25928;&#30340;&#21487;&#35745;&#31639;&#30340;&#26694;&#26550;&#36873;&#25321;&#33021;&#22815;&#20445;&#25345;&#34987;&#24179;&#22343;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#65292;&#20294;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21152;&#26435;&#26694;&#26550;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#24378;&#21046;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#36817;&#26399;&#24191;&#21463;&#20851;&#27880;&#30340;&#27867;&#21270;&#26041;&#27861;&#22914;&#26694;&#26550;&#24179;&#22343;&#21270;&#25104;&#20026;&#19968;&#31181;&#36731;&#37327;&#19988;&#28789;&#27963;&#30340;&#31561;&#21464;&#26550;&#26500;&#26367;&#20195;&#26041;&#26696;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#33021;&#22815;&#24102;&#26469;&#23454;&#35777;&#25928;&#30410;&#65292;&#36825;&#20123;&#26694;&#26550;&#23398;&#20064;&#32676;&#20803;&#32032;&#19978;&#30340;&#21152;&#26435;&#20998;&#24067;&#12290; &#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#19968;&#29616;&#35937;&#30340;&#24378;&#26377;&#21147;&#29702;&#35770;&#35777;&#25454;&#65306;&#23545;&#20110;&#24120;&#29992;&#30340;&#32676;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#21487;&#35745;&#31639;&#30340;&#26694;&#26550;&#36873;&#25321;&#33021;&#22815;&#20445;&#25345;&#34987;&#24179;&#22343;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#12290; &#25442;&#21477;&#35805;&#35828;&#65292;&#38750;&#21152;&#26435;&#30340;&#26694;&#26550;&#24179;&#22343;&#21487;&#20197;&#23558;&#19968;&#20010;&#24179;&#28369;&#30340;&#12289;&#38750;&#23545;&#31216;&#30340;&#20989;&#25968;&#36716;&#21464;&#20026;&#19968;&#20010;&#19981;&#36830;&#32493;&#12289;&#23545;&#31216;&#30340;&#20989;&#25968;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22522;&#26412;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#24182;&#26500;&#24314;&#20102;\emph{&#21152;&#26435;}&#26694;&#26550;&#65292;&#25454;&#35777;&#26126;&#33021;&#22815;&#20445;&#25345;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#39640;&#25928;&#36830;&#32493;&#30340;&#21152;&#26435;&#26694;&#26550;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16077v1 Announce Type: new  Abstract: Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct \emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the act
&lt;/p&gt;</description></item><item><title>RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2402.15814</link><description>&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#24402;&#32435;&#20559;&#24046;&#30340;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Result on the Inductive Bias of RNN Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15814
&lt;/p&gt;
&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32463;&#39564;&#25104;&#21151;&#21487;&#33021;&#24615;&#30340;&#19968;&#20010;&#35299;&#37322;&#12290; &#23427;&#26174;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#22312;&#20154;&#31867;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26377;&#30028;&#20998;&#23618;&#32467;&#26500;&#12290; &#36825;&#34920;&#26126;RNNs&#30340;&#25104;&#21151;&#21487;&#33021;&#19982;&#23427;&#20204;&#24314;&#27169;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290; &#28982;&#32780;&#65292;&#23545;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#26500;&#36896;&#30340;&#26356;&#35814;&#32454;&#26816;&#26597;&#34920;&#26126;&#65292;&#23427;&#19981;&#38480;&#20110;&#20998;&#23618;LMs&#65292;&#36825;&#24341;&#20986;&#20102;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#21738;&#20123;\emph{&#20854;&#20182;&#31867;&#22411;} LMs&#30340;&#38382;&#39064;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#25324;&#20182;&#20204;&#30340;&#26500;&#36896;&#20197;&#23637;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;LMs&#65306;&#21487;&#20197;&#36890;&#36807;&#24102;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#30340;&#19979;&#25512;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#37027;&#20123;&#12290; &#36825;&#31867;&#20284;&#20110;&#19968;&#20010;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#26356;&#26032;&#35760;&#24518;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15301</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15301
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22270;&#24674;&#22797;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#26159;&#22522;&#20110;&#30693;&#35782;&#25110;&#32479;&#35745;&#20272;&#35745;&#65292;&#21463;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#20851;&#20110;&#24433;&#21709;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#31185;&#23398;&#25991;&#29486;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25512;&#23548;&#19968;&#33324;&#22240;&#26524;&#22270;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;LLMs&#31995;&#32479;&#22320;&#20998;&#26512;&#21644;&#25552;&#21462;&#26469;&#33258;&#24191;&#27867;&#30740;&#31350;&#35770;&#25991;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20174;&#27719;&#24635;&#30340;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26412;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;LLM&#34987;&#29992;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#22240;&#32032;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09370</link><description>&lt;p&gt;
&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;
&lt;/p&gt;
&lt;p&gt;
Pseudorandom Error-Correcting Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09370
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65288;&#25110;&#31616;&#31216;&#20026;&#20266;&#38543;&#26426;&#30721;&#65289;&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#32416;&#38169;&#30721;&#65306;&#23545;&#20110;&#20219;&#20309;&#35745;&#31639;&#21463;&#38480;&#30340;&#23545;&#25163;&#26469;&#35828;&#65292;&#20219;&#24847;&#22810;&#20010;&#32534;&#30721;&#35789;&#37117;&#26159;&#20266;&#38543;&#26426;&#30340;&#12290;&#36890;&#36807;&#35299;&#30721;&#23494;&#38053;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32416;&#27491;&#26377;&#38169;&#35823;&#30340;&#32534;&#30721;&#35789;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#23545;&#26367;&#25442;&#38169;&#35823;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#20266;&#38543;&#26426;&#30721;&#65292;&#20854;&#20013;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;&#26631;&#20934;&#23494;&#30721;&#23398;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;LPN&#38382;&#39064;&#30340;$2^{O(\sqrt{n})}$&#22256;&#38590;&#31243;&#24230;&#65292;&#25110;&#32773;&#22522;&#20110;LPN&#38382;&#39064;&#21644;&#20302;&#23494;&#24230;&#19979;&#30340;&#25554;&#20837;&#24322;&#25110;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.08526</link><description>&lt;p&gt;
Concept-1K&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Concept-1K: A Novel Benchmark for Instance Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#65288;IL&#65289;&#23545;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20154;&#31867;&#32423;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IL&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#26080;&#27861;&#35780;&#20272;PLM&#20013;&#30340;&#36951;&#24536;&#65292;&#20351;&#20154;&#35823;&#20197;&#20026;PLM&#19981;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;IL&#22330;&#26223;&#65292;&#31216;&#20026;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25903;&#25345;&#25968;&#37327;&#32423;&#26356;&#22823;&#30340;IL&#27493;&#39588;&#30340;&#26032;&#25968;&#25454;&#38598;Concept-1K&#12290;&#22522;&#20110;&#23545;Concept-1K&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#36951;&#24536;&#21463;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;&#19968;&#31181;&#27969;&#34892;&#30340;&#24494;&#35843;&#25216;&#26415;LoRA&#37117;&#26410;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;PLM&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#40723;&#21169;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#20197;&#20943;&#36731;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07721</link><description>&lt;p&gt;
LoRA-drop&#65306;&#22522;&#20110;&#36755;&#20986;&#35780;&#20272;&#30340;&#39640;&#25928;LoRA&#21442;&#25968;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20026;&#27599;&#20010;&#23618;&#24341;&#20837;&#36741;&#21161;&#21442;&#25968;&#65292;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#24403;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#36164;&#28304;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#23618;&#30340;LoRA&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#26469;&#37319;&#29992;&#21098;&#26525;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#21482;&#20998;&#26512;&#20102;&#21442;&#25968;&#30340;&#29305;&#24449;&#20197;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#21442;&#25968;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;LoRA&#30340;&#36755;&#20986;&#26159;&#30452;&#25509;&#24433;&#21709;&#20923;&#32467;&#27169;&#22411;&#30340;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRA-drop&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#26469;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#32780;&#20854;&#20182;&#23618;&#30340;LoRA&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#12290;&#22312;NLU&#21644;NLG&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LoRA-drop&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.06465</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Differentially Private Subspace Estimation Without Distributional Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#38754;&#20020;&#30528;&#19968;&#20010;&#34987;&#31216;&#20026;&#32500;&#25968;&#35781;&#21650;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#20855;&#26377;&#22266;&#26377;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26799;&#24230;&#32463;&#24120;&#20301;&#20110;&#19968;&#20010;&#20302;&#32500;&#23376;&#31354;&#38388;&#38468;&#36817;&#12290;&#22914;&#26524;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#28857;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#36825;&#31181;&#20302;&#32500;&#32467;&#26500;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;&#22240;&#39640;&#32500;&#24230;&#32780;&#25903;&#20184;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying (in terms of privacy and accuracy) for the high ambient dimension.   On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed this limitation by considering points that are i.i.d. samples from a Gaussian distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was still left unclear whether we could provide similar upper bounds without distributional assumptions and whether we 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#35757;&#32451;&#25439;&#22833;&#30340;&#39640;&#26041;&#24046;&#65292;&#20174;&#32780;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.05453</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#35757;&#32451;&#25439;&#22833;&#30340;&#39640;&#26041;&#24046;&#65292;&#20174;&#32780;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#21363;&#25512;&#26029;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#12290;&#29616;&#26377;&#24037;&#20316;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#26469;&#22686;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#25439;&#22833;&#26041;&#24046;&#65292;&#32531;&#35299;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#21521;&#30456;&#21453;&#26041;&#21521;&#20248;&#21270;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;&#22312;&#23616;&#37096;&#26368;&#23567;&#20540;&#38468;&#36817;&#25391;&#33633;&#65292;&#23548;&#33268;&#19981;&#31283;&#23450;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#30340;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#20998;&#24067;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20542;&#21521;&#20110;&#20943;&#23569;&#25439;&#22833;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;CCL&#30340;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20985;&#20989;&#25968;&#39033;&#20943;&#23567;&#25439;&#22833;&#20989;&#25968;&#30340;&#20984;&#24615;&#12290;&#20351;&#29992;CCL&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#26041;&#24046;&#25439;&#22833;&#65292;&#21152;&#24378;&#20102;&#23545;MIAs&#30340;&#38450;&#24481;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;CCL&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#65292;&#39318;&#27425;&#23581;&#35797;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#21407;&#22987;&#22270;&#35889;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05011</link><description>&lt;p&gt;
&#23548;&#33322;&#22797;&#26434;&#24615;&#65306;&#36890;&#36807;&#25193;&#23637;&#31383;&#21475;&#21305;&#37197;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;
&lt;/p&gt;
&lt;p&gt;
Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#65292;&#39318;&#27425;&#23581;&#35797;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#21407;&#22987;&#22270;&#35889;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35889;&#31934;&#31616;&#26088;&#22312;&#36890;&#36807;&#21512;&#25104;&#32039;&#20945;&#30340;&#22270;&#35889;&#26469;&#20943;&#23569;&#22823;&#35268;&#27169;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20026;&#20943;&#23569;&#35757;&#32451;GNNs&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#22270;&#35889;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#26080;&#25439;&#31934;&#31616;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#65292;&#24182;&#25581;&#31034;&#20102;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#21305;&#37197;&#26041;&#27861;&#22312;&#20248;&#21270;&#31934;&#31616;&#22270;&#35889;&#26102;&#25552;&#20379;&#20102;&#26469;&#33258;&#21407;&#22987;&#22270;&#35889;&#30340;&#20559;&#20506;&#21644;&#21463;&#38480;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#31934;&#31616;&#22270;&#35889;&#30340;&#35268;&#27169;&#21644;&#21151;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04513</link><description>&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Cascade Learning for Efficient Inference over Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#22238;&#31572;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#30340;&#20248;&#21183;&#65292;&#20294;&#26159; LLM &#25512;&#29702;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#36825;&#37324;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#22120;&#65289;&#24320;&#22987;&#65292;&#21040;&#24378;&#22823;&#30340; LLM &#32467;&#26463;&#65292;&#24182;&#37197;&#22791;&#19968;&#20010;&#20915;&#23450;&#22312;&#32473;&#23450;&#36755;&#20837;&#19978;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22312;&#32447;&#23398;&#20064;&#32423;&#32852;&#30340;&#20219;&#21153;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982; LLM &#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25512;&#29702;&#25104;&#26412;&#21066;&#20943;&#20102;&#22810;&#36798; 90%&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#27969;&#22788;&#29702;&#20013;&#30340;&#25928;&#33021;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#23433;&#20840;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20182;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#26377;&#23475;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;VLGuard&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23558;&#35813;&#25968;&#25454;&#38598;&#38598;&#25104;&#21040;&#35270;&#35273;&#35821;&#35328;&#24494;&#35843;&#20013;&#25110;&#36827;&#34892;&#20107;&#21518;&#24494;&#35843;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#40784;VLLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#26377;&#30410;&#24615;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#29978;&#33267;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#23545;&#29616;&#26377;VLLMs&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#12289;&#35757;&#32451;&#26032;&#27169;&#22411;&#25110;&#20445;&#25252;&#39044;&#35757;&#32451;VLLMs&#12290;</title><link>https://arxiv.org/abs/2402.02207</link><description>&lt;p&gt;
&#23433;&#20840;&#24494;&#35843;&#20960;&#20046;&#38646;&#25104;&#26412;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35270;&#35273;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#23433;&#20840;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20182;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#26377;&#23475;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;VLGuard&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23558;&#35813;&#25968;&#25454;&#38598;&#38598;&#25104;&#21040;&#35270;&#35273;&#35821;&#35328;&#24494;&#35843;&#20013;&#25110;&#36827;&#34892;&#20107;&#21518;&#24494;&#35843;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#40784;VLLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#26377;&#30410;&#24615;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#29978;&#33267;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#23545;&#29616;&#26377;VLLMs&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#12289;&#35757;&#32451;&#26032;&#27169;&#22411;&#25110;&#20445;&#25252;&#39044;&#35757;&#32451;VLLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#26377;&#23475;&#25968;&#25454;&#65292;&#24182;&#19988;VLLM&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#23545;&#24213;&#23618;LLM&#20043;&#21069;&#23398;&#20064;&#30340;&#23433;&#20840;&#23545;&#40784;&#24615;&#30340;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#26377;&#23475;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;VLGuard&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#35813;&#25968;&#25454;&#38598;&#38598;&#25104;&#21040;&#26631;&#20934;&#30340;&#35270;&#35273;&#35821;&#35328;&#24494;&#35843;&#20013;&#25110;&#23558;&#20854;&#29992;&#20110;&#20107;&#21518;&#24494;&#35843;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#40784;VLLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#31181;&#23545;&#40784;&#26159;&#22312;&#23545;&#27169;&#22411;&#30340;&#26377;&#30410;&#24615;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#29978;&#33267;&#26377;&#25152;&#25552;&#21319;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#23433;&#20840;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#25104;&#20026;&#23545;&#29616;&#26377;VLLMs&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#65292;&#35757;&#32451;&#26032;&#27169;&#22411;&#25110;&#20445;&#25252;&#39044;&#35757;&#32451;VLLMs&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.00743</link><description>&lt;p&gt;
Transformer&#30340;&#22909;&#22788;&#65306;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;\citet{zhang2023trained,huang2023context}&#23545;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#26159;&#20182;&#20204;&#20551;&#35774;&#27599;&#20010;&#26679;&#26412;&#30340;&#36755;&#20837;$x_i$&#21644;&#36755;&#20986;$y_i$&#37117;&#34987;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#20196;&#29260;&#20013;&#65288;&#21363;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#23427;&#20204;&#21576;&#29616;&#20026;&#20004;&#20010;&#20196;&#29260;&#65288;&#21363;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;\cite{wibisono2023role}&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#30456;&#24212;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Transformer&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Transformer&#20013;&#36215;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#29992;&#30340;&#30830;&#20999;&#32452;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24102;&#26377;&#20004;&#23618;softmax&#65288;&#33258;&#25105;&#65289;&#27880;&#24847;&#21147;&#21644;&#21069;&#30651;&#24615;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;Transformer&#21487;&#20197;&#20174;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#22914;&#26524;$y_i$&#22312;&#20196;&#29260;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00564</link><description>&lt;p&gt;
&#19968;&#27425;&#22270;&#21367;&#31215;&#23601;&#22815;&#20102;&#65306;&#39640;&#25928;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#32780;CNN&#30456;&#27604;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#26356;&#21152;&#24222;&#22823;&#65292;&#36825;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36866;&#29992;&#20110;RGB&#21644;&#28784;&#24230;&#25968;&#25454;&#38598;&#65292;&#20294;&#20165;&#20165;&#20351;&#29992;&#28784;&#24230;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#30456;&#23545;&#36739;&#23569;&#35265;&#12290;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(ATR)&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#30340;&#30690;&#37327;&#21270;&#35270;&#22270;&#30340;&#26032;&#22411;&#28784;&#24230;(&#21333;&#36890;&#36947;)&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#23558;&#38382;&#39064;&#35774;&#32622;&#20026;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;MLP&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25209;&#27425;&#32423;&#21035;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23567;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#20934;&#30830;&#29575;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2401.17541</link><description>&lt;p&gt;
&#36879;&#36807;&#26657;&#20934;&#30340;&#35270;&#35282;&#29702;&#35299;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27979;&#35797;&#20998;&#24067;&#24448;&#24448;&#19982;&#35757;&#32451;&#19981;&#21516;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#36234;&#22495;&#27867;&#21270;&#65292;&#22312;&#24120;&#35268;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#20316;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#26088;&#22312;&#35782;&#21035;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#36234;&#22495;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;IRM&#30340;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;&#21452;&#23618;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20123;&#36817;&#20284;IRM&#25216;&#26415;&#65292;&#20351;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#12290;ECE&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#23427;&#26159;&#34913;&#37327;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#25429;&#25417;&#21040;&#29615;&#22659;&#19981;&#21464;&#29305;&#24449;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20102;...&#65288;&#25509;&#19979;&#37096;&#20998;&#25688;&#35201;&#36229;&#36807;200&#23383;&#65292;&#25552;&#21462;&#21069;200&#23383;&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#28041;&#21450;&#26680;-&#22270;&#23545;&#40784;&#29616;&#35937;&#65292;&#20174;&#20248;&#21270;&#35282;&#24230;&#35299;&#37322;&#20102;&#23398;&#21040;&#30340;&#20989;&#25968;&#20309;&#26102;&#21644;&#20026;&#20309;&#27867;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22312;&#24322;&#28304;&#22270;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2310.05105</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#23398;&#20064;&#30340;&#65306;&#26469;&#33258;&#35757;&#32451;&#21160;&#24577;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
How Graph Neural Networks Learn: Lessons from Training Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05105
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#28041;&#21450;&#26680;-&#22270;&#23545;&#40784;&#29616;&#35937;&#65292;&#20174;&#20248;&#21270;&#35282;&#24230;&#35299;&#37322;&#20102;&#23398;&#21040;&#30340;&#20989;&#25968;&#20309;&#26102;&#21644;&#20026;&#20309;&#27867;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22312;&#24322;&#28304;&#22270;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#20197;&#26356;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#34920;&#24449;&#40657;&#30418;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#12290;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#22312;&#27491;&#24335;&#21270;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;GNNs&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#26399;&#26395;&#30340;&#20989;&#25968;&#20173;&#19981;&#22826;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;GNNs&#38544;&#24335;&#21033;&#29992;&#22270;&#32467;&#26500;&#26469;&#26356;&#26032;&#23398;&#21040;&#30340;&#20989;&#25968;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#26680;-&#22270;&#23545;&#40784;&#65292;&#24050;&#32463;&#32463;&#39564;&#24615;&#21644;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#31181;&#26469;&#33258;&#20248;&#21270;&#35282;&#24230;&#30340;&#26032;&#20998;&#26512;&#26694;&#26550;&#33021;&#22815;&#35299;&#37322;&#20102;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#23398;&#20064;&#21040;&#30340;GNN&#20989;&#25968;&#27867;&#21270;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#22312;&#24322;&#28304;&#22270;&#19978;&#30340;&#38480;&#21046;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#20174;&#23454;&#29992;&#30340;&#35282;&#24230;&#30475;&#65292;&#23427;&#20063;&#25552;&#20379;&#20102;&#23545;&#20110;GNNs&#22914;&#20309;&#23398;&#20064;&#20989;&#25968;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05105v2 Announce Type: replace  Abstract: A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, but whether GNNs will learn desired functions during the optimization process remains less clear. To fill this gap, we study their training dynamics in function space. In particular, we find that the optimization of GNNs through gradient descent implicitly leverages the graph structure to update the learned function. This phenomenon is dubbed as kernel-graph alignment, which has been empirically and theoretically corroborated. This new analytical framework from the optimization perspective enables interpretable explanations of when and why the learned GNN functions generalize, which are relevant to their limitations on heterophilic graphs. From a practical standpoint, it also prov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#28024;&#36719;&#32420;&#32500;&#21644;&#23548;&#31649;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16937</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#28024;&#36719;&#32420;&#32500;&#21644;&#23548;&#31649;&#36827;&#34892;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning. (arXiv:2401.16937v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#28024;&#36719;&#32420;&#32500;&#21644;&#23548;&#31649;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26408;&#26448;&#30001;&#32420;&#32500;&#21644;&#23548;&#31649;&#31561;&#19981;&#21516;&#31181;&#31867;&#30340;&#32454;&#32990;&#32452;&#25104;&#65292;&#36825;&#20123;&#32454;&#32990;&#30340;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#25490;&#21015;&#23545;&#20110;&#29702;&#35299;&#26408;&#26448;&#26679;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#36825;&#28041;&#21450;&#23558;&#26679;&#26412;&#28024;&#27873;&#22312;&#28342;&#28082;&#20013;&#20197;&#20998;&#31163;&#32454;&#32990;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#20998;&#25955;&#22312;&#36733;&#29627;&#29255;&#19978;&#65292;&#29992;&#26174;&#24494;&#38236;&#36827;&#34892;&#24191;&#22495;&#25104;&#20687;&#65292;&#25429;&#25417;&#25968;&#21315;&#20010;&#32454;&#32990;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32454;&#32990;&#22312;&#22270;&#20687;&#20013;&#32463;&#24120;&#32858;&#38598;&#21644;&#37325;&#21472;&#65292;&#20351;&#29992;&#26631;&#20934;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#20998;&#21106;&#21464;&#24471;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;&#32467;&#26524;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#38454;YOLOv8&#27169;&#22411;&#26469;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#23545;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#32420;&#32500;&#21644;&#23548;&#31649;&#36827;&#34892;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;32640 x 25920&#20687;&#32032;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#26377;&#25928;&#30340;&#32454;&#32990;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#36798;&#21040;78%&#30340;mAP_0.5-0.95&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23545;&#32463;&#36807;&#22522;&#22240;&#25913;&#36896;&#30340;&#32420;&#32500;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Wood comprises different cell types, such as fibers and vessels, defining its properties. Studying their shape, size, and arrangement in microscopic images is crucial for understanding wood samples. Typically, this involves macerating (soaking) samples in a solution to separate cells, then spreading them on slides for imaging with a microscope that covers a wide area, capturing thousands of cells. However, these cells often cluster and overlap in images, making the segmentation difficult and time-consuming using standard image-processing methods. Results: In this work, we develop an automatic deep learning segmentation approach that utilizes the one-stage YOLOv8 model for fast and accurate fiber and vessel segmentation and characterization in microscopy images. The model can analyze 32640 x 25920 pixels images and demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95 of 78 %. To assess the model's robustness, we examined fibers from a genetically modi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.15935</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65306;&#29983;&#25104;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33719;&#21462;&#20107;&#20214;&#24207;&#21015;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27169;&#24577;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#38134;&#34892;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#25105;&#20204;&#23545;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#21035;&#24212;&#29992;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#32477;&#23545;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#24403;&#20195;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#20114;&#26021;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#23427;&#20204;&#30340;&#32852;&#21512;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#33267;&#23569;&#19982;&#29616;&#26377;&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26222;&#36866;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SSLMem&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2401.12233</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#25552;&#39640;&#20102;&#19979;&#28216;&#27010;&#25324;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12233
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SSLMem&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22240;&#20854;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26469;&#28304;&#20110;&#20114;&#32852;&#32593;&#30340;&#25235;&#21462;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;SSL&#32534;&#30721;&#22120;&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#27844;&#38706;&#36825;&#20123;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#21270;&#30340;&#29702;&#35770;&#23450;&#20041;&#20381;&#36182;&#20110;&#26631;&#31614;&#65292;&#22240;&#27492;&#26080;&#27861;&#36866;&#29992;&#20110;SSL&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSLMem&#65292;&#19968;&#20010;&#22312;SSL&#20869;&#23450;&#20041;&#35760;&#24518;&#21270;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#36890;&#36807;&#27604;&#36739;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#28857;&#19978;&#30340;&#32534;&#30721;&#22120;&#21644;&#26410;&#34987;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#28857;&#19978;&#30340;&#32534;&#30721;&#22120;&#36820;&#22238;&#30340;&#25968;&#25454;&#28857;&#21644;&#20182;&#20204;&#30340;&#22686;&#24378;&#35270;&#22270;&#30340;&#34920;&#31034;&#30340;&#23545;&#40784;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;SSL&#20381;&#36182;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#37117;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#20316;&#20026;&#27491;&#21017;&#21270;&#25163;&#27573;&#30340;&#24050;&#30693;&#25216;&#26415;&#65292;&#35760;&#24518;&#21270;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#24494;&#22937;&#24046;&#24322;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03482</link><description>&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification on Clinical Trial Outcome Prediction. (arXiv:2401.03482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#24494;&#22937;&#24046;&#24322;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#34987;&#35748;&#35782;&#21040;&#12290;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21644;&#22686;&#21152;&#20449;&#24515;&#12290;&#36825;&#22312;&#21307;&#23398;&#35786;&#26029;&#21644;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#21487;&#38752;&#30340;&#39044;&#27979;&#30452;&#25509;&#24433;&#21709;&#30740;&#31350;&#36136;&#37327;&#21644;&#24739;&#32773;&#20581;&#24247;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32435;&#20837;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#36776;&#21035;&#24494;&#22937;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#24182;&#23558;&#20854;&#19982;&#23618;&#27425;&#20132;&#20114;&#32593;&#32476;(HINT)&#26080;&#32541;&#38598;&#25104;&#65292;HINT&#26159;&#20020;&#24202;&#35797;&#39564;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#21069;&#27839;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20445;&#30041;&#20449;&#24687;&#20197;&#20379;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of uncertainty quantification is increasingly recognized in the diverse field of machine learning. Accurately assessing model prediction uncertainty can help provide deeper understanding and confidence for researchers and practitioners. This is especially critical in medical diagnosis and drug discovery areas, where reliable predictions directly impact research quality and patient health.  In this paper, we proposed incorporating uncertainty quantification into clinical trial outcome predictions. Our main goal is to enhance the model's ability to discern nuanced differences, thereby significantly improving its overall performance.  We have adopted a selective classification approach to fulfill our objective, integrating it seamlessly with the Hierarchical Interaction Network (HINT), which is at the forefront of clinical trial prediction modeling. Selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold de
&lt;/p&gt;</description></item><item><title>&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.15591</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15591
&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#31995;&#32479;&#26102;&#20195;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#32034;&#25968;&#25454;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#65288;NGDB&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#23558;&#22270;&#25968;&#25454;&#24211;&#65288;&#22270;&#24418;&#25968;&#25454;&#24211;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#31070;&#32463;&#23884;&#20837;&#23384;&#20648;&#21644;&#22797;&#26434;&#31070;&#32463;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;&#20026;NGDB&#25552;&#20379;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#22270;&#24418;&#19981;&#23436;&#25972;&#26102;&#65292;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#28508;&#22312;&#27169;&#24335;&#21644;&#34920;&#31034;&#26469;&#22635;&#34917;&#22270;&#32467;&#26500;&#20013;&#30340;&#31354;&#32570;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#20851;&#31995;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#32452;&#21512;&#26597;&#35810;&#25512;&#26029;&#20986;&#26356;&#22810;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#27604;&#36739;&#22270;&#25968;&#25454;&#24211;&#20013;Turing&#22870;&#24471;&#20027;&#30340;&#31572;&#26696;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#25104;&#26412;&#20809;&#28304;&#21644;&#20256;&#24863;&#22120;&#36827;&#34892;&#38750;&#25509;&#35302;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;96.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#38169;&#35823;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2311.01367</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#36827;&#34892;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals. (arXiv:2311.01367v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01367
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#25104;&#26412;&#20809;&#28304;&#21644;&#20256;&#24863;&#22120;&#36827;&#34892;&#38750;&#25509;&#35302;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;96.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#38169;&#35823;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#26800;&#26426;&#22120;&#20154;&#33016;&#37096;&#21453;&#23556;&#30340;&#38750;&#30456;&#24178;&#20809;&#27874;&#20449;&#21495;&#36827;&#34892;&#38750;&#25509;&#35302;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38647;&#36798;&#21644;&#25668;&#20687;&#22836;&#24863;&#24212;&#31995;&#32479;&#30456;&#27604;&#65292;&#36825;&#39033;&#25216;&#26415;&#21482;&#20351;&#29992;&#20102;&#20302;&#25104;&#26412;&#30340;&#26222;&#36941;&#20809;&#28304;&#65288;&#22914;&#32418;&#22806;&#21457;&#20809;&#20108;&#26497;&#31649;&#65289;&#21644;&#20256;&#24863;&#22120;&#65288;&#22914;&#20809;&#30005;&#26816;&#27979;&#22120;&#65289;&#12290;&#36825;&#20010;&#20809;&#27874;&#24863;&#24212;&#65288;LWS&#65289;&#31995;&#32479;&#36890;&#36807;&#27979;&#37327;&#26426;&#22120;&#20154;&#33016;&#37096;&#21453;&#23556;&#30340;&#20809;&#24378;&#21464;&#21270;&#26469;&#35782;&#21035;&#19981;&#21516;&#30340;&#21628;&#21560;&#24322;&#24120;&#65292;&#22312;0.5&#31859;&#33267;1.5&#31859;&#33539;&#22260;&#20869;&#12290;&#35813;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33021;&#22815;&#20197;96.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20998;&#31867;7&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#25968;&#25454;&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#26816;&#27979;&#21040;&#31995;&#32479;&#25910;&#38598;&#30340;&#19981;&#21253;&#21547;&#21628;&#21560;&#20449;&#24687;&#30340;&#38169;&#35823;&#25968;&#25454;&#12290;&#24320;&#21457;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#26234;&#33021;&#12289;&#38750;&#25509;&#35302;&#21644;&#38544;&#34109;&#30340;&#21628;&#21560;&#30417;&#27979;&#26041;&#27861;&#65292;&#22312;&#23478;&#24237;&#25110;&#21307;&#30103;&#26426;&#26500;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a non-contact respiratory anomaly detection method using incoherent light-wave signals reflected from the chest of a mechanical robot that can breathe like human beings. In comparison to existing radar and camera-based sensing systems for vitals monitoring, this technology uses only a low-cost ubiquitous light source (e.g., infrared light emitting diode) and sensor (e.g., photodetector). This light-wave sensing (LWS) system recognizes different breathing anomalies from the variations of light intensity reflected from the chest of the robot within a 0.5m-1.5m range. The anomaly detection model demonstrates up to 96.6% average accuracy in classifying 7 different types of breathing data using machine learning. The model can also detect faulty data collected by the system that does not contain breathing information. The developed system can be utilized at home or healthcare facilities as a smart, non-contact and discreet respiration monitoring method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16113</link><description>&lt;p&gt;
&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compressed representation of brain genetic transcription. (arXiv:2310.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#30452;&#35266;&#22320;&#36827;&#34892;&#35266;&#23519;&#65292;&#38656;&#35201;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#23558;&#20854;&#21464;&#21270;&#25237;&#24433;&#21040;&#32039;&#20945;&#12289;&#21487;&#23548;&#33322;&#30340;&#31354;&#38388;&#20013;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22522;&#22240;&#34920;&#36798;&#65289;&#20013;&#65292;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#35299;&#21078;&#21644;&#36716;&#24405;&#27169;&#24335;&#30340;&#32852;&#21512;&#22797;&#26434;&#24615;&#35201;&#27714;&#26368;&#22823;&#21387;&#32553;&#12290;&#30446;&#21069;&#30340;&#23454;&#36341;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#20854;&#35745;&#31639;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#22312;&#22823;&#21387;&#32553;&#27604;&#19979;&#34920;&#29616;&#21147;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20840;&#33041;&#20307;&#32032;&#32423;Allen&#22823;&#33041;&#22270;&#35889;&#36716;&#24405;&#25968;&#25454;&#65292;&#31995;&#32479;&#27604;&#36739;&#20102;&#22522;&#20110;&#26368;&#24191;&#27867;&#25903;&#25345;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65288;PCA&#65292;&#26680;PCA&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#65292;t-&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;t-SNE&#65289;&#65292;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#21644;&#25237;&#24433;&#65288;UMAP&#65289;&#65292;&#28145;&#24230;&#33258;&#32534;&#30721;&#65289;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#37327;&#21270;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#35299;&#21078;&#36830;&#36143;&#24615;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11604</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20104;&#20302;&#32423;&#25216;&#33021;&#36873;&#25321;&#26102;&#33021;&#22815;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;LLMs&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#29992;&#20110;&#20302;&#32423;&#36712;&#36857;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#35843;&#26597;&#20102;&#24403;&#32473;&#20104;LLM&#65288;GPT-4&#65289;&#20165;&#33021;&#35775;&#38382;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#26102;&#65292;&#23427;&#33021;&#21542;&#30452;&#25509;&#39044;&#27979;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#29992;&#20110;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#27809;&#26377;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#12289;&#36816;&#21160;&#21407;&#35821;&#25110;&#22806;&#37096;&#36712;&#36857;&#20248;&#21270;&#22120;&#65292;&#23427;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#22914;&#8220;&#25171;&#24320;&#29942;&#30422;&#8221;&#21644;&#8220;&#29992;&#28023;&#32501;&#25830;&#25325;&#30424;&#23376;&#8221;&#65292;&#20197;&#21450;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#25552;&#31034;&#20013;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;LLaMA-2&#33021;&#22815;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36825;&#26159;&#22240;&#20026;LLMs&#20855;&#26377;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07820</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Zero-Shot Time Series Forecasters. (arXiv:2310.07820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07820
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;LLaMA-2&#33021;&#22815;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36825;&#26159;&#22240;&#20026;LLMs&#20855;&#26377;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#32534;&#30721;&#20026;&#19968;&#31995;&#21015;&#25968;&#23383;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35270;&#20026;&#25991;&#26412;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#22312;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20363;&#22914;GPT-3&#21644;LLaMA-2&#21487;&#20197;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#26631;&#35760;&#21270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24182;&#23558;&#31163;&#25955;&#20998;&#24067;&#36716;&#25442;&#20026;&#39640;&#24230;&#28789;&#27963;&#30340;&#36830;&#32493;&#20540;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25104;&#21151;&#28304;&#20110;&#23427;&#20204;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#30340;&#20559;&#35265;&#65292;&#36825;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#31561;&#26174;&#33879;&#29305;&#24449;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#36890;&#36807;&#38750;&#25968;&#23383;&#25991;&#26412;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#20197;&#21450;&#22914;&#20309;&#36866;&#24212;&#25991;&#26412;&#38468;&#21152;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04006</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#30340;&#21152;&#36895;&#26159;&#19968;&#20010;&#38750;&#24120;&#23454;&#29992;&#21644;&#29702;&#35770;&#19978;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20248;&#21270;&#19978;&#65292;&#20294;&#32771;&#34385;&#21040;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#38656;&#35201;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#23454;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Segment Anything Model (SAM)&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#21644;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20004;&#31181;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#26356;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16992</link><description>&lt;p&gt;
Segment Anything Model&#23545;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#20855;&#26377;&#33391;&#22909;&#30340;&#25945;&#23548;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model is a Good Teacher for Local Feature Learning. (arXiv:2309.16992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Segment Anything Model (SAM)&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#21644;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20004;&#31181;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#26356;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#29305;&#24449;&#30340;&#26816;&#27979;&#21644;&#25551;&#36848;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#25551;&#36848;&#8220;&#20219;&#20309;&#22330;&#26223;&#8221;&#21644;&#8220;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#8221;&#30340;&#20851;&#38190;&#28857;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#33719;&#24471;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAMFeat&#26469;&#24341;&#20837;SAM&#65288;segment anything model&#65289;&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#28608;&#21457;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#65288;PSRD&#65289;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#23558;SAM&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#35821;&#20041;&#20449;&#24687;&#36890;&#36807;&#29305;&#24449;&#20851;&#31995;&#33976;&#39311;&#21040;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#39640;&#36890;&#36807;&#35821;&#20041;&#21306;&#20998;&#25913;&#21892;&#26412;&#22320;&#29305;&#24449;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#36739;&#24378;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.00508</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. (arXiv:2309.00508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#36739;&#24378;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#32467;&#26500;&#65292;&#30830;&#23450;&#20102;&#33021;&#22815;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#30340;&#21442;&#25968;&#38598;&#65292;&#24182;&#23436;&#25972;&#25551;&#36848;&#20102;&#20854;&#21608;&#22260;&#30340;&#26799;&#24230;&#27969;&#21160;&#24577;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#19968;&#20123;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#12289;&#30446;&#26631;&#20989;&#25968;&#12289;&#26679;&#26412;&#21644;&#21021;&#22987;&#21270;&#23545;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#65288;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65289;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#36827;&#34892;&#22270;&#32858;&#31867;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#22823;&#32858;&#31867;&#65292;&#26080;&#35770;&#20854;&#20182;&#32858;&#31867;&#30340;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20013;&#31561;&#22823;&#23567;&#30340;&#32858;&#31867;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15642</link><description>&lt;p&gt;
&#26080;&#38656;&#29305;&#24449;&#38388;&#38548;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering Without an Eigengap. (arXiv:2308.15642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#36827;&#34892;&#22270;&#32858;&#31867;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#22823;&#32858;&#31867;&#65292;&#26080;&#35770;&#20854;&#20182;&#32858;&#31867;&#30340;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20013;&#31561;&#22823;&#23567;&#30340;&#32858;&#31867;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#20013;&#30740;&#31350;&#20102;&#20855;&#26377;&#22823;&#32858;&#31867;&#21644;&#23567;&#19981;&#21487;&#24674;&#22797;&#32858;&#31867;&#30340;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#20801;&#35768;&#23567;&#20110;$ o&#65288;\sqrt {n}&#65289;$&#22823;&#23567;&#30340;&#23567;&#32858;&#31867;&#65292;&#35201;&#20040;&#35201;&#27714;&#26368;&#23567;&#21487;&#24674;&#22797;&#32858;&#31867;&#21644;&#26368;&#22823;&#19981;&#21487;&#24674;&#22797;&#32858;&#31867;&#20043;&#38388;&#23384;&#22312;&#22823;&#23567;&#38388;&#38548;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#36825;&#20123;&#35201;&#27714;&#65292;&#24182;&#21487;&#20197;&#30830;&#23450;&#22320;&#24674;&#22797;&#22823;&#32858;&#31867;&#65292;&#32780;&#19981;&#32771;&#34385;&#20854;&#20182;&#32858;&#31867;&#30340;&#22823;&#23567;&#12290;&#20013;&#31561;&#22823;&#23567;&#30340;&#32858;&#31867;&#23545;&#20998;&#26512;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#25509;&#36817;&#24674;&#22797;&#38408;&#20540;&#65292;&#38750;&#24120;&#25935;&#24863;&#20110;&#23567;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#19981;&#20801;&#35768;&#38381;&#21512;&#24418;&#24335;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;leave-one-out&#39118;&#26684;&#30340;&#35770;&#35777;&#65292;&#21363;&#20351;&#21435;&#25481;&#19968;&#34892;&#22122;&#22768;&#20063;&#21487;&#33021;&#22823;&#24133;&#25913;&#21464;SDP&#35299;&#20915;&#26041;&#26696;&#65292;&#20173;&#28982;&#21487;&#20197;&#25511;&#21046;SDP&#35299;&#20915;&#26041;&#26696;&#19982;&#22122;&#22768;&#21521;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous approaches achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09638</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#38454;&#27573;&#20419;&#36827;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#65292;&#29305;&#21035;&#26159;Adam&#65292;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#23545;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#26356;&#21152;&#40065;&#26834;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#27867;&#21270;&#25928;&#26524;&#26356;&#24046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#24402;&#22240;&#20110;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#65306;&#33258;&#36866;&#24212;&#26041;&#27861;&#20542;&#21521;&#20110;&#22312;&#25439;&#22833;&#20989;&#25968;&#26354;&#38754;&#20013;&#26356;&#23574;&#38160;&#30340;&#30406;&#22320;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#20351;&#24471;&#20248;&#21270;&#22120;&#22914;&#26524;&#30406;&#22320;&#30340;&#21560;&#24341;&#33539;&#22260;&#19981;&#22815;&#23485;&#65292;&#23601;&#20250;&#36229;&#20986;&#20854;&#33539;&#22260;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;CatBoost&#12289;XGBoost&#21644;LightGBM&#19977;&#31181;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#22312;&#22788;&#29702;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;</title><link>http://arxiv.org/abs/2307.07771</link><description>&lt;p&gt;
CatBoost&#23545;&#27604;XGBoost&#21644;LightGBM&#65306;&#24320;&#21457;&#22686;&#24378;&#30340;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;CatBoost&#12289;XGBoost&#21644;LightGBM&#19977;&#31181;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#22312;&#22788;&#29702;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36130;&#20135;&#21644;&#24847;&#22806;&#20107;&#25925;&#20445;&#38505;&#34892;&#19994;&#20013;&#65292;&#30001;&#20110;&#27491;&#21521;&#29702;&#36180;&#25968;&#25454;&#20855;&#26377;&#39640;&#24230;&#21491;&#20559;&#20998;&#24067;&#21644;&#36807;&#37327;&#30340;&#38646;&#20540;&#65292;&#26500;&#24314;&#29702;&#36180;&#39044;&#27979;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#65292;&#22914;&#27850;&#26494;&#25110;&#36127;&#20108;&#39033;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(GLM)&#65292;&#32463;&#24120;&#22312;&#22788;&#29702;&#36807;&#37327;&#38646;&#20540;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#31934;&#31639;&#31185;&#23398;&#30340;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#37319;&#29992;&#20102;&#8220;&#38646;&#33192;&#32960;&#8221;&#27169;&#22411;&#65292;&#23558;&#20256;&#32479;&#35745;&#25968;&#27169;&#22411;&#21644;&#20108;&#20803;&#27169;&#22411;&#21512;&#24182;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#25552;&#21319;&#31639;&#27861;&#26469;&#22788;&#29702;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#65292;&#21253;&#25324;&#38646;&#33192;&#32960;&#30340;&#36965;&#27979;&#25968;&#25454;&#65292;&#20197;&#26500;&#24314;&#29702;&#36180;&#39057;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#19977;&#20010;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211; - XGBoost&#12289;LightGBM&#21644;CatBoost&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is sup
&lt;/p&gt;</description></item><item><title>&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#31934;&#30830;&#24230;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24182;&#22312;&#30828;&#20214;&#24863;&#30693;&#21644;&#30828;&#20214;&#26080;&#24863;&#30693;&#30340;NAS&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01998</link><description>&lt;p&gt;
&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65306;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities. (arXiv:2307.01998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01998
&lt;/p&gt;
&lt;p&gt;
&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#31934;&#30830;&#24230;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24182;&#22312;&#30828;&#20214;&#24863;&#30693;&#21644;&#30828;&#20214;&#26080;&#24863;&#30693;&#30340;NAS&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38646;&#20195;&#20215;&#65288;&#25110;&#26080;&#38656;&#35757;&#32451;&#65289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#23558;NAS&#20174;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#35299;&#25918;&#20986;&#26469;&#12290;&#38646;&#20195;&#20215;NAS&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#39044;&#27979;&#26576;&#20123;&#32473;&#23450;&#32593;&#32476;&#31934;&#30830;&#24230;&#30340;&#20195;&#29702;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#32593;&#32476;&#21442;&#25968;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24050;&#32463;&#25552;&#20986;&#30340;&#20195;&#29702;&#36890;&#24120;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#21644;NAS&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#23457;&#26597;&#21644;&#27604;&#36739;&#26368;&#20808;&#36827;&#30340;&#38646;&#20195;&#20215;NAS&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#30828;&#20214;&#30340;&#24847;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20027;&#27969;&#30340;&#38646;&#20195;&#20215;&#20195;&#29702;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#27604;&#36739;&#36825;&#20123;&#38646;&#20195;&#20215;&#20195;&#29702;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#30828;&#20214;&#24863;&#30693;&#21644;&#30828;&#20214;&#26080;&#24863;&#30693;&#30340;NAS&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#22909;&#30340;&#20195;&#29702;&#30340;&#20960;&#20010;&#26377;&#21069;&#36884;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate NAS from the expensive training process. The key idea behind zero-shot NAS approaches is to design proxies that can predict the accuracy of some given networks without training the network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical understanding of deep learning and have shown great potential on several datasets and NAS benchmarks. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#22914;&#20309;&#20197;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#21644;&#25277;&#26679;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#34987;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2306.15083</link><description>&lt;p&gt;
&#24179;&#34913;&#36807;&#28388;&#20351;&#29992;&#38750;&#27844;&#38706;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Balanced Filtering via Non-Disclosive Proxies. (arXiv:2306.15083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#22914;&#20309;&#20197;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#21644;&#25277;&#26679;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#34987;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#22312;&#25910;&#38598;&#26102;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#19982;&#25935;&#24863;&#32676;&#32452;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25910;&#38598;&#26426;&#21046;&#19981;&#20250;&#27604;&#22522;&#26412;&#27604;&#29575;&#33021;&#22815;&#30830;&#23450;&#30340;&#20219;&#20309;&#20010;&#20307;&#26679;&#26412;&#30340;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#26356;&#22810;&#22320;&#36879;&#38706;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20844;&#24179;&#27969;&#31243;&#30340;&#35266;&#28857;&#65292;&#21363;&#23398;&#20064;&#32773;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20195;&#29702;&#20989;&#25968;&#65292;&#36825;&#20010;&#20195;&#29702;&#20989;&#25968;&#20197;&#21518;&#21487;&#20197;&#29992;&#20110;&#36825;&#20010;&#36807;&#28388;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#20989;&#25968;&#30340;&#33539;&#22260;&#19982;&#25277;&#26679;&#27010;&#29575;&#30456;&#20851;&#32852;&#65307;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#20505;&#36873;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#26681;&#25454;&#19982;&#20854;&#20195;&#29702;&#20998;&#31867;&#23545;&#24212;&#30340;&#25277;&#26679;&#27010;&#29575;&#36873;&#25321;&#23427;&#20316;&#20026;&#25105;&#20204;&#30340;&#26679;&#26412;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35201;&#27714;&#20195;&#29702;&#20998;&#31867;&#26412;&#36523;&#19981;&#20250;&#36879;&#38706;&#20219;&#20309;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#30340;&#37325;&#35201;&#20449;&#24687;&#65288;&#21363;&#65292;&#23427;&#24212;&#35813;&#26159;&#38750;&#27844;&#38706;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of non-disclosively collecting a sample of data that is balanced with respect to sensitive groups when group membership is unavailable or prohibited from use at collection time. Specifically, our collection mechanism does not reveal significantly more about group membership of any individual sample than can be ascertained from base rates alone. To do this, we adopt a fairness pipeline perspective, in which a learner can use a small set of labeled data to train a proxy function that can later be used for this filtering task. We then associate the range of the proxy function with sampling probabilities; given a new candidate, we classify it using our proxy function, and then select it for our sample with probability proportional to the sampling probability corresponding to its proxy classification. Importantly, we require that the proxy classification itself not reveal significant information about the sensitive group membership of any individual sample (i.e., it sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#65292;State-wise Constrained Policy Optimization (SCPO)&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#29366;&#24577;&#38480;&#21046;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26399;&#26395;&#29366;&#24577;&#32422;&#26463;&#20445;&#35777;&#21644;&#26368;&#22351;&#23433;&#20840;&#36829;&#21453;&#30340;&#26377;&#30028;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12594</link><description>&lt;p&gt;
&#32422;&#26463;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#65306;State-wise Constrained Policy Optimization
&lt;/p&gt;
&lt;p&gt;
State-wise Constrained Policy Optimization. (arXiv:2306.12594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#65292;State-wise Constrained Policy Optimization (SCPO)&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#29366;&#24577;&#38480;&#21046;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26399;&#26395;&#29366;&#24577;&#32422;&#26463;&#20445;&#35777;&#21644;&#26368;&#22351;&#23433;&#20840;&#36829;&#21453;&#30340;&#26377;&#30028;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#24212;&#29992;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#20854;&#20013;&#23433;&#20840;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#24378;&#21046;&#25191;&#34892;&#29366;&#24577;&#38480;&#21046;&#26159;&#21313;&#20998;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#26694;&#26550;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#29366;&#24577;&#32422;&#26463;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;State-wise Constrained Policy Optimization&#65288;SCPO&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#22788;&#29702;&#29366;&#24577;&#38480;&#21046;&#30340;&#36890;&#29992;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#12290;SCPO&#33021;&#22815;&#22312;&#26399;&#26395;&#19978;&#20445;&#35777;&#29366;&#24577;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;SCPO&#19979;&#26368;&#22351;&#30340;&#23433;&#20840;&#36829;&#21453;&#26159;&#26377;&#30028;&#30340;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#26102;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.08041</link><description>&lt;p&gt;
&#20851;&#20110;&#20266;&#36896;&#32435;&#20160;&#22343;&#34913;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#26356;&#25913;&#25968;&#25454;&#38598;&#20197;&#23433;&#35013;&#65288;&#28508;&#22312;&#34394;&#20551;&#30340;&#65289;&#21807;&#19968;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#32435;&#20160;&#22343;&#34913;&#28857;(Nash equilibrium)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#21363;&#30001;&#20854;Q&#20989;&#25968;&#35268;&#23450;&#30340;&#28216;&#25103;&#30340;&#38598;&#21512;&#65292;&#20854;&#20855;&#26377;&#21807;&#19968;&#30340;&#32852;&#21512;&#31574;&#30053;&#20316;&#20026;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23545;&#20110;&#27745;&#26579;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#25968;&#25454;&#27745;&#26579;&#20351;&#25152;&#26377;&#21512;&#29702;&#30340;&#28216;&#25103;&#37117;&#22312;&#20854;&#20013;&#26102;&#65292;&#25915;&#20987;&#25165;&#25104;&#21151;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23558;&#24120;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#22810;&#38754;&#20307;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#23545;&#20110;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#36870;&#32435;&#20160;&#38598;&#20197;&#21450;&#30001;&#25968;&#25454;&#24341;&#36215;&#30340;&#21512;&#29702;&#28216;&#25103;&#38598;&#37117;&#26159;Q&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#22810;&#38754;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26368;&#20248;&#30340;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#24517;&#35201;&#30340;&#27493;&#39588;&#25581;&#31034;&#20102;&#31163;&#32447;MARL&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#32467;&#26500;&#30340;&#19968;&#20123;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
&lt;/p&gt;</description></item><item><title>GPT-FL&#26159;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#32467;&#21512;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#31561;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;FL&#35757;&#32451;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02210</link><description>&lt;p&gt;
GPT-FL: &#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02210
&lt;/p&gt;
&lt;p&gt;
GPT-FL&#26159;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#32467;&#21512;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#31561;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;FL&#35757;&#32451;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-FL&#65292;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#12290;GPT-FL&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#25968;&#25454;&#29992;&#20110;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#19979;&#28216;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#26631;&#20934;FL&#26694;&#26550;&#19979;&#20351;&#29992;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-FL&#22312;&#27169;&#22411;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#28040;&#34701;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;FL&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;GPT-FL&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#25552;&#21319;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#30446;&#26631;&#25968;&#25454;&#26159;&#21542;&#22312;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#39046;&#22495;&#20869;&#25110;&#22806;&#65292;GPT-FL&#22987;&#32456;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;&#65292;&#24182;&#35777;&#26126;&#20102;Barron&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#31867;&#20284;&#20110;Sobolev&#31354;&#38388;$H^m$&#12290;&#20854;&#20013;&#65292;&#20462;&#27491;&#21151;&#29575;&#21333;&#20301;&#28608;&#27963;&#20989;&#25968;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#29305;&#21035;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.15839</link><description>&lt;p&gt;
&#20855;&#26377;&#39640;&#38454;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Embeddings between Barron spaces with higher order activation functions. (arXiv:2305.15839v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;&#65292;&#24182;&#35777;&#26126;&#20102;Barron&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#31867;&#20284;&#20110;Sobolev&#31354;&#38388;$H^m$&#12290;&#20854;&#20013;&#65292;&#20462;&#27491;&#21151;&#29575;&#21333;&#20301;&#28608;&#27963;&#20989;&#25968;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#29305;&#21035;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#23485;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#24615;&#36136;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#34920;&#31034;&#20989;&#25968;$f$&#30340;&#27979;&#37327;$\mu$&#19978;&#30340;&#25512;&#36827;&#26144;&#23556;&#26469;&#35777;&#26126;&#36825;&#20123;&#23884;&#20837;&#12290;&#19968;&#31181;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#32473;&#23450;&#20026;$\operatorname{RePU}_s(x)=\max(0,x)^s$&#30340;&#20462;&#27491;&#21151;&#29575;&#21333;&#20301;($\operatorname{RePU}$)&#12290;&#23545;&#20110;&#35768;&#22810;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#27888;&#21202;&#20313;&#39033;&#23450;&#29702;&#26500;&#36896;&#25512;&#36827;&#26144;&#23556;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#30456;&#20851;Barron&#31354;&#38388;&#23884;&#20837;&#21040;&#20855;&#26377;$\operatorname{RePU}$&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;Barron&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#19982;$\operatorname{RePU}_s$&#30456;&#20851;&#30340;Barron&#31354;&#38388;&#20855;&#26377;&#31867;&#20284;&#20110;Sobolev&#31354;&#38388;$H^m$&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The approximation properties of infinitely wide shallow neural networks heavily depend on the choice of the activation function. To understand this influence, we study embeddings between Barron spaces with different activation functions. These embeddings are proven by providing push-forward maps on the measures $\mu$ used to represent functions $f$. An activation function of particular interest is the rectified power unit ($\operatorname{RePU}$) given by $\operatorname{RePU}_s(x)=\max(0,x)^s$. For many commonly used activation functions, the well-known Taylor remainder theorem can be used to construct a push-forward map, which allows us to prove the embedding of the associated Barron space into a Barron space with a $\operatorname{RePU}$ as activation function. Moreover, the Barron spaces associated with the $\operatorname{RePU}_s$ have a hierarchical structure similar to the Sobolev spaces $H^m$.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#35813;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#24230;&#37327;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.15016</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#24182;&#24212;&#29992;&#21040;LLMs&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning. (arXiv:2305.15016v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15016
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#35813;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#24230;&#37327;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#23545;&#20960;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20272;&#35745;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#19982;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#30340;&#30417;&#30563;&#24230;&#37327;&#65288;&#22914;Fisher&#21028;&#21035;&#27604;&#29575;&#65288;FDR&#65289;&#21644;&#20998;&#31867;&#22120;&#30340;&#20132;&#21449;&#39564;&#35777;&#65289;&#20043;&#38388;&#23384;&#22312;&#28165;&#26224;&#30340;&#30456;&#20851;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#20174;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#24403;&#25105;&#20204;&#26377;&#38480;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#21644;&#30456;&#23545;&#36739;&#22823;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#26102;&#65292;&#36825;&#23558;&#29305;&#21035;&#26377;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#30417;&#27979;&#23884;&#20837;&#31354;&#38388;&#27969;&#24418;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised method that leverages topological characteristics of data manifolds to estimate class separability of the data without requiring labels. Experiments conducted in this paper on several datasets demonstrate a clear correlation and consistency between the class separability estimated by the proposed method with supervised metrics like Fisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which both require labels. This can enable implementing learning paradigms aimed at learning from both labeled and unlabeled data, like semi-supervised and transductive learning. This would be particularly useful when we have limited labeled data and a relatively large unlabeled dataset that can be used to enhance the learning process. The proposed method is implemented for language model fine-tuning with automated stopping criterion by monitoring class separability of the embedding-space manifold in an unsupervised setting. The proposed methodology has 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26041;&#38754;&#36827;&#34892;&#20102;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09605</link><description>&lt;p&gt;
&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expressiveness Remarks for Denoising Diffusion Models and Samplers. (arXiv:2305.09605v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26041;&#38754;&#36827;&#34892;&#20102;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#26368;&#36817;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#28459;&#25193;&#36807;&#31243;&#36880;&#28176;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#23558;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#27169;&#25311;&#35813;&#28459;&#25193;&#30340;&#26102;&#38388;&#21453;&#28436;&#30340;&#36924;&#36817;&#26469;&#33719;&#21462;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21018;&#24320;&#22987;&#36825;&#20010;&#28459;&#25193;&#27169;&#25311;&#30340;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#28459;&#25193;&#27169;&#22411;&#36866;&#24212;&#20110;&#37319;&#26679;&#21644;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#19982;F\"ollmer&#28418;&#31227;&#31867;&#20284;&#30340;&#38543;&#26426;&#25511;&#21046;&#32852;&#31995;&#65292;&#23558;&#38024;&#23545;F\"ollmer&#28418;&#31227;&#30340;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the F\"ollmer drift to extend established neural network approximation results for the F\"ollmer drift to denoising diffusion models and samplers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01311</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30693;&#35782;&#22270;&#35889;&#29992;&#25143;&#12289;&#25361;&#25112;&#21644;&#21487;&#35270;&#21270;&#38656;&#27714;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;19&#20301;&#26469;&#33258;&#20225;&#19994;&#21644;&#23398;&#26415;&#29615;&#22659;&#19979;&#12289;&#28041;&#21450;&#21508;&#31181;&#29992;&#20363;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#30340;&#35775;&#35848;&#65292;&#25552;&#20986;&#20102;KG&#23454;&#36341;&#32773;&#22312;&#21019;&#24314;&#12289;&#25506;&#32034;&#21644;&#20998;&#26512;KG&#26102;&#36935;&#21040;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#21487;&#35270;&#21270;&#35774;&#35745;&#26469;&#32531;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;KG&#23454;&#36341;&#32773;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;KG&#26500;&#24314;&#32773;&#12289;&#20998;&#26512;&#24072;&#21644;&#28040;&#36153;&#32773;&#65292;&#27599;&#20010;&#20154;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#38656;&#27714;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;KG&#26500;&#24314;&#32773;&#21487;&#20197;&#20174;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#20013;&#33719;&#30410;&#65292;&#32780;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#25552;&#20379;&#20013;&#38388;&#26597;&#35810;&#32467;&#26524;&#30340;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#12290;&#23545;&#20110;KG&#28040;&#36153;&#32773;&#65292;&#25105;&#20204;&#30830;&#23450;&#33410;&#28857;&#38142;&#25509;&#22270;&#30340;&#25928;&#21147;&#19981;&#36275;&#65292;&#24182;&#38656;&#35201;&#23450;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#26469;&#20419;&#36827;KG&#30340;&#37319;&#29992;&#21644;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#22320;&#23454;&#26045;KG&#38656;&#35201;&#19981;&#20165;&#25216;&#26415;&#19978;&#30340;&#65292;&#36824;&#26377;&#31038;&#20132;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30446;&#21069;&#24182;&#26410;&#34987;&#24403;&#21069;&#30340;&#24037;&#20855;&#12289;&#25216;&#26415;&#21644;&#26368;&#20339;&#23454;&#36341;&#25152;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22266;&#23450;&#35774;&#35745;&#19979;&#30340;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#31639;&#27861;&#22312;&#36951;&#24536;&#21644;&#19981;&#22949;&#21327;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#21487;&#33021;&#20250;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10263</link><description>&lt;p&gt;
&#22266;&#23450;&#35774;&#35745;&#19979;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Fixed Design Analysis of Regularization-Based Continual Learning. (arXiv:2303.10263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22266;&#23450;&#35774;&#35745;&#19979;&#30340;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#31639;&#27861;&#22312;&#36951;&#24536;&#21644;&#19981;&#22949;&#21327;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#21487;&#33021;&#20250;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#20004;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#65292;&#29305;&#24449;&#21521;&#37327;&#34987;&#20551;&#23450;&#20026;&#22266;&#23450;&#30340;&#65292;&#26631;&#31614;&#34987;&#20551;&#23450;&#20026;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;$\ell_2$-&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#35745;&#31639;&#19968;&#20010;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#21442;&#25968;&#26469;&#25311;&#21512;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#35745;&#31639;&#21478;&#19968;&#20010;&#21442;&#25968;&#65292;&#22312;$\ell_2$-&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#65292;&#25311;&#21512;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#24182;&#36755;&#20986;&#31532;&#20108;&#20010;&#21442;&#25968;&#12290;&#23545;&#20110;&#36825;&#20010;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#24179;&#22343;&#39118;&#38505;&#30340;&#20005;&#26684;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#39118;&#38505;&#30028;&#38480;&#25581;&#31034;&#20102;$\ell_2$-&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20013;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#36951;&#24536;&#21644;&#19981;&#22949;&#21327;&#30340;&#26435;&#34913;&#20851;&#31995;&#65306;&#20351;&#29992;&#22823;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#31639;&#27861;&#36755;&#20986;&#27604;&#36739;&#19981;&#20250;&#36951;&#24536;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#24895;&#20174;&#31532;&#20108;&#20010;&#20219;&#21153;&#20013;&#25552;&#21462;&#26032;&#20449;&#24687;&#65307;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#19981;&#30456;&#20284;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#21487;&#33021;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a continual learning (CL) problem with two linear regression tasks in the fixed design setting, where the feature vectors are assumed fixed and the labels are assumed to be random variables. We consider an $\ell_2$-regularized CL algorithm, which computes an Ordinary Least Squares parameter to fit the first dataset, then computes another parameter that fits the second dataset under an $\ell_2$-regularization penalizing its deviation from the first parameter, and outputs the second parameter. For this algorithm, we provide tight bounds on the average risk over the two tasks. Our risk bounds reveal a provable trade-off between forgetting and intransigence of the $\ell_2$-regularized CL algorithm: with a large regularization parameter, the algorithm output forgets less information about the first task but is intransigent to extract new information from the second task; and vice versa. Our results suggest that catastrophic forgetting could happen for CL with dissimilar tasks (u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#29615;&#38754;&#19978;&#24314;&#31435;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#30340;&#26680;&#30456;&#20851;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#23450;&#20041;&#22312;&#36229;&#29615;&#38754;&#19978;&#30340;&#21521;&#37327;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992; HvM-based GP &#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36882;&#24402;&#23450;&#20301;&#65292;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36319;&#36394;&#31934;&#24230;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.06799</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#21521;&#27969;&#24418;&#31215;&#30340;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process on the Product of Directional Manifolds. (arXiv:2303.06799v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#29615;&#38754;&#19978;&#24314;&#31435;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#30340;&#26680;&#30456;&#20851;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#23450;&#20041;&#22312;&#36229;&#29615;&#38754;&#19978;&#30340;&#21521;&#37327;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992; HvM-based GP &#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36882;&#24402;&#23450;&#20301;&#65292;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36319;&#36394;&#31934;&#24230;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26041;&#21521;&#27969;&#24418;&#31215;&#19978;&#24314;&#31435;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110; von Mises &#20998;&#24067;&#30340;&#24490;&#29615;&#26680;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#36229;&#29615;&#32500; von Mises&#65288;HvM&#65289;&#26680;&#65292;&#20197;&#32771;&#34385;&#24490;&#29615;&#20851;&#32852;&#32452;&#20214;&#26469;&#24314;&#31435;&#36229;&#29615;&#38754;&#19978;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#22312;&#30340;&#26680;&#30456;&#20851;&#27169;&#22411;&#65292;&#36816;&#29992;&#22810;&#36755;&#20986; GP &#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#29992;&#20110;&#23450;&#20041;&#22312;&#36229;&#29615;&#38754;&#19978;&#30340;&#21521;&#37327;&#20540;&#20989;&#25968;&#12290;&#20026;&#36816;&#34892;&#26102;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#20998;&#26512;&#23548;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992; HvM-based GP &#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36882;&#24402;&#23450;&#20301;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21442;&#25968;&#27169;&#22411;&#21644;&#22522;&#20110;&#20256;&#32479;&#26680;&#35774;&#35745;&#30340;&#39640;&#26031;&#36807;&#31243;&#30456;&#27604;&#65292;HvM-based GP &#23454;&#29616;&#20102;&#26356;&#20248;&#30340;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a principled study on establishing Gaussian processes (GPs) with inputs on the product of directional manifolds. A circular kernel is first presented according to the von Mises distribution. Based thereon, the so-called hypertoroidal von Mises (HvM) kernel is proposed to establish GPs on hypertori with consideration of correlational circular components. The proposed HvM kernel is demonstrated with multi-output GP regression for learning vector-valued functions defined on hypertori using the intrinsic coregionalization model. Analytical derivatives in hyperparameter optimization are provided for runtime-critical applications. For evaluation, we synthesize a ranging-based sensor network and employ the HvM-based GPs for data-driven recursive localization. The numerical results show that the HvM-based GP achieves superior tracking accuracy compared to parametric model and GPs based on conventional kernel designs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#23545;&#22810;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05342</link><description>&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#21512;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Joint Representations for Reinforcement Learning with Multiple Sensors. (arXiv:2302.05342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#23545;&#22810;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#22320;&#32467;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#65292;&#22914;&#26426;&#22120;&#20154;&#26412;&#20307;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#26412;&#20307;&#24863;&#30693;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#31639;&#27861;&#32858;&#28966;&#20110;&#30456;&#20851;&#26041;&#38754;&#65292;&#24182;&#25351;&#23548;&#20854;&#23547;&#25214;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20174;&#22810;&#20010;&#20256;&#24863;&#22120;&#20013;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32852;&#21512;&#34920;&#31034;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27599;&#20010;&#27169;&#24577;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#26080;&#27169;&#22411;&#21644;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21253;&#21547;&#20998;&#25955;&#30340;&#35270;&#35273;&#20449;&#24687;&#25110;&#32570;&#23569;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining inputs from multiple sensor modalities effectively in reinforcement learning (RL) is an open problem. While many self-supervised representation learning approaches exist to improve performance and sample complexity for image-based RL, they usually neglect other available information, such as robot proprioception. However, using this proprioception for representation learning can help algorithms to focus on relevant aspects and guide them toward finding better representations. In this work, we systematically analyze representation learning for RL from multiple sensors by building on Recurrent State Space Models. We propose a combination of reconstruction-based and contrastive losses, which allows us to choose the most appropriate method for each sensor modality. We demonstrate the benefits of joint representations, particularly with distinct loss functions for each modality, for model-free and model-based RL on complex tasks. Those include tasks where the images contain distra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#26500;&#24314;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;PCL&#21487;&#20197;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;DFPV&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.03907</link><description>&lt;p&gt;
&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#21450;&#20854;&#22312;&#28151;&#28102;&#36172;&#21338;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#26500;&#24314;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;PCL&#21487;&#20197;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;DFPV&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26159;&#19968;&#31181;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#21033;&#29992;&#20195;&#29702;&#65288;&#32467;&#26500;&#21270;&#20391;&#38754;&#20449;&#24687;&#65289;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#23454;&#29616;&#30340;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#27169;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#26469;&#23398;&#20064;&#22312;&#32473;&#23450;&#20195;&#29702;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#19979;&#65292;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;PCL&#22312;&#21487;&#35782;&#21035;&#26465;&#20214;&#19979;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PCL&#26041;&#27861;&#65292;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#65292;&#20197;&#35299;&#20915;&#20195;&#29702;&#12289;&#27835;&#30103;&#21644;&#32467;&#26524;&#20026;&#39640;&#32500;&#19988;&#20855;&#26377;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#34920;&#26126;DFPV&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#65292;&#21253;&#25324;&#28041;&#21450;&#39640;&#32500;&#22270;&#20687;&#25968;&#25454;&#30340;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;PCL&#30340;&#24212;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be app
&lt;/p&gt;</description></item></channel></rss>