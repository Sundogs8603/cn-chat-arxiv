<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;</title><link>https://arxiv.org/abs/2403.13748</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#22240;&#23376;&#21270;&#39640;&#26031;&#36817;&#20284;&#30340;&#24046;&#24322;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13748
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20998;&#24067;$p$&#65292;&#38382;&#39064;&#26159;&#20174;&#19968;&#20123;&#26356;&#26131;&#22788;&#29702;&#30340;&#26063;$\mathcal{Q}$&#20013;&#35745;&#31639;&#26368;&#20339;&#36817;&#20284;$q$&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;Kullback-Leibler (KL)&#25955;&#24230;&#26469;&#25214;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20854;&#20182;&#26377;&#25928;&#30340;&#25955;&#24230;&#36873;&#25321;&#65292;&#24403;$\mathcal{Q}$&#19981;&#21253;&#21547;$p$&#26102;&#65292;&#27599;&#20010;&#25955;&#24230;&#37117;&#25903;&#25345;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39640;&#26031;&#30340;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#34987;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36817;&#20284;&#25152;&#24433;&#21709;&#30340;VI&#32467;&#26524;&#20013;&#65292;&#25955;&#24230;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;VI&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#22914;&#26041;&#24046;&#12289;&#31934;&#24230;&#21644;&#29109;&#65292;&#36827;&#34892;\textit{&#25490;&#24207;}&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#23450;&#29702;&#65292;&#34920;&#26126;&#26080;&#27861;&#36890;&#36807;&#22240;&#23376;&#21270;&#36817;&#20284;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;&#65307;&#22240;&#27492;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13748v1 Announce Type: cross  Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to compute the best approximation $q$ from some more tractable family $\mathcal{Q}$. Most commonly the approximation is found by minimizing a Kullback-Leibler (KL) divergence. However, there exist other valid choices of divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence champions a different solution. We analyze how the choice of divergence affects the outcome of VI when a Gaussian with a dense covariance matrix is approximated by a Gaussian with a diagonal covariance matrix. In this setting we show that different divergences can be \textit{ordered} by the amount that their variational approximations misestimate various measures of uncertainty, such as the variance, precision, and entropy. We also derive an impossibility theorem showing that no two of these measures can be simultaneously matched by a factorized approximation; henc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13502</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#39033;&#32508;&#21512;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#65288;ACS&#65289;&#20013;&#22686;&#24378;&#20102;&#24037;&#19994;&#36807;&#31243;&#31649;&#29702;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#39033;&#38480;&#21046;&#24037;&#19994;&#26222;&#36941;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Tennessee Eastman&#36807;&#31243;&#25968;&#25454;&#38598;&#22312;ACS&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#26102;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#20845;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#25506;&#35752;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24378;&#22823;&#33030;&#24369;&#24615;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#30340;&#19981;&#21516;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#30830;&#20445;&#24037;&#19994;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#65292;&#30830;&#20445;&#20102;&#24037;&#19994;&#20013;&#31283;&#20581;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13502v1 Announce Type: new  Abstract: Integrating machine learning into Automated Control Systems (ACS) enhances decision-making in industrial process management. One of the limitations to the widespread adoption of these technologies in industry is the vulnerability of neural networks to adversarial attacks. This study explores the threats in deploying deep learning models for fault diagnosis in ACS using the Tennessee Eastman Process dataset. By evaluating three neural networks with different architectures, we subject them to six types of adversarial attacks and explore five different defense methods. Our results highlight the strong vulnerability of models to adversarial samples and the varying effectiveness of defense strategies. We also propose a novel protection approach by combining multiple defense methods and demonstrate it's efficacy. This research contributes several insights into securing machine learning within ACS, ensuring robust fault diagnosis in industrial 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07294</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Graph Data Condensation via Self-expressive Graph Structure Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#22270;&#25968;&#25454;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#22312;&#35757;&#32451;&#38454;&#27573;&#20943;&#36731;&#23384;&#20648;&#21644;&#26102;&#38388;&#25104;&#26412;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#23558;&#21407;&#22987;&#22823;&#35268;&#27169;&#22270;&#21387;&#32553;&#20026;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#35757;&#32451;&#19979;&#28216;GNN&#25152;&#38656;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#20165;&#20248;&#21270;&#33410;&#28857;&#29305;&#24449;&#65292;&#35201;&#20040;&#21162;&#21147;&#29420;&#31435;&#23398;&#20064;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#22120;&#12290;&#23427;&#20204;&#26080;&#27861;&#26126;&#30830;&#21033;&#29992;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#24182;&#26410;&#33021;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31361;&#20986;&#20043;&#22788;&#22312;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04696</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20135;&#29983;&#38169;&#35823;&#30340;&#22768;&#26126;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#31181;&#24187;&#35273;&#21487;&#33021;&#24456;&#21361;&#38505;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20598;&#23572;&#20986;&#29616;&#30340;&#20107;&#23454;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#34987;&#25972;&#20307;&#19978;&#26159;&#20107;&#23454;&#30340;&#25991;&#26412;&#25513;&#30422;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#26497;&#20854;&#38590;&#20197;&#21457;&#29616;&#12290;&#21033;&#29992;LLMs&#30340;&#24403;&#21069;&#26381;&#21153;&#36890;&#24120;&#19981;&#25552;&#20379;&#26816;&#27979;&#19981;&#21487;&#38752;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#12290;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#25110;&#20854;&#23618;&#36755;&#20986;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26469;&#26816;&#27979;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#24182;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26680;&#26597;LLM&#36755;&#20986;&#20013;&#30340;&#21508;&#31181;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#20107;&#23454;&#25552;&#20986;&#24576;&#30097;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15555</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24635;&#26159;&#29702;&#35299;&#24182;&#19988;&#36825;&#23601;&#26159;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Deep Networks Always Grok and Here is Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15555
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#65292;&#25110;&#32773;&#24310;&#36831;&#27867;&#21270;&#65292;&#26159;&#25351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36798;&#21040;&#25509;&#36817;&#20110;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#21518;&#24456;&#38271;&#26102;&#38388;&#20869;&#25165;&#21457;&#29983;&#27867;&#21270;&#30340;&#29616;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#29305;&#23450;&#21463;&#25511;&#29615;&#22659;&#20013;&#20986;&#29616;grokking&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#20351;&#29992;&#22823;&#33539;&#25968;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;DNN&#25110;&#32773;&#22312;&#31639;&#27861;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;transformers&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;grokking&#23454;&#38469;&#19978;&#26356;&#21152;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#21576;&#29616;&#65292;&#20363;&#22914;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#32773;&#22312;Imagenette&#19978;&#35757;&#32451;&#30340;Resnet&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;&#65292;&#21363;DNN&#22312;&#25554;&#20540;&#21644;/&#25110;&#27867;&#21270;&#20043;&#21518;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#29702;&#35299;&#24182;&#21464;&#24471;&#40065;&#26834;&#12290;&#25105;&#20204;&#38024;&#23545;DNN&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#25552;&#20986;&#20102;&#20986;&#29616;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#34913;&#37327;&#20102;DNN&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;</title><link>https://arxiv.org/abs/2402.15478</link><description>&lt;p&gt;
Transformer&#26159;&#34920;&#29616;&#21147;&#24378;&#22823;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#26469;&#35828;&#34920;&#29616;&#21147;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformers are Expressive, But Are They Expressive Enough for Regression?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15478
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#25351;&#30340;&#26159;&#23427;&#33021;&#22815;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#12290;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#23436;&#20840;&#34920;&#29616;&#21147;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#20805;&#24403;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#19982;&#29616;&#26377;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#22312;&#21487;&#38752;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20381;&#36182;&#20110;&#20855;&#26377;&#21487;&#35266;&#21306;&#38388;&#30340;&#20998;&#27573;&#24120;&#25968;&#36924;&#36817;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#8220;Transformer&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#36890;&#36807;&#23454;&#39564;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#8230;&#8230;&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#20110;&#26368;&#23567;&#28145;&#24230;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26368;&#23567;&#28145;&#24230;&#19982;CPWL&#20989;&#25968;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15315</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#23567;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Minimal Depth in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#20110;&#26368;&#23567;&#28145;&#24230;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26368;&#23567;&#28145;&#24230;&#19982;CPWL&#20989;&#25968;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;ReLU&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#19982;&#34920;&#31034;&#20219;&#20309;&#36830;&#32493;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65288;CPWL&#65289;&#25152;&#38656;&#30340;&#26368;&#23567;&#28145;&#24230;&#30456;&#20851;&#30340;&#29468;&#24819;&#30340;&#20851;&#31995;&#36827;&#34892;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#29305;&#24615;&#12290;&#30740;&#31350;&#37325;&#28857;&#21253;&#25324;&#23545;&#27714;&#21644;&#21644;&#26368;&#22823;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#34920;&#31034;&#65292;&#20197;&#21450;&#23545;&#22810;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#27714;&#21644;&#36816;&#31639;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#25805;&#20316;&#25968;&#26368;&#23567;&#28145;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#20197;&#25214;&#21040;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#12290;&#30456;&#21453;&#65292;&#20851;&#20110;&#26368;&#22823;&#36816;&#31639;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#20165;&#20381;&#36182;&#20110;&#25805;&#20316;&#25968;&#28145;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19981;&#20250;&#26263;&#31034;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#12290;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;&#20984;CPWL&#20989;&#25968;&#20043;&#38388;&#30340;&#26368;&#23567;&#28145;&#24230;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15315v1 Announce Type: new  Abstract: A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence. This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL). The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks. For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation. The study also examine the minimal depth relationship between convex CPWL functions. On polytope neural ne
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#21487;&#33021;&#20250;&#23548;&#33268;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#20063;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15018</link><description>&lt;p&gt;
LLM&#23545;&#20840;&#29699;&#34920;&#31034;&#30340;&#24847;&#22806;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unintended Impacts of LLM Alignment on Global Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15018
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#21487;&#33021;&#20250;&#23548;&#33268;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#20063;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#38754;&#21521;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#20043;&#21069;&#65292;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#21508;&#31181;&#31243;&#24207;&#65288;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;&#30446;&#21069;&#23545;&#36825;&#20123;&#31243;&#24207;&#30340;&#35780;&#20272;&#20391;&#37325;&#20110;&#36981;&#24490;&#25351;&#23548;&#12289;&#25512;&#29702;&#21644;&#30495;&#23454;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#24182;&#38750;&#26222;&#36941;&#65292;&#23545;&#29305;&#23450;&#20559;&#22909;&#38598;&#36827;&#34892;&#23545;&#40784;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#22806;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#19977;&#20010;&#20840;&#29699;&#34920;&#31034;&#32500;&#24230;&#65306;&#33521;&#35821;&#26041;&#35328;&#12289;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20840;&#29699;&#21508;&#22269;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#31243;&#24207;&#22312;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#20135;&#29983;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#40784;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23548;&#33268;&#36825;&#20123;&#24847;&#22806;&#24433;&#21709;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#20026;&#26356;&#20844;&#24179;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14865</link><description>&lt;p&gt;
DyVal 2: &#20803;&#25506;&#27979;&#20195;&#29702;&#21160;&#24577;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#35774;&#35745;&#20102;&#20351;&#29992;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#31639;&#27861;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;&#21327;&#35758;&#26080;&#27861;&#36731;&#26494;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21482;&#33021;&#25552;&#20379;&#25972;&#20307;&#22522;&#20934;&#32467;&#26524;&#65292;&#19981;&#33021;&#25903;&#25345;&#23545;LLMs&#33021;&#21147;&#36827;&#34892;&#32454;&#31890;&#24230;&#21644;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#21551;&#21457;&#30340;&#36890;&#29992;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290; MPA &#26159; DyVal 2 &#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340; DyVal&#12290; MPA &#35774;&#35745;&#20102;&#25506;&#27979;&#21644;&#35780;&#21028;&#20195;&#29702;&#65292;&#20197;&#33258;&#21160;&#23558;&#21407;&#22987;&#35780;&#20272;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36981;&#24490;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#22312;&#19977;&#20010;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#19978;&#30340;&#24212;&#29992;: &#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31283;&#23450;&#20102;&#38543;&#26426;&#38797;&#28857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#19981;&#26029;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.13903</link><description>&lt;p&gt;
&#22788;&#29702;&#38543;&#26426;&#38797;&#28857;&#20248;&#21270;&#20013;&#30340;&#26080;&#30028;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dealing with unbounded gradients in stochastic saddle-point optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31283;&#23450;&#20102;&#38543;&#26426;&#38797;&#28857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#19981;&#26029;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#23547;&#25214;&#20984;&#20985;&#20989;&#25968;&#38797;&#28857;&#30340;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#31867;&#26041;&#27861;&#38754;&#20020;&#30340;&#19968;&#20010;&#20030;&#19990;&#38395;&#21517;&#30340;&#25361;&#25112;&#26159;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26799;&#24230;&#21487;&#33021;&#20250;&#20219;&#24847;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#21644;&#21457;&#25955;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31283;&#23450;&#20102;&#36845;&#20195;&#24182;&#20135;&#29983;&#20102;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#21363;&#20351;&#23450;&#20041;&#22495;&#21644;&#26799;&#24230;&#22122;&#22768;&#38543;&#36845;&#20195;&#30340;&#35268;&#27169;&#32447;&#24615;&#21464;&#21270;&#65288;&#22240;&#27492;&#21487;&#33021;&#26159;&#26080;&#30028;&#30340;&#65289;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#31995;&#21015;&#19968;&#33324;&#24615;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20855;&#20307;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#23548;&#33268;&#22312;&#19981;&#38656;&#35201;&#26377;&#20851;&#20559;&#32622;&#36328;&#24230;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#24179;&#22343;&#22870;&#21169;MDP&#20013;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13903v1 Announce Type: new  Abstract: We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13852</link><description>&lt;p&gt;
&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural Control System for Continuous Glucose Monitoring and Maintenance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#33889;&#33796;&#31958;&#27700;&#24179;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#21033;&#29992;&#24494;&#20998;&#39044;&#27979;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21463;&#21040;&#22797;&#26434;&#31070;&#32463;&#31574;&#30053;&#21644;&#21487;&#21306;&#20998;&#24314;&#27169;&#30340;&#25351;&#23548;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#12290;&#36825;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#26368;&#22823;&#21270;&#25928;&#29575;&#65292;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#21644;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#32463;&#39564;&#21457;&#29616;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDEA&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#21464;&#36801;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.12767</link><description>&lt;p&gt;
&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#65306;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#36827;&#34892;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDEA&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#21464;&#36801;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20998;&#24067;&#30340;&#36716;&#31227;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20551;&#23450;&#26102;&#38388;&#20998;&#24067;&#30340;&#36716;&#31227;&#26159;&#22343;&#21248;&#21457;&#29983;&#30340;&#65292;&#20197;&#21306;&#20998;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#24456;&#38590;&#28385;&#36275;&#65292;&#22240;&#20026;&#25105;&#20204;&#19981;&#30693;&#36947;&#20998;&#24067;&#20309;&#26102;&#21457;&#29983;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#65288;IDEA&#65289;&#26469;&#26816;&#27979;&#20998;&#24067;&#20309;&#26102;&#21457;&#29983;&#36716;&#31227;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20805;&#20998;&#35266;&#23519;&#20551;&#35774;&#26469;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#21270;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19982;&#29615;&#22659;&#19981;&#30456;&#20851;&#30340;&#31283;&#23450;&#21464;&#37327;&#21644;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#38750;&#24179;&#31283;&#21464;&#37327;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28508;&#22312;&#29615;&#22659;&#21644;&#31283;&#23450;/&#38750;&#31283;&#23450;&#21464;&#37327;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;IDEA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12767v1 Announce Type: new  Abstract: Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated station- ary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#23545;&#35937;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;PANN&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#36924;&#36817;&#35823;&#24046;...</title><link>https://arxiv.org/abs/2402.11224</link><description>&lt;p&gt;
&#20855;&#26377;&#65288;&#20302;&#31934;&#24230;&#65289;&#22810;&#39033;&#24335;&#36924;&#36817;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#20934;&#30830;&#24615;&#25552;&#39640;&#30340;&#26032;&#35265;&#35299;&#21644;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#23545;&#35937;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;PANN&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#36924;&#36817;&#35823;&#24046;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#22810;&#39033;&#24335;&#36924;&#36817;&#26367;&#25442;&#38750;&#22810;&#39033;&#24335;&#20989;&#25968;&#65288;&#20363;&#22914;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#65289;&#26159;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#20570;&#27861;&#12290;&#26412;&#25991;&#20013;&#31216;&#20043;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#65288;PANN&#65289;&#30340;&#32467;&#26524;&#31070;&#32463;&#32593;&#32476;&#19982;&#20808;&#36827;&#30340;&#23494;&#30721;&#31995;&#32479;&#20860;&#23481;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#25512;&#26029;&#12290;&#21033;&#29992;&#8220;&#39640;&#31934;&#24230;&#8221;&#36924;&#36817;&#65292;&#26368;&#20808;&#36827;&#30340;PANN&#25552;&#20379;&#20102;&#19982;&#22522;&#30784;&#39592;&#24178;&#27169;&#22411;&#30456;&#20284;&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36924;&#36817;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#65292;&#24182;&#19988;&#29616;&#26377;&#25991;&#29486;&#36890;&#24120;&#26159;&#36890;&#36807;&#23454;&#35777;&#30830;&#23450;&#25152;&#38656;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#23545;PANN&#36827;&#34892;&#29420;&#31435;&#23545;&#35937;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;PANN&#20013;&#36817;&#20284;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;PANN&#23481;&#26131;&#21463;&#21040;&#26576;&#31181;&#31867;&#22411;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11224v1 Announce Type: new  Abstract: Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.10412</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#21152;&#26435;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;LLM&#22312;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#30340;&#34394;&#26500;
&lt;/p&gt;
&lt;p&gt;
Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10412
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#20107;&#23454;&#19981;&#27491;&#30830;&#20294;&#30475;&#20284;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#65292;&#30446;&#21069;&#26159;LLM&#21487;&#20449;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#20854;&#36827;&#34892;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#38656;&#35201;&#20855;&#26377;&#20855;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21363;&#20154;&#31867;&#32534;&#20889;&#30340;&#8220;&#26368;&#20339;&#8221;&#25110;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#36825;&#31181;&#35201;&#27714;&#20351;&#24187;&#35273;&#27979;&#37327;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21152;&#26435;LLM&#23545;&#20107;&#23454;&#24615;&#36827;&#34892;&#35780;&#20272;&#65288;FEWL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#37329;&#26631;&#20934;&#31572;&#26696;&#32570;&#22833;&#26102;&#35774;&#35745;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#12290;FEWL&#21033;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#31572;&#26696;&#20316;&#20026;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#20195;&#29702;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#37327;&#21270;&#21442;&#32771;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;FEWL&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#23427;&#26356;&#20934;&#30830;&#12290;&#24230;&#37327;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#32463;&#39564;&#22238;&#25918;&#36896;&#25104;&#30340;&#20248;&#21270;&#19981;&#31283;&#23450;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#30340;&#20462;&#25913;&#26469;&#24179;&#34913;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22238;&#25918;&#24335;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09542</link><description>&lt;p&gt;
&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65306;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#36817;&#31471;&#28857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#32463;&#39564;&#22238;&#25918;&#36896;&#25104;&#30340;&#20248;&#21270;&#19981;&#31283;&#23450;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#30340;&#20462;&#25913;&#26469;&#24179;&#34913;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22238;&#25918;&#24335;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#20174;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#37117;&#20351;&#29992;&#32463;&#39564;&#22238;&#25918;&#26469;&#21516;&#26102;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#24230;&#25311;&#21512;&#20808;&#21069;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#65306;&#20351;&#29992;&#32463;&#39564;&#22238;&#25918;&#35757;&#32451;&#30340;&#32593;&#32476;&#24448;&#24448;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;&#24433;&#21709;&#20854;&#25972;&#20307;&#20934;&#30830;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22238;&#25918;&#32531;&#20914;&#21306;&#23384;&#20648;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#38382;&#39064;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20248;&#21270;&#20960;&#20309;&#30340;&#31616;&#21333;&#20462;&#25913;&#26469;&#26368;&#23567;&#21270;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#65292;&#22312;&#21482;&#20801;&#35768;&#36880;&#28176;&#25913;&#21464;&#36807;&#21435;&#25968;&#25454;&#30340;&#38544;&#34255;&#28608;&#27963;&#30340;&#21516;&#26102;&#65292;&#24179;&#34913;&#20102;&#20174;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#20013;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;LPR&#22312;&#22522;&#20110;&#22238;&#25918;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09542v1 Announce Type: new  Abstract: In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08679</link><description>&lt;p&gt;
COLD-Attack: &#29992;&#20110;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36234;&#29425;&#30340;&#27880;&#24847;&#21147;&#36234;&#26469;&#36234;&#22810;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#36234;&#29425;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#20197;&#21450;&#24773;&#24863;/&#39118;&#26684;&#21464;&#21270;&#65292;&#22240;&#27492;&#30740;&#31350;&#21487;&#25511;&#24615;&#36234;&#29425;&#26159;&#26377;&#30410;&#30340;&#65292;&#21363;&#22914;&#20309;&#23545;LLM&#25915;&#20987;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#24418;&#24335;&#21270;&#20102;&#21487;&#25511;&#24615;&#25915;&#20987;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#19982;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#26032;&#22411;&#20851;&#32852;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#65288;COLD&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#19988;&#33258;&#21160;&#21270;&#22320;&#25628;&#32034;&#21508;&#31181;&#25511;&#21046;&#35201;&#27714;&#19979;&#30340;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#65292;&#20363;&#22914;&#27969;&#30021;&#24615;&#12289;&#38544;&#31192;&#24615;&#12289;&#24773;&#24863;&#21644;&#24038;&#21491;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.07933</link><description>&lt;p&gt;
&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#26080;&#20195;&#30721;AutoML&#65306;&#27010;&#24565;&#26694;&#26550;&#12289;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#20135;&#21697;&#30340;&#29305;&#28857;&#26159;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#23545;&#38750;&#19987;&#23478;&#30340;&#19981;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#12290;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30340;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#26080;&#32541;&#25191;&#34892;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#65292;&#36825;&#23545;&#20110;&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#34892;&#19994;&#21644;&#21019;&#26032;&#30456;&#20851;&#65292;&#23427;&#24433;&#21709;&#25112;&#30053;&#20915;&#31574;&#21644;&#25237;&#36164;&#39118;&#38505;&#30340;&#38477;&#20302;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24819;&#27861;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26080;&#20195;&#30721;AutoML&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24320;&#21457;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#20351;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#21463;&#30410;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
&lt;/p&gt;</description></item><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>S$\Omega$I&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#65292;&#21487;&#20197;&#35745;&#31639;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#37327;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.05667</link><description>&lt;p&gt;
S$\Omega$I: &#22522;&#20110;&#20998;&#25968;&#30340;O-INFORMATION&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
S$\Omega$I: Score-based O-INFORMATION Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05667
&lt;/p&gt;
&lt;p&gt;
S$\Omega$I&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#65292;&#21487;&#20197;&#35745;&#31639;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#37327;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25968;&#25454;&#21644;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#20998;&#26512;&#38656;&#35201;&#25429;&#25417;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20449;&#24687;&#37327;&#12290;&#26368;&#36817;&#65292;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#24050;&#34987;&#21457;&#23637;&#20986;&#26469;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#37327;&#24230;&#65288;&#22914;&#20114;&#20449;&#24687;&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#21482;&#32771;&#34385;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#20449;&#24687;&#21327;&#21516;&#21644;&#20887;&#20313;&#30340;&#27010;&#24565;&#23545;&#20110;&#29702;&#35299;&#21464;&#37327;&#20043;&#38388;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#26368;&#33879;&#21517;&#21644;&#22810;&#29992;&#36884;&#30340;&#37327;&#24230;&#20043;&#19968;&#26159;O-information&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28165;&#26224;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#37327;&#21270;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#38480;&#20110;&#31616;&#21270;&#24773;&#20917;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;S$\Omega$I&#65292;&#35813;&#26041;&#27861;&#39318;&#27425;&#20801;&#35768;&#35745;&#31639;O-information&#32780;&#19981;&#21463;&#23545;&#31995;&#32479;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;S$&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#38236;&#20687;&#26144;&#23556;&#36873;&#25321;&#65288;NPG&#65289;&#22312;&#26631;&#20934;&#22522;&#20934;&#29615;&#22659;&#20013;&#24120;&#24120;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#25214;&#21040;&#20102;&#26356;&#39640;&#25928;&#30340;&#38236;&#20687;&#26144;&#23556;&#65292;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05187</link><description>&lt;p&gt;
&#22312;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#20013;&#20803;&#23398;&#20064;&#38236;&#20687;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Meta-learning the mirror map in policy mirror descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#38236;&#20687;&#26144;&#23556;&#36873;&#25321;&#65288;NPG&#65289;&#22312;&#26631;&#20934;&#22522;&#20934;&#29615;&#22659;&#20013;&#24120;&#24120;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#25214;&#21040;&#20102;&#26356;&#39640;&#25928;&#30340;&#38236;&#20687;&#26144;&#23556;&#65292;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#35270;&#35282;&#65292;&#23427;&#21253;&#21547;&#20102;&#35768;&#22810;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#26159;&#36890;&#36807;&#36873;&#25321;&#19968;&#20010;&#38236;&#20687;&#26144;&#23556;&#32780;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;PMD&#30340;&#20840;&#38754;&#28508;&#21147;&#30340;&#25506;&#32034;&#26159;&#26377;&#38480;&#30340;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#38236;&#20687;&#26144;&#23556;&#19978;&#65292;&#21363;&#36127;&#29109;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#33879;&#21517;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#36824;&#19981;&#30830;&#23450;&#38236;&#20687;&#26144;&#23556;&#30340;&#36873;&#25321;&#26159;&#21542;&#20250;&#23545;PMD&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#38236;&#20687;&#26144;&#23556;&#36873;&#25321;&#65288;NPG&#65289;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#29615;&#22659;&#20013;&#32463;&#24120;&#20135;&#29983;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#24212;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#39640;&#25928;&#30340;&#38236;&#20687;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#24179;&#22343;&#24615;&#33021;&#36824;&#26159;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along th
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20511;&#21161;&#26368;&#20248;&#20256;&#36755;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20998;&#24067;&#30340;&#37325;&#24515;&#12290;&#36890;&#36807;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21452;&#23618;&#23545;&#25239;&#23398;&#20064;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#19981;&#38656;&#35201;&#20351;&#29992;&#19977;&#23618;&#20248;&#21270;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#29702;&#35770;&#35823;&#24046;&#30028;&#65292;&#24182;&#22312;&#31034;&#20363;&#22330;&#26223;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#24212;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03828</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#20272;&#35745;&#20998;&#24067;&#30340;&#37325;&#24515;
&lt;/p&gt;
&lt;p&gt;
Estimating Barycenters of Distributions with Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20511;&#21161;&#26368;&#20248;&#20256;&#36755;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20998;&#24067;&#30340;&#37325;&#24515;&#12290;&#36890;&#36807;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21452;&#23618;&#23545;&#25239;&#23398;&#20064;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#19981;&#38656;&#35201;&#20351;&#29992;&#19977;&#23618;&#20248;&#21270;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#29702;&#35770;&#35823;&#24046;&#30028;&#65292;&#24182;&#22312;&#31034;&#20363;&#22330;&#26223;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#24212;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19968;&#32452;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#19994;&#32773;&#26377;&#26102;&#38656;&#35201;&#25214;&#21040;&#19968;&#31181;&#8220;&#24179;&#22343;&#8221;&#20998;&#24067;&#65292;&#20197;&#20805;&#20998;&#32858;&#21512;&#21442;&#32771;&#20998;&#24067;&#12290;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#21560;&#24341;&#21147;&#30340;&#24179;&#22343;&#20998;&#24067;&#27010;&#24565;&#26159;Wasserstein&#37325;&#24515;&#65292;&#32780;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#23601;&#26159;&#23427;&#12290;&#36890;&#36807;&#24314;&#31435;&#22312;&#26368;&#20248;&#20256;&#36755;&#30340;&#23545;&#20598;&#24418;&#24335;&#20043;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#26469;&#35299;&#20915;Wasserstein&#37325;&#24515;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#30340;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65306;&#23427;&#20855;&#26377;&#21452;&#23618;&#23545;&#25239;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#36825;&#20123;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#65292;&#22240;&#20026;&#20856;&#22411;&#30340;&#21033;&#29992;&#37325;&#24515;&#20219;&#21153;&#30340;&#23545;&#25239;&#24615;&#31639;&#27861;&#24448;&#24448;&#21033;&#29992;&#19977;&#23618;&#20248;&#21270;&#65292;&#24182;&#19988;&#20027;&#35201;&#38598;&#20013;&#22312;&#20108;&#27425;&#25104;&#26412;&#19978;&#12290;&#25105;&#20204;&#36824;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#29702;&#35770;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#31034;&#20363;&#22330;&#26223;&#21644;&#22270;&#20687;&#25968;&#25454;&#35774;&#32622;&#19978;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a collection of probability measures, a practitioner sometimes needs to find an "average" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.
&lt;/p&gt;</description></item><item><title>$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#26159;&#23558;$C^*$-&#20195;&#25968;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#36890;&#36807;&#32479;&#19968;&#29616;&#26377;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#26356;&#22810;&#20803;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02637</link><description>&lt;p&gt;
$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#65306;&#36808;&#21521;&#26032;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
$C^*$-Algebraic Machine Learning: Moving in a New Direction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02637
&lt;/p&gt;
&lt;p&gt;
$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#26159;&#23558;$C^*$-&#20195;&#25968;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#36890;&#36807;&#32479;&#19968;&#29616;&#26377;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#26356;&#22810;&#20803;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#25968;&#23398;&#30340;&#20960;&#20010;&#39046;&#22495;&#65288;&#22914;&#32479;&#35745;&#23398;&#12289;&#27010;&#29575;&#35770;&#21644;&#32447;&#24615;&#20195;&#25968;&#65289;&#26377;&#30528;&#38271;&#26399;&#30340;&#21512;&#20316;&#20256;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#26041;&#21521;&#65306;$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#65292;&#36825;&#26159;$C^*$-&#20195;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#20132;&#27969;&#21644;&#30456;&#20114;&#28363;&#20859;&#12290;$C^*$-&#20195;&#25968;&#26159;&#22797;&#25968;&#31354;&#38388;&#30340;&#33258;&#28982;&#25512;&#24191;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#32479;&#19968;&#29616;&#26377;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#26356;&#22810;&#20803;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;$C^*$-&#20195;&#25968;&#30340;&#21407;&#22240;&#21644;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#35774;&#35745;$C^*$-&#20195;&#25968;&#23398;&#20064;&#27169;&#22411;&#30340;&#25216;&#26415;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;$C^*$-&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#21457;&#23637;&#21644;&#24212;&#29992;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications.
&lt;/p&gt;</description></item><item><title>TimeSiam&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Siamese&#32593;&#32476;&#21644;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#23398;&#20064;&#20869;&#37096;&#26102;&#24207;&#20381;&#36182;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02475</link><description>&lt;p&gt;
TimeSiam&#65306;&#19968;&#31181;&#29992;&#20110;&#23402;&#29983;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02475
&lt;/p&gt;
&lt;p&gt;
TimeSiam&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Siamese&#32593;&#32476;&#21644;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#23398;&#20064;&#20869;&#37096;&#26102;&#24207;&#20381;&#36182;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#38477;&#20302;&#26631;&#27880;&#30340;&#25104;&#26412;&#24182;&#21463;&#30410;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#22312;&#35270;&#35273;&#25110;&#35821;&#35328;&#39046;&#22495;&#24191;&#20026;&#35748;&#21487;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#22914;&#36974;&#34109;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#36974;&#34109;&#26102;&#38388;&#24207;&#21015;&#25110;&#35745;&#31639;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23558;&#23548;&#33268;&#20002;&#22833;&#25110;&#24573;&#35270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20851;&#38190;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24378;&#35843;&#26102;&#38388;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeSiam&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TimeSiam&#23545;&#23402;&#29983;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25429;&#25417;&#38543;&#26426;&#37319;&#26679;&#30340;&#36807;&#21435;&#21644;&#24403;&#21069;&#23376;&#24207;&#21015;&#20043;&#38388;&#30340;&#20869;&#22312;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#36974;&#34109;&#65289;&#65292;TimeSiam&#21487;&#20197;&#20174;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#23376;&#24207;&#21015;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#20174;&#36807;&#21435;&#21040;&#24403;&#21069;&#30340;&#37325;&#24314;&#26469;&#23398;&#20064;&#20869;&#37096;&#26102;&#24207;&#20381;&#36182;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#21487;&#23398;&#20064;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPreFL&#30340;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#36866;&#24212;&#20219;&#20309;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02225</link><description>&lt;p&gt;
&#37325;&#24605;&#20986;&#21457;&#28857;&#65306;&#36890;&#36807;&#21327;&#20316;&#39044;&#35757;&#32451;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoPreFL&#30340;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#36866;&#24212;&#20219;&#20309;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#20174;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23454;&#35777;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20026;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#26377;&#30410;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#39044;&#35757;&#32451;&#26041;&#27861;CoPreFL&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#35774;&#35745;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20026;&#20219;&#20309;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#27169;&#20223;&#19979;&#28216;&#20998;&#24067;&#24335;&#22330;&#26223;&#30340;&#20803;&#23398;&#20064;&#36807;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26410;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#12290;CoPreFL&#30340;&#39044;&#35757;&#32451;&#20248;&#21270;&#36807;&#31243;&#20063;&#22312;&#24179;&#22343;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#21021;&#22987;&#21270;&#26469;&#35299;&#20915;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#25361;&#25112;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#20219;&#20309;&#26410;&#30693;&#30340;&#19979;&#28216;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#23558;&#20851;&#38190;&#20449;&#24687;&#19982;&#26597;&#35810;&#21644;&#25968;&#20540;&#20998;&#31163;&#24182;&#37319;&#29992;&#27969;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;ImageNet-1K&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00534</link><description>&lt;p&gt;
&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27969;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Manifold Representation of the Key in Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#23558;&#20851;&#38190;&#20449;&#24687;&#19982;&#26597;&#35810;&#21644;&#25968;&#20540;&#20998;&#31163;&#24182;&#37319;&#29992;&#27969;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;ImageNet-1K&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#36716;&#25442;&#22120;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27880;&#24847;&#21147;&#22359;&#23454;&#29616;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MSA&#65289;&#12290;&#22312;&#36825;&#20123;&#22359;&#20013;&#65292;&#26597;&#35810;&#12289;&#20851;&#38190;&#20449;&#24687;&#21644;&#25968;&#20540;&#36890;&#24120;&#32416;&#32544;&#22312;&#19968;&#36215;&#65292;&#24182;&#36890;&#36807;&#21333;&#20010;&#20849;&#20139;&#32447;&#24615;&#21464;&#25442;&#29983;&#25104;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#20851;&#38190;&#20449;&#24687;&#20174;&#26597;&#35810;&#21644;&#25968;&#20540;&#20013;&#35299;&#32806;&#65292;&#24182;&#20026;&#20851;&#38190;&#20449;&#24687;&#37319;&#29992;&#27969;&#24418;&#34920;&#31034;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35299;&#32806;&#24182;&#36171;&#20104;&#20851;&#38190;&#20449;&#24687;&#27969;&#24418;&#32467;&#26500;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ViT-B&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#30340;top-1&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;0.87&#65285;&#65292;&#32780;Swin-T&#21017;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#25552;&#39640;&#20102;0.52&#65285;&#65292;&#20351;&#29992;&#20102;&#20843;&#20010;&#27969;&#24418;&#20851;&#38190;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#22686;&#21152;&#20102;&#26356;&#22810;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#31616;&#21333;&#24615;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#20197;&#25506;&#32034;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;Transformer&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#25972;&#21512;&#20849;&#20139;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#20379;&#26356;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.00433</link><description>&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;Transformer&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#25972;&#21512;&#20849;&#20139;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#20379;&#26356;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;Transformer&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#25152;&#26377;&#20219;&#21153;&#12290;&#20197;&#20219;&#21153;&#31639;&#26415;&#20026;&#20363;&#30340;&#20808;&#21069;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26082;&#26377;&#25928;&#21448;&#21487;&#25193;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23547;&#25214;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#38745;&#24577;&#26368;&#20248;&#35299;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#26159;&#20943;&#36731;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#21066;&#24369;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22810;&#25968;&#21442;&#25968;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;Transformer&#23618;&#30340;MLP&#25193;&#23637;&#20026;&#26435;&#37325;&#38598;&#25104;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#21160;&#24577;&#22320;&#25972;&#21512;&#20849;&#20139;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#26356;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36866;&#24212;&#27599;&#20010;&#23454;&#20363;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#31163;&#20849;&#20139;&#30693;&#35782;&#21644;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#28982;&#21518;&#21160;&#24577;&#22320;&#25972;&#21512;&#23427;&#20204;&#65292;
&lt;/p&gt;
&lt;p&gt;
Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.15698</link><description>&lt;p&gt;
RepairLLaMA&#65306;&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#29992;&#20110;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15698
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24050;&#26377;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#23545;&#20110;&#31243;&#24207;&#20462;&#22797;&#36827;&#34892;LLMs&#30340;&#24494;&#35843;&#26159;&#26368;&#36817;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#26410;&#34987;&#25506;&#32034;&#30340;&#32500;&#24230;&#12290;&#29616;&#26377;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#31616;&#21333;&#30340;&#20195;&#30721;&#34920;&#31034;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#33021;&#22815;&#24494;&#35843;&#26356;&#22823;&#22411;LLMs&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#23616;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepairLLaMA&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;1&#65289;&#29992;&#20110;APR&#30340;&#20195;&#30721;&#34920;&#31034;&#21644;2&#65289;&#26368;&#20808;&#36827;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#25216;&#26415;LoRA&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;RepairLLaMA&#20135;&#29983;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31243;&#24207;&#20462;&#22797;&#36866;&#37197;&#22120;&#8221;&#65292;&#29992;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20855;&#26377;&#31243;&#24207;&#20462;&#22797;&#29305;&#23450;&#20195;&#30721;&#34920;&#31034;&#30340;&#24494;&#35843;&#36866;&#37197;&#22120;&#20351;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#20462;&#22797;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26377;&#21161;&#20110;&#24494;&#35843;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#21487;&#33021;&#25552;&#39640;&#20043;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;</title><link>https://arxiv.org/abs/2312.02592</link><description>&lt;p&gt;
FRAPP'E&#65306;&#19968;&#20010;&#29992;&#20110;&#21518;&#22788;&#29702;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FRAPP\'E: A Group Fairness Framework for Post-Processing Everything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02592
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#21487;&#33021;&#25552;&#39640;&#20043;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22788;&#29702;&#20013;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#20294;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#22312;&#35768;&#22810;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#25110;&#26080;&#27861;&#35775;&#38382;&#39044;&#27979;&#27169;&#22411;&#35757;&#32451;&#31649;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#21518;&#22788;&#29702;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#19987;&#20026;&#29305;&#23450;&#38382;&#39064;&#35774;&#32622;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#32780;&#35774;&#35745;&#65292;&#22240;&#27492;&#19981;&#22914;&#22788;&#29702;&#20013;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33719;&#24471;&#26356;&#24191;&#27867;&#38382;&#39064;&#35774;&#32622;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#36884;&#24452;&#65292;&#36828;&#36229;&#36807;&#20197;&#21069;&#30340;&#21518;&#22788;&#29702;&#25991;&#29486;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#30041;&#20102;&#22788;&#29702;&#20013;&#23454;&#29616;&#30340;&#33391;&#22909;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02592v2 Announce Type: replace  Abstract: Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HelmFluid&#65292;&#19968;&#20010;&#31934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#27969;&#20307;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#65292;&#23558;&#27969;&#20307;&#21160;&#21147;&#23398;&#20998;&#35299;&#20026;&#26356;&#21487;&#35299;&#30340;&#26080;&#26059;&#21644;&#26080;&#25955;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#22810;&#23610;&#24230;&#22810;&#22836;&#31215;&#20998;&#26550;&#26500;&#36827;&#34892;&#38598;&#25104;&#65292;HelmFluid&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#27969;&#20307;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2310.10565</link><description>&lt;p&gt;
HelmFluid&#65306;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#27969;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HelmFluid&#65292;&#19968;&#20010;&#31934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#27969;&#20307;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#65292;&#23558;&#27969;&#20307;&#21160;&#21147;&#23398;&#20998;&#35299;&#20026;&#26356;&#21487;&#35299;&#30340;&#26080;&#26059;&#21644;&#26080;&#25955;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#22810;&#23610;&#24230;&#22810;&#22836;&#31215;&#20998;&#26550;&#26500;&#36827;&#34892;&#38598;&#25104;&#65292;HelmFluid&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#27969;&#20307;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#20307;&#39044;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#28145;&#24230;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#33021;&#21147;&#30452;&#25509;&#20272;&#35745;&#26410;&#26469;&#39044;&#27979;&#30340;&#36895;&#24230;&#22330;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23398;&#20064;&#34920;&#38754;&#36895;&#24230;&#22330;&#32780;&#36339;&#36807;&#22266;&#26377;&#30340;&#29289;&#29702;&#29305;&#24615;&#23558;&#23548;&#33268;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#31934;&#30830;&#25110;&#20855;&#26377;&#29289;&#29702;&#21487;&#38752;&#24615;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HelmFluid&#65292;&#38024;&#23545;&#27969;&#20307;&#30340;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#22120;&#12290;&#21463;Helmholtz&#23450;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;HelmDynamics&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;Helmholtz&#21160;&#21147;&#23398;&#65292;&#23558;&#27969;&#20307;&#21160;&#21147;&#23398;&#20998;&#35299;&#20026;&#26356;&#21487;&#35299;&#30340;&#26080;&#26059;&#21644;&#26080;&#25955;&#37096;&#20998;&#65292;&#29289;&#29702;&#19978;&#23545;&#24212;&#20110;&#27969;&#20307;&#30340;&#21183;&#20989;&#25968;&#21644;&#27969;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;HelmDynamics&#27169;&#22359;&#23884;&#20837;&#21040;&#22810;&#23610;&#24230;&#22810;&#22836;&#31215;&#20998;&#26550;&#26500;&#20013;&#65292;HelmFluid&#21487;&#20197;&#22312;&#22810;&#20010;&#31354;&#38388;&#23610;&#24230;&#19978;&#27839;&#26102;&#38388;&#32500;&#24230;&#38598;&#25104;&#23398;&#21040;&#30340;Helmholtz&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#20135;&#29983;&#26410;&#26469;&#30340;&#27969;&#20307;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HelmFluid&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27969;&#20307;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#27969;&#20307;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fluid prediction is a long-standing challenge due to the intrinsic high-dimensional non-linear dynamics. Previous methods usually utilize the non-linear modeling capability of deep models to directly estimate velocity fields for future prediction. However, skipping over inherent physical properties but directly learning superficial velocity fields will overwhelm the model from generating precise or physics-reliable results. In this paper, we propose the HelmFluid toward an accurate and interpretable predictor for fluid. Inspired by the Helmholtz theorem, we design a HelmDynamics block to learn Helmholtz dynamics, which decomposes fluid dynamics into more solvable curl-free and divergence-free parts, physically corresponding to potential and stream functions of fluid. By embedding the HelmDynamics block into a Multiscale Multihead Integral Architecture, HelmFluid can integrate learned Helmholtz dynamics along temporal dimension in multiple spatial scales to yield future fluid. Compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04521</link><description>&lt;p&gt;
Lie&#31070;&#32463;&#20803;&#65306;&#21322;&#21333;Lie&#20195;&#25968;&#30340;&#20276;&#38543;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#25968;&#25454;&#23384;&#22312;&#20110;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#20013;&#12290;&#23545;&#24212;&#30340;&#32676;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20316;&#29992;&#20110;Lie&#20195;&#25968;&#19978;&#65292;&#20351;&#24471;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#20855;&#26377;&#20276;&#38543;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#31616;&#21333;&#30340;$\mathrm{SO}(3)$-&#31561;&#21464;&#32593;&#32476;&#8212;&#8212;&#21521;&#37327;&#31070;&#32463;&#20803;&#20174;3&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25512;&#24191;&#21040;Lie&#20195;&#25968;&#31354;&#38388;&#65292;&#21033;&#29992;Killing&#24418;&#24335;&#30340;&#19981;&#21464;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Lie&#25324;&#21495;&#23618;&#21644;&#20960;&#20309;&#36890;&#36947;&#28151;&#21512;&#23618;&#26469;&#25193;&#23637;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;$\mathfrak{so}(3)$&#21644;$\mathfrak{sl}(3)$ Lie&#20195;&#25968;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#25311;&#21512;&#31561;&#21464;&#21644;&#19981;&#21464;&#20989;&#25968;&#12289;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#28857;&#20113;&#37197;&#20934;&#21644;&#22522;&#20110;&#21333;&#24212;&#24615;&#30340;&#24418;&#29366;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31561;&#21464;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#24314;&#27169;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#24341;&#23548;&#37325;&#24314;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2212.07892</link><description>&lt;p&gt;
&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#29992;&#20110;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#32852;&#21512;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.07892
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#24314;&#27169;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#24341;&#23548;&#37325;&#24314;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#20013;&#24863;&#20852;&#36259;&#30340;&#31995;&#32479;&#26412;&#36136;&#19978;&#26159;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#26469;&#35775;&#38382;&#36825;&#20123;&#31995;&#32479;&#12290;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#21487;&#33021;&#30001;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#32780;&#38750;&#36830;&#32493;&#27979;&#37327;&#32452;&#25104;&#65292;&#25110;&#32773;&#21487;&#33021;&#30001;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#27979;&#37327;&#32452;&#25104;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#38500;&#20102;&#33033;&#20914;&#35745;&#25968;&#21644;&#36830;&#32493;&#29983;&#29702;&#35760;&#24405;&#22806;&#65292;&#36824;&#26377;&#34892;&#20026;&#26631;&#31614;&#12290;&#34429;&#28982;&#29616;&#22312;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#37325;&#24314;&#30340;&#25991;&#29486;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20960;&#20046;&#27809;&#26377;&#34987;&#32771;&#34385;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#28789;&#27963;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#31232;&#30095;&#25945;&#24072;&#20449;&#21495;&#65292;&#25351;&#23548;&#37325;&#24314;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#21033;&#29992;&#20102;DSR&#35757;&#32451;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.07892v2 Announce Type: replace  Abstract: Many, if not most, systems of interest in science are naturally described as nonlinear dynamical systems. Empirically, we commonly access these systems through time series measurements. Often such time series may consist of discrete random variables rather than continuous measurements, or may be composed of measurements from multiple data modalities observed simultaneously. For instance, in neuroscience we may have behavioral labels in addition to spike counts and continuous physiological recordings. While by now there is a burgeoning literature on deep learning for dynamical systems reconstruction (DSR), multimodal data integration has hardly been considered in this context. Here we provide such an efficient and flexible algorithmic framework that rests on a multimodal variational autoencoder for generating a sparse teacher signal that guides training of a reconstruction model, exploiting recent advances in DSR training techniques. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;WAVES&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#22810;&#26679;&#21270;&#21387;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#27700;&#21360;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08573</link><description>&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Robustness of Image Watermarks. (arXiv:2401.08573v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;WAVES&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#22810;&#26679;&#21270;&#21387;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#27700;&#21360;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#24369;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;WAVES&#65288;&#36890;&#36807;&#22686;&#24378;&#30340;&#21387;&#21147;&#27979;&#35797;&#36827;&#34892;&#27700;&#21360;&#20998;&#26512;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;WAVES&#25972;&#21512;&#20102;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#30001;&#22810;&#26679;&#21270;&#21387;&#21147;&#27979;&#35797;&#32452;&#25104;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#12290;WAVES&#20013;&#30340;&#25915;&#20987;&#33539;&#22260;&#20174;&#20256;&#32479;&#30340;&#22270;&#20687;&#22833;&#30495;&#21040;&#25193;&#25955;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#39640;&#32423;&#21644;&#26032;&#39062;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#22270;&#20687;&#36136;&#37327;&#38477;&#20302;&#31243;&#24230;&#21644;&#25915;&#20987;&#21518;&#27700;&#21360;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#24615;&#33021;&#19982;&#36136;&#37327;2D&#22270;&#65292;&#21464;&#21270;&#22522;&#20110;&#20960;&#31181;&#31361;&#20986;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#28982;&#21518;&#29992;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26032;&#39062;&#26041;&#27861;&#23558;&#23427;&#20204;&#32858;&#21512;&#65292;&#20174;&#32780;&#20026;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#21644;&#25915;&#20987;&#33021;&#21147;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#30011;&#38754;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#25581;&#31034;&#20102;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the weaknesses of image watermarking techniques. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel benchmark for assessing watermark robustness, overcoming the limitations of current evaluation methods.WAVES integrates detection and identification tasks, and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced and novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. We develop a series of Performance vs. Quality 2D plots, varying over several prominent image similarity metrics, which are then aggregated in a heuristically novel manner to paint an overall picture of watermark robustness and attack potency. Our comprehensive evaluation reveals previously undetected vulnerabilities of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>WinNet&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;CNN-based&#27169;&#22411;&#65292;&#36890;&#36807;&#31383;&#21475;&#22686;&#24378;&#30340;&#21608;&#26399;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#22312;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#24615;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#31616;&#21333;&#32467;&#26500;&#12290;&#22312;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WinNet&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;CNN&#12289;MLP&#12289;Transformer&#26041;&#27861;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.00214</link><description>&lt;p&gt;
WinNet:&#20855;&#26377;&#31383;&#21475;&#22686;&#24378;&#21608;&#26399;&#25552;&#21462;&#21644;&#20132;&#20114;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
WinNet:time series forecasting with a window-enhanced period extracting and interacting. (arXiv:2311.00214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00214
&lt;/p&gt;
&lt;p&gt;
WinNet&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;CNN-based&#27169;&#22411;&#65292;&#36890;&#36807;&#31383;&#21475;&#22686;&#24378;&#30340;&#21608;&#26399;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#22312;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#24615;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#31616;&#21333;&#32467;&#26500;&#12290;&#22312;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WinNet&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;CNN&#12289;MLP&#12289;Transformer&#26041;&#27861;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26102;&#24207;&#39044;&#27979;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#26080;&#27861;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20934;&#30830;&#19988;&#31616;&#21333;&#32467;&#26500;&#30340;&#22522;&#20110;CNN&#30340;&#27169;&#22411;WinNet&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#21253;&#25324;&#65306;(i) Inter-Intra Period Encoder (I2PE) &#23558;1D&#24207;&#21015;&#36716;&#25442;&#20026;&#20108;&#32500;&#24352;&#37327;&#65292;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21608;&#26399;&#31383;&#21475;&#20855;&#26377;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#24615;&#65292;(ii) Two-Dimensional Period Decomposition (TDPD) &#23545;&#27169;&#22411;&#36827;&#34892;&#21608;&#26399;-&#36235;&#21183;&#21644;&#25391;&#33633;&#39033;&#24314;&#27169;&#65292;(iii) Decomposition Correlation Block (DCB) &#21033;&#29992;&#21608;&#26399;-&#36235;&#21183;&#21644;&#25391;&#33633;&#39033;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;CNNs&#25903;&#25345;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;WinNet&#22312;CNN&#12289;MLP&#21644;Transformer&#26041;&#27861;&#19978;&#21487;&#20197;&#23454;&#29616;SOTA&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;WinNet&#20026;&#22522;&#20110;CNN&#30340;&#20803;&#26041;&#27861;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-based methods have significantly improved state-of-the-art time series forecasting results, but they suffer from high computational costs and the inability to capture the long and short periodicity of time series. We present a highly accurate and simply structured CNN-based model for long-term time series forecasting tasks, called WinNet, including (i) Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with long and short periodicity according to the predefined periodic window, (ii) Two-Dimensional Period Decomposition (TDPD) to model period-trend and oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage the correlations of the period-trend and oscillation terms to support the prediction tasks by CNNs. Results on nine benchmark datasets show that the WinNet can achieve SOTA performance and lower computational complexity over CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the CNN-based met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#29109;&#36825;&#19968;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24357;&#34917;&#20102;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;(SEDD)&#24182;&#22312;GPT-2&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16834</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#20998;&#24067;&#27604;&#20363;&#30340;&#31163;&#25955;&#25193;&#25955;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#29109;&#36825;&#19968;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24357;&#34917;&#20102;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;(SEDD)&#24182;&#22312;GPT-2&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#31561;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#20851;&#38190;&#26159;&#65292;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#20381;&#36182;&#20110;&#25104;&#29087;&#30340;&#24471;&#20998;&#21305;&#37197;&#29702;&#35770;&#65292;&#20294;&#26159;&#23558;&#20854;&#25512;&#24191;&#21040;&#31163;&#25955;&#32467;&#26500;&#24182;&#27809;&#26377;&#21462;&#24471;&#30456;&#21516;&#30340;&#32463;&#39564;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#24471;&#20998;&#29109;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#23427;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#31283;&#23450;&#65292;&#21487;&#20197;&#24418;&#25104;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30340;ELBO&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#21464;&#20307;&#39640;&#25928;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65288;SEDD&#65289;&#25193;&#23637;&#21040;GPT-2&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#20284;&#28982;&#24230;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#31639;&#27861;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27604;&#36739;&#22823;&#23567;&#30456;&#20284;&#30340;SEDD&#21644;GPT-2&#27169;&#22411;&#26102;&#65292;SEDD&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#22256;&#24785;&#24230;&#65288;&#36890;&#24120;&#22312;&#22522;&#32447;&#30340;+$10\%$&#20869;&#65292;&#24182;&#19988;&#26377;&#26102;&#36229;&#36807;&#22522;&#32447;&#65289;&#12290;&#27492;&#22806;&#65292;SEDD&#27169;&#22411;&#23398;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#21464;&#37327;&#20998;&#32452;&#30340;&#26032;&#22411;&#24369;&#32422;&#26463;&#30340;&#21487;&#36776;&#35782;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#32467;&#26500;&#12289;&#24178;&#39044;&#25110;&#30417;&#30563;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.15709</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#27979;&#21464;&#37327;&#30340;&#20998;&#32452;&#20351;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21487;&#36776;&#35782;&#21270;
&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning Made Identifiable by Grouping of Observational Variables. (arXiv:2310.15709v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#21464;&#37327;&#20998;&#32452;&#30340;&#26032;&#22411;&#24369;&#32422;&#26463;&#30340;&#21487;&#36776;&#35782;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#32467;&#26500;&#12289;&#24178;&#39044;&#25110;&#30417;&#30563;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#24456;&#26377;&#24847;&#20041;&#30340;&#35805;&#39064;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;Causal Representation Learning&#65292;&#31616;&#31216;CRL&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#38544;&#34255;&#29305;&#24449;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CRL&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36866;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#32467;&#21512;&#20102;&#34920;&#31034;&#23398;&#20064;&#21644;&#22240;&#26524;&#21457;&#29616;&#36825;&#20004;&#20010;&#23481;&#26131;&#19981;&#36866;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35201;&#25214;&#21040;&#33021;&#20445;&#35777;&#21807;&#19968;&#35299;&#30340;&#23454;&#38469;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#23545;&#20110;&#20854;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#22522;&#20110;&#23545;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#26102;&#38388;&#22240;&#26524;&#24615;&#12289;&#30417;&#30563;&#25110;&#24178;&#39044;&#30340;&#23384;&#22312;&#65307;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#38480;&#21046;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35266;&#27979;&#28151;&#21512;&#34920;&#29616;&#20986;&#21512;&#36866;&#30340;&#21464;&#37327;&#20998;&#32452;&#30340;&#26032;&#22411;&#24369;&#32422;&#26463;&#65292;&#23637;&#31034;&#20102;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#19968;&#33268;&#30340;&#26032;&#22411;&#33258;&#25105;&#30417;&#30563;&#20272;&#35745;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A topic of great current interest is Causal Representation Learning (CRL), whose goal is to learn a causal model for hidden features in a data-driven manner. Unfortunately, CRL is severely ill-posed since it is a combination of the two notoriously ill-posed problems of representation learning and causal discovery. Yet, finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most approaches so far have been based on assumptions on the latent causal mechanisms, such as temporal causality, or existence of supervision or interventions; these can be too restrictive in actual applications. Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision. The approach is based assuming the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework consistent with the model, 
&lt;/p&gt;</description></item><item><title>&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.12324</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#34913;&#25945;&#24072;&#21644;&#30740;&#31350;&#32773;&#30340;&#28608;&#21169;&#65292;&#25506;&#32034;&#36866;&#24212;&#24615;&#23454;&#39564;&#20419;&#36827;&#25345;&#32493;&#25913;&#36827;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12324
&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#27604;&#36739;&#19981;&#21516;&#25945;&#23398;&#31574;&#30053;&#30340;&#26426;&#20250;&#21487;&#20197;&#20026;&#25945;&#24072;&#30340;&#20915;&#31574;&#25552;&#20379;&#26377;&#29992;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#23454;&#39564;&#32570;&#20047;&#28165;&#26224;&#31616;&#26126;&#30340;&#20351;&#29992;&#25968;&#25454;&#24555;&#36895;&#22686;&#21152;&#23454;&#39564;&#23398;&#29983;&#33719;&#24471;&#26368;&#20339;&#26465;&#20214;&#26426;&#20250;&#30340;&#36884;&#24452;&#12290;&#21463;&#39046;&#20808;&#31185;&#25216;&#20844;&#21496;&#22312;&#20135;&#21697;&#24320;&#21457;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23454;&#39564;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36866;&#24212;&#24615;&#23454;&#39564;&#26469;&#25345;&#32493;&#25913;&#36827;&#35838;&#31243;&#12290;&#22312;&#36866;&#24212;&#24615;&#23454;&#39564;&#20013;&#65292;&#19981;&#21516;&#30340;&#26465;&#20214;&#23558;&#34987;&#24212;&#29992;&#20110;&#23398;&#29983;&#36523;&#19978;&#65292;&#25968;&#25454;&#23558;&#34987;&#20998;&#26512;&#24182;&#29992;&#20110;&#25913;&#21464;&#26410;&#26469;&#23398;&#29983;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21738;&#20123;&#34892;&#21160;&#21487;&#20197;&#26356;&#26377;&#24076;&#26395;&#25913;&#21892;&#23398;&#29983;&#30340;&#20307;&#39564;&#25110;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#21160;&#24577;&#22320;&#23558;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#23398;&#29983;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10375</link><description>&lt;p&gt;
GTA&#65306;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#22810;&#35270;&#22270;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#23545;&#36755;&#20837;&#26631;&#35760;&#30340;&#25490;&#21015;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#23545;&#26631;&#35760;&#30340;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#23545;&#35768;&#22810;&#20219;&#21153;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#23545;&#20110;&#36890;&#24120;&#22312;&#20854;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#32467;&#26500;&#29305;&#24615;&#30340;&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#23545;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23562;&#37325;&#20854;&#24213;&#23618;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#23558;&#26631;&#35760;&#30340;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30001;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#25152;&#30830;&#23450;&#30340;&#30456;&#23545;&#21464;&#25442;&#12290;&#36890;&#36807;&#22312;&#31232;&#30095;&#23485;&#22522;&#32447;&#22810;&#35270;&#22270;&#35774;&#32622;&#20013;&#35780;&#20272;&#22810;&#20010;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#20960;&#20309;&#21464;&#25442;&#27880;&#24847;&#21147;&#65288;GTA&#65289;&#22914;&#20309;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Ring-A-Bell&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#21487;&#20197;&#20107;&#20808;&#20934;&#22791;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.10012</link><description>&lt;p&gt;
&#21709;&#38083;&#65281;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?. (arXiv:2310.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Ring-A-Bell&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#21487;&#20197;&#20107;&#20808;&#20934;&#22791;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#30340;&#25193;&#25955;(SD)&#65292;&#26368;&#36817;&#23637;&#31034;&#20986;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#28389;&#29992;&#30340;&#20960;&#20010;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21019;&#24314;&#21463;&#29256;&#26435;&#38480;&#21046;&#12289;&#31105;&#27490;&#21644;&#21463;&#38480;&#20869;&#23481;&#65292;&#25110;&#32773;&#19981;&#36866;&#23452;&#24037;&#20316;&#30340;(NSFW)&#22270;&#29255;&#26041;&#38754;&#12290;&#34429;&#28982;&#24050;&#32463;&#37319;&#21462;&#20102;&#19968;&#20123;&#25514;&#26045;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#35780;&#20272;&#38454;&#27573;&#23454;&#26045;&#23433;&#20840;&#36807;&#28388;&#22120;&#25110;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#28040;&#38500;&#19981;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#25110;&#39118;&#26684;&#65292;&#20294;&#36825;&#20123;&#23433;&#20840;&#25514;&#26045;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Ring-A-Bell&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;T2I&#25193;&#25955;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#25972;&#20010;&#35780;&#20272;&#21487;&#20197;&#22312;&#27809;&#26377;&#30446;&#26631;&#27169;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#21069;&#20934;&#22791;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Ring-A-Bell&#39318;&#20808;&#23545;&#28608;&#21169;&#36827;&#34892;&#20998;&#35299;&#65292;&#28982;&#21518;&#36890;&#36807;&#21435;&#38500;&#20505;&#36873;&#27010;&#24565;&#21644;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#30456;&#20851;&#24230;&#26469;&#35774;&#35745;&#31579;&#36873;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first pe
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;</title><link>http://arxiv.org/abs/2310.04361</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#25512;&#29702;&#26469;&#21033;&#29992;Transformer&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24120;&#38754;&#20020;&#23454;&#38469;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#26174;&#33879;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#34920;&#26126;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#21270;Transformer&#25512;&#29702;&#65288;DSTI&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#20854;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#29256;&#26412;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38477;&#20302;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#35757;&#32451;&#20986;&#25104;&#21151;&#39044;&#27979;&#27599;&#20010;&#19987;&#23478;&#30456;&#23545;&#36129;&#29486;&#30340;&#23567;&#22411;&#38376;&#25511;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#30830;&#23450;&#27599;&#20010;&#20196;&#29260;&#25191;&#34892;&#30340;&#19987;&#23478;&#25968;&#37327;&#30340;&#26426;&#21046;&#12290;DSTI&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#38024;&#23545;BERT-base&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06887</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06887
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#26159;&#31070;&#32463;&#32593;&#32476;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#22914;&#20309;&#20197;&#21450;&#20026;&#20309;&#21457;&#29983;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#38543;&#26426;&#29305;&#24449;&#33539;&#20363;&#20013;&#24494;&#19981;&#36275;&#36947;&#30340;&#29305;&#24449;&#23398;&#20064;&#30340;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#32463;&#24120;&#22320;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#20123;&#20808;&#21069;&#30340;&#20998;&#26512;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#26368;&#24120;&#35265;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.16978</link><description>&lt;p&gt;
&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26159;&#23547;&#25214;&#35206;&#30422;&#32473;&#23450;&#23553;&#38381;&#21306;&#22495;&#25972;&#20010;&#33258;&#30001;&#31354;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#26426;&#22120;&#20154;&#21106;&#33609;&#21644;&#21560;&#23576;&#21040;&#22320;&#38647;&#28165;&#38500;&#21644;&#25628;&#25937;&#20219;&#21153;&#12290;&#34429;&#28982;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#20026;&#24050;&#30693;&#29615;&#22659;&#25214;&#21040;&#21487;&#35777;&#26126;&#23436;&#22791;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#36335;&#24452;&#65292;&#20294;&#22312;&#22312;&#32447;&#22330;&#26223;&#19979;&#65292;&#29615;&#22659;&#20107;&#20808;&#26410;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38750;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#20215;&#20540;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#26500;&#24314;&#35266;&#23519;&#31354;&#38388;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#35268;&#21010;&#38271;&#26399;&#36335;&#24452;&#65292;&#24182;&#21516;&#26102;&#23545;&#30701;&#26399;&#38556;&#30861;&#29289;&#36827;&#34892;&#34892;&#21160;&#12290;&#20026;&#20102;&#32771;&#34385;&#22823;&#35268;&#27169;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#23610;&#24230;&#22320;&#22270;&#36755;&#20837;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#36335;&#24452;&#20559;&#31163;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#38469;&#25968;&#25454;&#30340;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22823;&#23398;&#20064;&#29575;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#24182;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#38544;&#24615;&#20559;&#24046;&#30340;&#20248;&#21183;&#12290;&#22312;&#32447;&#23398;&#20064;&#20013;SGD&#22122;&#38899;&#30340;&#22909;&#22788;&#21482;&#26159;&#35745;&#31639;&#19978;&#30340;&#20415;&#21033;&#65292;&#21487;&#20197;&#20419;&#36827;&#26356;&#22823;&#25110;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2306.08590</link><description>&lt;p&gt;
&#36229;&#36234;&#38544;&#24615;&#20559;&#24046;&#65306;SGD&#22122;&#22768;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#19981;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning. (arXiv:2306.08590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#38469;&#25968;&#25454;&#30340;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22823;&#23398;&#20064;&#29575;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#24182;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#38544;&#24615;&#20559;&#24046;&#30340;&#20248;&#21183;&#12290;&#22312;&#32447;&#23398;&#20064;&#20013;SGD&#22122;&#38899;&#30340;&#22909;&#22788;&#21482;&#26159;&#35745;&#31639;&#19978;&#30340;&#20415;&#21033;&#65292;&#21487;&#20197;&#20419;&#36827;&#26356;&#22823;&#25110;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;SGD&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#24402;&#22240;&#20110;&#39640;&#23398;&#20064;&#29575;&#25110;&#23567;&#25209;&#37327;&#22823;&#23567;&#25152;&#24341;&#36215;&#30340;&#38544;&#24615;&#20559;&#24046;&#65288;&#8220;SGD&#22122;&#22768;&#8221;&#65289;&#12290;&#32780;&#25105;&#20204;&#30740;&#31350;&#20102;SGD&#22122;&#22768;&#22312;&#22312;&#32447;&#65288;&#21363;&#21333;&#20010;epoch&#65289;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#30340;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22823;&#23398;&#20064;&#29575;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#24182;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#38544;&#24615;&#20559;&#24046;&#30340;&#20248;&#21183;&#12290;&#19982;&#31163;&#32447;&#23398;&#20064;&#30456;&#21453;&#65292;&#22312;&#32447;&#23398;&#20064;&#20013;SGD&#22122;&#22768;&#30340;&#22909;&#22788;&#20005;&#26684;&#26469;&#35828;&#21482;&#26159;&#35745;&#31639;&#19978;&#30340;&#20415;&#21033;&#65292;&#21487;&#20197;&#20419;&#36827;&#26356;&#22823;&#25110;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SGD&#22312;&#22312;&#32447;&#27169;&#24335;&#19979;&#21487;&#20197;&#34987;&#35270;&#20026;&#26159;&#22312;&#8220;&#26080;&#22122;&#22768;&#26799;&#24230;&#27969;&#31639;&#27861;&#8221;&#30340;&#8220;&#40644;&#37329;&#36335;&#24452;&#8221;&#19978;&#36393;&#36367;&#22024;&#26434;&#27493;&#20240;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#26399;&#38388;&#30340;SGD&#22122;&#22768;&#21644;&#27979;&#37327;&#27169;&#22411;&#20043;&#38388;&#30340;&#36880;&#28857;&#21151;&#33021;&#36317;&#31163;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#27492;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size ("SGD noise"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the "golden path" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models 
&lt;/p&gt;</description></item><item><title>Sy-CON&#26159;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23427;&#30001;&#20004;&#20010;&#25439;&#22833;&#32452;&#25104;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#25214;&#21040;&#33391;&#22909;&#30340;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05101</link><description>&lt;p&gt;
Sy-CON&#65306;&#29992;&#20110;&#25345;&#32493;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#31216;&#23545;&#27604;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning. (arXiv:2306.05101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05101
&lt;/p&gt;
&lt;p&gt;
Sy-CON&#26159;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23427;&#30001;&#20004;&#20010;&#25439;&#22833;&#32452;&#25104;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#25214;&#21040;&#33391;&#22909;&#30340;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;Symmetric Contrastive&#65288;Sy-CON&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CSSL&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#20256;&#32479;&#30340;&#25345;&#32493;&#23398;&#20064;&#25439;&#22833;&#24418;&#24335;&#30001;&#21333;&#20010;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#65288;&#29992;&#20110;&#21487;&#22609;&#24615;&#65289;&#21644;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65288;&#29992;&#20110;&#31283;&#23450;&#24615;&#65289;&#32452;&#25104;&#65292;&#23545;&#20110;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;CSSL&#26469;&#35828;&#21487;&#33021;&#19981;&#29702;&#24819;&#65292;&#22240;&#20026;&#22312;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#20250;&#36973;&#21463;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#38477;&#20302;&#30340;&#22256;&#25200;&#65292;&#32780;&#27491;&#21017;&#21270;&#22120;&#21487;&#33021;&#20250;&#38459;&#30861;&#23398;&#20064;&#26032;&#30340;&#26377;&#21306;&#21035;&#30340;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;Sy-CON&#65292;&#23427;&#30001;&#20004;&#20010;&#25439;&#22833;&#65288;&#19968;&#20010;&#29992;&#20110;&#21487;&#22609;&#24615;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#24615;&#65289;&#32452;&#25104;&#65292;&#23545;&#24403;&#21069;&#21644;&#36807;&#21435;&#27169;&#22411;&#30340;&#36127;&#26679;&#26412;&#23884;&#20837;&#20855;&#26377;&#23545;&#31216;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#28982;&#22320;&#25214;&#21040;&#33391;&#22909;&#30340;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#30340;&#22806;&#37096;&#21644;&#20869;&#37096;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;Sy-CON&#22312;&#31283;&#23450;&#24615;&#21644;&#34920;&#24449;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel and general loss function, called Symmetric Contrastive (Sy-CON) loss, for effective continual self-supervised learning (CSSL). We first argue that the conventional loss form of continual learning which consists of single task-specific loss (for plasticity) and a regularizer (for stability) may not be ideal for contrastive loss based CSSL that focus on representation learning. Our reasoning is that, in contrastive learning based methods, the task-specific loss would suffer from decreasing diversity of negative samples and the regularizer may hinder learning new distinctive representations. To that end, we propose Sy-CON that consists of two losses (one for plasticity and the other for stability) with symmetric dependence on current and past models' negative sample embeddings. We argue our model can naturally find good trade-off between the plasticity and stability without any explicit hyperparameter tuning. We validate the effectiveness of our approach through exte
&lt;/p&gt;</description></item><item><title>BOtied &#26159;&#19968;&#31181;&#24102;&#26377;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#36739;&#22909;&#30340;&#36817;&#20284;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.00344</link><description>&lt;p&gt;
BOtied: &#24102;&#26377;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
BOtied: Multi-objective Bayesian optimization with tied multivariate ranks. (arXiv:2306.00344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00344
&lt;/p&gt;
&lt;p&gt;
BOtied &#26159;&#19968;&#31181;&#24102;&#26377;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#36739;&#22909;&#30340;&#36817;&#20284;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#38656;&#35201;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#28508;&#22312;&#30340;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#12290;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270; (MOBO) &#26159;&#19968;&#31181;&#39640;&#25928;&#22320;&#35782;&#21035; Pareto &#26368;&#20248;&#35299;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#25903;&#37197;&#35299;&#21644;&#26368;&#39640;&#22810;&#20803;&#31561;&#32423;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#65292;&#23427;&#19982;&#32852;&#21512;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#26368;&#22806;&#23618;&#31561;&#39640;&#32447;&#37325;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; CDF indicator&#65292;&#36825;&#26159;&#19968;&#31181; Pareto &#21512;&#35268;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#36817;&#20284; Pareto &#38598;&#21512;&#30340;&#36136;&#37327;&#65292;&#23427;&#34917;&#20805;&#20102;&#27969;&#34892;&#30340; hypervolume indicator&#12290;MOBO &#30340;&#26680;&#24515;&#26159;&#37319;&#38598;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23548;&#33322;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#25240;&#20013;&#26469;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#35780;&#20272;&#30340;&#20505;&#36873;&#39033;&#12290; &#22522;&#20110;&#30418;&#23376;&#20998;&#35299;&#30446;&#26631;&#31354;&#38388;&#30340;&#22810;&#30446;&#26631;&#37319;&#38598;&#20989;&#25968;&#65288;&#20363;&#22914;&#26399;&#26395;&#30340; hypervolume &#25913;&#36827;&#65288;EHVI&#65289;&#21644;&#29109;&#25628;&#32034;&#65289;&#22312;&#23384;&#22312;&#22823;&#37327;&#30446;&#26631;&#26102;&#30340;&#24615;&#33021;&#32553;&#25918;&#24456;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#37319;&#38598;&#20989;&#25968;&#65292;&#31216;&#20026; BOtied&#65292;&#23427;&#21033;&#29992;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#26469;&#39640;&#25928;&#25628;&#32034; Pareto frontier&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BOtied &#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many scientific and industrial applications require joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. We show a natural connection between non-dominated solutions and the highest multivariate rank, which coincides with the outermost level line of the joint cumulative distribution function (CDF). We propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets that complements the popular hypervolume indicator. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Multi-objective acquisition functions that rely on box decomposition of the objective space, such as the expected hypervolume improvement (EHVI) and entropy search, scale poorly to a large number of objectives. We propose an acquisition function, call
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PDExplain&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#65292;&#26497;&#22823;&#22320;&#21327;&#21161;&#20102;&#24314;&#31435;&#29289;&#29702;&#31185;&#23398;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#29616;&#35937;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.15827</link><description>&lt;p&gt;
PDExplain&#65306;PDEs &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24773;&#22659;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15827
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PDExplain&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#65292;&#26497;&#22823;&#22320;&#21327;&#21161;&#20102;&#24314;&#31435;&#29289;&#29702;&#31185;&#23398;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#29616;&#35937;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;PDExplain&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#25805;&#20316;&#21592;&#23450;&#20041;&#30340;PDE&#23478;&#26063;&#30340;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#23478;&#26063;&#30340;&#19968;&#33324;&#24418;&#24335;&#36827;&#34892;&#39304;&#36865;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#29616;&#35937;&#20013;&#25910;&#38598;&#21040;&#30340;&#26368;&#23567;&#26679;&#26412;&#65292;&#20854;&#20013;&#26679;&#26412;&#19982; PDE &#23478;&#26063;&#30456;&#20851;&#65292;&#20294;&#19981;&#19968;&#23450;&#23646;&#20110;&#35757;&#32451;&#38454;&#27573;&#30475;&#21040;&#30340;&#20855;&#20307; PDE &#38598;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#27861;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;PDE&#30340;&#21487;&#35299;&#37322;&#24418;&#24335;&#65292;&#36825;&#31181;&#29305;&#24449;&#21487;&#20197;&#21327;&#21161;&#36890;&#36807;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#26469;&#23545;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32771;&#23519;&#20102;&#20854;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26032;&#20219;&#21153;DVS-GC&#24182;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#29616;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#23545;&#20107;&#20214;&#39034;&#24207;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2209.14915</link><description>&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#65306;&#29702;&#35299;&#20854;&#20248;&#21183;&#30340;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks for event-based action recognition: A new task to understand their advantage. (arXiv:2209.14915v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26032;&#20219;&#21153;DVS-GC&#24182;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#29616;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#23545;&#20107;&#20214;&#39034;&#24207;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#29420;&#29305;&#30340;&#26102;&#38388;&#21160;&#24577;&#29305;&#24615;&#65292;&#20294;&#26159;&#36825;&#31181;&#35745;&#31639;&#30340;&#24615;&#36136;&#21644;&#20248;&#21183;&#20173;&#28982;&#19981;&#22826;&#34987;&#20102;&#35299;&#12290;&#20026;&#20102;&#25552;&#20379;&#31572;&#26696;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#23454;&#29616;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#65292;&#26080;&#38656;&#36882;&#24402;&#31361;&#35302;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20256;&#32479;&#31070;&#32463;&#20803;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#20219;&#21153;DVS-Gesture-Chain&#65288;DVS-GC&#65289;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#29616;&#23454;&#20013;&#30340;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;DVS Gesture&#22522;&#20934;&#21487;&#20197;&#34987;&#19981;&#36827;&#34892;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#30340;&#32593;&#32476;&#35299;&#20915;&#65292;&#32780;&#26032;&#30340;DVS-GC&#21017;&#38656;&#35201;&#23545;&#20107;&#20214;&#30340;&#39034;&#24207;&#36827;&#34892;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNN) are characterised by their unique temporal dynamics, but the properties and advantages of such computations are still not well understood. In order to provide answers, in this work we demonstrate how Spiking neurons can enable temporal feature extraction in feed-forward neural networks without the need for recurrent synapses, showing how their bio-inspired computing principles can be successfully exploited beyond energy efficiency gains and evidencing their differences with respect to conventional neurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain (DVS-GC), which allows, for the first time, to evaluate the perception of temporal dependencies in a real event-based action recognition dataset. Our study proves how the widely used DVS Gesture benchmark could be solved by networks without temporal feature extraction, unlike the new DVS-GC which demands an understanding of the ordering of the events. Furthermore, this setup allowed us to un
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26126;&#30830;&#36981;&#23432;&#35268;&#33539;&#23545;&#31216;&#24615;&#25110;&#20219;&#24847;&#23376;&#32422;&#26463;&#65292;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#34920;&#31034;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.07243</link><description>&lt;p&gt;
&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#35268;&#33539;&#19981;&#21464;&#24615;&#21644;&#20219;&#24847;&#23376;&#23545;&#31216;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Gauge Invariant and Anyonic Symmetric Autoregressive Neural Networks for Quantum Lattice Models. (arXiv:2101.07243v3 [cond-mat.str-el] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26126;&#30830;&#36981;&#23432;&#35268;&#33539;&#23545;&#31216;&#24615;&#25110;&#20219;&#24847;&#23376;&#32422;&#26463;&#65292;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#34920;&#31034;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#65292;&#22914;&#35268;&#33539;&#19981;&#21464;&#24615;&#21644;&#20219;&#24847;&#23376;&#23545;&#31216;&#24615;&#65292;&#22312;&#37327;&#23376;&#22810;&#20307;&#29289;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#26500;&#24314;&#38024;&#23545;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#35268;&#33539;&#19981;&#21464;&#24615;&#25110;&#20219;&#24847;&#23376;&#23545;&#31216;&#24615;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324; Transformer &#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#21508;&#31181;&#26550;&#26500;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#39640;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#26126;&#30830;&#22320;&#36981;&#23432;&#35268;&#33539;&#23545;&#31216;&#24615;&#25110;&#20219;&#24847;&#23376;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#34920;&#31034;2D&#21644;3D&#25197;&#26354;&#30721;&#20197;&#21450;X-&#31435;&#26041;&#20307;&#20998;&#24418;&#27169;&#22411;&#30340;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#12290;&#25105;&#20204;&#21464;&#20998;&#20248;&#21270;&#20102;&#25105;&#20204;&#30340;&#23545;&#31216;&#24615;&#21512;&#24182;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#30340;&#22522;&#24577;&#20197;&#21450;&#23454;&#26102;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102; $\text{U(1)}$ &#32593;&#26684;&#35268;&#33539;&#29702;&#35770;&#30340;&#37327;&#23376;&#38142;&#25509;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#21644;&#22522;&#24577;&#65292;&#24471;&#21040;&#20102; 2D $\mathbb{Z}_2$ &#35268;&#33539;&#29702;&#35770;&#30340;&#30456;&#22270;&#65292;&#30830;&#23450;&#20102; $\text{SU(2)}$ &#20020;&#30028;&#38142;&#30340;&#30456;&#21464;&#21644;&#20013;&#24515;&#33655;&#65292;&#20197;&#21450;&#30740;&#31350;&#20102; 2D &#19977;&#35282;&#26230;&#26684;&#30340;&#33258;&#26059;&#27169;&#22411;&#30340;&#28909;&#21270;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries such as gauge invariance and anyonic symmetry play a crucial role in quantum many-body physics. We develop a general approach to constructing gauge invariant or anyonic symmetric autoregressive neural networks, including a wide range of architectures such as Transformer and recurrent neural network, for quantum lattice models. These networks can be efficiently sampled and explicitly obey gauge symmetries or anyonic constraint. We prove that our methods can provide exact representation for the ground and excited states of the 2D and 3D toric codes, and the X-cube fracton model. We variationally optimize our symmetry incorporated autoregressive neural networks for ground states as well as real-time dynamics for a variety of models. We simulate the dynamics and the ground states of the quantum link model of $\text{U(1)}$ lattice gauge theory, obtain the phase diagram for the 2D $\mathbb{Z}_2$ gauge theory, determine the phase transition and the central charge of the $\text{SU(2
&lt;/p&gt;</description></item></channel></rss>