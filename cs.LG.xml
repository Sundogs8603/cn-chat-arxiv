<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.14608</link><description>&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14608
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20195;&#34920;&#20102;&#19968;&#39033;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#31354;&#21069;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#25191;&#34892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20026;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#22823;&#22411;&#27169;&#22411;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#38480;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#65292;&#35268;&#27169;&#24222;&#22823;&#21644;&#35745;&#31639;&#35201;&#27714;&#24040;&#22823;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25972;&#22823;&#22411;&#27169;&#22411;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PEFT&#26159;&#25351;&#35843;&#25972;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#21518;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#21453;&#39304;&#29615;&#36335;&#30340;&#30896;&#25758;&#36991;&#20813;&#23646;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#35745;&#31639;&#30456;&#23545;&#21453;&#25237;&#24433;&#38598;&#65292;&#24182;&#19988;&#35813;&#36880;&#23545;&#26041;&#27861;&#21487;&#24182;&#34892;&#21270;&#65292;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;</title><link>https://arxiv.org/abs/2403.03314</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30896;&#25758;&#36991;&#20813;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Collision Avoidance Verification of Multiagent Systems with Learned Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#21518;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#21453;&#39304;&#29615;&#36335;&#30340;&#30896;&#25758;&#36991;&#20813;&#23646;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#35745;&#31639;&#30456;&#23545;&#21453;&#25237;&#24433;&#38598;&#65292;&#24182;&#19988;&#35813;&#36880;&#23545;&#26041;&#27861;&#21487;&#24182;&#34892;&#21270;&#65292;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#38382;&#39064;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#31995;&#32479;&#32570;&#20047;&#27491;&#24335;&#30340;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#30896;&#25758;&#36991;&#20813;&#12289;&#40065;&#26834;&#24615;&#65289;&#65292;&#36825;&#38459;&#27490;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#37492;&#20110;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#22788;&#29702;&#22810;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#24773;&#20917;&#65292;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#21453;&#39304;&#29615;&#36335;&#65288;MA-NFLs&#65289;&#30340;&#30896;&#25758;&#36991;&#20813;&#23646;&#24615;&#30340;&#22522;&#20110;&#21521;&#21518;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;&#23450;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#35757;&#32451;&#25511;&#21046;&#31574;&#30053;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#20026;&#27599;&#23545;&#26234;&#33021;&#20307;&#35299;&#20915;&#19968;&#31995;&#21015;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILPs&#65289;&#26469;&#35745;&#31639;&#30456;&#23545;&#21453;&#25237;&#24433;&#38598;&#12290;&#25105;&#20204;&#30340;&#36880;&#23545;&#26041;&#27861;&#21487;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#21487;&#20197;&#24456;&#22909;&#22320;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#19988;&#25105;&#20204;&#32771;&#34385;&#20102;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03314v1 Announce Type: cross  Abstract: For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities. However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings. While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent. To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs). Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our pair-wise approach is parallelizable and thus scales well with increasing number of agents, and we account for state 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00642</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rethinking The Uniformity Metric in Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#21248;&#24615;&#22312;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20043;&#21069;&#30340;&#19968;&#39033;&#24320;&#21019;&#24615;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#23450;&#37327;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#23849;&#28291;&#31243;&#24230;&#12290;&#30452;&#25509;&#20248;&#21270;&#36825;&#19968;&#24230;&#37327;&#19982;&#23545;&#40784;&#19968;&#36215;&#65292;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#19981;&#26029;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#36825;&#19968;&#24230;&#37327;&#32570;&#20047;&#23545;&#32500;&#24230;&#23849;&#28291;&#30340;&#25935;&#24863;&#24615;&#65292;&#20984;&#26174;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#24182;&#35774;&#35745;&#19968;&#20010;&#26356;&#26377;&#25928;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#28385;&#36275;&#20854;&#20013;&#30340;&#19968;&#20123;&#12290;&#25105;&#20204;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#26399;&#26395;&#65292;&#24182;&#19988;&#23545;&#32500;&#24230;&#23849;&#28291;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00642v1 Announce Type: cross  Abstract: Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various 
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12237</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#25512;&#36831;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer in Content Moderation: The Human-AI Interplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#22312;&#32447;&#24179;&#21488;&#20869;&#23481;&#23457;&#26680;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31639;&#27861;&#35266;&#23519;&#21040;&#21363;&#23558;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20570;&#20986;&#20998;&#31867;&#21644;&#20934;&#20837;&#20915;&#31574;&#65292;&#24182;&#23433;&#25490;&#24086;&#23376;&#36827;&#34892;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#65292;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08522</link><description>&lt;p&gt;
&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fairness Auditing with Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#65292;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#24037;&#20316;&#20551;&#35774;&#20195;&#29702;&#20154;&#29420;&#31435;&#25805;&#20316;&#12290;&#26412;&#25991;&#32771;&#34385;&#22810;&#20010;&#20195;&#29702;&#20154;&#23545;&#21516;&#19968;&#24179;&#21488;&#36827;&#34892;&#19981;&#21516;&#20219;&#21153;&#30340;&#23457;&#35745;&#24773;&#20917;&#12290;&#20195;&#29702;&#20154;&#26377;&#20004;&#20010;&#26464;&#26438;&#65306;&#21327;&#20316;&#31574;&#30053;&#65288;&#26159;&#21542;&#36827;&#34892;&#21327;&#35843;&#65289;&#21644;&#25277;&#26679;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20195;&#29702;&#20154;&#29420;&#31435;&#25805;&#20316;&#25110;&#21327;&#20316;&#26102;&#23545;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#30340;&#23457;&#35745;&#20934;&#30830;&#24615;&#19982;&#21327;&#20316;&#30340;&#26368;&#20248;&#25277;&#26679;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;WyScout&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#29992;&#20110;&#26500;&#24314;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.06820</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Forecasting Events in Soccer Matches Through Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;WyScout&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#29992;&#20110;&#26500;&#24314;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#38382;&#39064;&#38750;&#24120;&#30456;&#20284;&#30340;&#25361;&#25112;&#12290;&#19982;&#20854;&#20182;&#20005;&#37325;&#38480;&#21046;&#36275;&#29699;&#20107;&#20214;&#21160;&#24577;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20174;&#24456;&#22810;&#21464;&#37327;&#20013;&#25277;&#35937;&#20986;&#26469;&#25110;&#20381;&#36182;&#20110;&#28151;&#21512;&#39034;&#24207;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;LLMs&#26041;&#27861;&#23398;&#21551;&#21457;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#20102;&#32452;&#25104;&#19968;&#20010;&#20107;&#20214;&#30340;&#23436;&#25972;&#21464;&#37327;&#38142;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#26500;&#24314;&#36275;&#29699;&#22823;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#30340;&#36807;&#31243;&#12290;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#19979;&#19968;&#20010;&#20107;&#20214;&#31867;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65289;&#26174;&#33879;&#36229;&#36234;&#20102;&#20197;&#24448;LEM&#25552;&#26696;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#31361;&#26174;&#20102;LEM&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LEM&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#35768;&#22810;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be bu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#24212;&#29992;&#20110;&#36275;&#29699;&#20998;&#26512;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#8220;&#36275;&#29699;&#35821;&#35328;&#8221;&#65292;LEMs&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20107;&#20214;&#30340;&#21464;&#37327;&#65292;&#20174;&#32780;&#27169;&#25311;&#27604;&#36187;&#24182;&#39044;&#27979;&#29699;&#21592;&#22312;&#19981;&#21516;&#22242;&#38431;&#32972;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;2017-2018&#33521;&#36229;&#36187;&#23395;&#20351;&#29992;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;LEMs&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#21457;&#29616;&#20102;LEMs&#22312;&#36275;&#29699;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#31361;&#20986;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29699;&#38431;&#25490;&#21517;&#21644;&#25506;&#32034;&#39640;&#32423;&#22330;&#26223;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06815</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#20272;&#35745;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#29699;&#21592;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#24212;&#29992;&#20110;&#36275;&#29699;&#20998;&#26512;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#8220;&#36275;&#29699;&#35821;&#35328;&#8221;&#65292;LEMs&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20107;&#20214;&#30340;&#21464;&#37327;&#65292;&#20174;&#32780;&#27169;&#25311;&#27604;&#36187;&#24182;&#39044;&#27979;&#29699;&#21592;&#22312;&#19981;&#21516;&#22242;&#38431;&#32972;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;2017-2018&#33521;&#36229;&#36187;&#23395;&#20351;&#29992;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;LEMs&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#21457;&#29616;&#20102;LEMs&#22312;&#36275;&#29699;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#31361;&#20986;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29699;&#38431;&#25490;&#21517;&#21644;&#25506;&#32034;&#39640;&#32423;&#22330;&#26223;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#22312;&#36275;&#29699;&#20998;&#26512;&#39046;&#22495;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23398;&#20064;&#36275;&#29699;&#30340;&#8220;&#35821;&#35328;&#8221; - &#39044;&#27979;&#21518;&#32493;&#20107;&#20214;&#30340;&#21464;&#37327;&#32780;&#19981;&#26159;&#21333;&#35789;&#65292;LEMs&#21487;&#20197;&#27169;&#25311;&#27604;&#36187;&#24182;&#25552;&#20379;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#19981;&#21516;&#22242;&#38431;&#32972;&#26223;&#19979;&#30340;&#29699;&#21592;&#34920;&#29616;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;WyScout&#25968;&#25454;&#38598;&#23545;2017-2018&#33521;&#36229;&#36187;&#23395;&#36827;&#34892;LEMs&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#20197;&#33719;&#21462;&#20851;&#20110;&#29699;&#21592;&#36129;&#29486;&#21644;&#22242;&#38431;&#25112;&#30053;&#30340;&#20855;&#20307;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#21453;&#26144;&#36275;&#29699;&#30340;&#24494;&#22937;&#21160;&#24577;&#65292;&#20174;&#32780;&#35780;&#20272;&#20551;&#35774;&#30340;&#36716;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;LEMs&#22312;&#36275;&#29699;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#21644;&#23616;&#38480;&#24615;&#65292;&#31361;&#26174;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#29699;&#38431;&#39044;&#26399;&#25490;&#21517;&#24182;&#25506;&#32034;&#39640;&#32423;&#22330;&#26223;&#65288;&#20363;&#22914;&#23558;Cristiano Ronaldo&#25110;Lionel Messi&#36716;&#20250;&#33267;&#19981;&#21516;&#29699;&#38431;&#30340;&#28508;&#22312;&#24433;&#21709;&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative application of Large Event Models (LEMs), akin to Large Language Models, to the domain of soccer analytics. By learning the "language" of soccer - predicting variables for subsequent events rather than words LEMs facilitate the simulation of matches and offer various applications, including player performance prediction across different team contexts. We focus on fine-tuning LEMs with the WyScout dataset for the 2017-2018 Premier League season to derive specific insights into player contributions and team strategies. Our methodology involves adapting these models to reflect the nuanced dynamics of soccer, enabling the evaluation of hypothetical transfers. Our findings confirm the effectiveness and limitations of LEMs in soccer analytics, highlighting the model's capability to forecast teams' expected standings and explore high-profile scenarios, such as the potential effects of transferring Cristiano Ronaldo or Lionel Messi to different teams in the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05677</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Batched Low-Rank Adaptation of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05677
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22240;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#24182;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;LoRA&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#20854;&#22312;&#23454;&#26102;&#20026;&#21508;&#31181;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#22810;&#20010;&#29305;&#23450;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#20256;&#20837;&#35831;&#27714;&#20010;&#24615;&#21270;&#12289;&#29305;&#23450;&#20219;&#21153;&#36866;&#24212;&#30340;&#22330;&#26223;&#20013;&#36896;&#25104;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#22788;&#29702;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#21487;&#20197;&#19982;&#20854;&#29420;&#29305;&#30340;&#20302;&#31209;&#36866;&#24212;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#26500;&#35831;&#27714;&#30340;&#39640;&#25928;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#34920;&#26126;&#65292;FLoRA&#20445;&#30041;&#20102;LoRA&#30340;&#32489;&#25928;&#20248;&#28857;&#65292;&#22312;&#36328;&#36234;8&#31181;&#35821;&#35328;&#30340;MultiPL-E&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#31454;&#20105;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
&lt;/p&gt;</description></item><item><title>ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;</title><link>https://arxiv.org/abs/2310.20431</link><description>&lt;p&gt;
&#25552;&#21319;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#30340;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Raising the ClaSS of Streaming Time Series Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20431
&lt;/p&gt;
&lt;p&gt;
ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#65292;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#21457;&#23556;&#39640;&#39057;&#25968;&#20540;&#27979;&#37327;&#27969;&#65292;&#21453;&#26144;&#20102;&#20154;&#31867;&#12289;&#21160;&#29289;&#12289;&#24037;&#19994;&#12289;&#21830;&#19994;&#21644;&#33258;&#28982;&#36807;&#31243;&#30340;&#29305;&#24615;&#12290;&#36825;&#20123;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#20363;&#22914;&#30001;&#22806;&#37096;&#20107;&#20214;&#25110;&#20869;&#37096;&#29366;&#24577;&#21464;&#21270;&#24341;&#36215;&#30340;&#65292;&#20250;&#34920;&#29616;&#20026;&#35760;&#24405;&#20449;&#21495;&#20013;&#30340;&#21464;&#21270;&#12290;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#65288;STSS&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#27969;&#20998;&#21106;&#20026;&#23545;&#24212;&#20110;&#25152;&#35266;&#23519;&#30340;&#36807;&#31243;&#25110;&#23454;&#20307;&#29366;&#24577;&#30340;&#36830;&#32493;&#21487;&#21464;&#22823;&#23567;&#30340;&#20998;&#27573;&#12290;&#20998;&#21106;&#25805;&#20316;&#26412;&#36523;&#24517;&#39035;&#33021;&#22815;&#24212;&#23545;&#36755;&#20837;&#20449;&#21495;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ClaSS&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;STSS&#31639;&#27861;&#12290;ClaSS&#20351;&#29992;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#28508;&#22312;&#20998;&#21106;&#30340;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#65288;CPs&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#35780;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22823;&#22411;&#22522;&#20934;&#21644;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26723;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20431v2 Announce Type: replace-cross  Abstract: Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2310.00724</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;:&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subtractive Mixture Models via Squaring: Representation and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00724
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#20256;&#32479;&#19978;&#26159;&#36890;&#36807;&#23558;&#20960;&#20010;&#20998;&#24067;&#20316;&#20026;&#32452;&#20214;&#30456;&#21152;&#26469;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#12290;&#20801;&#35768;&#28151;&#21512;&#20943;&#21435;&#27010;&#29575;&#36136;&#37327;&#25110;&#23494;&#24230;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#24314;&#27169;&#22797;&#26434;&#20998;&#24067;&#25152;&#38656;&#30340;&#32452;&#20214;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#31181;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#24182;&#30830;&#20445;&#23427;&#20204;&#20173;&#28982;&#32534;&#30721;&#38750;&#36127;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24179;&#26041;&#26469;&#23398;&#20064;&#21644;&#25191;&#34892;&#28145;&#24230;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#30005;&#36335;&#26694;&#26550;&#20013;&#36827;&#34892;&#36825;&#20123;&#30740;&#31350;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#24352;&#37327;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#24182;&#27867;&#21270;&#20854;&#20182;&#20943;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20801;&#35768;&#20943;&#27861;&#30340;&#24179;&#26041;&#30005;&#36335;&#31867;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#20855;&#34920;&#36798;&#21147;&#65307;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#19978;&#23454;&#35777;&#23637;&#31034;&#20102;&#36825;&#31181;&#22686;&#21152;&#30340;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#21453;&#28436;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#25311;&#22120;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#19981;&#21487;&#24494;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13695</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#39063;&#31890;&#27969;&#30340;&#21453;&#28436;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inverse analysis of granular flows using differentiable graph neural network simulator. (arXiv:2401.13695v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#21453;&#28436;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#25311;&#22120;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#19981;&#21487;&#24494;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39063;&#31890;&#27969;&#20013;&#30340;&#21453;&#28436;&#38382;&#39064;&#65292;&#22914;&#23665;&#20307;&#28369;&#22369;&#21644;&#30862;&#23633;&#27969;&#65292;&#28041;&#21450;&#22522;&#20110;&#30446;&#26631;&#27874;&#21160;&#21078;&#38754;&#20272;&#35745;&#26448;&#26009;&#21442;&#25968;&#25110;&#36793;&#30028;&#26465;&#20214;&#12290;&#20256;&#32479;&#30340;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#23545;&#36825;&#20123;&#21453;&#28436;&#38382;&#39064;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#38480;&#21046;&#20102;&#21487;&#33021;&#30340;&#27169;&#25311;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#19981;&#21487;&#24494;&#24615;&#20351;&#24471;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#20197;&#20854;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#24494;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#20302;&#32500;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#39063;&#31890;&#27969;&#30340;&#23436;&#25972;&#29289;&#29702;&#36807;&#31243;&#65292;&#22240;&#27492;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;(GNS)&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#28436;&#38382;&#39064;&#12290;GNS&#23398;&#20064;&#20102;&#39063;&#31890;&#27969;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granula
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.13555</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13555
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#21512;&#25104;&#23186;&#20307;&#65288;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#35825;&#20154;&#65292;&#20294;&#35780;&#20272;&#20854;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#21463;&#30417;&#30563;&#20844;&#24179;&#24615;&#30340;&#28789;&#24863;&#26469;&#28304;&#8212;&#8212;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#36825;&#20010;&#29305;&#23450;&#24212;&#29992;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#29616;&#20195;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UnfairFace&#65292;&#36825;&#26159;FairFace&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22797;&#21046;&#20102;&#24120;&#35265;&#22823;&#35268;&#27169;&#20154;&#33080;&#25968;&#25454;&#38598;&#30340;&#31181;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01373</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01373
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#26816;&#27979;&#26159;&#21046;&#36896;&#19994;&#36136;&#37327;&#25511;&#21046;&#38454;&#27573;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#65292;&#24182;&#22312;&#32599;&#20271;&#29305;&#183;&#21338;&#19990;&#21046;&#36896;&#21378;&#29983;&#20135;&#30340;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#32452;&#20214;&#30340;&#30495;&#23454;&#32570;&#38519;&#26816;&#27979;&#24212;&#29992;&#20013;&#32771;&#23519;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;T-CNN&#22312;&#20943;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#19978;&#36816;&#34892;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31561;&#25928;CNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;T-CNN&#21487;&#20197;&#36890;&#36807;&#36136;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#19982;&#20256;&#32479;CNN&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#21482;&#26377;&#20854;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;T-CNN&#22823;&#22823;&#36229;&#36234;&#20102;&#20256;&#32479;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30340;&#32467;&#26524;&#65292;&#22312;&#24403;&#21069;&#21046;&#36896;&#24212;&#29992;&#20013;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4% to 19% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.02246</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#26088;&#22312;&#20174;&#35266;&#27979;&#20013;&#30830;&#23450;&#21442;&#25968;&#65292;&#36825;&#26159;&#24037;&#31243;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#33391;&#22909;&#30340;&#25968;&#23398;&#29305;&#24615;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#23545;&#26041;&#24046;&#35843;&#24230;&#30340;&#36873;&#25321;&#25935;&#24863;&#65292;&#35813;&#35843;&#24230;&#25511;&#21046;&#30528;&#25193;&#25955;&#36807;&#31243;&#30340;&#21160;&#24577;&#12290;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#24494;&#35843;&#36825;&#20010;&#35843;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#26102;&#38388;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#23545;&#25968;&#25454;&#30340;&#27010;&#29575;&#26465;&#20214;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#24320;&#38144;&#19979;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#30456;&#20851;&#30340;&#36870;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#21644;&#23450;&#37327;&#30456;&#20301;&#25104;&#20687;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#36739;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.07550</link><description>&lt;p&gt;
Tabdoor&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07550
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#19982;&#36825;&#20123;&#21457;&#23637;&#21516;&#26102;&#65292;&#19982;DNN&#35757;&#32451;&#30456;&#20851;&#30340;&#28431;&#27934;&#65292;&#22914;&#21518;&#38376;&#25915;&#20987;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24494;&#22937;&#22320;&#25554;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20801;&#35768;&#25805;&#32437;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;DNNs&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#36716;&#25442;&#22120;&#12290;&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#23884;&#20837;&#21518;&#38376;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#20351;&#21482;&#26377;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;XGBoost&#21644;DeepFM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20960;&#20046;&#34920;&#26126;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions.More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models.  Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, particularly focusing on transformers. Given the inherent complexities of tabular data, we explore the challenges of embedding backdoors. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results indicate nearly perfect attac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLoad&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22635;&#20805;&#37327;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10879</link><description>&lt;p&gt;
BLoad&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#39640;&#25928;&#39034;&#24207;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling. (arXiv:2310.10879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLoad&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22635;&#20805;&#37327;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#21644;&#25968;&#25454;&#38598;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#24320;&#21457;&#20248;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#26412;&#30333;&#30382;&#20070;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25928;&#24615;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#24207;&#21015;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#20013;&#23454;&#29616;&#39640;&#25928;&#22788;&#29702;&#65292;&#21516;&#26102;&#36824;&#33021;&#26368;&#23567;&#21270;&#39069;&#22806;&#24320;&#38144;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#26041;&#26696;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#22635;&#20805;&#37327;&#20943;&#23569;&#36229;&#36807;100&#20493;&#65292;&#21516;&#26102;&#19981;&#21024;&#38500;&#20219;&#20309;&#24103;&#65292;&#20174;&#32780;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24635;&#20307;&#19978;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.
&lt;/p&gt;</description></item><item><title>ZeroSwap &#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340; DeFi &#24066;&#22330;&#20570;&#24066;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#24066;&#22330;&#20570;&#24066;&#21830;&#38646;&#21033;&#28070;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24212;&#20132;&#26131;&#32773;&#34892;&#20026;&#26469;&#35299;&#20915;&#20102;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09413</link><description>&lt;p&gt;
ZeroSwap: &#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; DeFi &#20013;&#30340;&#26368;&#20248;&#24066;&#22330;&#20570;&#24066;
&lt;/p&gt;
&lt;p&gt;
ZeroSwap: Data-driven Optimal Market Making in DeFi. (arXiv:2310.09413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09413
&lt;/p&gt;
&lt;p&gt;
ZeroSwap &#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340; DeFi &#24066;&#22330;&#20570;&#24066;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#24066;&#22330;&#20570;&#24066;&#21830;&#38646;&#21033;&#28070;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24212;&#20132;&#26131;&#32773;&#34892;&#20026;&#26469;&#35299;&#20915;&#20102;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20570;&#24066;&#21830; (AMMs) &#26159;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#20013;&#21305;&#37197;&#27969;&#21160;&#24615;&#20379;&#32473;&#21644;&#38656;&#27714;&#30340;&#20027;&#35201;&#20013;&#24515;&#12290;&#23427;&#20204;&#30340;&#21151;&#33021;&#20027;&#35201;&#20381;&#36182;&#20110;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773; (LPs) &#23558;&#20854;&#36164;&#20135;&#25237;&#36164;&#20110;&#27969;&#21160;&#24615;&#27744;&#12290;&#28982;&#32780;&#65292;&#27744;&#20013;&#36164;&#20135;&#20132;&#26131;&#30340;&#20215;&#26684;&#36890;&#24120;&#27604;&#38598;&#20013;&#21270;&#21644;&#26356;&#27969;&#21160;&#30340;&#20132;&#26131;&#25152;&#20215;&#26684;&#24310;&#36831;&#26356;&#22810;&#12290;&#36825;&#23548;&#33268;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992; Glosten &#21644; Milgrom &#30340;&#32463;&#20856;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#27169;&#22411;&#65292;&#23558;&#24066;&#22330;&#20215;&#26684;&#36866;&#24212;&#20110;&#20132;&#26131;&#32773;&#34892;&#20026;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26368;&#20248;&#36125;&#21494;&#26031;&#21644;&#31532;&#19968;&#20010;&#26080;&#27169;&#22411;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#26469;&#26368;&#20248;&#22320;&#36319;&#36394;&#36164;&#20135;&#30340;&#22806;&#37096;&#20215;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26368;&#20248;&#24615;&#27010;&#24565;&#22312;&#24066;&#22330;&#20570;&#24066;&#21830;&#30340;&#20215;&#26684;&#19978;&#24378;&#21046;&#25191;&#34892;&#20102;&#38646;&#21033;&#28070;&#26465;&#20214;&#65292;&#22240;&#27492;&#21462;&#21517;&#20026; ZeroSwap&#12290;&#36825;&#30830;&#20445;&#20102;&#24066;&#22330;&#20570;&#24066;&#21830;&#22312;&#25439;&#22833;&#30693;&#24773;&#20132;&#26131;&#32773;&#30340;&#21516;&#26102;&#20174;&#22122;&#22768;&#20132;&#26131;&#32773;&#37027;&#37324;&#33719;&#24471;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Market Makers (AMMs) are major centers of matching liquidity supply and demand in Decentralized Finance. Their functioning relies primarily on the presence of liquidity providers (LPs) incentivized to invest their assets into a liquidity pool. However, the prices at which a pooled asset is traded is often more stale than the prices on centralized and more liquid exchanges. This leads to the LPs suffering losses to arbitrage. This problem is addressed by adapting market prices to trader behavior, captured via the classical market microstructure model of Glosten and Milgrom. In this paper, we propose the first optimal Bayesian and the first model-free data-driven algorithm to optimally track the external price of the asset. The notion of optimality that we use enforces a zero-profit condition on the prices of the market maker, hence the name ZeroSwap. This ensures that the market maker balances losses to informed traders with profits from noise traders. The key property of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01717</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#30340;&#38598;&#25104;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#23558;&#21477;&#23376;&#30340;&#35789;&#21644;&#30701;&#35821;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#19981;&#20351;&#29992;&#35821;&#35328;&#23398;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#25429;&#25417;&#21040;&#20102;&#35299;&#26512;&#32467;&#26500;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26641;&#24179;&#22343;&#8221;&#30340;&#27010;&#24565;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65307;&#36825;&#31181;&#38598;&#25104;-&#33976;&#39311;&#30340;&#36807;&#31243;&#26159;&#32531;&#35299;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#34920;&#29616;&#20986;&#20854;&#22312;&#19981;&#21516;&#38598;&#25104;&#32452;&#20214;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.05855</link><description>&lt;p&gt;
&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#33021;&#37327;&#20445;&#25345;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#24418;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#20026;&#20160;&#20040;&#22914;&#27492;&#22256;&#38590;&#65311;&#23613;&#31649;&#26377;&#22810;&#27425;&#23581;&#35797;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(convnets)&#36827;&#34892;&#28388;&#27874;&#22120;&#35774;&#35745;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#36229;&#36234;&#25163;&#24037;&#21019;&#24314;&#30340;&#22522;&#32447;&#12290;&#36825;&#26356;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#32447;&#26159;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65306;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#20256;&#36882;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#22823;&#24863;&#21463;&#37326;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#36825;&#22312;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#26159;&#20856;&#22411;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#26399;&#26395;&#33021;&#37327;&#20445;&#25345;&#23545;&#20110;&#25968;&#20540;&#31283;&#23450;&#24615;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#30340;&#20808;&#36827;&#21644;&#24120;&#35268;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#28151;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#35299;&#28151;&#36719;&#20214;&#21253;HySUPP&#12290;</title><link>http://arxiv.org/abs/2308.09375</link><description>&lt;p&gt;
&#22270;&#20687;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#39640;&#20809;&#35889;&#35299;&#28151;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#27010;&#36848;&#21644;HySUPP Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package. (arXiv:2308.09375v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#30340;&#20808;&#36827;&#21644;&#24120;&#35268;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#28151;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#35299;&#28151;&#36719;&#20214;&#21253;HySUPP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39640;&#20809;&#35889;&#20256;&#24863;&#22120;&#30340;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;&#21452;&#37325;&#25955;&#23556;&#21644;&#22330;&#26223;&#20013;&#26448;&#26009;&#30340;&#28151;&#21512;&#29616;&#35937;&#65292;&#20809;&#35889;&#20687;&#32032;&#24448;&#24448;&#26159;&#26448;&#26009;&#30340;&#32431;&#20809;&#35889;&#25104;&#20998;&#30340;&#28151;&#21512;&#29289;&#65292;&#34987;&#31216;&#20026;&#31471;&#20803;&#12290;&#35299;&#28151;&#21363;&#20272;&#35745;&#20687;&#32032;&#28857;&#20013;&#21508;&#31471;&#20803;&#30340;&#27604;&#20363;&#12290;&#26681;&#25454;&#31471;&#20803;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32447;&#24615;&#35299;&#28151;&#21487;&#20998;&#20026;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#65288;&#30450;&#35299;&#28151;&#65289;&#19977;&#22823;&#31867;&#12290;&#22270;&#20687;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#23545;&#35299;&#28151;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20808;&#36827;&#21644;&#24120;&#35268;&#30340;&#35299;&#28151;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#19977;&#20010;&#31867;&#21035;&#20013;&#20808;&#36827;&#21644;&#24120;&#35268;&#25216;&#26415;&#36827;&#34892;&#20102;&#37325;&#35201;&#23545;&#27604;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35299;&#28151;&#25216;&#26415;&#22312;&#19977;&#20010;&#27169;&#25311;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;&#19981;&#21516;&#35299;&#28151;&#22330;&#26223;&#26469;&#35828;&#65292;&#19981;&#21516;&#35299;&#28151;&#31867;&#21035;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36719;&#20214;&#21253;&#8212;&#8212;HySUPP&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#28608;&#27963;&#23618;&#33021;&#22815;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#28382;&#21518;&#32467;&#26500;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#30340;&#20960;&#23618;&#20013;&#36880;&#28176;&#20007;&#22833;&#36825;&#20123;&#20449;&#24687;&#65292;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#21464;&#24046;&#65292;&#21516;&#26102;&#28608;&#27963;&#23618;&#20063;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#31227;&#21160;&#24179;&#22343;&#21644;&#24322;&#26041;&#24046;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.15830</link><description>&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#26469;&#21051;&#30011;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting. (arXiv:2307.15830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#28608;&#27963;&#23618;&#33021;&#22815;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#28382;&#21518;&#32467;&#26500;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#30340;&#20960;&#23618;&#20013;&#36880;&#28176;&#20007;&#22833;&#36825;&#20123;&#20449;&#24687;&#65292;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#21464;&#24046;&#65292;&#21516;&#26102;&#28608;&#27963;&#23618;&#20063;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#31227;&#21160;&#24179;&#22343;&#21644;&#24322;&#26041;&#24046;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#20316;&#20026;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#31181;&#24120;&#29992;&#27169;&#22411;&#20043;&#19968;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#20851;&#20110;RNNs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#24046;&#24322;&#32570;&#20047;&#28145;&#20837;&#27934;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36317;&#31163;&#30456;&#20851;&#24615;&#36825;&#19968;&#22810;&#21151;&#33021;&#25351;&#26631;&#26469;&#23558;&#26102;&#38388;&#24207;&#21015;&#30340;&#29305;&#24449;&#19982;RNNs&#30340;&#32452;&#25104;&#37096;&#20998;&#32852;&#31995;&#36215;&#26469;&#30340;&#26041;&#27861;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;RNN&#28608;&#27963;&#23618;&#30340;&#20449;&#24687;&#27969;&#26469;&#35299;&#37322;&#21644;&#35828;&#26126;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;RNN&#30340;&#28608;&#27963;&#23618;&#33021;&#22815;&#24456;&#22909;&#22320;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#28382;&#21518;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#30340;&#20960;&#23618;&#20013;&#65292;&#23427;&#20204;&#36880;&#28176;&#20007;&#22833;&#20102;&#36825;&#20123;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#20855;&#26377;&#22823;&#28382;&#21518;&#32467;&#26500;&#30340;&#24207;&#21015;&#30340;&#39044;&#27979;&#36136;&#37327;&#21464;&#24046;&#12290;&#25105;&#20204;&#36824;&#26174;&#31034;&#65292;&#28608;&#27963;&#23618;&#19981;&#33021;&#20805;&#20998;&#24314;&#27169;&#31227;&#21160;&#24179;&#22343;&#21644;&#24322;&#26041;&#24046;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#32531;&#23384;&#12289;&#21327;&#20316;&#21368;&#36733;&#21644;&#35745;&#31639;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#26469;&#25552;&#39640;MEC&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#26381;&#21153;&#36136;&#37327;&#24182;&#38477;&#20302;&#32531;&#23384;&#20999;&#25442;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.09691</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#24335;MEC&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#26381;&#21153;&#32531;&#23384;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65306;&#22522;&#20110;DRL&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach. (arXiv:2307.09691v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#32531;&#23384;&#12289;&#21327;&#20316;&#21368;&#36733;&#21644;&#35745;&#31639;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#26469;&#25552;&#39640;MEC&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#26381;&#21153;&#36136;&#37327;&#24182;&#38477;&#20302;&#32531;&#23384;&#20999;&#25442;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#32500;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#28385;&#36275;&#32456;&#31471;&#30340;&#20005;&#26684;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#23545;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#24335;MEC&#26694;&#26550;&#65292;&#20419;&#36827;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#32531;&#23384;&#12289;&#21327;&#20316;&#21368;&#36733;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#22823;&#21270;&#38271;&#26399;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#38477;&#20302;&#32531;&#23384;&#20999;&#25442;&#25104;&#26412;&#12290;&#26381;&#21153;&#32531;&#23384;&#21644;&#20854;&#20182;&#36164;&#28304;&#20998;&#37197;&#20043;&#38388;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#29305;&#24615;&#21644;&#26102;&#38388;&#22238;&#24402;&#20851;&#31995;&#20351;&#35299;&#20915;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#26696;&#65292;&#31216;&#20026;DGL-DDPG&#65292;&#23427;&#30001;&#30701;&#26399;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#21644;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;LSTM-DDPG&#65289;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.09055</link><description>&lt;p&gt;
&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24352;&#37327;&#25968;&#25454;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#20540;&#25110;&#26679;&#26412;&#29305;&#23450;&#30340;&#27745;&#26579;&#12290;&#22914;&#20309;&#24674;&#22797;&#34987;&#24322;&#24120;&#20540;&#25439;&#22351;&#30340;&#24352;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#24352;&#37327;&#25968;&#25454;&#32858;&#31867;&#30340;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#65288;OR-TLRR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#24341;&#36215;&#30340;&#24352;&#37327;&#24352;&#37327;&#31215;&#30340;&#21551;&#21457;&#12290;&#23545;&#20110;&#24102;&#26377;&#20219;&#24847;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#24352;&#37327;&#35266;&#27979;&#65292;OR-TLRR&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#33021;&#22815;&#30830;&#20999;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#24182;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;OR-TLRR&#30340;&#25193;&#23637;&#26469;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05272</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36895;&#29575;&#38477;&#20302;&#21407;&#21017;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24050;&#32463;&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#20294;&#26159;&#32858;&#31867;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992; CLIP &#31561;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#29305;&#24449;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#65292;&#26356;&#20855;&#26377;&#32467;&#26500;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#20174; ImageNet-1k &#30340; 57&#65285;&#25552;&#39640;&#21040; 66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;&#20309;&#23548;&#33268;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#26631;&#35760;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914; MS-COCO &#21644; LAION-Aesthetics&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item><item><title>IMAP&#26159;&#19968;&#20010;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#65292;&#26080;&#38656;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02605</link><description>&lt;p&gt;
IMAP: &#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02605
&lt;/p&gt;
&lt;p&gt;
IMAP&#26159;&#19968;&#20010;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#65292;&#26080;&#38656;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#23481;&#26131;&#21463;&#21040;&#35268;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23545;&#31574;&#30053;&#25110;&#20540;&#32593;&#32476;&#30340;&#36755;&#20837;&#25110;&#36755;&#20986;&#27880;&#20837;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65307;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#23545;&#25163;&#38388;&#25509;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#12290; &#23545;&#25239;&#24615;&#31574;&#30053;&#20026;&#35299;&#20915;&#27492;&#31867;&#25915;&#20987;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#21463;&#23475;&#32773;&#25919;&#31574;&#30340;&#23436;&#32654;&#25110;&#37096;&#20998;&#30693;&#35782;&#65292;&#35201;&#20040;&#30001;&#20110;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#32780;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#25919;&#31574;&#65288;IMAP&#65289;&#65292;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#39640;&#25928;&#30340;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#20219;&#20309;&#20851;&#20110;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#30693;&#35782;&#12290; IMAP&#21033;&#29992;&#22522;&#20110;&#29366;&#24577;&#35206;&#30422;&#29575;&#65292;&#31574;&#30053;&#35206;&#30422;&#29575;&#65292;&#39118;&#38505;&#21644;&#25919;&#31574;&#20998;&#27495;&#30340;&#22235;&#20010;&#20869;&#22312;&#30446;&#26631;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#24182;&#21457;&#29616;&#26356;&#24378;&#30340;&#25915;&#20987;&#25216;&#33021;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#23454;&#21147;&#30340;&#23545;&#25163;&#30340;&#21487;&#25512;&#24191;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMAP&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;Atari&#28216;&#25103;&#65292;&#19968;&#20010;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#21644;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#35780;&#20272;&#20102;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22312;112&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#31867;&#27861;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#20026;&#20116;&#31867;&#65292;&#20026;TSC&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.13029</link><description>&lt;p&gt;
Bake Off&#37325;&#35775;&#65306;&#23545;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#30340;&#35780;&#36848;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bake off redux: a review and experimental evaluation of recent time series classification algorithms. (arXiv:2304.13029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#35780;&#20272;&#20102;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22312;112&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#31867;&#27861;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#20026;&#20116;&#31867;&#65292;&#20026;TSC&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#30740;&#31350;&#35770;&#25991;&#22312;2017&#24180;&#27604;&#36739;&#20102;18&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#31639;&#27861;&#22312;&#26469;&#33258;&#21152;&#24030;&#22823;&#23398;&#27827;&#28392;&#20998;&#26657;&#65288;UCR&#65289;&#23384;&#26723;&#30340;85&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#28888;&#28953;&#27604;&#36187;&#8221;&#65292;&#21457;&#29616;&#21482;&#26377;9&#20010;&#31639;&#27861;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#26059;&#36716;&#26862;&#26519;&#22522;&#20934;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#31639;&#27861;&#31867;&#22411;&#23545;&#27599;&#20010;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24418;&#25104;&#20102;&#20116;&#31181;&#20027;&#35201;&#31639;&#27861;&#31867;&#22411;&#30340;&#20998;&#31867;&#27861;&#12290;&#19982;&#21487;&#20197;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#21644;&#32467;&#26524;&#30340;&#25552;&#20379;&#30456;&#32467;&#21512;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20998;&#31867;&#21644;&#21487;&#35775;&#38382;&#30340;&#32467;&#26524;&#25512;&#21160;&#20102;TSC&#39046;&#22495;&#30340;&#26222;&#21450;&#12290;&#20845;&#24180;&#36807;&#21435;&#20102;&#65292;UCR&#23384;&#26723;&#24050;&#25193;&#23637;&#21040;112&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#30475;&#30475;&#27599;&#20010;&#25552;&#20986;&#30340;&#31867;&#21035;&#33258;&#21407;&#22987;&#20986;&#29256;&#20197;&#26469;&#30340;&#36827;&#23637;&#65292;&#24182;&#35780;&#20272;&#26032;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2017, a research paper compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;SALAMA&#29992;&#20110;&#39044;&#27979;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#65292;&#21487;&#20197;&#22312;&#38271;&#36798;11&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#65292;&#19988;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.08736</link><description>&lt;p&gt;
&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#36890;&#36807;&#21518;&#22788;&#29702;&#27169;&#25311;&#25968;&#25454;&#39044;&#27979;&#38647;&#26292;&#22825;&#27668;(arXiv:2303.08736v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
A machine-learning approach to thunderstorm forecasting through post-processing of simulation data. (arXiv:2303.08736v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08736
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;SALAMA&#29992;&#20110;&#39044;&#27979;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#65292;&#21487;&#20197;&#22312;&#38271;&#36798;11&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#65292;&#19988;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38647;&#26292;&#23545;&#31038;&#20250;&#21644;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#38647;&#26292;&#39044;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SALAMA&#65292;&#19968;&#31181;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#25968;&#25454;&#20013;&#30340;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#22312;&#20013;&#27431;&#23545;&#27969;&#23618;&#35299;&#26512;&#38598;&#21512;&#39044;&#25253;&#21644;&#38647;&#30005;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20165;&#32473;&#20986;&#20174;NWP&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#19982;&#38647;&#26292;&#21457;&#23637;&#30456;&#20851;&#30340;&#20687;&#32032;&#36755;&#20837;&#21442;&#25968;&#38598;&#65292;SALAMA&#20197;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#24335;&#25512;&#26029;&#38647;&#26292;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#38271;&#36798;11&#23567;&#26102;&#30340;&#21069;&#32622;&#26102;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20165;&#22522;&#20110;&#23545;&#27969;&#26377;&#25928;&#20301;&#33021;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#25913;&#21464;&#23558;&#38378;&#30005;&#35266;&#27979;&#19982;NWP&#25968;&#25454;&#30456;&#20851;&#32852;&#30340;&#26102;&#31354;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29087;&#32451;&#30340;&#38647;&#26292;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#30340;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thunderstorms pose a major hazard to society and economy, which calls for reliable thunderstorm forecasts. In this work, we introduce SALAMA, a feedforward neural network model for identifying thunderstorm occurrence in numerical weather prediction (NWP) data. The model is trained on convection-resolving ensemble forecasts over Central Europe and lightning observations. Given only a set of pixel-wise input parameters that are extracted from NWP data and related to thunderstorm development, SALAMA infers the probability of thunderstorm occurrence in a reliably calibrated manner. For lead times up to eleven hours, we find a forecast skill superior to classification based only on convective available potential energy. Varying the spatiotemporal criteria by which we associate lightning observations with NWP data, we show that the time scale for skillful thunderstorm predictions increases linearly with the spatial scale of the forecast.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#20197;&#21450;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20351;&#24471;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#25512;&#26029;&#26102;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.00009</link><description>&lt;p&gt;
&#23545;Riesz Representer&#30340;&#25932;&#23545;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00009
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#20197;&#21450;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20351;&#24471;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#25512;&#26029;&#26102;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22240;&#26524;&#21644;&#32467;&#26500;&#21442;&#25968;&#26159;&#22522;&#20110;&#24213;&#23618;&#22238;&#24402;&#30340;&#32447;&#24615;&#27867;&#20989;&#12290;Riesz Representer&#26159;&#21322;&#21442;&#25968;&#32447;&#24615;&#27867;&#20989;&#28176;&#36817;&#26041;&#24046;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#20010;&#31216;&#20026;&#20020;&#30028;&#21322;&#24452;&#30340;&#25277;&#35937;&#37327;&#65292;&#28982;&#21518;&#23558;&#20854;&#19987;&#38376;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20316;&#20026;&#20027;&#35201;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20020;&#30028;&#21322;&#24452;&#29702;&#35770;&#26469;&#35777;&#26126;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26679;&#26412;&#20998;&#21106;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#22797;&#26434;&#24230;-&#36895;&#29575;&#40065;&#26834;&#24615;&#8221;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20855;&#26377;&#23454;&#38469;&#21518;&#26524;&#65306;&#22312;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#30340;&#25512;&#26029;&#65292;&#36825;&#21487;&#33021;&#20250;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#20013;&#23454;&#29616;&#20102;&#21517;&#20041;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo
&lt;/p&gt;</description></item></channel></rss>