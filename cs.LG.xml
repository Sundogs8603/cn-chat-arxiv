<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.07874</link><description>&lt;p&gt;
&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#12289;&#22522;&#20110;Vision Transformer&#30340;&#38750;&#22343;&#36136;&#21435;&#38654;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#21435;&#38654;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#22343;&#36136;&#38654;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#22312;&#24212;&#29992;&#20110;&#23384;&#22312;&#38750;&#22343;&#36136;&#38654;&#30340;&#22270;&#20687;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;NTIRE&#25361;&#25112;&#20171;&#32461;&#30340;NH-HAZE23&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#19968;&#20010;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#38750;&#22343;&#36136;&#38654;&#19981;&#31526;&#21512;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#23545;&#22823;&#37327;&#30340;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#19982;&#20854;&#28165;&#26224;&#23545;&#24212;&#39033;&#36827;&#34892;&#37197;&#23545;&#65292;&#32780;NH-HAZE23&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#38750;&#22343;&#36136;&#21435;&#38654;&#25968;&#25454;&#38598;&#25193;&#20805;NH-HAZE23&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#36866;&#24403;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#20197;&#20943;&#23569;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#29109;&#22686;&#24378;(CEBoosting)&#31574;&#30053;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#32447;&#27169;&#22411;&#35782;&#21035;&#26469;&#26816;&#27979;&#21046;&#24230;&#36716;&#25442;&#21644;&#21457;&#29616;&#26032;&#21046;&#24230;&#30456;&#20851;&#21160;&#21147;&#23398;&#12290;&#23427;&#33021;&#22815;&#39640;&#25928;&#22320;&#35745;&#31639;&#22240;&#26524;&#29109;&#26469;&#25552;&#20379;&#36923;&#36753;&#20540;&#65292;&#26816;&#27979;&#21040;&#21046;&#24230;&#36716;&#25442;&#21518;&#20250;&#25913;&#27491;&#27169;&#22411;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.07863</link><description>&lt;p&gt;
CEBoosting:&#22522;&#20110;&#22240;&#26524;&#29109;&#22686;&#24378;&#30340;&#21160;&#24577;&#31995;&#32479;&#24102;&#21046;&#24230;&#36716;&#25442;&#30340;&#22312;&#32447;&#31232;&#30095;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CEBoosting: Online Sparse Identification of Dynamical Systems with Regime Switching by Causation Entropy Boosting. (arXiv:2304.07863v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#29109;&#22686;&#24378;(CEBoosting)&#31574;&#30053;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#32447;&#27169;&#22411;&#35782;&#21035;&#26469;&#26816;&#27979;&#21046;&#24230;&#36716;&#25442;&#21644;&#21457;&#29616;&#26032;&#21046;&#24230;&#30456;&#20851;&#21160;&#21147;&#23398;&#12290;&#23427;&#33021;&#22815;&#39640;&#25928;&#22320;&#35745;&#31639;&#22240;&#26524;&#29109;&#26469;&#25552;&#20379;&#36923;&#36753;&#20540;&#65292;&#26816;&#27979;&#21040;&#21046;&#24230;&#36716;&#25442;&#21518;&#20250;&#25913;&#27491;&#27169;&#22411;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29305;&#24449;&#12289;&#28151;&#27788;&#34892;&#20026;&#21644;&#26497;&#31471;&#20107;&#20214;&#20351;&#24471;&#21046;&#24230;&#36716;&#25442;&#26222;&#36941;&#23384;&#22312;&#20110;&#35768;&#22810;&#22797;&#26434;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#29109;&#22686;&#24378;(CEBoosting)&#31574;&#30053;&#65292;&#20197;&#20415;&#36890;&#36807;&#22312;&#32447;&#27169;&#22411;&#35782;&#21035;&#20419;&#36827;&#21046;&#24230;&#36716;&#25442;&#30340;&#26816;&#27979;&#21644;&#26032;&#21046;&#24230;&#30456;&#20851;&#21160;&#21147;&#23398;&#30340;&#21457;&#29616;&#12290;&#36890;&#36807;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22240;&#26524;&#29109;&#65292;&#20026;&#39044;&#20808;&#30830;&#23450;&#30340;&#24211;&#20013;&#30340;&#27599;&#20010;&#20505;&#36873;&#20989;&#25968;&#25552;&#20379;&#19968;&#20010;&#36923;&#36753;&#20540;&#12290;&#19982;&#24403;&#21069;&#21046;&#24230;&#26657;&#20934;&#30340;&#27169;&#22411;&#30456;&#20851;&#30340;&#23569;&#25968;&#19968;&#20010;&#25110;&#20960;&#20010;&#22240;&#26524;&#29109;&#25351;&#26631;&#30340;&#21453;&#36716;&#24847;&#21619;&#30528;&#26816;&#27979;&#21040;&#21046;&#24230;&#36716;&#25442;&#12290;&#23613;&#31649;&#30001;&#36830;&#32493;&#25968;&#25454;&#32452;&#25104;&#30340;&#27599;&#20010;&#25209;&#27425;&#30340;&#38271;&#24230;&#24456;&#30701;&#65292;&#20294;&#19982;&#19968;&#31995;&#21015;&#25968;&#25454;&#25209;&#27425;&#23545;&#24212;&#30340;&#22240;&#26524;&#29109;&#32047;&#31215;&#20540;&#23548;&#33268;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#26816;&#27979;&#27169;&#22411;&#32467;&#26500;&#30340;&#25913;&#27491;&#65292;&#38543;&#21518;&#30340;&#21442;&#25968;&#20272;&#35745;&#25104;&#20026;&#19968;&#20010;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#36827;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regime switching is ubiquitous in many complex dynamical systems with multiscale features, chaotic behavior, and extreme events. In this paper, a causation entropy boosting (CEBoosting) strategy is developed to facilitate the detection of regime switching and the discovery of the dynamics associated with the new regime via online model identification. The causation entropy, which can be efficiently calculated, provides a logic value of each candidate function in a pre-determined library. The reversal of one or a few such causation entropy indicators associated with the model calibrated for the current regime implies the detection of regime switching. Despite the short length of each batch formed by the sequential data, the accumulated value of causation entropy corresponding to a sequence of data batches leads to a robust indicator. With the detected rectification of the model structure, the subsequent parameter estimation becomes a quadratic optimization problem, which is solved using
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#26469;&#25913;&#21892;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#23457;&#26597;&#21644;&#20195;&#30721;&#26356;&#25913;&#30340;&#25968;&#25454;&#38598;&#24494;&#35843;&#65292;&#33021;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07840</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#23457;&#26597;&#30340;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;: &#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?. (arXiv:2304.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#26469;&#25913;&#21892;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#23457;&#26597;&#21644;&#20195;&#30721;&#26356;&#25913;&#30340;&#25968;&#25454;&#38598;&#24494;&#35843;&#65292;&#33021;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#23558;&#38169;&#35823;&#30340;&#31243;&#24207;&#36716;&#25442;&#20026;&#27491;&#30830;&#30340;&#31243;&#24207;&#65292;&#24403;&#23427;&#20204;&#34987;&#35757;&#32451;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#20195;&#30721;&#23457;&#26597;(&#20851;&#20110;&#20195;&#30721;&#20013;&#24314;&#35758;&#24615;&#26356;&#25913;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;)&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#31243;&#24207;&#20462;&#22797;&#12290;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35745;&#31639;&#26426;&#31243;&#24207;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21253;&#21547;&#20004;&#31181;&#35821;&#35328;&#30340;&#22266;&#26377;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20195;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22266;&#26377;&#30693;&#35782;&#26469;&#25552;&#39640;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;PLBART&#21644;CodeT5&#36825;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20004;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31243;&#24207;&#20462;&#22797;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21253;&#21547;&#20195;&#30721;&#23457;&#26597;&#21644;&#38543;&#21518;&#20195;&#30721;&#26356;&#25913;&#30340;&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20462;&#22797;&#33021;&#21147;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;SOTA&#25216;&#26415;&#65292;CODET5&#21462;&#24471;&#20102;3%&#24038;&#21491;&#30340;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review (natural language instruction about suggestive changes in code) can improve the program repair further. Large language models, trained with Natural Language (NL) and computer program corpora, have the capacity to contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of code and NL can be utilized to improve automated program repair. We applied PLBART and CodeT5, two state-of-the-art language models that are pre-trained with both Programming Language (PL) and Natural Language (NL), on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperform each of the previous models. We observed that the pre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#36127;&#36733;&#21160;&#24577;&#20998;&#35299;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#30721;&#30005;&#32593;&#36127;&#36733;&#21160;&#24577;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.07832</link><description>&lt;p&gt;
&#22522;&#20110;Koopman&#27169;&#24577;&#20998;&#35299;&#30340;&#30005;&#32593;&#36127;&#36733;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Characterizing the load profile in power grids by Koopman mode decomposition of interconnected dynamics. (arXiv:2304.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#36127;&#36733;&#21160;&#24577;&#20998;&#35299;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#30721;&#30005;&#32593;&#36127;&#36733;&#21160;&#24577;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#36733;&#39044;&#27979;&#23545;&#20110;&#26377;&#25928;&#31649;&#29702;&#21644;&#20248;&#21270;&#30005;&#32593;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31639;&#23376;&#29702;&#35770;&#26694;&#26550;&#20869;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35782;&#21035;&#36127;&#36733;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;Koopman&#31639;&#23376;&#26469;&#34920;&#31034;&#36127;&#36733;&#25968;&#25454;&#65292;&#35813;&#31639;&#23376;&#22266;&#26377;&#20110;&#24213;&#23618;&#21160;&#24577;&#12290;&#36890;&#36807;&#35745;&#31639;&#30456;&#24212;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#36127;&#36733;&#21160;&#24577;&#20998;&#35299;&#20026;&#30456;&#24178;&#30340;&#26102;&#31354;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#26159;&#21160;&#24577;&#30340;&#26368;&#24378;&#29305;&#24449;&#12290;&#27599;&#20010;&#27169;&#24335;&#26681;&#25454;&#20854;&#21333;&#19968;&#39057;&#29575;&#29420;&#31435;&#28436;&#21270;&#65292;&#22522;&#20110;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#36127;&#36733;&#21160;&#24577;&#26159;&#22522;&#20110;&#22266;&#26377;&#20110;&#21160;&#24577;&#30340;&#30456;&#24178;&#30340;&#26102;&#31354;&#27169;&#24335;&#26500;&#24314;&#30340;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#32534;&#30721;&#20016;&#23500;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#30005;&#32593;&#30340;&#29289;&#29702;&#29305;&#24449;&#65288;&#22914;&#23395;&#33410;&#24615;&#21644;&#23567;&#26102;&#27169;&#24335;&#65289;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#24213;&#23618;&#21160;&#24577;&#30340;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity load forecasting is crucial for effectively managing and optimizing power grids. Over the past few decades, various statistical and deep learning approaches have been used to develop load forecasting models. This paper presents an interpretable machine learning approach that identifies load dynamics using data-driven methods within an operator-theoretic framework. We represent the load data using the Koopman operator, which is inherent to the underlying dynamics. By computing the corresponding eigenfunctions, we decompose the load dynamics into coherent spatiotemporal patterns that are the most robust features of the dynamics. Each pattern evolves independently according to its single frequency, making its predictability based on linear dynamics. We emphasize that the load dynamics are constructed based on coherent spatiotemporal patterns that are intrinsic to the dynamics and are capable of encoding rich dynamical features at multiple time scales. These features are relate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;TDI&#26041;&#27861;&#35299;&#20915;&#20020;&#24202;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#38382;&#39064;&#65292;&#36890;&#36807;&#38598;&#25104;&#21069;&#21521;&#22635;&#20805;&#21644;&#36845;&#20195;&#25554;&#20540;&#22120;&#35299;&#20915;&#22810;&#20803;&#21644;&#32437;&#21521;&#25968;&#25454;&#38382;&#39064;&#65292;&#37319;&#29992;&#21160;&#24577;&#21152;&#26435;&#31574;&#30053;&#22788;&#29702;&#32570;&#22833;&#29575;&#21644;&#27979;&#37327;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07821</link><description>&lt;p&gt;
&#22810;&#20803;&#32437;&#21521;&#20020;&#24202;&#25968;&#25454;&#30340;&#26102;&#21464;&#36845;&#20195;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Time-dependent Iterative Imputation for Multivariate Longitudinal Clinical Data. (arXiv:2304.07821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;TDI&#26041;&#27861;&#35299;&#20915;&#20020;&#24202;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#38382;&#39064;&#65292;&#36890;&#36807;&#38598;&#25104;&#21069;&#21521;&#22635;&#20805;&#21644;&#36845;&#20195;&#25554;&#20540;&#22120;&#35299;&#20915;&#22810;&#20803;&#21644;&#32437;&#21521;&#25968;&#25454;&#38382;&#39064;&#65292;&#37319;&#29992;&#21160;&#24577;&#21152;&#26435;&#31574;&#30053;&#22788;&#29702;&#32570;&#22833;&#29575;&#21644;&#27979;&#37327;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#26159;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#30005;&#23376;&#30149;&#21382;&#20013;&#65292;&#24120;&#24120;&#26377;&#22823;&#37327;&#30340;&#23454;&#39564;&#23460;&#26816;&#26597;&#21644;&#29983;&#21629;&#20307;&#24449;&#25968;&#20540;&#32570;&#22833;&#12290;&#32570;&#22833;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#20559;&#20506;&#20272;&#35745;&#24182;&#38480;&#21046;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#32467;&#35770;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21482;&#33021;&#24212;&#29992;&#20110;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25968;&#25454;&#34917;&#20805;&#65292;&#21363;&#22635;&#34917;&#32570;&#22833;&#20540;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#27969;&#34892;&#30340;&#34917;&#20805;&#26041;&#27861;&#22312;&#20020;&#24202;&#25968;&#25454;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#20381;&#36182;&#36845;&#20195;&#25554;&#34917; (TDI)&#65292;&#20026;&#22635;&#34917;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#21069;&#21521;&#22635;&#20805;&#21644;&#36845;&#20195;&#25554;&#20540;&#22120;&#26469;&#22788;&#29702;&#22810;&#20803;&#21644;&#32437;&#21521;&#25968;&#25454;&#12290;&#35813;&#38598;&#25104;&#37319;&#29992;&#22522;&#20110;&#20020;&#24202;&#25968;&#25454;&#27169;&#24335;&#30340;&#24739;&#32773;&#12289;&#21464;&#37327;&#21644;&#35266;&#27979;&#29305;&#23450;&#30340;&#21160;&#24577;&#21152;&#26435;&#31574;&#30053;&#65292;&#21253;&#25324;&#32570;&#22833;&#29575;&#21644;&#27979;&#37327;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;TDI&#24212;&#29992;&#20110;&#38543;&#26426;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a major challenge in clinical research. In electronic medical records, often a large fraction of the values in laboratory tests and vital signs are missing. The missingness can lead to biased estimates and limit our ability to draw conclusions from the data. Additionally, many machine learning algorithms can only be applied to complete datasets. A common solution is data imputation, the process of filling-in the missing values. However, some of the popular imputation approaches perform poorly on clinical data. We developed a simple new approach, Time-Dependent Iterative imputation (TDI), which offers a practical solution for imputing time-series data. It addresses both multivariate and longitudinal data, by integrating forward-filling and Iterative Imputer. The integration employs a patient, variable, and observation-specific dynamic weighting strategy, based on the clinical patterns of the data, including missing rates and measurement frequency. We tested TDI on random
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#31934;&#30830;&#22320;&#20272;&#35745;&#20102;&#22826;&#38451;&#33021;&#30005;&#27744;&#21644;&#20809;&#20239;&#27169;&#22359;&#30340;PVLIB&#27169;&#22411;&#21442;&#25968;&#65292;&#31934;&#24230;&#36229;&#36807;95&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.07817</link><description>&lt;p&gt;
MPPT&#19982;PV&#30005;&#27744;&#21442;&#25968;&#20272;&#35745;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of MPPT and Parameter Estimation of PV cells. (arXiv:2304.07817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#31934;&#30830;&#22320;&#20272;&#35745;&#20102;&#22826;&#38451;&#33021;&#30005;&#27744;&#21644;&#20809;&#20239;&#27169;&#22359;&#30340;PVLIB&#27169;&#22411;&#21442;&#25968;&#65292;&#31934;&#24230;&#36229;&#36807;95&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20934;&#30830;&#20272;&#35745;&#22826;&#38451;&#33021;&#30005;&#27744;&#21644;&#20809;&#20239;&#27169;&#22359;&#30340;PVLIB&#27169;&#22411;&#30340;&#24050;&#30693;&#21644;&#26410;&#30693;&#21442;&#25968;&#30340;&#20934;&#30830;&#20540;&#12290;&#20351;&#29992;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36752;&#29031;&#24230;&#21644;&#28201;&#24230;&#22522;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#34892;&#20102;&#24067;&#20848;&#24503;-&#22885;&#23572;&#29305;&#26364;&#65288;Bland Altman&#65289;&#27979;&#35797;&#24182;&#24471;&#20986;&#20102;&#36229;&#36807;95&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;&#39564;&#35777;&#21518;&#65292;&#20351;&#29992;ANN&#31639;&#27861;&#20272;&#35745;&#21442;&#25968;&#21450;&#20854;&#30456;&#24212;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presented work focuses on utilising machine learning techniques to accurately estimate accurate values for known and unknown parameters of the PVLIB model for solar cells and photovoltaic modules.Finding accurate model parameters of circuits for photovoltaic (PV) cells is important for a variety of tasks. An Artificial Neural Network (ANN) algorithm was employed, which outperformed other metaheuristic and machine learning algorithms in terms of computational efficiency. To validate the consistency of the data and output, the results were compared against other machine learning algorithms based on irradiance and temperature. A Bland Altman test was conducted that resulted in more than 95 percent accuracy rate. Upon validation, the ANN algorithm was utilised to estimate the parameters and their respective values.
&lt;/p&gt;</description></item><item><title>VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2304.07810</link><description>&lt;p&gt;
VISAR&#65306;&#19968;&#31181;&#24102;&#26377;&#21487;&#35270;&#21270;&#32534;&#31243;&#21644;&#24555;&#36895;&#33609;&#26696;&#21407;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#35770;&#35777;&#20889;&#20316;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07810
&lt;/p&gt;
&lt;p&gt;
VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#65292;&#20316;&#32773;&#24517;&#39035;&#26500;&#24605;&#20998;&#23618;&#20889;&#20316;&#30446;&#26631;&#65292;&#30830;&#20445;&#20854;&#35770;&#28857;&#30340;&#35828;&#26381;&#21147;&#65292;&#24182;&#36890;&#36807;&#36215;&#33609;&#26469;&#20462;&#35746;&#21644;&#32452;&#32455;&#20182;&#20204;&#30340;&#35745;&#21010;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#65288;&#20363;&#22914;ChatGPT&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#20889;&#20316;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24847;&#22270;&#65292;&#32570;&#20047;&#29992;&#25143;&#25511;&#21046;&#21644;&#33258;&#20027;&#26435;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#38480;&#30340;&#24110;&#21161;&#26469;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#21644;&#20462;&#35746;&#20889;&#20316;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VISAR&#65292;&#19968;&#31181;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#22312;&#20854;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#26500;&#24605;&#21644;&#20462;&#35746;&#20998;&#23618;&#30446;&#26631;&#65292;&#36890;&#36807;&#21516;&#27493;&#25991;&#26412;&#32534;&#36753;&#21644;&#21487;&#35270;&#21270;&#32534;&#31243;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#28779;&#33457;&#25512;&#33616;&#22686;&#24378;&#35828;&#26381;&#21147;&#12290;VISAR&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#39564;&#35777;&#20182;&#20204;&#30340;&#20889;&#20316;&#35745;&#21010;&#12290;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;&#23460;&#30740;&#31350;&#35777;&#23454;&#65292;VISAR&#21487;&#20197;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.07788</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#27010;&#29575;&#20915;&#31574;&#26641;&#30340;&#20020;&#24202;&#23454;&#36341;&#36741;&#21161;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assisting clinical practice with fuzzy probabilistic decision trees. (arXiv:2304.07788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07788
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24847;&#35782;&#21040;&#38656;&#35201;&#23436;&#20840;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#26102;&#65292;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36741;&#21161;&#25935;&#24863;&#39046;&#22495;&#20915;&#31574;&#30340;&#36235;&#21183;&#23558;&#20250;&#22686;&#38271;&#65292;&#24182;&#19988;&#21363;&#23558;&#20986;&#21488;&#30340;&#27861;&#35268;&#23558;&#20250;&#21152;&#24378;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20542;&#26012;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20999;&#20837;&#28857;&#20043;&#19968;&#26159;&#21307;&#23398;&#23454;&#36341;&#65292;&#23427;&#21487;&#20197;&#21463;&#30410;&#20110;&#31934;&#30830;&#30340;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20250;&#20135;&#29983;&#20449;&#20219;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;MedFP&#65292;&#23427;&#32467;&#21512;&#20102;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#26469;&#36741;&#21161;&#20020;&#24202;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65307;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#22330;&#26223;&#20013;&#65306;&#32959;&#30244;&#20998;&#31867;&#21644;&#31958;&#23615;&#30149;&#31867;&#22411;2&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for fully human-understandable models is increasingly being recognised as a central theme in AI research. The acceptance of AI models to assist in decision making in sensitive domains will grow when these models are interpretable, and this trend towards interpretable models will be amplified by upcoming regulations. One of the killer applications of interpretable AI is medical practice, which can benefit from accurate decision support methodologies that inherently generate trust. In this work, we propose FPT, (MedFP), a novel method that combines probabilistic trees and fuzzy logic to assist clinical practice. This approach is fully interpretable as it allows clinicians to generate, control and verify the entire diagnosis procedure; one of the methodology's strength is the capability to decrease the frequency of misdiagnoses by providing an estimate of uncertainties and counterfactuals. Our approach is applied as a proof-of-concept to two real medical scenarios: classifying ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#22240;&#26524;&#23398;&#20064;&#27169;&#22411;&#65292;&#25512;&#26029;EoE&#30340;&#32452;&#32455;&#23398;&#29305;&#24449;&#21644;&#21046;&#23450;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#28040;&#38500;&#35745;&#21010;&#65292;&#20197;&#25913;&#36827;EoE&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07787</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#22240;&#26524;&#23398;&#20064;&#25913;&#36827;&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#33203;&#39135;&#27835;&#30103;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Harnessing Digital Pathology And Causal Learning To Improve Eosinophilic Esophagitis Dietary Treatment Assignment. (arXiv:2304.07787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#22240;&#26524;&#23398;&#20064;&#27169;&#22411;&#65292;&#25512;&#26029;EoE&#30340;&#32452;&#32455;&#23398;&#29305;&#24449;&#21644;&#21046;&#23450;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#28040;&#38500;&#35745;&#21010;&#65292;&#20197;&#25913;&#36827;EoE&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#65288;EoE&#65289;&#26159;&#19968;&#31181;&#19982;&#39135;&#29289;&#36807;&#25935;&#28814;&#30151;&#26377;&#20851;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#19982;&#39135;&#31649;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21319;&#39640;&#26377;&#20851;&#12290;EoE&#26159;GERD&#21518;&#24930;&#24615;&#21534;&#21693;&#22256;&#38590;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;EoE&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#35745;&#25968;&#32452;&#32455;&#23398;&#20999;&#29255;&#20013;&#30340;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#65292;&#36825;&#26159;&#19968;&#39033;&#25163;&#21160;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38480;&#21046;&#20102;&#25552;&#21462;&#22797;&#26434;&#30340;&#24739;&#32773;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;EoE&#30340;&#27835;&#30103;&#21253;&#25324;&#33647;&#29289;&#21644;&#39135;&#29289;&#28040;&#38500;&#12290;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#28040;&#38500;&#35745;&#21010;&#23545;&#20110;&#24739;&#32773;&#30340;&#21442;&#19982;&#21644;&#27835;&#30103;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20808;&#21069;&#30340;&#23581;&#35797;&#26410;&#33021;&#20135;&#29983;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19968;&#26041;&#38754;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20174;&#25972;&#20010;&#27963;&#26816;&#20999;&#29255;&#20013;&#25512;&#26029;&#32452;&#32455;&#23398;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#26080;&#27861;&#25163;&#21160;&#25552;&#21462;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22240;&#26524;&#23398;&#20064;&#27169;&#22411;&#26469;&#22788;&#29702;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#8220;&#20845;&#31181;&#39135;&#29289;&#19982;&#19968;&#31181;&#39135;&#29289;&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#39278;&#39135;&#30740;&#31350;&#8221;&#65292;&#38024;&#23545;112&#21517;18-60&#23681;&#30340;&#26377;&#30151;&#29366;&#25104;&#24180;&#24739;&#32773;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eosinophilic esophagitis (EoE) is a chronic, food antigen-driven, allergic inflammatory condition of the esophagus associated with elevated esophageal eosinophils. EoE is a top cause of chronic dysphagia after GERD. Diagnosis of EoE relies on counting eosinophils in histological slides, a manual and time-consuming task that limits the ability to extract complex patient-dependent features. The treatment of EoE includes medication and food elimination. A personalized food elimination plan is crucial for engagement and efficiency, but previous attempts failed to produce significant results. In this work, on the one hand, we utilize AI for inferring histological features from the entire biopsy slide, features that cannot be extracted manually. On the other hand, we develop causal learning models that can process this wealth of data. We applied our approach to the 'Six-Food vs. One-Food Eosinophilic Esophagitis Diet Study', where 112 symptomatic adults aged 18-60 years with active EoE were 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#30340;&#22797;&#21046;&#26426;&#21046;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#39046;&#22495;&#22312;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#23558;&#22797;&#21046;&#26426;&#21046;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;NMT&#30340;SPARQL&#29983;&#25104;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28155;&#21152;&#22797;&#21046;&#26426;&#21046;&#25110;&#20351;&#29992;&#38382;&#39064;&#27880;&#37322;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RCALAD&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#21644;&#26032;&#30340;&#37492;&#21035;&#22120;&#22686;&#24378;&#20102;GAN&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#34917;&#20805;&#20998;&#24067;&#24341;&#23548;&#37325;&#24314;&#21644;&#24341;&#20837;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07769</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#23436;&#25972;&#24490;&#29615;&#19968;&#33268;&#24615;GAN&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Regularized Complete Cycle Consistent GAN for Anomaly Detection. (arXiv:2304.07769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RCALAD&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#21644;&#26032;&#30340;&#37492;&#21035;&#22120;&#22686;&#24378;&#20102;GAN&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#34917;&#20805;&#20998;&#24067;&#24341;&#23548;&#37325;&#24314;&#21644;&#24341;&#20837;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#35823;&#24046;&#20013;&#24490;&#29615;&#19968;&#33268;&#24615;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23041;&#21147;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#30001;&#20110;&#31867;&#21035;&#38388;&#31934;&#24230;&#39640;&#24230;&#24046;&#24322;&#32780;&#26410;&#34987;&#24212;&#29992;&#20110;&#25152;&#26377;&#31867;&#22411;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;RCALAD&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#23558;&#26032;&#30340;&#37492;&#21035;&#22120;&#24341;&#20837;&#21040;&#32467;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;RCALAD&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#34917;&#20805;&#20998;&#24067;&#65292;&#23558;&#37325;&#24314;&#24341;&#23548;&#21040;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#26679;&#26412;&#19982;&#20854;&#37325;&#24314;&#20998;&#31163;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24471;&#20986;&#20102;&#23637;&#31034;&#20986;&#20854;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an adversarial method for anomaly detection in real-world applications, leveraging the power of generative adversarial neural networks (GANs) through cycle consistency in reconstruction error. Previous methods suffer from the high variance between class-wise accuracy which leads to not being applicable for all types of anomalies. The proposed method named RCALAD tries to solve this problem by introducing a novel discriminator to the structure, which results in a more efficient training process. Additionally, RCALAD employs a supplementary distribution in the input space to steer reconstructions toward the normal data distribution, effectively separating anomalous samples from their reconstructions and facilitating more accurate anomaly detection. To further enhance the performance of the model, two novel anomaly scores are introduced. The proposed model has been thoroughly evaluated through extensive experiments on six various datasets, yielding results that demonst
&lt;/p&gt;</description></item><item><title>Data-OOB&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;out-of-bag&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.07718</link><description>&lt;p&gt;
Data-OOB:&#20197;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#30340;Out-of-bag&#20272;&#35745;&#20026;&#20934;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07718
&lt;/p&gt;
&lt;p&gt;
Data-OOB&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;out-of-bag&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#35780;&#20272;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#32479;&#35745;&#27934;&#23519;&#21147;&#65292;&#20197;&#21306;&#20998;&#21738;&#20123;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26159;&#26377;&#30410;&#30340;&#65292;&#21738;&#20123;&#26159;&#26377;&#23475;&#30340;&#12290;&#32437;&#35266;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#35768;&#22810;&#20197;Shapley&#20026;&#22522;&#30784;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#22343;&#26174;&#31034;&#20986;&#20102;&#24456;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#27492;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Data-OOB&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;bagging&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;out-of-bag&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#35780;&#20272;100&#20010;&#36755;&#20837;&#32500;&#24230;&#19988;&#23384;&#22312;$10^6$&#20010;&#26679;&#26412;&#26102;&#65292;Data-OOB&#20165;&#38656;&#35201;&#22312;&#21333;&#20010;CPU&#22788;&#29702;&#22120;&#19978;&#25191;&#34892;&#19981;&#21040;2.25&#20010;&#23567;&#26102;&#12290;&#27492;&#22806;&#65292;Data-OOB&#22312;&#29702;&#35770;&#19978;&#26377;&#22362;&#23454;&#30340;&#35299;&#37322;&#65292;&#24403;&#20004;&#20010;&#31163;&#24046;&#20540;&#20989;&#25968;&#30456;&#21516;&#26102;&#65292;&#20854;&#35782;&#21035;&#20855;&#26377;&#30456;&#21516;&#37325;&#35201;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07689</link><description>&lt;p&gt;
&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#29992;&#20110;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#26041;&#27861;&#37319;&#29992;&#22266;&#23450;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21487;&#33021;&#23548;&#33268;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#26368;&#20248;&#24615;&#33021;&#12290;Bregman&#25955;&#24230;&#27010;&#25324;&#20102;&#21508;&#31181;&#36317;&#31163;&#24230;&#37327;&#30340;&#24230;&#37327;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39046;&#22495;&#20013;&#20135;&#29983;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;Bregman&#25955;&#24230;&#33719;&#24471;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#32622;&#23545;Bregman&#25955;&#24230;&#19979;&#30340;&#20984;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;SOTA&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#27969;&#34892;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#20351;&#29992;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#20844;&#20849;&#21355;&#29983;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24739;&#32773;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#30340;&#22320;&#29702;&#20998;&#36776;&#29575;&#36739;&#39640;&#26102;&#65292;&#25928;&#26524;&#26356;&#21152;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.07679</link><description>&lt;p&gt;
&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#20351;&#29992;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#20844;&#20849;&#21355;&#29983;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Using Geographic Location-based Public Health Features in Survival Analysis. (arXiv:2304.07679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#20351;&#29992;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#20844;&#20849;&#21355;&#29983;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24739;&#32773;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#30340;&#22320;&#29702;&#20998;&#36776;&#29575;&#36739;&#39640;&#26102;&#65292;&#25928;&#26524;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20449;&#24687;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#24314;&#27169;&#26041;&#24335;&#65292;&#36890;&#24120;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#26469;&#20272;&#35745;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#29983;&#23384;&#24471;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31561;&#29616;&#20195;&#24037;&#20855;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#21307;&#30103;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26356;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#21644;&#26356;&#39057;&#32321;&#30340;&#35266;&#27979;&#26377;&#21161;&#20110;&#25552;&#39640;&#24739;&#32773;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#21152;&#20837;&#24739;&#32773;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#20844;&#20849;&#21355;&#29983;&#32479;&#35745;&#25968;&#25454;&#23545;&#20010;&#20307;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#25913;&#36827;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20844;&#20849;&#21355;&#29983;&#32479;&#35745;&#20449;&#24687;&#32435;&#20837;&#36755;&#20837;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#21253;&#21547;&#20840;&#22269;&#24615;&#30284;&#30151;&#21457;&#30149;&#25968;&#25454;&#30340; Surveillance&#65292;Epidemiology &#21644; End Results&#65288;SEER&#65289;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#21547;&#22320;&#29702;&#23450;&#20301;&#30340;&#20844;&#20849;&#20581;&#24247;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19968;&#33268;&#24615;&#25351;&#25968;&#12290;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#30340;&#22320;&#29702;&#20998;&#36776;&#29575;&#36739;&#39640;&#26102;&#65292;&#36825;&#31181;&#25913;&#36827;&#25928;&#26524;&#26356;&#21152;&#26126;&#26174;&#65292;&#24182;&#20197;&#20122;&#29305;&#20848;&#22823;&#21644;&#26087;&#37329;&#23665;&#22320;&#21306;&#30340;&#30284;&#30151;&#21457;&#30149;&#30149;&#20363;&#30740;&#31350;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time elapsed till an event of interest is often modeled using the survival analysis methodology, which estimates a survival score based on the input features. There is a resurgence of interest in developing more accurate prediction models for time-to-event prediction in personalized healthcare using modern tools such as neural networks. Higher quality features and more frequent observations improve the predictions for a patient, however, the impact of including a patient's geographic location-based public health statistics on individual predictions has not been studied. This paper proposes a complementary improvement to survival analysis models by incorporating public health statistics in the input features. We show that including geographic location-based public health information results in a statistically significant improvement in the concordance index evaluated on the Surveillance, Epidemiology, and End Results (SEER) dataset containing nationwide cancer incidence data. The improv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21464;&#37327;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26041;&#21521;&#29305;&#24449;&#20132;&#20114;&#25429;&#33719;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#22312;Shapley&#20540;&#35299;&#37322;&#20013;&#24212;&#29992;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07670</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#21521;&#29305;&#24449;&#20132;&#20114;&#30340;&#40657;&#30418;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explanations of Black-Box Models based on Directional Feature Interactions. (arXiv:2304.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21464;&#37327;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26041;&#21521;&#29305;&#24449;&#20132;&#20114;&#25429;&#33719;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#22312;Shapley&#20540;&#35299;&#37322;&#20013;&#24212;&#29992;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20351;&#20854;&#24120;&#24120;&#26159;&#40657;&#30418;&#27169;&#22411;&#21464;&#24471;&#36879;&#26126;&#21270;&#38750;&#24120;&#37325;&#35201;&#12290;&#20960;&#20010;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#25235;&#21462;&#27599;&#20010;&#23454;&#20363;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#39044;&#27979;&#29305;&#24449;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;;&#36825;&#31181;&#35299;&#37322;&#26041;&#27861;&#26159;&#21333;&#21464;&#37327;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#24449;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#21333;&#21464;&#37327;&#35299;&#37322;&#25193;&#23637;&#21040;&#39640;&#38454;&#65292;&#36890;&#36807;&#21452;&#21464;&#37327;&#26041;&#27861;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#21452;&#21464;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#12290;&#20998;&#26512;&#36825;&#20010;&#22270;&#21487;&#20197;&#21457;&#29616;&#21516;&#31561;&#37325;&#35201;&#30340;&#29305;&#24449;&#32452;&#65292;&#32780;&#26041;&#21521;&#24615;&#30340;&#27010;&#24565;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21452;&#21464;&#37327;&#26041;&#27861;&#24212;&#29992;&#20110;Shapley&#20540;&#35299;&#37322;&#65292;&#24182;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#26041;&#21521;&#24615;&#35299;&#37322;&#21457;&#29616;&#29305;&#24449;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR10&#12289;IMDB&#12289;Census&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent. Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature. We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph. Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features. We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;IoT&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.07668</link><description>&lt;p&gt;
FedBlockHealth&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#30340;IoT&#21307;&#30103;&#20445;&#20581;&#38544;&#31169;&#21644;&#23433;&#20840;&#21327;&#21516;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedBlockHealth: A Synergistic Approach to Privacy and Security in IoT-Enabled Healthcare through Federated Learning and Blockchain. (arXiv:2304.07668v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;IoT&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#25968;&#25454;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#24739;&#32773;&#23433;&#20840;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#38656;&#35201;&#20445;&#35777;&#23433;&#20840;&#21644;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#20026;IoT&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20844;&#38053;&#21152;&#23494;&#31995;&#32479;&#20026;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#25552;&#20379;&#35821;&#20041;&#23433;&#20840;&#65292;&#32780;&#21306;&#22359;&#38142;&#25216;&#26415;&#30830;&#20445;&#36825;&#20123;&#26356;&#26032;&#30340;&#23436;&#25972;&#24615;&#24182;&#24378;&#21046;&#35775;&#38382;&#25511;&#21046;&#21644;&#38382;&#36131;&#21046;&#12290;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;EMNIST&#25968;&#25454;&#38598;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of Internet of Things (IoT) devices in healthcare has introduced new challenges in preserving data privacy, security and patient safety. Traditional approaches need to ensure security and privacy while maintaining computational efficiency, particularly for resource-constrained IoT devices. This paper proposes a novel hybrid approach combining federated learning and blockchain technology to provide a secure and privacy-preserved solution for IoT-enabled healthcare applications. Our approach leverages a public-key cryptosystem that provides semantic security for local model updates, while blockchain technology ensures the integrity of these updates and enforces access control and accountability. The federated learning process enables a secure model aggregation without sharing sensitive patient data. We implement and evaluate our proposed framework using EMNIST datasets, demonstrating its effectiveness in preserving data privacy and security while maintaining computatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#65292;&#21160;&#24577;&#24179;&#34913;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20197;&#26356;&#22909;&#22320;&#26597;&#35810;&#25968;&#25454;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.07665</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#20013;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#20013;&#30340;&#21160;&#24577;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling. (arXiv:2304.07665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#65292;&#21160;&#24577;&#24179;&#34913;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20197;&#26356;&#22909;&#22320;&#26597;&#35810;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26368;&#20855;&#20449;&#24687;&#30340;&#23454;&#39564;&#20197;&#23398;&#20064;&#26410;&#30693;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20998;&#23618;&#26041;&#27861;&#26469;&#21160;&#24577;&#24179;&#34913;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20197;&#26356;&#22909;&#22320;&#26597;&#35810;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning provides a framework to adaptively sample the most informative experiments towards learning an unknown black-box function. Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space. Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal. In this paper, we develop a Bayesian hierarchical approach to dynamically balance the exploration-exploitation trade-off as more data points are queried. We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of data samples in the feature space to sample from the posterior distribution of the trade-off parameter obtained from the Bayesian hierarchical model. Simulated and real-world examples show the proposed approach achieves at least 6% and 11% average improvement when compared to pure exploration and ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ProbDR&#21464;&#20998;&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#38477;&#32500;&#31639;&#27861;&#35299;&#37322;&#20026;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#23436;&#25104;&#25512;&#26029;&#25805;&#20316;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#23436;&#25104;&#24120;&#35268;&#38477;&#32500;&#31639;&#27861;&#65292;&#36824;&#25903;&#25345;&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#38477;&#32500;&#25805;&#20316;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07658</link><description>&lt;p&gt;
&#20316;&#20026;&#27010;&#29575;&#25512;&#26029;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction as Probabilistic Inference. (arXiv:2304.07658v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ProbDR&#21464;&#20998;&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#38477;&#32500;&#31639;&#27861;&#35299;&#37322;&#20026;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#23436;&#25104;&#25512;&#26029;&#25805;&#20316;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#23436;&#25104;&#24120;&#35268;&#38477;&#32500;&#31639;&#27861;&#65292;&#36824;&#25903;&#25345;&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#38477;&#32500;&#25805;&#20316;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#31639;&#27861;&#23558;&#39640;&#32500;&#25968;&#25454;&#21387;&#32553;&#21040;&#20302;&#32500;&#34920;&#31034;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#38477;&#32500;&#26159;&#35768;&#22810;&#20998;&#26512;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#12289;&#22122;&#22768;&#38477;&#20302;&#21644;&#39640;&#25928;&#30340;&#19979;&#28216;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbDR&#21464;&#20998;&#26694;&#26550;&#65292;&#23558;&#24191;&#27867;&#30340;&#32463;&#20856;DR&#31639;&#27861;&#35299;&#37322;&#20026;&#35813;&#26694;&#26550;&#20013;&#30340;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#12290;ProbDR&#21253;&#25324;PCA&#12289;CMDS&#12289;LLE&#12289;LE&#12289;MVU&#12289;&#25193;&#25955;&#26144;&#23556;&#12289;kPCA&#12289;Isomap&#12289;(t-)SNE&#21644;UMAP&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#20302;&#32500;&#28508;&#21464;&#37327;&#29992;&#20110;&#26500;&#24314;&#21327;&#26041;&#24046;&#12289;&#31934;&#24230;&#25110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#12290;&#25512;&#26029;&#26159;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#25903;&#25345;&#20351;&#29992;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#65288;PPL&#65289;&#36827;&#34892;DR&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#21487;&#20197;&#23436;&#25104;&#24120;&#35268;DR&#31639;&#27861;&#30340;&#25805;&#20316;&#65292;&#24182;&#36171;&#20104;&#20102;&#23427;&#36890;&#36807;&#27010;&#29575;&#21464;&#20998;&#25512;&#26029;&#30340;&#24378;&#22823;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) algorithms compress high-dimensional data into a lower dimensional representation while preserving important features of the data. DR is a critical step in many analysis pipelines as it enables visualisation, noise reduction and efficient downstream processing of the data. In this work, we introduce the ProbDR variational framework, which interprets a wide range of classical DR algorithms as probabilistic inference algorithms in this framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps, kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latent variable is used to construct a covariance, precision, or a graph Laplacian matrix, which can be used as part of a generative model for the data. Inference is done by optimizing an evidence lower bound. We demonstrate the internal consistency of our framework and show that it enables the use of probabilistic programming languages (PPLs) for DR. Additionally, we illustrate that the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.07655</link><description>&lt;p&gt;
EEG SN&#65306;&#38754;&#21521; EEG &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20302;&#24310;&#36831;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
EEGSN: Towards Efficient Low-latency Decoding of EEG with Graph Spiking Neural Networks. (arXiv:2304.07655v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#22810;&#20010;&#38656;&#35201;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290; &#22522;&#20110;&#30456;&#20851;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#25512;&#26029;&#22823;&#33041;&#34892;&#20026;&#23601;&#26159;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#23398;&#20064;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#20250;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25928;&#29575;&#12290;&#30446;&#21069;&#65292;SNN&#20165;&#20165;&#20381;&#38752;&#19968;&#33324;&#24402;&#32435;&#20559;&#24046;&#26469;&#27169;&#25311;&#19981;&#21516;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;EEGSN&#65289;&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#20998;&#24067;&#22312; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#21516;&#26102;&#22312;&#36816;&#21160;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#35757;&#32451; EEG &#25968;&#25454;&#30340;&#22270;&#24418;SNN&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast majority of spiking neural networks (SNNs) are trained based on inductive biases that are not necessarily a good fit for several critical tasks that require low-latency and power efficiency. Inferring brain behavior based on the associated electroenchephalography (EEG) signals is an example of how networks training and inference efficiency can be heavily impacted by learning spatio-temporal dependencies. Up to now, SNNs rely solely on general inductive biases to model the dynamic relations between different data streams. Here, we propose a graph spiking neural network architecture for multi-channel EEG classification (EEGSN) that learns the dynamic relational information present in the distributed EEG sensors. Our method reduced the inference computational complexity by $\times 20$ compared to the state-of-the-art SNNs, while achieved comparable accuracy on motor execution classification tasks. Overall, our work provides a framework for interpretable and efficient training of gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24212;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#25554;&#20540;&#31639;&#27861;&#26469;&#25552;&#39640;&#27969;&#25968;&#25454;&#20998;&#20301;&#25968;&#38382;&#39064;&#30340;&#23454;&#38469;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21487;&#25511;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2304.07652</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#25554;&#20540;&#31639;&#27861;&#22312;&#27969;&#25968;&#25454;&#20998;&#20301;&#25968;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#19982;&#24615;&#33021;&#20445;&#38556;&#65288;arXiv:2304.07652v1 [cs.DS]&#65289;
&lt;/p&gt;
&lt;p&gt;
Learned Interpolation for Better Streaming Quantile Approximation with Worst-Case Guarantees. (arXiv:2304.07652v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24212;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#25554;&#20540;&#31639;&#27861;&#26469;&#25552;&#39640;&#27969;&#25968;&#25454;&#20998;&#20301;&#25968;&#38382;&#39064;&#30340;&#23454;&#38469;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21487;&#25511;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;$\varepsilon$&#36817;&#20284;&#20998;&#20301;&#25968;&#33609;&#22270;&#65292;&#23545;&#20110;&#30001;$n$&#20010;&#36755;&#20837;&#32452;&#25104;&#30340;&#27969;&#65292;&#23427;&#21487;&#20197;&#22312;&#28040;&#32791;$o(n)$&#31354;&#38388;&#20043;&#19979;&#65292;&#20197;&#33267;&#23569;$1-1/\mathrm{poly}(n)$&#30340;&#27010;&#29575;&#36817;&#20284;&#35745;&#31639;&#20986;&#20219;&#20309;&#26597;&#35810;&#28857;$q$&#30340;&#25490;&#21517; - &#21363;&#23567;&#20110;$q$&#30340;&#36755;&#20837;&#28857;&#25968;&#65292;&#20855;&#26377;$\varepsilon n$&#30340;&#38468;&#21152;&#35823;&#24046;&#12290;&#34429;&#28982;Karnin&#12289;Lang&#21644;Liberty&#30340;KLL&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20248;&#30340;&#20998;&#20301;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#20294;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25152;&#23454;&#29616;&#30340;&#20272;&#35745;&#24448;&#24448;&#36828;&#36828;&#19981;&#22914;&#26368;&#20248;&#12290;&#20107;&#23454;&#19978;&#65292;&#23454;&#36341;&#20013;&#26368;&#24120;&#29992;&#30340;&#25216;&#26415;&#26159;Dunning&#30340;$t$-digest&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#19978;&#24448;&#24448;&#27604;KLL&#23454;&#29616;&#26356;&#22909;&#30340;&#36817;&#20284;&#25928;&#26524;&#65292;&#20294;&#24050;&#30693;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#35823;&#24046;&#21487;&#20197;&#20219;&#24847;&#22823;&#12290;&#26412;&#25991;&#23558;&#25554;&#20540;&#25216;&#26415;&#24212;&#29992;&#20110;&#27969;&#24335;&#21462;&#20998;&#20301;&#25968;&#38382;&#39064;&#65292;&#20197;&#35797;&#22270;&#22312;&#20445;&#25345;&#31867;&#20284;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#38556;&#30340;&#21516;&#26102;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
An $\varepsilon$-approximate quantile sketch over a stream of $n$ inputs approximates the rank of any query point $q$ - that is, the number of input points less than $q$ - up to an additive error of $\varepsilon n$, generally with some probability of at least $1 - 1/\mathrm{poly}(n)$, while consuming $o(n)$ space. While the celebrated KLL sketch of Karnin, Lang, and Liberty achieves a provably optimal quantile approximation algorithm over worst-case streams, the approximations it achieves in practice are often far from optimal. Indeed, the most commonly used technique in practice is Dunning's t-digest, which often achieves much better approximations than KLL on real-world data but is known to have arbitrarily large errors in the worst case. We apply interpolation techniques to the streaming quantiles problem to attempt to achieve better approximations on real-world data sets than KLL while maintaining similar guarantees in the worst case.
&lt;/p&gt;</description></item><item><title>LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.07647</link><description>&lt;p&gt;
LASER&#65306;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LASER: Neuro-Symbolic Learning of Semantic Video Representations. (arXiv:2304.07647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07647
&lt;/p&gt;
&lt;p&gt;
LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28041;&#21450;&#35270;&#39057;&#30340;AI&#24212;&#29992;&#65288;&#22914;&#35270;&#39057;-&#25991;&#26412;&#23545;&#40784;&#12289;&#35270;&#39057;&#25628;&#32034;&#21644;&#35270;&#39057;&#23383;&#24149;&#65289;&#21463;&#30410;&#20110;&#23545;&#35270;&#39057;&#35821;&#20041;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#65292;&#35201;&#20040;&#22522;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#23884;&#20837;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LASER&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#22815;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#26102;&#31354;&#23646;&#24615;&#30340;&#36923;&#36753;&#35268;&#33539;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#35270;&#39057;&#19982;&#35268;&#33539;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#20844;&#24335;&#21270;&#38382;&#39064;&#12290;&#23545;&#40784;&#36807;&#31243;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;&#20302;&#23618;&#24863;&#30693;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#23618;&#35268;&#33539;&#30340;&#32454;&#31890;&#24230;&#35270;&#39057;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#24182;&#21487;&#32435;&#20837;&#20174;&#35268;&#33539;&#23548;&#20986;&#30340;&#23545;&#27604;&#21644;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#20016;&#23500;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern AI applications involving video, such as video-text alignment, video search, and video captioning, benefit from a fine-grained understanding of video semantics. Existing approaches for video understanding are either data-hungry and need low-level annotation, or are based on general embeddings that are uninterpretable and can miss important details. We propose LASER, a neuro-symbolic approach that learns semantic video representations by leveraging logic specifications that can capture rich spatial and temporal properties in video data. In particular, we formulate the problem in terms of alignment between raw videos and specifications. The alignment process efficiently trains low-level perception models to extract a fine-grained video representation that conforms to the desired high-level specification. Our pipeline can be trained end-to-end and can incorporate contrastive and semantic loss functions derived from specifications. We evaluate our method on two datasets with rich sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.07645</link><description>&lt;p&gt;
&#38024;&#23545;&#31283;&#23450;&#30340;&#36229;&#32593;&#32476;&#23398;&#20064;&#30340;&#38750;&#27604;&#20363;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Proportional Parametrizations for Stable Hypernetwork Learning. (arXiv:2304.07645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#26159;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24403;&#21069;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#27604;&#38750;&#36229;&#32593;&#32476;&#27169;&#22411;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#19982;&#20351;&#29992;&#24120;&#35265;&#30340;&#36229;&#32593;&#32476;&#26550;&#26500;&#21644;&#21021;&#22987;&#21270;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#26377;&#20851;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#25968;&#20540;&#38382;&#39064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#29978;&#33267;&#38459;&#27490;&#25910;&#25947;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24402;&#19968;&#21270;&#31574;&#30053;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#35746;&#30340;&#36229;&#32593;&#32476;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#36229;&#32593;&#32476;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24182;&#35777;&#26126;&#23427;&#22987;&#32456;&#21487;&#20197;&#23548;&#33268;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks are neural networks that generate the parameters of another neural network. In many scenarios, current hypernetwork training strategies are unstable, and convergence is often far slower than for non-hypernetwork models. We show that this problem is linked to an issue that arises when using common choices of hypernetwork architecture and initialization. We demonstrate analytically and experimentally how this numerical issue can lead to an instability during training that slows, and sometimes even prevents, convergence. We also demonstrate that popular deep learning normalization strategies fail to address these issues. We then propose a solution to the problem based on a revised hypernetwork formulation that uses non-proportional additive parametrizations. We test the proposed reparametrization on several tasks, and demonstrate that it consistently leads to more stable training, achieving faster convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24358;&#22270;&#35821;&#35328;&#20013;&#30340;&#32593;&#32476;&#22270;&#26469;&#24314;&#31435;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#20013;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#25193;&#23637;&#20102;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#23545;&#27169;&#22411;&#30340;&#19968;&#33324;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#30452;&#35266;&#19988;&#20005;&#26684;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07638</link><description>&lt;p&gt;
&#24358;&#22270;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal models in string diagrams. (arXiv:2304.07638v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24358;&#22270;&#35821;&#35328;&#20013;&#30340;&#32593;&#32476;&#22270;&#26469;&#24314;&#31435;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#20013;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#25193;&#23637;&#20102;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#23545;&#27169;&#22411;&#30340;&#19968;&#33324;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#30452;&#35266;&#19988;&#20005;&#26684;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#27169;&#22411;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#22240;&#26524;&#25512;&#29702;&#26041;&#27861;&#65292;&#20170;&#22825;&#24050;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#33539;&#30068;&#35770;&#26469;&#24418;&#24335;&#21270;&#22320;&#35299;&#37322;&#24358;&#22270;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#19968;&#31867;&#31216;&#20026;&#32593;&#32476;&#22270;&#30340;&#24358;&#22270;&#19982;&#26377;&#21521;&#26080;&#29615;&#22270;&#19968;&#19968;&#23545;&#24212;&#12290;&#20351;&#29992;&#20855;&#26377;&#8220;&#22797;&#21046;-&#20002;&#24323;&#8221;&#32467;&#26500;&#30340;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#20013;&#30340;&#38543;&#26426;&#26144;&#23556;&#12289;&#20989;&#25968;&#25110;&#19968;&#33324;&#36890;&#36947;&#20316;&#20026;&#27169;&#22411;&#30340;&#32452;&#20214;&#65292;&#32473;&#20986;&#20102;&#36825;&#26679;&#19968;&#20010;&#22270;&#34920;&#26469;&#34920;&#31034;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#23558;&#27169;&#22411;&#36716;&#21270;&#20026;&#19968;&#20010;&#21487;&#20197;&#30452;&#35266;&#24182;&#20005;&#26684;&#25512;&#29702;&#30340;&#21333;&#19968;&#25968;&#23398;&#23545;&#35937;&#12290;&#24314;&#31435;&#22312;Fong&#21644;Jacobs&#12289;Kissinger&#21644;Zanasi&#20197;&#21450;Fritz&#21644;Klingler&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;cd-&#33539;&#30068;&#20013;&#25552;&#20986;&#20102;&#22240;&#26524;&#27169;&#22411;&#21644;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#30340;&#22270;&#34920;&#23450;&#20041;&#65292;&#36825;&#25193;&#23637;&#20102;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#20998;&#21035;&#24418;&#24335;&#21270;&#20102;&#23545;&#27169;&#22411;&#30340;&#19968;&#33324;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
The framework of causal models provides a principled approach to causal reasoning, applied today across many scientific domains. Here we present this framework in the language of string diagrams, interpreted formally using category theory. A class of string diagrams, called network diagrams, are in 1-to-1 correspondence with directed acyclic graphs. A causal model is given by such a diagram with its components interpreted as stochastic maps, functions, or general channels in a symmetric monoidal category with a 'copy-discard' structure (cd-category), turning a model into a single mathematical object that can be reasoned with intuitively and yet rigorously. Building on prior works by Fong and Jacobs, Kissinger and Zanasi, as well as Fritz and Klingler, we present diagrammatic definitions of causal models and functional causal models in a cd-category, generalising causal Bayesian networks and structural causal models, respectively. We formalise general interventions on a model, including
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;LSTM-based seq2seq&#26550;&#26500;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;OCR&#25216;&#26415;&#19982;&#35789;&#32423;&#32763;&#35793;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25991;&#26723;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07637</link><description>&lt;p&gt;
TransDocs&#65306;&#22522;&#20110;&#35789;&#32423;&#32763;&#35793;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
TransDocs: Optical Character Recognition with word to word translation. (arXiv:2304.07637v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;LSTM-based seq2seq&#26550;&#26500;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;OCR&#25216;&#26415;&#19982;&#35789;&#32423;&#32763;&#35793;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25991;&#26723;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;OCR&#25216;&#26415;&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#20854;&#36755;&#20986;&#24182;&#19981;&#24635;&#26159;&#20934;&#30830;&#30340;&#65292;&#23548;&#33268;&#38169;&#37197;&#23383;&#35789;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#21892;OCR&#25216;&#26415;&#65292;&#23558;OCR&#25216;&#26415;&#19982;&#22522;&#20110;LSTM&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25972;&#21512;&#20197;&#36827;&#34892;&#25991;&#26723;&#32763;&#35793;&#65292;&#24182;&#22522;&#20110;ANKI&#25968;&#25454;&#38598;&#36827;&#34892;&#33521;&#35821;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#32763;&#35793;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;LSTM-based seq2seq&#26550;&#26500;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;OCR&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#38754;&#21521;&#23545;OCR&#25216;&#26415;&#21450;&#20854;&#22312;&#25991;&#26723;&#32763;&#35793;&#20013;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
While OCR has been used in various applications, its output is not always accurate, leading to misfit words. This research work focuses on improving the optical character recognition (OCR) with ML techniques with integration of OCR with long short-term memory (LSTM) based sequence to sequence deep learning models to perform document translation. This work is based on ANKI dataset for English to Spanish translation. In this work, I have shown comparative study for pre-trained OCR while using deep learning model using LSTM-based seq2seq architecture with attention for machine translation. End-to-end performance of the model has been expressed in BLEU-4 score. This research paper is aimed at researchers and practitioners interested in OCR and its applications in document translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;</title><link>http://arxiv.org/abs/2304.07633</link><description>&lt;p&gt;
&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#21270;&#31070;&#32463;&#27169;&#22411;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#22810;&#27169;&#24577;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#28436;&#21270;&#25345;&#32493;&#22686;&#38271;&#65292;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#35875;&#35328;&#25110;&#34394;&#20551;&#26032;&#38395;&#32534;&#36753;&#20027;&#35201;&#20381;&#36182;&#20110;&#29983;&#25104;&#21644;/&#25110;&#20266;&#36896;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#35270;&#39057;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;&#34394;&#20551;&#20449;&#24687;&#21019;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#19981;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65289;&#26469;&#27450;&#39575;&#20844;&#20247;&#21644;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#31181;&#26032;&#22411;&#30340;&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#38590;&#24230;&#65292;&#20063;&#22686;&#21152;&#20102;&#28548;&#28165;&#30340;&#38590;&#24230;&#65292;&#22240;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#27169;&#24577;&#37117;&#36275;&#22815;&#25509;&#36817;&#30495;&#23454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#21435;&#19978;&#19979;&#25991;&#26816;&#27979;&#65292;&#21516;&#26102;&#35782;&#21035;&#19981;&#21305;&#37197;&#30340;&#23545;&#21644;&#36328;&#27169;&#24577;&#30683;&#30462;&#65292;&#36825;&#23545;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#30340;&#35760;&#24405;&#28548;&#28165;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22522;&#20110;Abstract M&#36827;&#34892;&#31526;&#21495;&#21270;&#20998;&#35299;&#65292;&#24471;&#21040;&#19968;&#32452;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the sustained evolution of misinformation that aims at manipulating public opinions. Unlike traditional rumors or fake news editors who mainly rely on generated and/or counterfeited images, text and videos, current misinformation creators now more tend to use out-of-context multimedia contents (e.g. mismatched images and captions) to deceive the public and fake news detection systems. This new type of misinformation increases the difficulty of not only detection but also clarification, because every individual modality is close enough to true information. To address this challenge, in this paper we explore how to achieve interpretable cross-modal de-contextualization detection that simultaneously identifies the mismatched pairs and the cross-modal contradictions, which is helpful for fact-check websites to document clarifications. The proposed model first symbolically disassembles the text-modality information to a set of fact queries based on the Abstract M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;N2&#22686;&#27833;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65292;&#34920;&#26126;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07617</link><description>&lt;p&gt;
&#22522;&#20110;&#32431;&#20928;/&#19981;&#32431;&#30340;N2&#22686;&#27833;&#36807;&#31243;&#20013;&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65288;MMP&#65289;&#30340;&#20272;&#35745;&#65306;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of minimum miscibility pressure (MMP) in impure/pure N2 based enhanced oil recovery process: A comparative study of statistical and machine learning algorithms. (arXiv:2304.07617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;N2&#22686;&#27833;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65292;&#34920;&#26126;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65288;MMP&#65289;&#30340;&#39044;&#27979;&#22312;&#35774;&#35745;&#21644;&#25805;&#20316;&#22522;&#20110;&#27694;&#30340;&#22686;&#27833;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;MMP&#20272;&#35745;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#22823;&#22810;&#25968;&#39044;&#27979;&#27169;&#22411;&#26174;&#31034;&#20986;&#27604;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#30456;&#20851;&#21644;&#39044;&#27979;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum miscibility pressure (MMP) prediction plays an important role in design and operation of nitrogen based enhanced oil recovery processes. In this work, a comparative study of statistical and machine learning methods used for MMP estimation is carried out. Most of the predictive models developed in this study exhibited superior performance over correlation and predictive models reported in literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20316;&#20026;PyTorch&#25509;&#21475;&#30340;&#31232;&#30095;&#32534;&#31243;&#27169;&#22411;STen&#65292;&#23427;&#25903;&#25345;&#20960;&#20046;&#25152;&#26377;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#39640;&#25928;&#12289;&#21487;&#23450;&#21046;&#12289;&#21487;&#25193;&#23637;&#19988;&#33021;&#22815;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07613</link><description>&lt;p&gt;
STen: PyTorch&#20013;&#39640;&#25928;&#30340;&#31232;&#30095;&#24615;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
STen: Productive and Efficient Sparsity in PyTorch. (arXiv:2304.07613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20316;&#20026;PyTorch&#25509;&#21475;&#30340;&#31232;&#30095;&#32534;&#31243;&#27169;&#22411;STen&#65292;&#23427;&#25903;&#25345;&#20960;&#20046;&#25152;&#26377;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#39640;&#25928;&#12289;&#21487;&#23450;&#21046;&#12289;&#21487;&#25193;&#23637;&#19988;&#33021;&#22815;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22686;&#38271;&#65292;&#31232;&#30095;&#24615;&#27491;&#22312;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#24182;&#20943;&#23569;&#23384;&#20648;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26694;&#26550;&#23545;&#31232;&#30095;&#24615;&#30340;&#25903;&#25345;&#36739;&#24046;&#12290;&#19987;&#29992;&#30340;&#31232;&#30095;&#24341;&#25806;&#19987;&#27880;&#20110;&#31232;&#30095;&#25512;&#29702;&#65292;&#32780;&#19968;&#33324;&#26694;&#26550;&#20027;&#35201;&#38598;&#20013;&#22312;&#32463;&#20856;&#26684;&#24335;&#30340;&#31232;&#30095;&#24352;&#37327;&#19978;&#65292;&#24182;&#24573;&#30053;&#20102;&#20351;&#29992;&#31232;&#30095;&#27169;&#22411;&#25152;&#38656;&#30340;&#26356;&#24191;&#27867;&#30340;&#31232;&#30095;&#21270;&#27969;&#31243;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26694;&#26550;&#19981;&#23481;&#26131;&#25193;&#23637;&#65306;&#28155;&#21152;&#26032;&#30340;&#31232;&#30095;&#24352;&#37327;&#26684;&#24335;&#25110;&#25805;&#20316;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STen&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;PyTorch&#35774;&#35745;&#30340;&#31232;&#30095;&#32534;&#31243;&#27169;&#22411;&#21644;&#25509;&#21475;&#65292;&#23427;&#21253;&#25324;&#26377;&#25928;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#25193;&#23637;&#30340;&#31232;&#30095;&#24067;&#23616;&#12289;&#36816;&#31639;&#31526;&#21644;&#31232;&#30095;&#21270;&#31243;&#24207;&#65292;&#25903;&#25345;&#20960;&#20046;&#25152;&#26377;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22312;CPU&#25512;&#29702;&#20013;&#36827;&#34892;&#20013;&#31561;&#31232;&#30095;&#24230;&#30340;&#39640;&#24615;&#33021;&#20998;&#32452;n:m&#31232;&#30095;&#24067;&#23616;&#26469;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models grow, sparsity is becoming an increasingly critical component of deep neural networks, enabling improved performance and reduced storage. However, existing frameworks offer poor support for sparsity. Specialized sparsity engines focus exclusively on sparse inference, while general frameworks primarily focus on sparse tensors in classical formats and neglect the broader sparsification pipeline necessary for using sparse models, especially during training. Further, existing frameworks are not easily extensible: adding a new sparse tensor format or operator is challenging and time-consuming. To address this, we propose STen, a sparsity programming model and interface for PyTorch, which incorporates sparsity layouts, operators, and sparsifiers, in an efficient, customizable, and extensible framework that supports virtually all sparsification methods. We demonstrate this by developing a high-performance grouped n:m sparsity layout for CPU inference at moderate sparsi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#21487;&#37325;&#26500;&#35745;&#31639;&#65292;&#24182;&#33021;&#22788;&#29702;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.07608</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#30340;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-Speed and Energy-Efficient Non-Binary Computing with Polymorphic Electro-Optic Circuits and Architectures. (arXiv:2304.07608v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#21487;&#37325;&#26500;&#35745;&#31639;&#65292;&#24182;&#33021;&#22788;&#29702;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#21487;&#37325;&#26500;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#21487;&#20197;&#21160;&#24577;&#32534;&#31243;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#23454;&#29616;&#19981;&#21516;&#30340;&#36923;&#36753;&#21644;&#31639;&#26415;&#21151;&#33021;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#32039;&#20945;&#24615;&#21644;&#22810;&#24577;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#25805;&#20316;&#25968;&#22788;&#29702;&#33021;&#21147;&#65292;&#20943;&#23569;&#31354;&#38386;&#26102;&#38388;&#65292;&#24182;&#22686;&#21152;&#38754;&#31215;&#21644;&#38745;&#24577;&#21151;&#29575;&#24320;&#38144;&#30340;&#25674;&#38144;&#12290;&#24403;&#19982;&#26580;&#24615;&#20809;&#30005;&#25506;&#27979;&#22120;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#30005;&#36335;&#21487;&#20197;&#25903;&#25345;&#38750;&#20108;&#36827;&#21046;&#26684;&#24335;&#65288;&#22914;&#38543;&#26426;/&#19968;&#20803;&#21644;&#39640;&#32500;&#20648;&#22791;&#26684;&#24335;&#65289;&#30340;&#25968;&#25454;&#30340;&#33410;&#33021;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#36824;&#21487;&#20197;&#23454;&#29616;&#21487;&#37197;&#32622;&#30340;&#30005;&#20809;&#35745;&#31639;&#21152;&#36895;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#20108;&#36827;&#21046;&#21644;&#25972;&#25968;&#37327;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present microring resonator (MRR) based polymorphic E-O circuits and architectures that can be employed for high-speed and energy-efficient non-binary reconfigurable computing. Our polymorphic E-O circuits can be dynamically programmed to implement different logic and arithmetic functions at different times. They can provide compactness and polymorphism to consequently improve operand handling, reduce idle time, and increase amortization of area and static power overheads. When combined with flexible photodetectors with the innate ability to accumulate a high number of optical pulses in situ, our circuits can support energy-efficient processing of data in non-binary formats such as stochastic/unary and high-dimensional reservoir formats. Furthermore, our polymorphic E-O circuits enable configurable E-O computing accelerator architectures for processing binarized and integer quantized convolutional neural networks (CNNs). We compare our designed polymorphic E-O circuit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(L-DeepONet)&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#36716;&#25442;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07599</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#25913;&#21892;&#20102;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning in latent spaces improves the predictive accuracy of deep neural operators. (arXiv:2304.07599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(L-DeepONet)&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#36716;&#25442;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#22238;&#24402;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#26500;&#24314;&#25551;&#36848;&#29289;&#29702;&#31995;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#19981;&#21464;&#31163;&#25955;&#21270;&#20223;&#30495;&#22120;&#30340;&#26041;&#27861;&#12290;&#31070;&#32463;&#31639;&#23376;&#29305;&#21035;&#26159;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#26080;&#38480;&#32500;Banach&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#20316;&#20026;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#31070;&#32463;&#31639;&#23376;&#38656;&#35201;&#29983;&#25104;&#26631;&#35760;&#35266;&#27979;&#25968;&#25454;&#65292;&#22312;&#22797;&#26434;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#39640;&#32500;&#30340;&#65292;&#21253;&#21547;&#20887;&#20313;&#21644;&#22122;&#22768;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#23558;&#36825;&#20123;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21487;&#20197;&#20351;&#25968;&#25454;&#22788;&#29702;&#26356;&#26041;&#20415;&#65292;&#20063;&#21487;&#20197;&#22686;&#24378;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28508;&#22312;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(L-DeepONet)&#65292;&#23427;&#26159;&#26631;&#20934;DeepONet&#30340;&#25193;&#23637;&#65292;&#21033;&#29992;&#36866;&#24403;&#30340;&#33258;&#32534;&#30721;&#22120;&#35782;&#21035;&#39640;&#32500;PDE&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;L-DeepONet&#20248;&#20110;&#20256;&#32479;&#30340;DeepONet&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operator regression provides a powerful means of constructing discretization-invariant emulators for partial-differential equations (PDEs) describing physical systems. Neural operators specifically employ deep neural networks to approximate mappings between infinite-dimensional Banach spaces. As data-driven models, neural operators require the generation of labeled observations, which in cases of complex high-fidelity models result in high-dimensional datasets containing redundant and noisy features, which can hinder gradient-based optimization. Mapping these high-dimensional datasets to a low-dimensional latent space of salient features can make it easier to work with the data and also enhance learning. In this work, we investigate the latent deep operator network (L-DeepONet), an extension of standard DeepONet, which leverages latent representations of high-dimensional PDE input and output functions identified with suitable autoencoders. We illustrate that L-DeepONet outperforms the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;ICOs&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26059;&#36716;&#19981;&#21464;&#30340;&#26041;&#24335;&#23545;3D&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#36866;&#29992;&#20110;&#29699;&#24418;&#25110;&#20108;&#21313;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#25968;&#25454;&#38598;&#25193;&#23637;&#21644;&#25552;&#39640;&#21270;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07558</link><description>&lt;p&gt;
Icospherical Chemical Objects&#65288;ICOs&#65289;&#20801;&#35768;&#23545;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#25193;&#20805;&#24182;&#20445;&#25345;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Icospherical Chemical Objects (ICOs) allow for chemical data augmentation and maintain rotational, translation and permutation invariance. (arXiv:2304.07558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;ICOs&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26059;&#36716;&#19981;&#21464;&#30340;&#26041;&#24335;&#23545;3D&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#36866;&#29992;&#20110;&#29699;&#24418;&#25110;&#20108;&#21313;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#25968;&#25454;&#38598;&#25193;&#23637;&#21644;&#25552;&#39640;&#21270;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#26159;&#22788;&#29702;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#20294;&#21270;&#23398;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#29699;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#21644;&#20108;&#21313;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#65288;IcoNNs&#65289;&#26159;&#19968;&#31181;&#20445;&#25345;&#26059;&#36716;&#23545;&#31216;&#24615;&#30340;&#20960;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20998;&#23376;&#32467;&#26500;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#21644;&#22825;&#28982;&#30340;3D&#23646;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;3D&#32534;&#30721;&#26041;&#27861;&#23558;&#20998;&#23376;&#32467;&#26500;&#36755;&#20837;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Icospherical Chemical Objects&#65288;ICOs&#65289;&#65292;&#20197;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#30340;&#26041;&#24335;&#23545;3D&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#21487;&#36866;&#29992;&#20110;&#29699;&#24418;&#25110;&#20108;&#21313;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20801;&#35768;&#36827;&#34892;&#25968;&#25454;&#38598;&#22686;&#24378;&#12290;&#25105;&#22312;&#39044;&#27979;&#26222;&#36890;&#20998;&#23376;&#24615;&#36136;&#12289;&#39044;&#27979;&#33647;&#29289;&#31867;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;&#21644;&#34507;&#30333;&#36136;&#32467;&#21512;&#38382;&#39064;&#31561;&#20219;&#21153;&#20013;&#28436;&#31034;&#20102;ICO&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;ICO&#21644;SphNN&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset augmentation is a common way to deal with small datasets; Chemistry datasets are often small. Spherical convolutional neural networks (SphNNs) and Icosahedral neural networks (IcoNNs) are a type of geometric machine learning algorithm that maintains rotational symmetry. Molecular structure has rotational invariance and is inherently 3-D, and thus we need 3-D encoding methods to input molecular structure into machine learning. In this paper I present Icospherical Chemical Objects (ICOs) that enable the encoding of 3-D data in a rotationally invariant way which works with spherical or icosahedral neural networks and allows for dataset augmentation. I demonstrate the ICO featurisation method on the following tasks: predicting general molecular properties, predicting solubility of drug like molecules and the protein binding problem and find that ICO and SphNNs perform well on all problems.
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#26159;&#19968;&#31181;&#32534;&#30721;&#20998;&#23376;&#24418;&#29366;&#30340;&#26041;&#27861;&#65292;&#23427;&#25429;&#25417;&#20102;&#21270;&#23398;&#20998;&#23376;&#30340;&#25299;&#25169;&#24418;&#29366;&#29305;&#24615;&#65292;&#24182;&#22312;&#33021;&#37327;&#20351;&#29992;&#25928;&#29575;&#19978;&#27604;&#20854;&#20182;&#32534;&#30721;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#34920;&#26126;&#22312;&#21270;&#23398;&#20013;&#65292;&#31934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20027;&#35201;&#20381;&#36182;&#20110;&#20998;&#23376;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2304.07554</link><description>&lt;p&gt;
&#24418;&#29366;&#65288;&#20960;&#20046;&#65289;&#26159;&#20840;&#37096;&#65281;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#65288;PHFs&#65289;&#26159;&#39640;&#25928;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#20016;&#23500;&#20449;&#24687;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape is (almost) all!: Persistent homology features (PHFs) are an information rich input for efficient molecular machine learning. (arXiv:2304.07554v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07554
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#26159;&#19968;&#31181;&#32534;&#30721;&#20998;&#23376;&#24418;&#29366;&#30340;&#26041;&#27861;&#65292;&#23427;&#25429;&#25417;&#20102;&#21270;&#23398;&#20998;&#23376;&#30340;&#25299;&#25169;&#24418;&#29366;&#29305;&#24615;&#65292;&#24182;&#22312;&#33021;&#37327;&#20351;&#29992;&#25928;&#29575;&#19978;&#27604;&#20854;&#20182;&#32534;&#30721;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#34920;&#26126;&#22312;&#21270;&#23398;&#20013;&#65292;&#31934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20027;&#35201;&#20381;&#36182;&#20110;&#20998;&#23376;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#24418;&#29366;&#23545;&#20110;&#21270;&#23398;&#24456;&#37325;&#35201;&#65292;&#20294;&#26377;&#22810;&#37325;&#35201;&#21602;&#65311;&#26426;&#22120;&#23398;&#20064;&#22312;&#36755;&#20837;&#31616;&#21333;&#12289;&#21305;&#37197;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25928;&#26524;&#26368;&#22909;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#26222;&#36941;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#21270;&#23398;&#25968;&#25454;&#38598;&#24448;&#24448;&#38750;&#24120;&#23567;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#20174;&#27599;&#20010;&#25968;&#25454;&#28857;&#20013;&#33719;&#21462;&#26368;&#22810;&#30340;&#20449;&#24687;&#12290;&#25345;&#32493;&#21516;&#35843;&#27979;&#37327;&#28857;&#20113;&#22312;&#19981;&#21516;&#27604;&#20363;&#19979;&#30340;&#25299;&#25169;&#24418;&#29366;&#29305;&#24615;&#65292;&#24182;&#29992;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#21516;&#35843;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#65288;PHFs&#65289;&#65292;&#29992;&#20110;&#32534;&#30721;&#20998;&#23376;&#30340;&#24418;&#29366;&#65292;&#21516;&#26102;&#22833;&#21435;&#22823;&#22810;&#25968;&#31526;&#21495;&#32454;&#33410;&#65292;&#22914;&#21407;&#23376;&#26631;&#31614;&#12289;&#20215;&#12289;&#30005;&#33655;&#12289;&#38190;&#31561;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;PHFs&#30340;&#26377;&#29992;&#24615;&#65306;QM7&#12289;&#30095;&#27700;&#24615;&#12289;Delaney&#21644;Tox21&#12290;PHFs&#21644;&#26368;&#20339;&#22522;&#20934;&#19968;&#26679;&#26377;&#25928;&#12290;PHFs&#38750;&#24120;&#20449;&#24687;&#23494;&#38598;&#65292;&#27604;&#20854;&#20182;&#21457;&#29616;&#30340;&#32534;&#30721;&#26041;&#27861;&#35201;&#23567;&#24471;&#22810;&#65292;&#36825;&#24847;&#21619;&#30528;ML&#31639;&#27861;&#26356;&#21152;&#33410;&#33021;&#12290;&#23613;&#31649;&#22833;&#21435;&#20102;&#22823;&#37327;&#31526;&#21495;&#20449;&#24687;&#65292;PHFs&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#22312;&#21270;&#23398;&#20013;&#36827;&#34892;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20165;&#38656;&#35201;&#19977;&#32500;&#24418;&#29366;&#65288;&#20960;&#20046;&#65289;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
3-D shape is important to chemistry, but how important? Machine learning works best when the inputs are simple and match the problem well. Chemistry datasets tend to be very small compared to those generally used in machine learning so we need to get the most from each datapoint. Persistent homology measures the topological shape properties of point clouds at different scales and is used in topological data analysis. Here we investigate what persistent homology captures about molecular structure and create persistent homology features (PHFs) that encode a molecule's shape whilst losing most of the symbolic detail like atom labels, valence, charge, bonds etc. We demonstrate the usefulness of PHFs on a series of chemical datasets: QM7, lipophilicity, Delaney and Tox21. PHFs work as well as the best benchmarks. PHFs are very information dense and much smaller than other encoding methods yet found, meaning ML algorithms are much more energy efficient. PHFs success despite losing a large am
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38750;&#27954;&#22320;&#21306;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#30456;&#20851;&#24212;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#26597;&#21450;&#20851;&#38190;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24403;&#21069;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#21512;&#20316;&#30740;&#31350;&#21644;&#30693;&#35782;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.07542</link><description>&lt;p&gt;
&#38750;&#27954;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36235;&#21183;&#65306;30&#24180;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Research Trends in Africa: A 30 Years Overview with Bibliometric Analysis Review. (arXiv:2304.07542v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38750;&#27954;&#22320;&#21306;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#30456;&#20851;&#24212;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#26597;&#21450;&#20851;&#38190;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24403;&#21069;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#21512;&#20316;&#30740;&#31350;&#21644;&#30693;&#35782;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#27954;&#22320;&#21306;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#30456;&#20851;&#24212;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#26597;&#65292;&#24182;&#36827;&#34892;&#20102;&#20851;&#38190;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#30740;&#31350;&#12290;&#35813;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#30740;&#31350;&#20849;&#25910;&#38598;&#20102;2761&#31687;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#25991;&#29486;&#65292;&#20854;&#20013;98&#65285;&#26159;&#21457;&#34920;&#22312;903&#20010;&#26399;&#21002;&#19978;&#33267;&#23569;&#26377;482&#27425;&#24341;&#29992;&#30340;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;&#36807;&#21435;&#30340;30&#24180;&#12290;&#21478;&#22806;&#65292;&#36825;&#20123;&#25991;&#29486;&#26159;&#20174;Science Citation Index EXPANDED&#26816;&#32034;&#20013;&#26469;&#33258;&#20110;1993&#24180;&#33267;2021&#24180;&#20043;&#38388;54&#20010;&#38750;&#27954;&#22269;&#23478;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#12290;&#25991;&#29486;&#35745;&#37327;&#30740;&#31350;&#26174;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24403;&#21069;&#29616;&#29366;&#21644;&#26410;&#26469;&#36235;&#21183;&#30340;&#21487;&#35270;&#21270;&#65292;&#20197;&#20419;&#36827;&#38750;&#27954;&#22823;&#38470;&#20998;&#24067;&#22312;&#19981;&#21516;&#30740;&#31350;&#26426;&#26500;&#30340;&#20316;&#32773;&#20043;&#38388;&#36827;&#34892;&#26410;&#26469;&#30340;&#21512;&#20316;&#30740;&#31350;&#21644;&#30693;&#35782;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a critical bibliometric analysis study is conducted, coupled with an extensive literature survey on recent developments and associated applications in machine learning research with a perspective on Africa. The presented bibliometric analysis study consists of 2761 machine learning-related documents, of which 98% were articles with at least 482 citations published in 903 journals during the past 30 decades. Furthermore, the collated documents were retrieved from the Science Citation Index EXPANDED, comprising research publications from 54 African countries between 1993 and 2021. The bibliometric study shows the visualization of the current landscape and future trends in machine learning research and its application to facilitate future collaborative research and knowledge exchange among authors from different research institutions scattered across the African continent.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31639;&#27861;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979; Twitter &#26426;&#22120;&#20154;&#36134;&#21495;&#65292;&#36890;&#36807;&#23558;&#20854;&#34892;&#20026;&#36716;&#25442;&#20026;&#22270;&#29255;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.07535</link><description>&lt;p&gt;
&#20174;&#22312;&#32447;&#34892;&#20026;&#21040;&#22270;&#29255;&#65306;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Online Behaviours to Images: A Novel Approach to Social Bot Detection. (arXiv:2304.07535v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31639;&#27861;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979; Twitter &#26426;&#22120;&#20154;&#36134;&#21495;&#65292;&#36890;&#36807;&#23558;&#20854;&#34892;&#20026;&#36716;&#25442;&#20026;&#22270;&#29255;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#28040;&#36153;&#21644;&#20849;&#20139;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#22823;&#22534;&#19981;&#21487;&#38752;&#21644;&#19981;&#20934;&#30830;&#30340;&#20869;&#23481;&#12290;&#20854;&#20013;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#31038;&#20132;&#36134;&#21495;&#65292;&#34987;&#31216;&#20026;&#8220;&#26426;&#22120;&#20154;&#8221;&#65292;&#23427;&#20204;&#36890;&#36807;&#33258;&#21160;&#21270;&#25805;&#20316;&#26469;&#23459;&#20256;&#19981;&#21487;&#20449;&#30340;&#20869;&#23481;&#12289;&#36807;&#24230;&#25903;&#25345;&#25919;&#27835;&#27966;&#31995;&#21644;&#23459;&#20256;&#26426;&#26500;&#21270;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110; Twitter &#36134;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#23558;&#36134;&#21495;&#25191;&#34892;&#30340;&#25805;&#20316;&#24207;&#21015;&#36716;&#25442;&#25104;&#22270;&#29255;&#65292;&#28982;&#21518;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#21151;&#33021;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Social Networks have revolutionized how we consume and share information, but they have also led to a proliferation of content not always reliable and accurate. One particular type of social accounts is known to promote unreputable content, hyperpartisan, and propagandistic information. They are automated accounts, commonly called bots. Focusing on Twitter accounts, we propose a novel approach to bot detection: we first propose a new algorithm that transforms the sequence of actions that an account performs into an image; then, we leverage the strength of Convolutional Neural Networks to proceed with image classification. We compare our performances with state-of-the-art results for bot detection on genuine accounts / bot accounts datasets well known in the literature. The results confirm the effectiveness of the proposal, because the detection capability is on par with the state of the art, if not better in some cases.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07520</link><description>&lt;p&gt;
STAS: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07520
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#26377;&#25928;&#30340;&#33539;&#20363;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#36171;&#20449;&#29992;&#20540;&#65292;&#21363;&#36890;&#36807;&#20195;&#29702;&#30340;&#36129;&#29486;&#26469;&#32473;&#20195;&#29702;&#36171;&#20449;&#29992;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#38544;&#24335;&#22320;&#20998;&#35299;&#32852;&#21512;&#20215;&#20540;&#20989;&#25968;&#25110;&#26174;&#24335;&#22320;&#35745;&#31639;&#25152;&#26377;&#20195;&#29702;&#30340;&#25903;&#20184;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#26377;&#22312;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20840;&#23616;&#22870;&#21169;&#21482;&#33021;&#22312;&#21608;&#26399;&#32467;&#26463;&#26102;&#26174;&#31034;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#12290;&#23427;&#20204;&#32570;&#20047;&#23545;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#22312;&#26102;&#38388;&#32500;&#24230;&#20013;&#22797;&#26434;&#20851;&#31995;&#30340;&#24314;&#27169;&#21151;&#33021;&#65292;&#24182;&#19988;&#21463;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#38388;&#26102;&#38388;&#20851;&#27880;&#19982; Shapley&#65288;STAS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22238;&#25253;&#20998;&#35299;&#65307;STAS &#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#23398;&#20064;&#20449;&#29992;&#20998;&#37197;&#12290;&#23427;&#39318;&#20808;&#23558;&#20840;&#23616;&#22238;&#25253;&#20998;&#35299;&#22238;&#21040;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#28982;&#21518;&#20351;&#29992;Shapley&#20540;&#26469;&#35780;&#20272;&#21327;&#20316;MARL&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#36129;&#29486;&#12290; STAS &#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388; - &#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#25429;&#33719;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#20013;&#65292;STAS &#33021;&#22815;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is yet credit assignment, which aims to credit agents by their contributions. Prior studies focus on either implicitly decomposing the joint value function or explicitly computing the payoff distribution of all agents. However, in episodic reinforcement learning settings where global rewards can only be revealed at the end of the episode, existing methods usually fail to work. They lack the functionality of modeling complicated relations of the delayed global reward in the temporal dimension and suffer from large variance and bias. We propose a novel method named Spatial-Temporal Attention with Shapley (STAS) for return decomposition; STAS learns credit assignment in both the temporal and the spatial dimension. It first decomposes the global return back to each time step, then utilizes Shapley Value to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#20960;&#20309;&#29305;&#24449;&#21644;&#21151;&#33021;&#23545;&#24212;&#26469;&#21516;&#26102;&#23398;&#20064;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#24418;&#29366;&#32467;&#26500;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#36275;&#22815;&#20581;&#22766;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.07515</link><description>&lt;p&gt;
S3M&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#24212;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#32479;&#35745;&#24418;&#29366;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences. (arXiv:2304.07515v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#20960;&#20309;&#29305;&#24449;&#21644;&#21151;&#33021;&#23545;&#24212;&#26469;&#21516;&#26102;&#23398;&#20064;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#24418;&#29366;&#32467;&#26500;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#36275;&#22815;&#20581;&#22766;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;(SSM)&#26159;&#19968;&#31181;&#20960;&#20309;&#19978;&#34920;&#31034;&#20154;&#32676;&#35299;&#21078;&#23398;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#36153;&#21147;&#30340;&#25163;&#21160;&#20998;&#21106;&#25110;&#22320;&#26631;&#27880;&#37322;&#26469;&#29983;&#25104;&#12290;&#23545;&#20110;SSM&#30340;&#23545;&#24212;&#20272;&#35745;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#20960;&#20309;&#29305;&#24449;&#21644;&#21151;&#33021;&#23545;&#24212;&#26469;&#21516;&#26102;&#23398;&#20064;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#24418;&#29366;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#26174;&#33879;&#25913;&#36827;&#20102;SSM&#30340;&#26080;&#30417;&#30563;&#23545;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#34920;&#38754;&#25299;&#25169;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#29992;&#30002;&#29366;&#33146;&#21644;&#22810;&#23460;&#24515;&#33039;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#20581;&#22766;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#25193;&#22823;SSMs&#35268;&#27169;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical shape models (SSMs) are an established way to geometrically represent the anatomy of a population with various clinically relevant applications. However, they typically require domain expertise and labor-intensive manual segmentations or landmark annotations to generate. Methods to estimate correspondences for SSMs typically learn with such labels as supervision signals. We address these shortcomings by proposing an unsupervised method that leverages deep geometric features and functional correspondences to learn local and global shape structures across complex anatomies simultaneously. Our pipeline significantly improves unsupervised correspondence estimation for SSMs compared to baseline methods, even on highly irregular surface topologies. We demonstrate this for two different anatomical structures: the thyroid and a multi-chamber heart dataset. Furthermore, our method is robust enough to learn from noisy neural network predictions, enabling scaling SSMs to larger patien
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;SVRS&#21644;AccSVRS&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20854;&#20013;&#65292;AccSVRS&#31639;&#27861;&#23454;&#29616;&#20102;&#23436;&#20840;&#26080;&#24179;&#28369;&#24615;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#26356;&#26159;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07504</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#22343;&#20108;&#38454;&#30456;&#20284;&#24615;&#30340;&#38543;&#26426;&#20998;&#24067;&#24335;&#20248;&#21270;&#65306;&#31639;&#27861;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis. (arXiv:2304.07504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;SVRS&#21644;AccSVRS&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20854;&#20013;&#65292;AccSVRS&#31639;&#27861;&#23454;&#29616;&#20102;&#23436;&#20840;&#26080;&#24179;&#28369;&#24615;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#26356;&#26159;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;$n$&#20010;&#23458;&#25143;&#31471;&#30340;&#26377;&#38480;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#28385;&#36275;&#27969;&#34892;&#30340;$\delta$-&#30456;&#20284;&#24615;&#26465;&#20214;&#21644;$\mu$-&#24378;&#20984;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65306;SVRS&#21644;AccSVRS&#65292;&#21551;&#21457;&#33258;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#38750;&#21152;&#36895;&#30340;SVRS&#26041;&#27861;&#32467;&#21512;&#20102;&#26799;&#24230;&#28369;&#21160;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;$\tilde{\gO}(n {+} \sqrt{n}\delta/\mu)$&#65292;&#19982;&#29616;&#26377;&#30340;&#38750;&#21152;&#36895;&#31639;&#27861;&#30456;&#27604;&#26377;&#25152;&#25552;&#39640;&#12290;&#24212;&#29992;Katyusha X&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;AccSVRS&#30340;&#30452;&#25509;&#21152;&#36895;&#23454;&#38469;&#29256;&#26412;&#65292;&#20854;&#23436;&#20840;&#26080;&#24179;&#28369;&#24615;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#20026;$\tilde{\gO}(n {+} n^{3/4}\sqrt{\delta/\mu})$&#65292;&#22312;&#30149;&#24577;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#25509;&#36817;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;AccSVRS&#26041;&#27861;&#30340;&#32039;&#23494;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study finite-sum distributed optimization problems with $n$-clients under popular $\delta$-similarity condition and $\mu$-strong convexity. We propose two new algorithms: SVRS and AccSVRS motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient-sliding and variance reduction, which achieves superior communication complexity $\tilde{\gO}(n {+} \sqrt{n}\delta/\mu)$ compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X, we also build a direct accelerated practical version named AccSVRS with totally smoothness-free $\tilde{\gO}(n {+} n^{3/4}\sqrt{\delta/\mu})$ communication complexity that improves upon existing algorithms on ill-conditioning cases. Furthermore, we show a nearly matched lower bound to verify the tightness of our AccSVRS method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; TAP-GNN&#65292;&#36890;&#36807;&#25972;&#20010;&#37051;&#22495;&#30340;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#22270;&#27969;&#22330;&#26223;&#20013;&#25903;&#25345;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.07503</link><description>&lt;p&gt;
&#21160;&#24577;&#34920;&#31034;&#30340;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Aggregation and Propagation Graph Neural Networks for Dynamic Representation. (arXiv:2304.07503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; TAP-GNN&#65292;&#36890;&#36807;&#25972;&#20010;&#37051;&#22495;&#30340;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#22270;&#27969;&#22330;&#26223;&#20013;&#25903;&#25345;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23637;&#31034;&#20102;&#33410;&#28857;&#20043;&#38388;&#22312;&#36830;&#32493;&#26102;&#38388;&#20869;&#30340;&#21160;&#24577;&#20132;&#20114;&#65292;&#20854;&#25299;&#25169;&#38543;&#26102;&#38388;&#27969;&#36893;&#32780;&#28436;&#21464;&#12290;&#33410;&#28857;&#30340;&#25972;&#20010;&#26102;&#38388;&#37051;&#22495;&#26174;&#31034;&#20102;&#33410;&#28857;&#30340;&#21464;&#21270;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#31616;&#21270;&#36215;&#35265;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#37051;&#23621;&#29983;&#25104;&#21160;&#24577;&#34920;&#31034;&#65292;&#36825;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#21644;&#22312;&#32447;&#25512;&#29702;&#24310;&#36831;&#39640;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#20010;&#37051;&#22495;&#30340;&#26102;&#38388;&#22270;&#21367;&#31215;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TAP-GNN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23637;&#24320;&#26102;&#38388;&#22270;&#26469;&#20998;&#26512;&#21160;&#24577;&#34920;&#31034;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20351;&#29992;&#32858;&#21512;&#21644;&#20256;&#25773;&#65288;AP&#65289;&#22359;&#26469;&#26174;&#33879;&#20943;&#23569;&#21382;&#21490;&#37051;&#23621;&#30340;&#37325;&#22797;&#35745;&#31639;&#12290;&#26368;&#32456;&#65292;TAP-GNN&#25903;&#25345;&#22312;&#22270;&#27969;&#22330;&#26223;&#20013;&#36827;&#34892;&#22312;&#32447;&#25512;&#29702;&#65292;&#26082;&#39640;&#25928;&#21448;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graphs exhibit dynamic interactions between nodes over continuous time, whose topologies evolve with time elapsing.  The whole temporal neighborhood of nodes reveals the varying preferences of nodes.  However, previous works usually generate dynamic representation with limited neighbors for simplicity, which results in both inferior performance and high latency of online inference.  Therefore, in this paper, we propose a novel method of temporal graph convolution with the whole neighborhood, namely Temporal Aggregation and Propagation Graph Neural Networks (TAP-GNN).  Specifically, we firstly analyze the computational complexity of the dynamic representation problem by unfolding the temporal graph in a message-passing paradigm.  The expensive complexity motivates us to design the AP (aggregation and propagation) block, which significantly reduces the repeated computation of historical neighbors.  The final TAP-GNN supports online inference in the graph stream scenario, which i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07499</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#40065;&#26834;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust Educational Dialogue Act Classifiers with Low-Resource and Imbalanced Datasets. (arXiv:2304.07499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#34892;&#20026;&#21487;&#20197;&#20195;&#34920;&#22312;&#36741;&#23548;&#23545;&#35805;&#26399;&#38388;&#21457;&#29983;&#30340;&#25945;&#32451;&#21592;&#25110;&#23398;&#29983;&#30340;&#23545;&#35805;&#21160;&#20316;&#12290;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#33258;&#21160;&#35782;&#21035;&#23545;&#35805;&#34892;&#20026;&#23545;&#20110;&#22522;&#20110;&#23545;&#35805;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#35774;&#35745;&#26159;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#36741;&#23548;&#23545;&#35805;&#20013;&#30340;&#23545;&#35805;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#20351;&#29992;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#20302;&#36164;&#28304;&#25968;&#25454;&#22330;&#26223;&#65289;&#26469;&#20248;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#21487;&#20197;&#21453;&#26144;&#20998;&#31867;&#22120;&#23398;&#20064;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#30340;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30740;&#31350;&#37319;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25439;&#22833;&#26469;&#20248;&#21270;&#20302;&#36164;&#28304;&#25968;&#25454;&#20013;&#30340;DA&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;DA&#20998;&#31867;&#22120;&#24448;&#24448;&#20197;&#29306;&#29298;&#23569;&#25968;&#31867;&#30340;&#20195;&#20215;&#26469;&#20248;&#20808;&#32771;&#34385;&#22823;&#22810;&#25968;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#22312;&#23569;&#25968;&#31867;&#19978;&#24615;&#33021;&#24046;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;DA&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;DA&#20998;&#31867;&#22120;&#22312;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MIREX&#22312;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#30340;DA&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue acts (DAs) can represent conversational actions of tutors or students that take place during tutoring dialogues. Automating the identification of DAs in tutoring dialogues is significant to the design of dialogue-based intelligent tutoring systems. Many prior studies employ machine learning models to classify DAs in tutoring dialogues and invest much effort to optimize the classification accuracy by using limited amounts of training data (i.e., low-resource data scenario). However, beyond the classification accuracy, the robustness of the classifier is also important, which can reflect the capability of the classifier on learning the patterns from different class distributions. We note that many prior studies on classifying educational DAs employ cross entropy (CE) loss to optimize DA classifiers on low-resource data with imbalanced DA distribution. The DA classifiers in these studies tend to prioritize accuracy on the majority class at the expense of the minority class which 
&lt;/p&gt;</description></item><item><title>SalientGrads&#26159;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#23376;&#32593;&#32476;&#21644;&#20165;&#20256;&#36755;&#39640;&#24230;&#31232;&#30095;&#30340;&#26799;&#24230;&#26469;&#31616;&#21270;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#39640;&#36798;90%&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#36798;60%&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07488</link><description>&lt;p&gt;
SalientGrads&#65306;&#38754;&#21521;&#36890;&#20449;&#25928;&#29575;&#21644;&#25968;&#25454;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#31232;&#30095;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SalientGrads: Sparse Models for Communication Efficient and Data Aware Distributed Federated Training. (arXiv:2304.07488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07488
&lt;/p&gt;
&lt;p&gt;
SalientGrads&#26159;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#23376;&#32593;&#32476;&#21644;&#20165;&#20256;&#36755;&#39640;&#24230;&#31232;&#30095;&#30340;&#26799;&#24230;&#26469;&#31616;&#21270;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#39640;&#36798;90%&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#36798;60%&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#31449;&#28857;&#21033;&#29992;&#20998;&#25955;&#30340;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#36890;&#36807;&#19981;&#25910;&#38598;&#25968;&#25454;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#22823;&#25361;&#25112;&#26159;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#23458;&#25143;&#31471;&#33410;&#28857;&#35745;&#31639;&#21644;&#36890;&#20449;&#24102;&#23485;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20256;&#36755;&#31232;&#30095;&#27169;&#22411;&#21644;&#36845;&#20195;&#23398;&#20064;&#21160;&#24577;&#25513;&#30721;&#31561;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#38656;&#35201;&#20256;&#36755;&#27169;&#22411;&#26435;&#37325;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#29305;&#23450;&#30340;&#21098;&#26525;&#26631;&#20934;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SalientGrads&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#22522;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#35745;&#31639;&#27169;&#22411;&#21442;&#25968;&#30340;&#26174;&#30528;&#24615;&#20998;&#25968;&#65292;&#20174;&#32780;&#36873;&#25321;&#19968;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#23376;&#32593;&#32476;&#26469;&#31616;&#21270;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#20165;&#20256;&#36755;&#39640;&#24230;&#31232;&#30095;&#30340;&#26799;&#24230;&#65292;&#32780;&#19981;&#20687;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#20256;&#36755;&#25972;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SalientGrads &#21487;&#20197;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#39640;&#36798; 90% &#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#36798; 60% &#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables the training of a model leveraging decentralized data in client sites while preserving privacy by not collecting data. However, one of the significant challenges of FL is limited computation and low communication bandwidth in resource limited edge client nodes. To address this, several solutions have been proposed in recent times including transmitting sparse models and learning dynamic masks iteratively, among others. However, many of these methods rely on transmitting the model weights throughout the entire training process as they are based on ad-hoc or random pruning criteria. In this work, we propose Salient Grads, which simplifies the process of sparse training by choosing a data aware subnetwork before training, based on the model-parameter's saliency scores, which is calculated from the local client data. Moreover only highly sparse gradients are transmitted between the server and client models during the training process unlike most methods that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07472</link><description>&lt;p&gt;
&#36890;&#29992;&#26680;&#23398;&#20064;&#30340;&#39640;&#25928;&#20984;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Convex Algorithms for Universal Kernel Learning. (arXiv:2304.07472v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#30340;&#26680;&#38598;&#12290;&#29702;&#24819;&#30340;&#26680;&#38598;&#24212;&#35813;&#65306;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#65288;&#20197;&#20415;&#20110;&#21487;&#22788;&#29702;&#24615;&#65289;&#65307;&#22312;&#25152;&#26377;&#26680;&#38598;&#20013;&#23494;&#38598;&#65288;&#20197;&#20415;&#20110;&#40065;&#26834;&#24615;&#65289;&#65307;&#26159;&#36890;&#29992;&#30340;&#65288;&#20197;&#20415;&#20110;&#20934;&#30830;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#23450;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#19968;&#31867;&#27491;&#21322;&#20998;&#31163;&#26680;&#12290;&#23613;&#31649;&#27492;&#31867;&#26680;&#33021;&#22815;&#28385;&#36275;&#25152;&#26377;&#19977;&#20010;&#26631;&#20934;&#65292;&#20294;&#20043;&#21069;&#29992;&#20110;&#20248;&#21270;&#27492;&#31867;&#26680;&#30340;&#31639;&#27861;&#20165;&#38480;&#20110;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#20381;&#36182;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#30340;&#38382;&#39064;&#20316;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#20854;&#19982;&#20043;&#21069;&#22522;&#20110;SDP&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy and complexity of machine learning algorithms based on kernel optimization are determined by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). Recently, a framework was proposed for using positive matrices to parameterize a class of positive semi-separable kernels. Although this class can be shown to meet all three criteria, previous algorithms for optimization of such kernels were limited to classification and furthermore relied on computationally complex Semidefinite Programming (SDP) algorithms. In this paper, we pose the problem of learning semiseparable kernels as a minimax optimization problem and propose a SVD-QCQP primal-dual algorithm which dramatically reduces the computational complexity as compared with previous SDP-based approaches. Furthermore, we provide an efficient implementation of thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.07470</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#30340;&#32593;&#32476;&#23433;&#20840;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Weakly-supervised Cybersecurity Anomaly Detection. (arXiv:2304.07470v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#22522;&#20110;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#25915;&#20987;&#32773;&#31363;&#21462;&#29992;&#25143;&#25935;&#24863;&#25968;&#25454;&#30340;&#32593;&#32476;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#31181;&#25915;&#20987;&#30340;&#35268;&#27169;&#21644;&#39057;&#29575;&#27491;&#22312;&#36805;&#36895;&#21319;&#32423;&#65292;&#24182;&#24433;&#21709;&#19982;&#20114;&#32852;&#32593;&#36830;&#25509;&#30340;&#21508;&#31181;&#31995;&#32479;&#21644;&#35774;&#22791;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#36275;&#22815;&#22320;&#24212;&#23545;&#22797;&#26434;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#26032;&#23041;&#32961;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#22823;&#31361;&#30772;&#24341;&#36215;&#20102;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#30028;&#30340;&#20852;&#36259;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#22686;&#24378;&#29616;&#26377;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25152;&#26377;&#26032;&#20852;&#21644;&#22797;&#26434;&#30340;&#25915;&#20987;&#25910;&#38598;&#24102;&#26631;&#31614;&#30340;&#24322;&#24120;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#21153;&#23454;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20196;&#20154;&#40723;&#33310;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased reliance on Internet based technologies, cyberattacks compromising users' sensitive data are becoming more prevalent. The scale and frequency of these attacks are escalating rapidly, affecting systems and devices connected to the Internet. The traditional defense mechanisms may not be sufficiently equipped to handle the complex and ever-changing new threats. The significant breakthroughs in the machine learning methods including deep learning, had attracted interests from the cybersecurity research community for further enhancements in the existing anomaly detection methods. Unfortunately, collecting labelled anomaly data for all new evolving and sophisticated attacks is not practical. Training and tuning the machine learning model for anomaly detection using only a handful of labelled data samples is a pragmatic approach. Therefore, few-shot weakly supervised anomaly detection is an encouraging research direction. In this paper, we propose an enhancement to an existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07460</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#25928;&#36890;&#20449;&#21644;&#33410;&#33021;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#22312;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#36991;&#20813;&#20102;&#20174;&#26412;&#22320;&#25968;&#25454;&#38598;&#27844;&#28431;&#30452;&#25509;&#20449;&#24687;&#65292;&#20294;&#20173;&#21487;&#33021;&#20174;&#20849;&#20139;&#27169;&#22411;&#25512;&#26029;&#20986;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;FL&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#25552;&#20379;&#27491;&#24335;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26080;&#32447;&#36793;&#32536;&#37096;&#32626;FL&#26102;&#65292;&#30830;&#20445;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;DP&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#31232;&#30095;&#21270;&#30340;&#31169;&#26377;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;PFELS&#65289;&#30340;&#26032;&#22411;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#20869;&#22312;&#20449;&#36947;&#22122;&#22768;&#30340;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;PFELS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#27599;&#20010;&#35774;&#22791;&#20808;&#21387;&#32553;&#20854;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#26681;&#25454;&#26080;&#32447;&#20449;&#36947;&#33258;&#36866;&#24212;&#35774;&#35745;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#30340;&#21457;&#36865;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#20004;&#20010;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#26082;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.07453</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Context-aware Domain Adaptation for Time Series Anomaly Detection. (arXiv:2304.07453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#20004;&#20010;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#26082;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#24191;&#27867;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#30001;&#20110;&#26631;&#31614;&#31232;&#30095;&#24615;&#65292;&#35757;&#32451;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#26368;&#36817;&#24050;&#32463;&#33268;&#21147;&#20110;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#20197;&#21033;&#29992;&#31867;&#20284;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#22240;&#20854;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#32780;&#22312;&#24322;&#24120;&#26041;&#38754;&#21463;&#21040;&#36127;&#38754;&#30693;&#35782;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#21463;&#21040;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#23454;&#35777;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#20004;&#20010;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21516;&#26102;&#24314;&#27169;&#22797;&#26434;&#30340;&#22495;&#20869;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#36328;&#22495;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#28304;&#22495;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#23558;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#24322;&#24120;&#26816;&#27979;&#32467;&#21512;&#21040;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#37319;&#26679;&#34920;&#36848;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#28304;&#22495;&#30340;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#21644;&#26377;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a challenging task with a wide range of real-world applications. Due to label sparsity, training a deep anomaly detector often relies on unsupervised approaches. Recent efforts have been devoted to time series domain adaptation to leverage knowledge from similar domains. However, existing solutions may suffer from negative knowledge transfer on anomalies due to their diversity and sparsity. Motivated by the empirical study of context alignment between two domains, we aim to transfer knowledge between two domains via adaptively sampling context information for two domains. This is challenging because it requires simultaneously modeling the complex in-domain temporal dependencies and cross-domain correlations while exploiting label information from the source domain. To this end, we propose a framework that combines context sampling and anomaly detection into a joint learning procedure. We formulate context sampling into the Markov decision process and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#26041;&#38754;&#24050;&#32463;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#30456;&#20284;&#24230;&#26816;&#32034;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#20449;&#21495;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36991;&#20813;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07449</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#29992;&#20110;&#22522;&#20110;&#38899;&#20048;&#30456;&#20284;&#24230;&#26816;&#32034;&#21644;&#33258;&#21160;&#26631;&#27880;&#30340;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Auxiliary Loss for Metric Learning in Music Similarity-based Retrieval and Auto-tagging. (arXiv:2304.07449v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#26041;&#38754;&#24050;&#32463;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#30456;&#20284;&#24230;&#26816;&#32034;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#20449;&#21495;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36991;&#20813;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26816;&#32034;&#21644;&#33258;&#21160;&#26631;&#35760;&#26159;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#32771;&#34385;&#21040;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#38480;&#21046;&#24615;&#21644;&#19981;&#21487;&#25193;&#23637;&#24615;&#65292;&#35753;&#27169;&#22411;&#20174;&#20854;&#20182;&#26469;&#28304;&#23398;&#20064;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20165;&#20381;&#36182;&#20110;&#20174;&#38899;&#20048;&#38899;&#39057;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#22312;&#33258;&#21160;&#26631;&#35760;&#30340;&#32972;&#26223;&#19979;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#21516;&#26102;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#20449;&#21495;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#32780;&#19981;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36991;&#20813;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of music information retrieval, similarity-based retrieval and auto-tagging serve as essential components. Given the limitations and non-scalability of human supervision signals, it becomes crucial for models to learn from alternative sources to enhance their performance. Self-supervised learning, which exclusively relies on learning signals derived from music audio data, has demonstrated its efficacy in the context of auto-tagging. In this study, we propose a model that builds on the self-supervised learning approach to address the similarity-based retrieval challenge by introducing our method of metric learning with a self-supervised auxiliary loss. Furthermore, diverging from conventional self-supervised learning methodologies, we discovered the advantages of concurrently training the model with both self-supervision and supervision signals, without freezing pre-trained models. We also found that refraining from employing augmentation during the fine-tuning phase yields
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#30446;&#26631;&#40657;&#30418;&#20248;&#21270;&#21644;&#19981;&#26029;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26448;&#26009;&#30340;&#20840;&#33258;&#21160;&#35774;&#35745;&#12290;&#36890;&#36807;&#33258;&#20027;&#25805;&#20316;&#23454;&#39564;&#23460;&#65292;&#35813;&#24037;&#20316;&#27969;&#25104;&#21151;&#30830;&#23450;&#20102;2,2,2-&#19977;&#27679;&#20057;&#22522;&#30002;&#22522;&#30899;&#37240;&#37231;&#30340;&#29702;&#24819;&#21046;&#36896;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.07445</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#26448;&#26009;&#20840;&#33258;&#21160;&#35774;&#35745;&#30340;&#26694;&#26550;&#65306;&#25361;&#25112;&#21644;&#19979;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps. (arXiv:2304.07445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#30446;&#26631;&#40657;&#30418;&#20248;&#21270;&#21644;&#19981;&#26029;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26448;&#26009;&#30340;&#20840;&#33258;&#21160;&#35774;&#35745;&#12290;&#36890;&#36807;&#33258;&#20027;&#25805;&#20316;&#23454;&#39564;&#23460;&#65292;&#35813;&#24037;&#20316;&#27969;&#25104;&#21151;&#30830;&#23450;&#20102;2,2,2-&#19977;&#27679;&#20057;&#22522;&#30002;&#22522;&#30899;&#37240;&#37231;&#30340;&#29702;&#24819;&#21046;&#36896;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#25968;&#25454;&#33719;&#21462;&#26114;&#36149;&#19988;&#23384;&#22312;&#22810;&#20010;&#31454;&#20105;&#35774;&#35745;&#26631;&#20934;&#30340;&#23454;&#38469;&#33258;&#21160;&#39550;&#39542;&#23454;&#39564;&#23460;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#65292;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#26234;&#33021;&#37319;&#26679;&#65292;&#24179;&#34913;&#24615;&#33021;&#26435;&#34913;&#21644;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#40657;&#30418;&#20248;&#21270;&#21644;&#19981;&#26029;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#20010;&#24037;&#20316;&#27969;&#24314;&#31435;&#22312;&#23454;&#26102;&#25968;&#25454;&#27969;&#21644;&#27169;&#22359;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#36719;&#20214;&#24320;&#21457;&#30340;&#24320;&#28304;&#25216;&#26415;&#20043;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#32493;&#27969;&#21270;&#23398;&#23454;&#39564;&#23460;&#30340;&#33258;&#20027;&#25805;&#20316;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#24037;&#20316;&#27969;&#30340;&#27010;&#24565;&#35777;&#26126;&#65292;&#35813;&#23454;&#39564;&#23460;&#30830;&#23450;&#20102;2,2,2-&#19977;&#27679;&#20057;&#22522;&#30002;&#22522;&#30899;&#37240;&#37231;&#30340;&#29702;&#24819;&#21046;&#36896;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to deploy machine learning in a real-world self-driving laboratory where data acquisition is costly and there are multiple competing design criteria, systems need to be able to intelligently sample while balancing performance trade-offs and constraints. For these reasons, we present an active learning process based on multiobjective black-box optimization with continuously updated machine learning models. This workflow is built on open-source technologies for real-time data streaming and modular multiobjective optimization software development. We demonstrate a proof of concept for this workflow through the autonomous operation of a continuous-flow chemistry laboratory, which identifies ideal manufacturing conditions for the electrolyte 2,2,2-trifluoroethyl methyl carbonate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21517;&#20026;Meta-Quantum Optimization (MQO)&#65292;&#30456;&#27604;&#24403;&#21069;&#20381;&#36182;&#20110;&#37327;&#23376;&#35774;&#22791;&#35745;&#31639;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25193;&#23637;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20316;&#32773;&#22312;&#20108;&#20803;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#25104;&#21151;&#22320;&#24212;&#29992;&#20102;MQO&#31639;&#27861;&#26469;&#20248;&#21270;QNNs&#65292;&#20026;QNNs&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07442</link><description>&lt;p&gt;
&#26080;&#38656;&#26799;&#24230;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning To Optimize Quantum Neural Network Without Gradients. (arXiv:2304.07442v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21517;&#20026;Meta-Quantum Optimization (MQO)&#65292;&#30456;&#27604;&#24403;&#21069;&#20381;&#36182;&#20110;&#37327;&#23376;&#35774;&#22791;&#35745;&#31639;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25193;&#23637;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20316;&#32773;&#22312;&#20108;&#20803;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#25104;&#21151;&#22320;&#24212;&#29992;&#20102;MQO&#31639;&#27861;&#26469;&#20248;&#21270;QNNs&#65292;&#20026;QNNs&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#23376;&#39046;&#22495;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#20043;&#19968;&#26159;&#36890;&#36807;&#23558;&#25968;&#25454;&#32534;&#30721;&#25104;&#37327;&#23376;&#24577;&#26469;&#25191;&#34892;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#12290;&#20174;&#32463;&#20856;&#21040;&#37327;&#23376;&#30340;&#25193;&#23637;&#24471;&#20197;&#23454;&#29616;&#26159;&#30001;&#20110;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#20351;&#29992;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#28151;&#21512;&#31639;&#27861;&#30340;&#35757;&#32451;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#30340;QNN&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#35745;&#31639;&#30456;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35745;&#31639;&#38750;&#24120;&#38590;&#20197;&#25193;&#23637;&#65292;&#24182;&#21463;&#21040;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#20013;&#23384;&#22312;&#30340;&#30828;&#20214;&#21644;&#37319;&#26679;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26799;&#24230;&#20449;&#24687;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#20248;&#21270;&#31639;&#27861;Meta-Quantum Optimization&#65288;MQO)&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;QNNs&#12290;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;QNNs&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MQO&#21487;&#20197;&#25104;&#21151;&#20248;&#21270;QNNs&#65292;&#24182;&#19988;&#26159;&#35757;&#32451;QNNs&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning is an emerging sub-field in machine learning where one of the goals is to perform pattern recognition tasks by encoding data into quantum states. This extension from classical to quantum domain has been made possible due to the development of hybrid quantum-classical algorithms that allow a parameterized quantum circuit to be optimized using gradient based algorithms that run on a classical computer. The similarities in training of these hybrid algorithms and classical neural networks has further led to the development of Quantum Neural Networks (QNNs). However, in the current training regime for QNNs, the gradients w.r.t objective function have to be computed on the quantum device. This computation is highly non-scalable and is affected by hardware and sampling noise present in the current generation of quantum hardware. In this paper, we propose a training algorithm that does not rely on gradient information. Specifically, we introduce a novel meta-optimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#29420;&#31435;&#36827;&#21270;&#30340;&#29289;&#31181;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#25216;&#33021;&#21457;&#29616;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#23384;&#26723;&#25110;&#39044;&#20808;&#23450;&#20041;&#34892;&#20026;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.07425</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#20248;&#36136;&#20010;&#20307;&#23454;&#29616;&#39640;&#25928;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Quality-Diversity Optimization through Diverse Quality Species. (arXiv:2304.07425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#29420;&#31435;&#36827;&#21270;&#30340;&#29289;&#31181;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#25216;&#33021;&#21457;&#29616;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#23384;&#26723;&#25110;&#39044;&#20808;&#23450;&#20041;&#34892;&#20026;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#30340;&#26222;&#36941;&#23616;&#38480;&#24615;&#26159;&#23427;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#31639;&#27861;&#26469;&#32416;&#27491;&#65292;&#20854;&#20013;&#39318;&#36873;&#38382;&#39064;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#35299;&#20915;&#26041;&#26696;&#30340;&#20154;&#32676;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;QD&#26041;&#27861;&#65292;&#20363;&#22914;MAP-Elites&#65292;&#26126;&#30830;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#34987;&#20998;&#35299;&#25104;&#39044;&#23450;&#20041;&#30340;&#22721;&#40859;&#30340;&#34892;&#20026;&#26723;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23384;&#26723;&#25110;&#39044;&#20808;&#23450;&#20041;&#34892;&#20026;&#33539;&#22260;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#31181;&#32676;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#29420;&#31435;&#36827;&#21270;&#30340;&#29289;&#31181;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#25216;&#33021;&#21457;&#29616;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#31361;&#21464;&#26469;&#23436;&#25104;&#65292;&#36825;&#20123;&#31361;&#21464;&#37319;&#21462;&#20114;&#20449;&#24687;&#21644;&#24615;&#33021;&#30340;&#20449;&#24687;&#29702;&#35770;&#35270;&#35282;&#20849;&#21516;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#20248;&#36136;&#29289;&#31181;(DQS)&#20316;&#20026;&#23384;&#26723;&#22411;QD&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prevalent limitation of optimizing over a single objective is that it can be misguided, becoming trapped in local optimum. This can be rectified by Quality-Diversity (QD) algorithms, where a population of high-quality and diverse solutions to a problem is preferred. Most conventional QD approaches, for example, MAP-Elites, explicitly manage a behavioral archive where solutions are broken down into predefined niches. In this work, we show that a diverse population of solutions can be found without the limitation of needing an archive or defining the range of behaviors in advance. Instead, we break down solutions into independently evolving species and use unsupervised skill discovery to learn diverse, high-performing solutions. We show that this can be done through gradient-based mutations that take on an information theoretic perspective of jointly maximizing mutual information and performance. We propose Diverse Quality Species (DQS) as an alternative to archive-based QD algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedPC&#65292;&#20854;&#20013;&#20351;&#29992;&#32780;&#19981;&#24433;&#21709;&#21496;&#26426;&#38544;&#31169;&#30340;&#36710;&#20869;&#30456;&#26426;&#25968;&#25454;&#36827;&#34892;&#33258;&#28982;&#39550;&#39542;&#34892;&#20026;&#35782;&#21035;&#65288;NDAR&#65289;&#27169;&#22411;&#30340;&#24314;&#31435;&#65292;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07421</link><description>&lt;p&gt;
&#33258;&#28982;&#39550;&#39542;&#34892;&#20026;&#35782;&#21035;&#30340;&#28857;&#23545;&#28857;&#32852;&#37030;&#19981;&#26029;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Peer-to-Peer Federated Continual Learning for Naturalistic Driving Action Recognition. (arXiv:2304.07421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedPC&#65292;&#20854;&#20013;&#20351;&#29992;&#32780;&#19981;&#24433;&#21709;&#21496;&#26426;&#38544;&#31169;&#30340;&#36710;&#20869;&#30456;&#26426;&#25968;&#25454;&#36827;&#34892;&#33258;&#28982;&#39550;&#39542;&#34892;&#20026;&#35782;&#21035;&#65288;NDAR&#65289;&#27169;&#22411;&#30340;&#24314;&#31435;&#65292;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#39550;&#39542;&#34892;&#20026;&#35782;&#21035;&#65288;NDAR&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26816;&#27979;&#39550;&#39542;&#21592;&#20998;&#24515;&#21644;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#39118;&#38505;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36710;&#20869;&#30456;&#26426;&#30340;&#20405;&#20837;&#24335;&#35774;&#35745;&#24341;&#36215;&#20102;&#23545;&#39550;&#39542;&#21592;&#38544;&#31169;&#30340;&#25285;&#24551;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28857;&#23545;&#28857;&#65288;P2P&#65289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#65292;&#21363;FedPC&#65292;&#23427;&#30830;&#20445;&#20102;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19987;&#27880;&#20110;&#22312;&#26080;&#26381;&#21153;FL&#26694;&#26550;&#20013;&#35299;&#20915;&#23458;&#25143;&#31471;&#30340;&#30446;&#26631;&#65292;&#26088;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#20934;&#30830;&#30340;NDAR&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;NDAR&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#21644;&#35780;&#20272;&#20102;FedPC&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2023&#24180;AICity&#25361;&#25112;&#36187;&#20013;&#30340;State Farm&#20998;&#24515;&#39550;&#39542;&#24102;&#26816;&#27979;&#21644;Track 3 NDAR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;FedPC&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#23458;&#25143;&#31471;&#21040;&#26381;&#21153;&#22120;&#65288;C2S&#65289;FL&#30340;&#24378;&#22823;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Naturalistic driving action recognition (NDAR) has proven to be an effective method for detecting driver distraction and reducing the risk of traffic accidents. However, the intrusive design of in-cabin cameras raises concerns about driver privacy. To address this issue, we propose a novel peer-to-peer (P2P) federated learning (FL) framework with continual learning, namely FedPC, which ensures privacy and enhances learning efficiency while reducing communication, computational, and storage overheads. Our framework focuses on addressing the clients' objectives within a serverless FL framework, with the goal of delivering personalized and accurate NDAR models. We demonstrate and evaluate the performance of FedPC on two real-world NDAR datasets, including the State Farm Distracted Driver Detection and Track 3 NDAR dataset in the 2023 AICity Challenge. The results of our experiments highlight the strong competitiveness of FedPC compared to the conventional client-to-server (C2S) FLs in ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Transformer&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#32431;&#24230;&#20316;&#20026;&#25351;&#26631;&#65292;&#37319;&#29992;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32500;&#25345;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;Cross-attention&#26426;&#21046;&#25552;&#39640;&#32858;&#31867;&#30340;&#32431;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07408</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#32858;&#31867;&#31639;&#27861;:&#19968;&#31181;&#26032;&#30340;Transformer&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fairness in Visual Clustering: A Novel Transformer Clustering Approach. (arXiv:2304.07408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Transformer&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#32431;&#24230;&#20316;&#20026;&#25351;&#26631;&#65292;&#37319;&#29992;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32500;&#25345;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;Cross-attention&#26426;&#21046;&#25552;&#39640;&#32858;&#31867;&#30340;&#32431;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#32858;&#31867;&#30340;&#24773;&#26223;&#19979;&#65292;&#20026;&#20102;&#20943;&#23569;&#20154;&#32676;&#20559;&#24046;&#32780;&#22686;&#21152;&#28145;&#24230;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#20174;&#32858;&#31867;&#32431;&#24230;&#30340;&#35282;&#24230;&#35780;&#20272;&#20102;&#28145;&#24230;&#32858;&#31867;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#20559;&#24046;&#65292;&#32858;&#31867;&#32431;&#24230;&#26159;&#25351;&#32858;&#31867;&#20013;&#27491;&#26679;&#26412;&#19982;&#23427;&#20204;&#30340;&#30456;&#20851;&#31243;&#24230;&#30340;&#27604;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#40723;&#21169;&#25152;&#26377;&#32858;&#31867;&#30340;&#32431;&#24230;&#19968;&#33268;&#24615;&#20197;&#32500;&#25345;&#23398;&#20064;&#21040;&#30340;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Cross-attention&#26426;&#21046;&#65292;&#29992;&#20110;&#27979;&#37327;&#22810;&#20010;&#32858;&#31867;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#24378;&#36828;&#36317;&#31163;&#30340;&#27491;&#26679;&#26412;&#65292;&#25552;&#39640;&#32858;&#31867;&#30340;&#32431;&#24230;&#12290;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#22810;&#31181;&#23646;&#24615;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.07407</link><description>&lt;p&gt;
&#26410;&#35266;&#27979;&#21040;&#20195;&#29702;&#22870;&#21169;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents. (arXiv:2304.07407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#22330;&#26223;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#19968;&#31181;&#32769;&#34382;&#26426;&#21518;&#20250;&#33719;&#24471;&#22870;&#21169;&#21644;&#28608;&#21169;&#65292;&#20294;&#36127;&#36131;&#20154;&#21482;&#33021;&#35266;&#23519;&#21040;&#20195;&#29702;&#36873;&#25321;&#20102;&#21738;&#20010;&#32769;&#34382;&#26426;&#20197;&#21450;&#20195;&#29702;&#30456;&#24212;&#30340;&#28608;&#21169;&#65292;&#32780;&#24819;&#35201;&#35774;&#35745;&#19968;&#31181;&#21512;&#36866;&#30340;&#31574;&#30053;&#21364;&#20805;&#28385;&#20102;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by a number of real-world applications from domains like healthcare and sustainable transportation, in this paper we study a scenario of repeated principal-agent games within a multi-armed bandit (MAB) framework, where: the principal gives a different incentive for each bandit arm, the agent picks a bandit arm to maximize its own expected reward plus incentive, and the principal observes which arm is chosen and receives a reward (different than that of the agent) for the chosen arm. Designing policies for the principal is challenging because the principal cannot directly observe the reward that the agent receives for their chosen actions, and so the principal cannot directly learn the expected reward using existing estimation techniques. As a result, the problem of designing policies for this scenario, as well as similar ones, remains mostly unexplored. In this paper, we construct a policy that achieves a low regret (i.e., square-root regret up to a log factor) in this scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07396</link><description>&lt;p&gt;
&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#30340;&#24739;&#32773;&#39044;&#31579;&#36873;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#24739;&#32773;&#30340;&#20020;&#24202;&#35797;&#39564;&#65292;&#21307;&#29983;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#26816;&#26597;&#65292;&#20197;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#25991;&#26412;&#22522;&#20934;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#21644;&#20020;&#24202;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23578;&#26410;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;InstructGPT&#36741;&#21161;&#21307;&#29983;&#26681;&#25454;&#24739;&#32773;&#30340;&#21307;&#30103;&#31616;&#20917;&#30830;&#23450;&#20854;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#30340;&#36164;&#26684;&#12290;&#20351;&#29992;&#19968;&#27425;&#24615;&#12289;&#36873;&#25321;-&#25512;&#29702;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#22235;&#20010;&#32423;&#21035;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65306;&#33021;&#21542;&#20174;&#20020;&#24202;&#35797;&#39564;&#20013;&#32473;&#20986;&#30340;&#21307;&#30103;&#31616;&#20917;&#20013;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#65307;&#33021;&#21542;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#20934;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#24739;&#32773;&#65307;&#25972;&#20307;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#38656;&#27714;&#39044;&#27979;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25910;&#30410;&#31649;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20165;&#20351;&#29992;&#21382;&#21490;&#39044;&#35746;&#25968;&#25454;&#26469;&#29983;&#25104;&#31454;&#20215;&#20215;&#26684;&#12290;</title><link>http://arxiv.org/abs/2304.07391</link><description>&lt;p&gt;
&#26080;&#38656;&#38656;&#27714;&#39044;&#27979;&#30340;&#25910;&#30410;&#31649;&#29702;&#65306;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31454;&#20215;&#20215;&#26684;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revenue Management without Demand Forecasting: A Data-Driven Approach for Bid Price Generation. (arXiv:2304.07391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#38656;&#27714;&#39044;&#27979;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25910;&#30410;&#31649;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20165;&#20351;&#29992;&#21382;&#21490;&#39044;&#35746;&#25968;&#25454;&#26469;&#29983;&#25104;&#31454;&#20215;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25910;&#30410;&#31649;&#29702;&#20381;&#36182;&#38271;&#26399;&#31283;&#23450;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#21487;&#39044;&#27979;&#30340;&#38656;&#27714;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#24182;&#38750;&#24635;&#26159;&#21487;&#33021;&#30340;&#12290;&#35768;&#22810;&#34892;&#19994;&#37117;&#38754;&#20020;&#25345;&#32493;&#30340;&#38656;&#27714;&#27874;&#21160;&#65292;&#20363;&#22914;&#31354;&#20013;&#36135;&#36816;&#65292;&#23427;&#20855;&#26377;&#26356;&#30701;&#30340;&#39044;&#35746;&#21608;&#26399;&#21644;&#39640;&#24230;&#21464;&#21270;&#30340;&#25209;&#37327;&#21040;&#36135;&#12290;&#21363;&#20351;&#23545;&#20110;&#25910;&#20837;&#31649;&#29702;&#65288;RM&#65289;&#25104;&#29087;&#30340;&#20056;&#23458;&#33322;&#31354;&#20844;&#21496;&#65292;&#24212;&#23545;&#22806;&#37096;&#20914;&#20987;&#20063;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#29992;&#25143;&#30417;&#25511;&#21644;&#25163;&#21160;&#24178;&#39044;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;RM&#36824;&#35201;&#27714;&#26377;&#22823;&#37327;&#21382;&#21490;&#39044;&#35746;&#21644;&#20215;&#26684;&#25968;&#25454;&#65292;&#21363;&#20351;&#27809;&#26377;&#20219;&#20309;&#39044;&#35746;&#65292;&#20063;&#38656;&#36328;&#36234;&#22810;&#24180;&#12290;&#23545;&#20110;&#27809;&#26377;&#24314;&#31435;RM&#24815;&#20363;&#30340;&#20844;&#21496;&#26469;&#35828;&#65292;&#36825;&#31181;&#24191;&#27867;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#21487;&#33719;&#21462;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;RM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#38656;&#27714;&#39044;&#27979;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#21382;&#21490;&#39044;&#35746;&#25968;&#25454;&#26469;&#29983;&#25104;&#31454;&#20215;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional revenue management relies on long and stable historical data and predictable demand patterns. However, meeting those requirements is not always possible. Many industries face demand volatility on an ongoing basis, an example would be air cargo which has much shorter booking horizon with highly variable batch arrivals. Even for passenger airlines where revenue management (RM) is well-established, reacting to external shocks is a well-known challenge that requires user monitoring and manual intervention. Moreover, traditional RM comes with strict data requirements including historical bookings and pricing even in the absence of any bookings, spanning multiple years. For companies that have not established a practice in RM, that type of extensive data is usually not available. We present a data-driven approach to RM which eliminates the need for demand forecasting and optimization techniques. We develop a methodology to generate bid prices using historical booking data only. O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoY&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#20363;&#31243;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20154;&#20307;&#31867;&#22411;&#19978;&#25552;&#39640;&#31934;&#30830;&#30340;3D&#24418;&#20307;&#20272;&#35745;&#20934;&#30830;&#24230;&#65292;&#20026;&#26102;&#23578;&#34892;&#19994;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.07389</link><description>&lt;p&gt;
Shape of You&#65306;&#38024;&#23545;&#22810;&#26679;&#21270;&#36523;&#26448;&#30340;&#31934;&#20934;3D&#24418;&#20307;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shape of You: Precise 3D shape estimations for diverse body types. (arXiv:2304.07389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoY&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#20363;&#31243;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20154;&#20307;&#31867;&#22411;&#19978;&#25552;&#39640;&#31934;&#30830;&#30340;3D&#24418;&#20307;&#20272;&#35745;&#20934;&#30830;&#24230;&#65292;&#20026;&#26102;&#23578;&#34892;&#19994;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoY&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;&#35270;&#35273;&#30340;&#26381;&#35013;&#25512;&#33616;&#31995;&#32479;&#20013;3D&#36523;&#20307;&#24418;&#29366;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#20272;&#35745;&#20102;3D&#23039;&#21183;&#65292;&#20294;&#22312;&#31934;&#30830;&#24418;&#29366;&#20272;&#35745;&#26041;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#30340;&#20154;&#20307;&#31867;&#22411;&#65292;&#20173;&#23384;&#22312;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#21442;&#25968;&#21270;&#30340;3D&#20154;&#20307;&#37325;&#24314;&#31649;&#36947;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#20248;&#21270;&#20363;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;SSP-3D&#25968;&#25454;&#38598;&#19978;&#27604;&#26368;&#36817;&#30340;SHAPY&#26041;&#27861;&#25552;&#39640;&#20102;17.7&#65285;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#26397;&#30528;&#26356;&#20934;&#30830;&#30340;3D&#24418;&#24577;&#20272;&#35745;&#31995;&#32479;&#36808;&#20986;&#30340;&#19968;&#27493;&#65292;&#35813;&#31995;&#32479;&#21487;&#22312;&#19981;&#21516;&#30340;&#20307;&#22411;&#19978;&#21487;&#38752;&#22320;&#24037;&#20316;&#65292;&#24182;&#26377;&#26395;&#22312;&#26102;&#23578;&#34892;&#19994;&#20013;&#24471;&#21040;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Shape of You (SoY), an approach to improve the accuracy of 3D body shape estimation for vision-based clothing recommendation systems. While existing methods have successfully estimated 3D poses, there remains a lack of work in precise shape estimation, particularly for diverse human bodies. To address this gap, we propose two loss functions that can be readily integrated into parametric 3D human reconstruction pipelines. Additionally, we propose a test-time optimization routine that further improves quality. Our method improves over the recent SHAPY method by 17.7% on the challenging SSP-3D dataset. We consider our work to be a step towards a more accurate 3D shape estimation system that works reliably on diverse body types and holds promise for practical applications in the fashion industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#26377;&#25928;&#22411;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#37327;&#23376;&#32416;&#38169;&#65292;&#36890;&#36807;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#36798;&#21040;&#20102;&#19982;&#20808;&#21069;&#31070;&#32463;&#35299;&#30721;&#22120;&#30456;&#27604;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07362</link><description>&lt;p&gt;
END&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#32416;&#38169;&#30340;&#31561;&#21464;&#31070;&#32463;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
The END: An Equivariant Neural Decoder for Quantum Error Correction. (arXiv:2304.07362v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#26377;&#25928;&#22411;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#37327;&#23376;&#32416;&#38169;&#65292;&#36890;&#36807;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#36798;&#21040;&#20102;&#19982;&#20808;&#21069;&#31070;&#32463;&#35299;&#30721;&#22120;&#30456;&#27604;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32416;&#38169;&#26159;&#25193;&#23637;&#37327;&#23376;&#35745;&#31639;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#37327;&#23376;&#30721;&#65292;&#26368;&#20339;&#35299;&#30721;&#22120;&#23558;&#27979;&#37327;&#21040;&#30340;&#30721;&#36829;&#35268;&#26144;&#23556;&#21040;&#26368;&#26377;&#21487;&#33021;&#21457;&#29983;&#30340;&#38169;&#35823;&#65292;&#20294;&#20854;&#25104;&#26412;&#38543;&#31995;&#32479;&#22823;&#23567;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#22120;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#36825;&#31181;&#26144;&#23556;&#30340;&#26377;&#25928;&#36817;&#20284;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#22122;&#22768;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#26377;&#25928;&#22411;&#31070;&#32463;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#25197;&#26354;&#30721;&#29702;&#35770;&#30340;&#26368;&#20339;&#35299;&#30721;&#22120;&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#65292;&#19982;&#20197;&#21069;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum error correction is a critical component for scaling up quantum computing. Given a quantum code, an optimal decoder maps the measured code violations to the most likely error that occurred, but its cost scales exponentially with the system size. Neural network decoders are an appealing solution since they can learn from data an efficient approximation to such a mapping and can automatically adapt to the noise distribution. In this work, we introduce a data efficient neural decoder that exploits the symmetries of the problem. We characterize the symmetries of the optimal decoder for the toric code and propose a novel equivariant architecture that achieves state of the art accuracy compared to previous neural decoders.
&lt;/p&gt;</description></item><item><title>PTW&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#21644;&#26356;&#22909;&#30340;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07361</link><description>&lt;p&gt;
PTW: &#38024;&#23545;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#20851;&#38190;&#35843;&#25972;&#22411;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PTW: Pivotal Tuning Watermarking for Pre-Trained Image Generators. (arXiv:2304.07361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07361
&lt;/p&gt;
&lt;p&gt;
PTW&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#21644;&#26356;&#22909;&#30340;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#27867;&#25351;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#22120;&#32508;&#21512;&#20986;&#26469;&#30340;&#20869;&#23481;&#65292;&#33509;&#34987;&#8220;&#35823;&#29992;&#8221;&#65292;&#21487;&#20197;&#30772;&#22351;&#25968;&#23383;&#23186;&#20307;&#30340;&#20449;&#20219;&#12290;&#32780;&#21046;&#20316;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#38656;&#35201;&#25509;&#35302;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#29983;&#25104;&#22120;&#65292;&#21482;&#26377;&#23569;&#25968;&#23454;&#20307;&#21487;&#20197;&#35757;&#32451;&#21644;&#25552;&#20379;&#36825;&#20123;&#29983;&#25104;&#22120;&#12290;&#23041;&#32961;&#30340;&#26159;&#65292;&#24694;&#24847;&#29992;&#25143;&#20250;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#21487;&#25506;&#27979;&#36215;&#26469;&#65292;&#28145;&#24230;&#20266;&#36896;&#38656;&#35201;&#22312;&#29983;&#25104;&#22120;&#20013;&#23884;&#20837;&#35782;&#21035;&#30721;&#65292;&#20197;&#20415;&#22312;&#21518;&#32493;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Pivotal Tuning Watermarking (PTW)&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;(i) &#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;(ii) &#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21516;&#26102;&#32553;&#25918;&#21040;&#20102;&#27604;&#30456;&#20851;&#24037;&#20316;&#22823;4&#20493;&#30340;&#29983;&#25104;&#22120;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;PTW&#21487;&#20197;&#23884;&#20837;&#26356;&#38271;&#30340;&#35782;&#21035;&#30721;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#27700;&#21360;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes refer to content synthesized using deep generators, which, when \emph{misused}, have the potential to erode trust in digital media. Synthesizing high-quality deepfakes requires access to large and complex generators only few entities can train and provide. The threat are malicious users that exploit access to the provided model and generate harmful deepfakes without risking detection. Watermarking makes deepfakes detectable by embedding an identifiable code into the generator that is later extractable from its generated images. We propose Pivotal Tuning Watermarking (PTW), a method for watermarking pre-trained generators (i) three orders of magnitude faster than watermarking from scratch and (ii) without the need for any training data. We improve existing watermarking methods and scale to generators $4 \times$ larger than related work. PTW can embed longer codes than existing methods while better preserving the generator's image quality. We propose rigorous, game-based defini
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25193;&#25955;&#31639;&#27861;&#30340;&#25512;&#24191;&#65292;&#24182;&#22312;&#32593;&#32476;&#20013;&#36827;&#34892;&#23376;&#31354;&#38388;&#32422;&#26463;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#24471;&#21040;&#20102;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.07358</link><description>&lt;p&gt;
&#20998;&#24067;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#31934;&#30830;&#23376;&#31354;&#38388;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Exact Subspace Diffusion for Decentralized Multitask Learning. (arXiv:2304.07358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25193;&#25955;&#31639;&#27861;&#30340;&#25512;&#24191;&#65292;&#24182;&#22312;&#32593;&#32476;&#20013;&#36827;&#34892;&#23376;&#31354;&#38388;&#32422;&#26463;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#24471;&#21040;&#20102;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#32852;&#37030;&#23398;&#20064;&#25110;&#20998;&#25955;&#24335;&#26799;&#24230;&#19979;&#38477;&#65292;&#37319;&#29992;&#20849;&#35782;&#26426;&#21046;&#26469;&#24378;&#21046;&#23454;&#29616;&#20195;&#29702;&#20043;&#38388;&#30340;&#21516;&#36136;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#20195;&#29702;&#36981;&#24490;&#24322;&#26500;&#30446;&#26631;&#25110;&#25968;&#25454;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#20197;&#26356;&#21152;&#24494;&#22937;&#30340;&#26041;&#24335;&#22312;&#20195;&#29702;&#20043;&#38388;&#24314;&#31435;&#20851;&#31995;&#65292;&#24182;&#40723;&#21169;&#21327;&#20316;&#32780;&#19981;&#26159;&#24378;&#21046;&#20849;&#35782;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#29992;&#20110;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#23376;&#31354;&#38388;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#31934;&#30830;&#25193;&#25955;&#31639;&#27861;&#30340;&#25512;&#24191;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#21033;&#29992;&#22122;&#22768;&#26799;&#24230;&#36924;&#36817;&#26102;&#20854;&#22343;&#26041;&#20559;&#24046;&#30340;&#20934;&#30830;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#19978;&#39564;&#35777;&#20102;&#39044;&#27979;&#24615;&#33021;&#34920;&#36798;&#24335;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#21450;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical paradigms for distributed learning, such as federated or decentralized gradient descent, employ consensus mechanisms to enforce homogeneity among agents. While these strategies have proven effective in i.i.d. scenarios, they can result in significant performance degradation when agents follow heterogeneous objectives or data. Distributed strategies for multitask learning, on the other hand, induce relationships between agents in a more nuanced manner, and encourage collaboration without enforcing consensus. We develop a generalization of the exact diffusion algorithm for subspace constrained multitask learning over networks, and derive an accurate expression for its mean-squared deviation when utilizing noisy gradient approximations. We verify numerically the accuracy of the predicted performance expressions, as well as the improved performance of the proposed approach over alternatives based on approximate projections.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#23376;&#22330;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#20307;&#31215;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#21160;&#24577;&#23454;&#26102;&#20840;&#23616;&#20809;&#29031;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#20307;&#31215;&#25968;&#25454;&#20013;&#23454;&#29616;&#21160;&#24577;&#20840;&#23616;&#20809;&#29031;&#21644;&#26448;&#36136;&#21464;&#21270;&#30340;&#20132;&#20114;&#24335;&#24103;&#29575;&#65292;&#20026;&#20307;&#31215;&#28210;&#26579;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.07338</link><description>&lt;p&gt;
&#20809;&#23376;&#22330;&#32593;&#32476;&#20026;&#21160;&#24577;&#23454;&#26102;&#20307;&#31215;&#20840;&#23616;&#20809;&#29031;&#25216;&#26415;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination. (arXiv:2304.07338v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#23376;&#22330;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#20307;&#31215;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#21160;&#24577;&#23454;&#26102;&#20840;&#23616;&#20809;&#29031;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#20307;&#31215;&#25968;&#25454;&#20013;&#23454;&#29616;&#21160;&#24577;&#20840;&#23616;&#20809;&#29031;&#21644;&#26448;&#36136;&#21464;&#21270;&#30340;&#20132;&#20114;&#24335;&#24103;&#29575;&#65292;&#20026;&#20307;&#31215;&#28210;&#26579;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#25968;&#25454;&#22312;&#35768;&#22810;&#31185;&#23398;&#23398;&#31185;&#20013;&#24120;&#24120;&#20986;&#29616;&#65292;&#22914;&#21307;&#23398;&#12289;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#19987;&#23478;&#20204;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#31185;&#23398;&#21487;&#35270;&#21270;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#36335;&#24452;&#36861;&#36394;&#34987;&#35748;&#20026;&#26159;&#20307;&#31215;&#28210;&#26579;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#30340;&#23454;&#26102;&#20840;&#29699;&#20809;&#29031;&#25928;&#26524;&#38750;&#24120;&#30495;&#23454;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#20307;&#31215;&#36335;&#24452;&#36861;&#36394;&#24448;&#24448;&#20250;&#21463;&#21040;&#38543;&#26426;&#22122;&#22768;&#21644;&#38271;&#26102;&#38388;&#30340;&#25910;&#25947;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20307;&#31215;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#23454;&#26102;&#20840;&#23616;&#20809;&#29031;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Photon Field Networks&#8212;&#8212;&#19968;&#31181;&#30456;&#20301;&#20989;&#25968;&#24863;&#30693;&#12289;&#22810;&#20809;&#28304;&#31070;&#32463;&#34920;&#31034;&#30340;&#38388;&#25509;&#20307;&#31215;&#20840;&#23616;&#20809;&#29031;&#25216;&#26415;&#12290;&#36825;&#20123;&#22330;&#22312;&#25105;&#20204;&#39044;&#20808;&#35745;&#31639;&#30340;&#22810;&#30456;&#20809;&#23376;&#32531;&#23384;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35757;&#32451;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#23436;&#25104;&#65292;&#20043;&#21518;&#36825;&#20123;&#22330;&#23601;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#28210;&#26579;&#20219;&#21153;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#28508;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#31070;&#32463;&#36335;&#24452;&#36861;&#36394;&#22120;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#20809;&#23376;&#22330;&#21487;&#20197;&#23454;&#29616;&#22312;&#20307;&#31215;&#25968;&#25454;&#20013;&#23454;&#29616;&#21160;&#24577;&#20840;&#23616;&#20809;&#29031;&#21644;&#26448;&#36136;&#21464;&#21270;&#30340;&#20132;&#20114;&#24335;&#24103;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Volume data is commonly found in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks -- a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HEAT&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;SimpleX&#22312;CPU&#19978;&#30340;&#25805;&#20316;&#65292;&#23454;&#29616;&#21327;&#21516;&#36807;&#28388;&#30340;&#39640;&#25928;&#29575;&#35757;&#32451;</title><link>http://arxiv.org/abs/2304.07334</link><description>&lt;p&gt;
HEAT&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#30340;&#22522;&#20110;CPU&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#35757;&#32451;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
HEAT: A Highly Efficient and Affordable Training System for Collaborative Filtering Based Recommendation on CPUs. (arXiv:2304.07334v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07334
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HEAT&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;SimpleX&#22312;CPU&#19978;&#30340;&#25805;&#20316;&#65292;&#23454;&#29616;&#21327;&#21516;&#36807;&#28388;&#30340;&#39640;&#25928;&#29575;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#24050;&#34987;&#35777;&#26126;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#25152;&#26377;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#20013;&#65292;SimpleX&#26159;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#36866;&#24403;&#25968;&#37327;&#30340;&#36127;&#26679;&#26412;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;SimpleX&#22312;&#22810;&#26680;CPU&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#23548;&#33268;&#24615;&#33021;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;SimpleX&#23454;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21253;&#25324;(1)&#19981;&#35268;&#21017;&#30340;&#20869;&#23384;&#35775;&#38382;&#65292;(2)&#19981;&#24517;&#35201;&#30340;&#20869;&#23384;&#22797;&#21046;&#65292;(3)&#20887;&#20313;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;CF&#35757;&#32451;&#31995;&#32479;(&#21517;&#20026;HEAT)&#65292;&#23427;&#20805;&#20998;&#21457;&#25381;&#20102;&#29616;&#20195;CPU&#30340;&#22810;&#32423;&#32531;&#23384;&#21644;&#22810;&#32447;&#31243;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HEAT&#30340;&#20248;&#21270;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#20351;&#29992;&#29926;&#29255;&#21270;&#25216;&#26415;&#22686;&#21152;&#25968;&#25454;&#23616;&#37096;&#24615;&#21644;&#20943;&#23569;&#32531;&#23384;&#22833;&#25928;(&#20174;&#32780;&#20943;&#23569;&#35835;&#21462;&#24310;&#36831;)&#65307;(2)&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#21644;&#37319;&#26679;&#20248;&#21270;&#65307;(3)&#20351;&#29992;&#21151;&#33021;&#20998;&#21306;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) has been proven to be one of the most effective techniques for recommendation. Among all CF approaches, SimpleX is the state-of-the-art method that adopts a novel loss function and a proper number of negative samples. However, there is no work that optimizes SimpleX on multi-core CPUs, leading to limited performance. To this end, we perform an in-depth profiling and analysis of existing SimpleX implementations and identify their performance bottlenecks including (1) irregular memory accesses, (2) unnecessary memory copies, and (3) redundant computations. To address these issues, we propose an efficient CF training system (called HEAT) that fully enables the multi-level caching and multi-threading capabilities of modern CPUs. Specifically, the optimization of HEAT is threefold: (1) It tiles the embedding matrix to increase data locality and reduce cache misses (thus reduce read latency); (2) It optimizes stochastic gradient descent (SGD) with sampling by par
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;STEGO&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#20854;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#21487;&#29992;&#20110;&#26032;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.07314</link><description>&lt;p&gt;
&#25581;&#31034;STEGO&#36827;&#34892;&#23433;&#20840;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation. (arXiv:2304.07314v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07314
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;STEGO&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#20854;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#21487;&#29992;&#20110;&#26032;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#39592;&#24178;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#32467;&#21512;Vision Transformer&#26550;&#26500;&#65292;DINO&#33258;&#33976;&#39311;&#25216;&#26415;&#20855;&#26377;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#22914;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#29983;&#25104;&#29305;&#24449;&#30340;&#35821;&#20041;&#23545;&#24212;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26174;&#24335;&#30340;&#20154;&#24037;&#26631;&#27880;&#26631;&#31614;&#12290;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;STEGO&#26041;&#27861;&#36890;&#36807;&#23545;DINO&#39044;&#35757;&#32451;&#30340;Vision Transformer&#30340;&#29305;&#24449;&#23545;&#24212;&#36827;&#34892;&#23545;&#27604;&#33976;&#39311;&#65292;&#26368;&#36817;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20248;&#24615;&#12290;&#28982;&#32780;&#65292;STEGO&#30340;&#35814;&#32454;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#20998;&#35299;&#65292;&#38459;&#30861;&#20102;&#23427;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;STEGO&#26550;&#26500;&#21644;&#22521;&#35757;&#31574;&#30053;&#30340;&#24037;&#20316;&#26426;&#21046;&#65292;&#37325;&#29616;&#21644;&#25193;&#23637;&#20854;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#30740;&#31350;STEGO&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-training strategies have recently shown impressive results for training general-purpose feature extraction backbones in computer vision. In combination with the Vision Transformer architecture, the DINO self-distillation technique has interesting emerging properties, such as unsupervised clustering in the latent space and semantic correspondences of the produced features without using explicit human-annotated labels. The STEGO method for unsupervised semantic segmentation contrastively distills feature correspondences of a DINO-pre-trained Vision Transformer and recently set a new state of the art. However, the detailed workings of STEGO have yet to be disentangled, preventing its usage in safety-critical applications. This paper provides a deeper understanding of the STEGO architecture and training strategy by conducting studies that uncover the working mechanisms behind STEGO, reproduce and extend its experimental validation, and investigate the ability of STEGO t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#39044;&#23450;&#20041;&#30340;&#30830;&#23450;&#24615;&#35843;&#24230;&#22312;&#22270;&#20687;&#21387;&#32553;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20351;&#29992;&#21452;&#21521;&#21464;&#25442;&#22120;&#65292;&#36974;&#25377;&#27880;&#24847;&#21147;&#21644;&#28608;&#27963;&#32531;&#23384;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07313</link><description>&lt;p&gt;
M2T: &#20004;&#27425;&#25513;&#34109;&#21464;&#25442;&#25552;&#39640;&#35299;&#30721;&#36895;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
M2T: Masking Transformers Twice for Faster Decoding. (arXiv:2304.07313v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#39044;&#23450;&#20041;&#30340;&#30830;&#23450;&#24615;&#35843;&#24230;&#22312;&#22270;&#20687;&#21387;&#32553;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20351;&#29992;&#21452;&#21521;&#21464;&#25442;&#22120;&#65292;&#36974;&#25377;&#27880;&#24847;&#21147;&#21644;&#28608;&#27963;&#32531;&#23384;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21452;&#21521;&#21464;&#25442;&#22120;&#22312;&#36974;&#25377;&#20196;&#29260;&#39044;&#27979;&#19978;&#30340;&#35757;&#32451;&#22914;&#20309;&#24212;&#29992;&#20110;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#31867;&#27169;&#22411;&#20197;&#21069;&#26366;&#34987;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#65292;&#37319;&#29992;&#26681;&#25454;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#35843;&#24230;&#36880;&#27493;&#37319;&#26679;&#36974;&#34109;&#30340;&#20196;&#29260;&#32452;&#12290;&#19982;&#36825;&#20123;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#39044;&#23450;&#20041;&#30340;&#30830;&#23450;&#24615;&#35843;&#24230;&#22312;&#22270;&#20687;&#21387;&#32553;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#29978;&#33267;&#26356;&#22909;&#12290;&#36825;&#31181;&#35265;&#35299;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#36974;&#25377;&#27880;&#24847;&#21147;&#65292;&#38500;&#20102;&#36974;&#25377;&#36755;&#20837;&#20043;&#22806;&#65292;&#36824;&#20351;&#29992;&#28608;&#27963;&#32531;&#23384;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#36895;&#24230;&#65288;&#25512;&#29702;&#36895;&#24230;&#32422;&#39640;4&#20493;&#65289;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#19968;&#28857;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how bidirectional transformers trained for masked token prediction can be applied to neural image compression to achieve state-of-the-art results. Such models were previously used for image generation by progressivly sampling groups of masked tokens according to uncertainty-adaptive schedules. Unlike these works, we demonstrate that predefined, deterministic schedules perform as well or better for image compression. This insight allows us to use masked attention during training in addition to masked inputs, and activation caching during inference, to significantly speed up our models (~4 higher inference speed) at a small increase in bitrate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#20221;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#21644;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#32852;&#37030;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#30740;&#31350;&#21457;&#29616;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#65292;&#26410;&#26469;&#24212;&#35813;&#25506;&#32034;FL&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#27169;&#22411;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.07310</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#21644;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#32852;&#37030;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#65306;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Federated and distributed learning applications for electronic health records and structured medical data: A scoping review. (arXiv:2304.07310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#20221;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#21644;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#32852;&#37030;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#30740;&#31350;&#21457;&#29616;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#65292;&#26410;&#26469;&#24212;&#35813;&#25506;&#32034;FL&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#27169;&#22411;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#30340;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#22312;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#26041;&#38754;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#20316;&#20026;&#26368;&#24120;&#35265;&#30340;&#20020;&#24202;&#25968;&#25454;&#24418;&#24335;&#20043;&#19968;&#65292;&#21516;&#26102;&#20063;&#32463;&#21382;&#20102;&#26174;&#30528;&#30340;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#26816;&#26597;&#20102;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#30830;&#23450;&#20102;&#24403;&#20195;&#30340;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;SCOPUS&#12289;MEDLINE&#12289;Web of Science&#12289;Embase&#21644;CINAHL&#31561;&#20116;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#35782;&#21035;&#23558;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#24182;&#26681;&#25454;PRISMA&#20934;&#21017;&#25253;&#21578;&#32467;&#26524;&#30340;&#25991;&#31456;&#12290;&#27599;&#31687;&#36873;&#23450;&#30340;&#20986;&#29256;&#29289;&#20174;&#25968;&#25454;&#36136;&#37327;&#12289;&#24314;&#27169;&#31574;&#30053;&#21644;FL&#26694;&#26550;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#32463;&#36807;&#31579;&#36873;&#65292;34&#31687;&#25991;&#31456;&#31526;&#21512;&#32435;&#20837;&#26631;&#20934;&#65292;&#27599;&#31687;&#25991;&#31456;&#21253;&#25324;&#19968;&#20010;&#25110;&#22810;&#20010;&#20351;&#29992;FL&#22788;&#29702;&#32467;&#26500;&#21270;&#20020;&#24202;/&#21307;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;&#12290;&#20854;&#20013;18&#20010;&#30740;&#31350;&#24212;&#29992;&#20102;FL&#26469;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#65292;&#32780;&#20165;&#26377;2&#20010;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;FL&#36827;&#34892;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#32034;&#20102;FL&#30340;&#20854;&#20182;&#20248;&#28857;&#65292;&#22914;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#27169;&#22411;&#36890;&#29992;&#24615;&#12290;&#26412;&#33539;&#22260;&#23457;&#26597;&#26368;&#21518;&#35752;&#35770;&#20102;FL&#22312;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#32972;&#26223;&#19979;&#30340;&#25361;&#25112;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has gained popularity in clinical research in recent years to facilitate privacy-preserving collaboration. Structured data, one of the most prevalent forms of clinical data, has experienced significant growth in volume concurrently, notably with the widespread adoption of electronic health records in clinical practice. This review examines FL applications on structured medical data, identifies contemporary limitations and discusses potential innovations. We searched five databases, SCOPUS, MEDLINE, Web of Science, Embase, and CINAHL, to identify articles that applied FL to structured medical data and reported results following the PRISMA guidelines. Each selected publication was evaluated from three primary perspectives, including data quality, modeling strategies, and FL frameworks. Out of the 1160 papers screened, 34 met the inclusion criteria, with each article consisting of one or more studies that used FL to handle structured clinical/medical data. Of these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#22768;&#23398;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#20316;&#20026;&#29305;&#24449;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#36724;&#25215;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2304.07307</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#30340;&#22768;&#23398;&#20998;&#26512;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Airborne-Sound Analysis for the Detection of Bearing Faults in Railway Vehicles with Real-World Data. (arXiv:2304.07307v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#22768;&#23398;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#20316;&#20026;&#29305;&#24449;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#36724;&#25215;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#36710;&#36742;&#27491;&#24120;&#36816;&#34892;&#26399;&#38388;&#35760;&#24405;&#30340;&#22768;&#23398;&#20449;&#21495;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#20316;&#20026;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#36755;&#20837;&#25552;&#20379;&#32473;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22312;&#38024;&#23545;&#29616;&#20195;&#36890;&#21220;&#38081;&#36335;&#36710;&#36742;&#36827;&#34892;&#27979;&#37327;&#27963;&#21160;&#20013;&#33719;&#24471;&#30340;&#23454;&#38469;&#25968;&#25454;&#35780;&#20272;&#30340;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25152;&#36873;&#25321;&#30340;MFCC&#29305;&#24449;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#36724;&#25215;&#25925;&#38556;&#65292;&#21363;&#20351;&#20986;&#29616;&#22312;&#35757;&#32451;&#20013;&#26410;&#21253;&#25324;&#30340;&#36724;&#25215;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenging problem of detecting bearing faults in railway vehicles by analyzing acoustic signals recorded during regular operation. For this, we introduce Mel Frequency Cepstral Coefficients (MFCCs) as features, which form the input to a simple Multi-Layer Perceptron classifier. The proposed method is evaluated with real-world data that was obtained for state-of-the-art commuter railway vehicles in a measurement campaign. The experiments show that with the chosen MFCC features bearing faults can be reliably detected even for bearing damages that were not included in training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#25152;&#38656;&#30340;&#19987;&#23478;&#39044;&#27979;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12289;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#39044;&#27979;&#21644;&#21033;&#29992;&#26679;&#26412;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07306</link><description>&lt;p&gt;
&#26377;&#38480;&#19987;&#23478;&#39044;&#27979;&#19979;&#30340;&#23398;&#20064;&#25512;&#36831;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer with Limited Expert Predictions. (arXiv:2304.07306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#25152;&#38656;&#30340;&#19987;&#23478;&#39044;&#27979;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12289;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#39044;&#27979;&#21644;&#21033;&#29992;&#26679;&#26412;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20154;&#31867;&#19987;&#23478;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#36229;&#20986;&#21333;&#29420;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#33021;&#21147;&#32467;&#21512;&#36890;&#24120;&#36890;&#36807;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#23454;&#29616;&#65292;&#36825;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#23398;&#20250;&#20915;&#23450;&#26159;&#21542;&#20026;&#29305;&#23450;&#26679;&#26412;&#20570;&#20986;&#39044;&#27979;&#25110;&#23558;&#20854;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20934;&#30830;&#22320;&#23398;&#20064;&#21738;&#20123;&#26679;&#26412;&#24212;&#35813;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#65292;&#38656;&#35201;&#22823;&#37327;&#20934;&#30830;&#21453;&#26144;&#19987;&#23478;&#33021;&#21147;&#30340;&#19987;&#23478;&#39044;&#27979;&#65292;&#38500;&#20102;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#25152;&#38656;&#30340;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#36825;&#26159;&#35768;&#22810;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#20849;&#20139;&#30340;&#35201;&#27714;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36131;&#20219;&#19987;&#23478;&#32463;&#24120;&#26356;&#25913;&#25110;&#33719;&#24471;&#36275;&#22815;&#30340;&#19987;&#23478;&#39044;&#27979;&#30340;&#25104;&#26412;&#36739;&#39640;&#30340;&#22330;&#26223;&#20013;&#30340;&#37319;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#20943;&#23569;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#25152;&#38656;&#30340;&#19987;&#23478;&#39044;&#27979;&#25968;&#37327;&#12290;&#23427;&#21253;&#25324;&#65288;1&#65289;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent research suggests that combining AI models with a human expert can exceed the performance of either alone. The combination of their capabilities is often realized by learning to defer algorithms that enable the AI to learn to decide whether to make a prediction for a particular instance or defer it to the human expert. However, to accurately learn which instances should be deferred to the human expert, a large number of expert predictions that accurately reflect the expert's capabilities are required -- in addition to the ground truth labels needed to train the AI. This requirement shared by many learning to defer algorithms hinders their adoption in scenarios where the responsible expert regularly changes or where acquiring a sufficient number of expert predictions is costly. In this paper, we propose a three-step approach to reduce the number of expert predictions required to train learning to defer algorithms. It encompasses (1) the training of an embedding model with ground 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#32500;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;ICPHM 2023&#25968;&#25454;&#25361;&#25112;&#36187;&#20013;&#24212;&#29992;&#20110;&#22826;&#38451;&#40831;&#36718;&#25925;&#38556;&#20998;&#31867;&#20219;&#21153;&#12290;&#21363;&#20351;&#38754;&#23545;&#22810;&#20010;&#25805;&#20316;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#34987;&#26816;&#27979;&#40831;&#36718;&#31665;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.07305</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#30340;&#19968;&#32500;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;ICPHM 2023&#25968;&#25454;&#25361;&#25112;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
1-D Residual Convolutional Neural Network coupled with Data Augmentation and Regularization Techniques for the ICPHM 2023 Data Challenge. (arXiv:2304.07305v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#32500;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;ICPHM 2023&#25968;&#25454;&#25361;&#25112;&#36187;&#20013;&#24212;&#29992;&#20110;&#22826;&#38451;&#40831;&#36718;&#25925;&#38556;&#20998;&#31867;&#20219;&#21153;&#12290;&#21363;&#20351;&#38754;&#23545;&#22810;&#20010;&#25805;&#20316;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#34987;&#26816;&#27979;&#40831;&#36718;&#31665;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;ICPHM 2023&#25361;&#25112;&#36187;&#20013;&#23545;&#20110;&#21033;&#29992;&#25391;&#21160;&#20998;&#26512;&#36827;&#34892;&#24037;&#19994;&#31995;&#32479;&#20581;&#24247;&#30417;&#27979;&#30340;&#22826;&#38451;&#40831;&#36718;&#25925;&#38556;&#20998;&#31867;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#36890;&#36947;&#26102;&#38388;&#22495;&#25391;&#21160;&#20449;&#21495;&#30340;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#20854;&#21487;&#35757;&#32451;&#21442;&#25968;&#23569;&#20110;30,000&#20010;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#22330;&#26223;&#20013;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20135;&#29983;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;&#21363;&#20351;&#38754;&#23545;&#22810;&#20010;&#25805;&#20316;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#34987;&#26816;&#27979;&#40831;&#36718;&#31665;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present our contribution to the ICPHM 2023 Data Challenge on Industrial Systems' Health Monitoring using Vibration Analysis. For the task of classifying sun gear faults in a gearbox, we propose a residual Convolutional Neural Network that operates on raw three-channel time-domain vibration signals. In conjunction with data augmentation and regularization techniques, the proposed model yields very good results in a multi-class classification scenario with real-world data despite its relatively small size, i.e., with less than 30,000 trainable parameters. Even when presented with data obtained from multiple operating conditions, the network is still capable to accurately predict the condition of the gearbox under inspection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#30340;&#28145;&#24230;&#34920;&#31034;&#65292;&#36890;&#36807;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#19982;&#26377;&#30417;&#30563;&#34920;&#31034;&#30340;&#21306;&#21035;&#65292;&#27604;&#36739;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#26174;&#33879;&#22270;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.07304</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#34920;&#36798;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition. (arXiv:2304.07304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#30340;&#28145;&#24230;&#34920;&#31034;&#65292;&#36890;&#36807;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#19982;&#26377;&#30417;&#30563;&#34920;&#31034;&#30340;&#21306;&#21035;&#65292;&#27604;&#36739;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#26174;&#33879;&#22270;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#65292;&#20197;&#20415;&#23398;&#20064;&#28145;&#24230;&#34920;&#31034;&#32780;&#19981;&#38656;&#35201;&#25968;&#25454;&#27880;&#37322;&#12290;&#34429;&#28982;SSL&#26694;&#26550;&#30340;&#24615;&#33021;&#20960;&#20046;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#65292;&#20294;&#23545;SSL&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#36827;&#34892;&#35299;&#37322;&#30340;&#30740;&#31350;&#21364;&#24456;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;SSL&#34920;&#31034;&#19982;&#30417;&#30563;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#65306;&#23427;&#20204;&#26159;&#22914;&#20309;&#34987;&#23398;&#20064;&#30340;&#12289;&#23427;&#20204;&#20445;&#30041;&#20102;&#21738;&#20123;&#36755;&#20837;&#25968;&#25454;&#23646;&#24615;&#65292;&#20197;&#21450;&#20309;&#26102;&#21487;&#20197;&#36873;&#25321;SSL&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#20004;&#20010;&#26368;&#36817;&#30340;SSL&#26694;&#26550;SimCLR&#21644;VICReg&#30340;&#28145;&#24230;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37325;&#28857;&#25918;&#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;(i) &#27604;&#36739;&#30417;&#30563;&#21644;SSL&#27169;&#22411;&#23545;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65307;(ii) &#20351;&#29992;&#26174;&#33879;&#22270;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#29992;&#20110;&#39044;&#27979;&#21508;&#31181;&#27963;&#21160;&#30340;&#20027;&#35201;&#36755;&#20837;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) frameworks have been extensively applied to sensor-based Human Activity Recognition (HAR) in order to learn deep representations without data annotations. While SSL frameworks reach performance almost comparable to supervised models, studies on interpreting representations learnt by SSL models are limited. Nevertheless, modern explainability methods could help to unravel the differences between SSL and supervised representations: how they are being learnt, what properties of input data they preserve, and when SSL can be chosen over supervised training. In this paper, we aim to analyze deep representations of two recent SSL frameworks, namely SimCLR and VICReg. Specifically, the emphasis is made on (i) comparing the robustness of supervised and SSL models to corruptions in input data; (ii) explaining predictions of deep learning models using saliency maps and highlighting what input channels are mostly used for predicting various activitie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#27599;&#22825;&#20056;&#23458;&#37327;&#30340;&#21464;&#21270;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35268;&#21010;&#20986;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.07303</link><description>&lt;p&gt;
&#26234;&#24935;&#22320;&#38081;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#20056;&#23458;&#37327;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership. (arXiv:2304.07303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#27599;&#22825;&#20056;&#23458;&#37327;&#30340;&#21464;&#21270;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35268;&#21010;&#20986;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;1999&#24180;&#24314;&#31435;&#20197;&#26469;&#65292;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#65288;MRT3&#65289;&#20026;&#20247;&#22810;&#20056;&#23458;&#25552;&#20379;&#20102;&#20132;&#36890;&#36873;&#25321;&#12290;&#33778;&#24459;&#23486;&#25919;&#24220;&#20132;&#36890;&#37096;&#35760;&#24405;&#27599;&#22825;&#26377;&#36229;&#36807;&#19968;&#21315;&#20154;&#20351;&#29992;MRT3&#65292;&#39044;&#27979;&#27599;&#22825;&#30340;&#20056;&#23458;&#25968;&#37327;&#21487;&#33021;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21463;&#20551;&#26399;&#12289;&#24037;&#20316;&#26085;&#21644;&#20854;&#20182;&#24847;&#22806;&#38382;&#39064;&#31561;&#21464;&#37327;&#24433;&#21709;&#65292;MRT3&#30340;&#26085;&#24120;&#20056;&#23458;&#37327;&#27874;&#21160;&#36739;&#22823;&#12290;&#20056;&#23458;&#26080;&#27861;&#30693;&#36947;&#26576;&#19968;&#22825;&#20182;&#20204;&#25152;&#20056;&#36335;&#32447;&#19978;&#26377;&#22810;&#23569;&#20854;&#20182;&#20056;&#23458;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#20182;&#20204;&#35268;&#21010;&#39640;&#25928;&#34892;&#31243;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#33778;&#24459;&#23486;&#20132;&#36890;&#36816;&#36755;&#37096;&#20381;&#36182;&#21253;&#21547;&#21382;&#21490;&#25968;&#25454;&#30340;&#30005;&#23376;&#34920;&#26684;&#65292;&#36825;&#21487;&#33021;&#38590;&#20197;&#26816;&#26597;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#29305;&#23450;&#26085;&#26399;&#26576;&#20010;&#36710;&#31449;&#30340;&#26410;&#26469;&#20986;&#21220;&#20154;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.
&lt;/p&gt;</description></item><item><title>HGWaveNet&#26159;&#19968;&#31181;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#21253;&#25324;&#36229;&#26354;&#29575;&#25193;&#25955;&#22270;&#21367;&#31215;&#21644;&#23567;&#27874;&#26102;&#38388;&#21367;&#31215;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65292;&#21487;&#26377;&#25928;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#21644;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07302</link><description>&lt;p&gt;
HGWaveNet: &#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#30340;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction. (arXiv:2304.07302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07302
&lt;/p&gt;
&lt;p&gt;
HGWaveNet&#26159;&#19968;&#31181;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#21253;&#25324;&#36229;&#26354;&#29575;&#25193;&#25955;&#22270;&#21367;&#31215;&#21644;&#23567;&#27874;&#26102;&#38388;&#21367;&#31215;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65292;&#21487;&#26377;&#25928;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#21644;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#21160;&#24577;&#22270;&#20013;&#25104;&#23545;&#33410;&#28857;&#20043;&#38388;&#30340;&#26410;&#26469;&#36793;&#32536;&#65292;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#24314;&#31435;&#22312;&#22343;&#21248;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#65292;&#36825;&#19982;&#29616;&#23454;&#19990;&#30028;&#22270;&#24418;&#30340;&#24130;&#24459;&#20998;&#24067;&#30456;&#30683;&#30462;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#34920;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20998;&#23618;&#36830;&#25509;&#12290;&#38024;&#23545;&#36825;&#31181;&#29305;&#27530;&#30340;&#25968;&#25454;&#29305;&#24449;&#65292;&#21452;&#26354;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#24819;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#25351;&#25968;&#25193;&#23637;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGWaveNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36229;&#20960;&#20309;&#31354;&#38388;&#19982;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#36866;&#24212;&#24615;&#65292;&#29992;&#20110;&#26102;&#38388;&#38142;&#25509;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#26469;&#20998;&#21035;&#23398;&#20064;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#26102;&#38388;&#28436;&#21464;&#20449;&#24687;&#12290;&#19968;&#26041;&#38754;&#65292;&#36229;&#26354;&#29575;&#25193;&#25955;&#22270;&#21367;&#31215; (HDGC) &#27169;&#22359;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#26356;&#24191;&#27867;&#30340;&#37051;&#23621;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#23567;&#27874;&#30340;&#26102;&#38388;&#21367;&#31215; (WTC) &#27169;&#22359;&#36890;&#36807;&#36716;&#25442;&#26102;&#38388;&#22495;&#29305;&#24449;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HGWaveNet&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the ot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20102;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;5&#24180;&#29983;&#23384;&#29575;&#65292;&#24182;&#27604;&#36739;&#20102;&#19971;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#26679;&#26412;&#30340;&#29983;&#23384;&#29575;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#21644;&#39118;&#38505;&#22240;&#32032;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.07299</link><description>&lt;p&gt;
&#29992;&#20110;&#20083;&#33146;&#30284;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#23384;&#39044;&#27979;&#30340;&#30417;&#30563;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Machine Learning for Breast Cancer Risk Factors Analysis and Survival Prediction. (arXiv:2304.07299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20102;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;5&#24180;&#29983;&#23384;&#29575;&#65292;&#24182;&#27604;&#36739;&#20102;&#19971;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#26679;&#26412;&#30340;&#29983;&#23384;&#29575;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#21644;&#39118;&#38505;&#22240;&#32032;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#21487;&#33021;&#20250;&#21463;&#21040;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#39044;&#27979;&#24739;&#32773;&#23384;&#27963;&#30340;&#26426;&#20250;&#65292;&#37319;&#29992;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;METABRIC&#25968;&#25454;&#38598;&#20013;&#30340;1904&#26465;&#24739;&#32773;&#35760;&#24405;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;5&#24180;&#20083;&#33146;&#30284;&#29983;&#23384;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19971;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#34920;&#29616;&#22914;&#19979;&#65306;&#21484;&#22238;&#29575;&#12289;AUC&#12289;&#28151;&#28102;&#30697;&#38453;&#12289;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#20551;&#27491;&#29575;&#21644;&#30495;&#27491;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Logistic&#22238;&#24402;(LR)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#12289;&#20915;&#31574;&#26641;(DT)&#12289;&#38543;&#26426;&#26862;&#26519;(RD)&#12289;&#26497;&#31471;&#38543;&#26426;&#21270;&#26641;(ET)&#12289;K-&#36817;&#37051;(KNN)&#21644;&#33258;&#36866;&#24212;&#22686;&#24378;(AdaBoost)&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#26679;&#26412;&#30340;&#29983;&#23384;&#29575;&#65292;&#20998;&#21035;&#26159;75.4&#65285;&#12289;74.7&#65285;&#12289;71.5&#65285;&#12289;75.5&#65285;&#12289;70.3&#65285;&#12289;72.5&#65285;&#21644;72.2&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#21644;&#39118;&#38505;&#22240;&#32032;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The choice of the most effective treatment may eventually be influenced by breast cancer survival prediction. To predict the chances of a patient surviving, a variety of techniques were employed, such as statistical, machine learning, and deep learning models. In the current study, 1904 patient records from the METABRIC dataset were utilized to predict a 5-year breast cancer survival using a machine learning approach. In this study, we compare the outcomes of seven classification models to evaluate how well they perform using the following metrics: recall, AUC, confusion matrix, accuracy, precision, false positive rate, and true positive rate. The findings demonstrate that the classifiers for Logistic Regression (LR), Support Vector Machines (SVM), Decision Tree (DT), Random Forest (RD), Extremely Randomized Trees (ET), K-Nearest Neighbor (KNN), and Adaptive Boosting (AdaBoost) can accurately predict the survival rate of the tested samples, which is 75,4\%, 74,7\%, 71,5\%, 75,5\%, 70,3
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#39640;&#38454;&#21644;&#36828;&#31243;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07298</link><description>&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Road Network Representation Learning: A Dual Graph based Approach. (arXiv:2304.07298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#39640;&#38454;&#21644;&#36828;&#31243;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#26159;&#25903;&#25745;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#31227;&#21160;&#24615;&#21644;&#29289;&#27969;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#12290;&#20026;&#20102;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#30340;&#36755;&#20837;&#65292;&#26377;&#24517;&#35201;&#23398;&#20064;&#36947;&#36335;&#30340;&#21521;&#37327;&#34920;&#31034;&#24418;&#24335;&#65292;&#31216;&#20026;&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#65288;RNRL&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181; RNRL &#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21482;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995; / &#36830;&#25509;&#65288;&#21363;&#20316;&#20026;&#31616;&#21333;&#22270;&#65289;&#65292;&#26080;&#27861;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#37027;&#20123;&#20849;&#21516;&#24418;&#25104;&#26412;&#22320;&#21306;&#22495;&#30340;&#36947;&#36335;&#36890;&#24120;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#36895;&#24230;&#38480;&#21046;&#65289;&#21644;&#36828;&#31243;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#19968;&#20123;&#30456;&#36317;&#36739;&#36828;&#30340;&#36947;&#36335;&#21487;&#33021;&#20855;&#26377;&#31867;&#20284;&#30340;&#35821;&#20041;&#65292;&#20363;&#22914;&#26159;&#20303;&#23429;&#21306;&#36947;&#36335;&#65289;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#26500;&#24314;&#19968;&#20010;&#8220;&#36229;&#22270;&#8221;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#36793;&#23545;&#24212;&#20110;&#26500;&#25104;&#21306;&#22495;&#30340;&#22810;&#26465;&#36947;&#36335;&#30340;&#38598;&#21512;&#12290;&#26500;&#24314;&#30340;&#36229;&#22270;&#20250;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#36947;&#36335;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#21644;&#36828;&#31243;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network is a critical infrastructure powering many applications including transportation, mobility and logistics in real life. To leverage the input of a road network across these different applications, it is necessary to learn the representations of the roads in the form of vectors, which is named \emph{road network representation learning} (RNRL). While several models have been proposed for RNRL, they capture the pairwise relationships/connections among roads only (i.e., as a simple graph), and fail to capture among roads the high-order relationships (e.g., those roads that jointly form a local region usually have similar features such as speed limit) and long-range relationships (e.g., some roads that are far apart may have similar semantics such as being roads in residential areas). Motivated by this, we propose to construct a \emph{hypergraph}, where each hyperedge corresponds to a set of multiple roads forming a region. The constructed hypergraph would naturally capture the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07297</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#21644;&#20154;&#31867;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#35753;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#20197;&#27492;&#35299;&#20915;&#22312;&#32570;&#20047;&#36739;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24120;&#24120;&#20250;&#25910;&#25947;&#21040;&#20154;&#31867;&#24182;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#22312;&#20154;&#31867;&#25351;&#20196;&#19979;&#30340;&#20808;&#39564;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#19982;&#20154;&#31867;&#21916;&#22909;&#19968;&#33268;&#30340;&#22343;&#34913;&#28857;&#12290;&#36890;&#36807;&#27010;&#24565;&#35777;&#26126;&#29615;&#22659;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Hanabi&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;instructRL&#25910;&#25947;&#20110;&#28385;&#36275;&#32473;&#23450;&#25351;&#20196;&#30340;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#36947;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#65292;&#21487;&#29992;&#20110;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.07213</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#21046;&#20316;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar. (arXiv:2304.07213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#65292;&#21487;&#29992;&#20110;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#34987;&#32467;&#26500;&#30340;&#26144;&#23556;&#23545;&#20110;&#29702;&#35299;&#20840;&#29699;&#30899;&#24490;&#29615;&#21644;&#30417;&#27979;&#22522;&#20110;&#33258;&#28982;&#30340;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#33322;&#31354;&#21644;GEDI&#28608;&#20809;&#36965;&#24863;&#25968;&#25454;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#21046;&#20316;&#30340;&#20896;&#23618;&#39640;&#24230;&#22270;&#21487;&#20197;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vegetation structure mapping is critical for understanding the global carbon cycle and monitoring nature-based approaches to climate adaptation and mitigation. Repeat measurements of these data allow for the observation of deforestation or degradation of existing forests, natural forest regeneration, and the implementation of sustainable agricultural practices like agroforestry. Assessments of tree canopy height and crown projected area at a high spatial resolution are also important for monitoring carbon fluxes and assessing tree-based land uses, since forest structures can be highly spatially heterogeneous, especially in agroforestry systems. Very high resolution satellite imagery (less than one meter (1m) ground sample distance) makes it possible to extract information at the tree level while allowing monitoring at a very large scale. This paper presents the first high-resolution canopy height map concurrently produced for multiple sub-national jurisdictions. Specifically, we produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06858</link><description>&lt;p&gt;
Vax-Culture: &#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#30123;&#33495;&#35752;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#30123;&#33495;&#29369;&#35947;&#32487;&#32493;&#26159;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#35813;&#29369;&#35947;&#30772;&#22351;&#20102;&#30123;&#33495;&#36816;&#21160;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#30830;&#23450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21453;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#38271;&#26159;&#35813;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#25512;&#29305;&#20316;&#20026;&#35823;&#23548;&#20869;&#23481;&#30340;&#26469;&#28304;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#19982;&#30123;&#33495;&#26377;&#20851;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#20511;&#21161;&#19987;&#19994;&#27807;&#36890;&#21644;&#26032;&#38395;&#32972;&#26223;&#30340;&#27880;&#37322;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#26368;&#32456;&#24076;&#26395;&#36825;&#21487;&#20197;&#24102;&#26469;&#26377;&#25928;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#25509;&#35302;&#37027;&#20123;&#25345;&#21453;&#30123;&#33495;&#20449;&#20208;&#32773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;</title><link>http://arxiv.org/abs/2304.06446</link><description>&lt;p&gt;
SpectFormer: &#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#26159;&#35270;&#35273;Transformer&#25152;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#20854;&#31181;&#31867;&#21253;&#25324;&#22522;&#20110;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#22914;ViT&#12289;DeIT&#65289;&#21644;&#22522;&#20110;&#35889;&#23618;&#65288;&#22914;Fnet&#12289;GFNet&#12289;AFNO&#65289;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#37117;&#23545;Transformer&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22240;&#27492;&#25552;&#20986;&#20102;&#26032;&#30340;Spectformer&#26550;&#26500;&#65292;&#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#34701;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Spectformer&#21487;&#24688;&#24403;&#22320;&#25429;&#25417;&#29305;&#24449;&#34920;&#31034;&#65292;&#19982;&#20854;&#20182;Transformer&#34920;&#24449;&#30456;&#27604;&#65292;&#21487;&#20197;&#25552;&#39640;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#22320;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;&#35745;&#31639;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#65292;&#20854;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06174</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#29289;&#20307;&#24863;&#30693;&#31561;&#21464;&#22522;&#20803;&#21453;&#24212;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#30830;&#36807;&#28193;&#24577;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. (arXiv:2304.06174v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#22320;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;&#35745;&#31639;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#65292;&#20854;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#28193;&#24577;&#25628;&#32034;&#22312;&#21270;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#29992;&#20110;&#38416;&#26126;&#21453;&#24212;&#26426;&#29702;&#21644;&#25506;&#32034;&#21453;&#24212;&#32593;&#32476;&#12290;&#20294;&#25628;&#32034;&#31934;&#30830;&#30340;&#19977;&#32500;&#36807;&#28193;&#24577;&#32467;&#26500;&#38656;&#35201;&#22823;&#37327;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#65292;&#22240;&#20026;&#21183;&#33021;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#28385;&#36275;&#29983;&#25104;&#21453;&#24212;&#29289;&#12289;&#36807;&#28193;&#24577;&#21644;&#29983;&#25104;&#29289;&#19977;&#31181;&#32467;&#26500;&#30340;&#25152;&#26377;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#24050;&#30693;&#21453;&#24212;&#29289;&#21644;&#29983;&#25104;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#32553;&#30701;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#12290;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230; (2.6 kcal/mol)&#65292;&#24182;&#19988;&#21482;&#38656;&#23545; 14% &#30340;&#32467;&#26524;&#36827;&#34892;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transition state (TS) search is key in chemistry for elucidating reaction mechanisms and exploring reaction networks. The search for accurate 3D TS structures, however, requires numerous computationally intensive quantum chemistry calculations due to the complexity of potential energy surfaces. Here, we developed an object-aware SE(3) equivariant diffusion model that satisfies all physical symmetries and constraints for generating pairs of structures, i.e., reactant, TS, and product, in an elementary reaction. Provided reactant and product, this model generates a TS structure in seconds instead of the hours required when performing quantum chemistry-based optimizations. The generated TS structures achieve an average error of 0.13 A root mean square deviation compared to true TS. With a confidence scoring model for uncertainty quantification, we approach an accuracy required for reaction rate estimation (2.6 kcal/mol) by only performing quantum chemistry-based optimizations on 14% of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#32534;&#36753;&#21451;&#22909;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06140</link><description>&lt;p&gt;
&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65306;&#21453;&#28436;&#19982;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
An Edit Friendly DDPM Noise Space: Inversion and Manipulations. (arXiv:2304.06140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#32534;&#36753;&#21451;&#22909;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21033;&#29992;&#19968;&#31995;&#21015;&#30333;&#22122;&#22768;&#26679;&#26412;&#29983;&#25104;&#22270;&#20687;&#12290;&#31867;&#20284;&#20110;GAN&#65292;&#36825;&#20123;&#22122;&#22768;&#22270;&#21487;&#20197;&#30475;&#20316;&#26159;&#29983;&#25104;&#22270;&#20687;&#30456;&#20851;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21407;&#22987;&#22122;&#22768;&#31354;&#38388;&#27809;&#26377;&#26041;&#20415;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#22312;&#32534;&#36753;&#20219;&#21153;&#20013;&#24456;&#38590;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;DDPM&#30340;&#28508;&#22312;&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#20219;&#20309;&#32473;&#23450;&#22270;&#20687;&#65288;&#30495;&#23454;&#25110;&#21512;&#25104;&#29983;&#25104;&#65289;&#30340;&#26131;&#20110;&#32534;&#36753;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g., shifting, color edits). Moreover, in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26500;&#24314;&#21487;&#38752;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#38081;&#36335;&#20449;&#21495;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26377;&#28508;&#21147;&#20026;&#23454;&#29616;&#27491;&#24335;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#25552;&#20379;&#23454;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2304.06052</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#21644;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26377;&#20449;&#24515;&#29289;&#20307;&#26816;&#27979;&#65306;&#38081;&#36335;&#20449;&#21495;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling. (arXiv:2304.06052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26500;&#24314;&#21487;&#38752;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#38081;&#36335;&#20449;&#21495;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26377;&#28508;&#21147;&#20026;&#23454;&#29616;&#27491;&#24335;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#25552;&#20379;&#23454;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#35748;&#35777;&#31995;&#32479;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#25552;&#20379;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#19981;&#30830;&#23450;&#24615;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26500;&#24314;&#21487;&#38752;&#30340;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#26816;&#27979;&#38081;&#36335;&#20449;&#21495;&#30340;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#28779;&#36710;&#25805;&#20316;&#21592;&#35270;&#35282;&#19979;&#30340;&#22270;&#20687;&#21644;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20960;&#31181;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#25552;&#20379;&#27491;&#24335;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#20855;&#26377;&#28508;&#21147;&#65292;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#25552;&#20379;&#20102;&#23454;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying deep learning models in real-world certified systems requires the ability to provide confidence estimates that accurately reflect their uncertainty. In this paper, we demonstrate the use of the conformal prediction framework to construct reliable and trustworthy predictors for detecting railway signals. Our approach is based on a novel dataset that includes images taken from the perspective of a train operator and state-of-the-art object detectors. We test several conformal approaches and introduce a new method based on conformal risk control. Our findings demonstrate the potential of the conformal prediction framework to evaluate model performance and provide practical guidance for achieving formally guaranteed uncertainty bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2304.05365</link><description>&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#21527;&#65311;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20010;&#24615;&#21270;&#27835;&#30103;&#24207;&#21015;&#20197;&#25903;&#25345;&#29992;&#25143;&#37319;&#21462;&#26356;&#20581;&#24247;&#30340;&#34892;&#20026;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;&#28041;&#21450;&#21040;&#22522;&#20110;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#27963;&#21160;&#27700;&#24179;&#12289;&#20301;&#32622;&#31561;&#65289;&#22312;&#20309;&#26102;&#27835;&#30103;&#20197;&#21450;&#22914;&#20309;&#27835;&#30103;&#30340;&#20915;&#23450;&#12290;&#22312;&#32447;RL&#31639;&#27861;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#21382;&#21490;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20010;&#24615;&#21270;&#36825;&#20123;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35201;&#20915;&#23450;&#26159;&#21542;&#24212;&#22312;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#20248;&#21270;&#8221;&#24178;&#39044;&#20013;&#21253;&#21547;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25968;&#25454;&#35777;&#25454;&#65292;&#34920;&#26126;RL&#31639;&#27861;&#23454;&#38469;&#19978;&#27491;&#22312;&#23558;&#27835;&#30103;&#20010;&#24615;&#21270;&#36866;&#24212;&#20854;&#29992;&#25143;&#12290;&#30001;&#20110;RL&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#22312;&#26576;&#20123;&#29366;&#24577;&#19979;&#30340;&#23398;&#20064;&#24182;&#20351;&#29992;&#27492;&#23398;&#20064;&#26469;&#25552;&#20379;&#29305;&#23450;&#27835;&#30103;&#30340;&#33021;&#21147;&#20135;&#29983;&#35823;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#23450;&#20041;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#37325;&#22797;&#37319;&#26679;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#22312;&#32447;RL&#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Perp-Neg&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#30446;&#21069;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#65292;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;3D&#24212;&#29992;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.04968</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#36127;&#25552;&#31034;&#31639;&#27861;&#65306;&#23558;2D&#25193;&#25955;&#36716;&#21270;&#20026;3D&#65292;&#32531;&#35299;&#8220;&#25196;&#23612;&#26031;&#38382;&#39064;&#8221;&#31561;&#31561;
&lt;/p&gt;
&lt;p&gt;
Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond. (arXiv:2304.04968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Perp-Neg&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#30446;&#21069;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#65292;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;3D&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#26377;&#26102;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#31867;&#20284;&#20110;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#30340;&#25991;&#26412;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;2D&#21644;3D&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#36127;&#38754;&#25552;&#31034;&#65292;&#20294;&#21457;&#29616;&#24403;&#21069;&#30340;&#23454;&#29616;&#26080;&#27861;&#20135;&#29983;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#20027;&#25552;&#31034;&#21644;&#36127;&#38754;&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;Perp-Neg&#65292;&#19968;&#31181;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#24403;&#21069;&#36127;&#24615;&#25552;&#31034;&#31639;&#27861;&#32570;&#28857;&#30340;&#26032;&#31639;&#27861;&#12290;Perp-Neg&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;Perp-Neg&#36890;&#36807;&#22312;2D&#24773;&#20917;&#19979;&#20351;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#29983;&#25104;&#22270;&#20687;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#30340;&#31639;&#27861;&#21040;3D&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;SAM&#19982;&#20256;&#32479;&#26041;&#27861;BET&#22312;MRI&#33041;&#25552;&#21462;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;SAM&#22312;&#20449;&#21495;&#19981;&#22343;&#21248;&#12289;&#38750;&#31561;&#21521;&#24615;&#20998;&#36776;&#29575;&#25110;&#30149;&#21464;&#38752;&#36817;&#33041;&#22806;&#21306;&#22495;&#21644;&#33041;&#33180;&#26102;&#25928;&#26524;&#26356;&#20339;&#65292;SAM&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#12289;&#26356;&#20581;&#22766;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.04738</link><description>&lt;p&gt;
SAM&#19982;BET&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30913;&#20849;&#25391;&#22270;&#20687;&#33041;&#25552;&#21462;&#21644;&#20998;&#21106;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning. (arXiv:2304.04738v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;SAM&#19982;&#20256;&#32479;&#26041;&#27861;BET&#22312;MRI&#33041;&#25552;&#21462;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;SAM&#22312;&#20449;&#21495;&#19981;&#22343;&#21248;&#12289;&#38750;&#31561;&#21521;&#24615;&#20998;&#36776;&#29575;&#25110;&#30149;&#21464;&#38752;&#36817;&#33041;&#22806;&#21306;&#22495;&#21644;&#33041;&#33180;&#26102;&#25928;&#26524;&#26356;&#20339;&#65292;SAM&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#12289;&#26356;&#20581;&#22766;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#25552;&#21462;&#26159;&#31070;&#32463;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#28041;&#21450;&#20351;&#29992;MRI&#25968;&#25454;&#23558;&#33041;&#32452;&#32455;&#19982;&#38750;&#33041;&#32452;&#32455;&#20998;&#31163;&#12290;FSL&#30340;&#33041;&#25552;&#21462;&#24037;&#20855;&#65288;BET&#65289;&#26159;&#24403;&#21069;&#30340;&#37329;&#26631;&#20934;&#65292;&#20294;&#30001;&#20110;&#22270;&#20687;&#36136;&#37327;&#38382;&#39064;&#23481;&#26131;&#20986;&#38169;&#12290;Meta AI&#30340;Segment Anything Model&#65288;SAM&#65289;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;&#28508;&#21147;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;SAM&#21644;BET&#22312;&#19981;&#21516;&#30340;&#33041;&#25195;&#25551;&#20013;&#30340;&#33041;&#25552;&#21462;&#25928;&#26524;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#36136;&#37327;&#12289;MRI&#24207;&#21015;&#21644;&#30149;&#21464;&#20301;&#32622;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#21442;&#25968;&#26041;&#38754;&#65292;SAM&#32988;&#36807;BET&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#21495;&#19981;&#22343;&#21248;&#12289;&#38750;&#31561;&#21521;&#24615;&#20307;&#32032;&#20998;&#36776;&#29575;&#25110;&#30149;&#21464;&#38752;&#36817;&#33041;&#22806;&#21306;&#22495;&#21644;&#33041;&#33180;&#30340;&#24773;&#20917;&#19979;&#12290;SAM&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#26126;&#20854;&#22312;&#33041;&#25552;&#21462;&#21644;&#20998;&#21106;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#20934;&#30830;&#12289;&#26356;&#20581;&#22766;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain extraction is a critical preprocessing step in neuroimaging studies, involving the separation of brain tissue from non-brain tissue using MRI data. FSL's Brain Extraction Tool (BET) is the current gold standard but is prone to errors due to image quality issues. The Segment Anything Model (SAM) by Meta AI has shown promising zero-shot segmentation potential. This paper compares SAM with BET for brain extraction on diverse brain scans, considering image quality, MRI sequences, and lesion locations. Results demonstrate that SAM outperforms BET in various evaluation parameters, particularly in cases with signal inhomogeneities, non-isotropic voxel resolutions, or lesions near the brain's outer regions and meninges. SAM's superior performance indicates its potential as a more accurate, robust, and versatile tool for brain extraction and segmentation applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#32500;&#32467;&#32928;&#34920;&#38754;&#37325;&#24314;&#65292;&#22312;&#22270;&#20687;&#39044;&#22788;&#29702;&#30340;&#36807;&#31243;&#20013;&#36890;&#36807;&#32416;&#27491;&#23616;&#37096;&#27424;&#26333;&#20809;&#21644;&#36807;&#26333;&#20809;&#26469;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03171</link><description>&lt;p&gt;
&#20197;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#26333;&#20809;&#22686;&#24378;&#20316;&#20026;&#20934;&#30830;&#30340;&#19977;&#32500;&#32467;&#32928;&#34920;&#38754;&#37325;&#24314;&#30340;&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based image exposure enhancement as a pre-processing for an accurate 3D colon surface reconstruction. (arXiv:2304.03171v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#32500;&#32467;&#32928;&#34920;&#38754;&#37325;&#24314;&#65292;&#22312;&#22270;&#20687;&#39044;&#22788;&#29702;&#30340;&#36807;&#31243;&#20013;&#36890;&#36807;&#32416;&#27491;&#23616;&#37096;&#27424;&#26333;&#20809;&#21644;&#36807;&#26333;&#20809;&#26469;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#22270;&#20687;&#39044;&#22788;&#29702;&#25913;&#21892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#32928;&#37096;&#20998;&#19977;&#32500;&#37325;&#24314;&#12290;&#20551;&#35774;&#22312;&#20869;&#31397;&#38236;&#26816;&#26597;&#20013;&#24212;&#35813;&#32416;&#27491;&#23616;&#37096;&#27424;&#26333;&#20809;&#21644;&#36807;&#26333;&#20809;&#32780;&#19981;&#26159;&#20840;&#23616;&#22270;&#20687;&#29031;&#26126;&#26657;&#27491;&#12290;&#39318;&#20808;&#27010;&#36848;&#20102;&#21253;&#25324;&#22270;&#20687;&#26333;&#20809;&#26657;&#27491;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;-SLAM&#30340;&#27969;&#31243;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#19982;&#36866;&#24403;&#29031;&#26126;&#26657;&#27491;&#21644;&#26080;&#26657;&#27491;&#30340;&#24773;&#20917;&#19979;&#32467;&#32928;&#20869;&#38236;&#36712;&#36857;&#30340;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This contribution shows how an appropriate image pre-processing can improve a deep-learning based 3D reconstruction of colon parts. The assumption is that, rather than global image illumination corrections, local under- and over-exposures should be corrected in colonoscopy. An overview of the pipeline including the image exposure correction and a RNN-SLAM is first given. Then, this paper quantifies the reconstruction accuracy of the endoscope trajectory in the colon with and without appropriate illumination correction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SSCL&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#38590;&#20197;&#21306;&#20998;&#20986;&#30340;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02971</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21512;&#25104;&#38590;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Synthetic Hard Negative Samples for Contrastive Learning. (arXiv:2304.02971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SSCL&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#38590;&#20197;&#21306;&#20998;&#20986;&#30340;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#30446;&#26631;&#26159;&#22312;&#26368;&#22823;&#21270;&#21516;&#19968;&#22270;&#20687;&#30340;&#20004;&#20010;&#22686;&#24378;&#29256;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65288;&#27491;&#23545;&#65289;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65288;&#36127;&#23545;&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38590;&#24230;&#26356;&#22823;&#30340;&#36127;&#26679;&#26412;&#65292;&#21363;&#38590;&#20197;&#20174;&#38170;&#23450;&#26679;&#26412;&#20013;&#21306;&#20998;&#20986;&#30340;&#26679;&#26412;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#21457;&#25381;&#20102;&#26356;&#20026;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#23618;&#26041;&#27861;&#65292;&#21363;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21512;&#25104;&#38590;&#36127;&#26679;&#26412;&#65288;SSCL&#65289;&#30340;&#37319;&#26679;&#65292;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#26356;&#38590;&#30340;&#36127;&#26679;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;1&#65289;&#25105;&#20204;&#36890;&#36807;&#28151;&#21512;&#36127;&#26679;&#26412;&#29983;&#25104;&#26356;&#22810;&#21644;&#26356;&#38590;&#30340;&#36127;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#25511;&#21046;&#38170;&#28857;&#26679;&#26412;&#19982;&#20854;&#20182;&#36127;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#26469;&#23545;&#36825;&#20123;&#36127;&#26679;&#26412;&#36827;&#34892;&#37319;&#26679;&#12290;2&#65289;&#32771;&#34385;&#21040;&#36890;&#36807;&#37319;&#26679;&#24471;&#21040;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#23384;&#22312;&#20551;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#27880;&#37325;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has emerged as an essential approach for self-supervised learning in computer vision. The central objective of contrastive learning is to maximize the similarities between two augmented versions of the same image (positive pairs), while minimizing the similarities between different images (negative pairs). Recent studies have demonstrated that harder negative samples, i.e., those that are difficult to distinguish from anchor sample, play a more critical role in contrastive learning. In this paper, we propose a novel featurelevel method, namely sampling synthetic hard negative samples for contrastive learning (SSCL), to exploit harder negative samples more effectively. Specifically, 1) we generate more and harder negative samples by mixing negative samples, and then sample them by controlling the contrast of anchor sample with the other negative samples. 2) Considering that the negative samples obtained by sampling may have the problem of false negative samples, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaIRNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#36739;&#24102;&#26377;&#25110;&#19981;&#24102;&#26377;&#39044;&#22788;&#29702;&#30340;&#32437;&#21521;&#22270;&#20687;&#23545;&#65292;&#20197;&#26816;&#27979;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#19982;&#30740;&#31350;&#38382;&#39064;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02531</link><description>&lt;p&gt;
&#23398;&#20064;&#27604;&#36739;&#32437;&#21521;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Learning to Compare Longitudinal Images. (arXiv:2304.02531v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaIRNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#36739;&#24102;&#26377;&#25110;&#19981;&#24102;&#26377;&#39044;&#22788;&#29702;&#30340;&#32437;&#21521;&#22270;&#20687;&#23545;&#65292;&#20197;&#26816;&#27979;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#19982;&#30740;&#31350;&#38382;&#39064;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#30740;&#31350;&#26159;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30740;&#31350;&#21644;&#25551;&#36848;&#26102;&#38388;&#21160;&#24577;&#30340;&#24120;&#29992;&#25216;&#26415;&#65292;&#20854;&#20013;&#20174;&#30456;&#21516;&#30340;&#20154;&#32676;&#20013;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#33719;&#21462;&#19968;&#31995;&#21015;&#22270;&#20687;&#12290;&#20256;&#32479;&#30340;&#32437;&#21521;&#27604;&#36739;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#22788;&#29702;&#26469;&#26631;&#20934;&#21270;&#22122;&#22768;&#21464;&#21270;&#65292;&#20363;&#22914;&#22270;&#20687;&#26041;&#21521;&#25110;&#23545;&#27604;&#24230;&#24046;&#24322;&#65292;&#28982;&#21518;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#20197;&#26816;&#27979;&#24863;&#20852;&#36259;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861; PaIRNet&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#27604;&#36739;&#32437;&#21521;&#22270;&#20687;&#23545;&#65292;&#26080;&#35770;&#26159;&#21542;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;PaIRNet&#23398;&#20064;&#35782;&#21035;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#30340;&#26368;&#20855;&#36776;&#21035;&#24615;&#30340;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#30456;&#20284;&#24230;&#24471;&#20998;&#20197;&#21453;&#26144;&#23427;&#20204;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal studies, where a series of images from the same set of individuals are acquired at different time-points, represent a popular technique for studying and characterizing temporal dynamics in biomedical applications. The classical approach for longitudinal comparison involves normalizing for nuisance variations, such as image orientation or contrast differences, via pre-processing. Statistical analysis is, in turn, conducted to detect changes of interest, either at the individual or population level. This classical approach can suffer from pre-processing issues and limitations of the statistical modeling. For example, normalizing for nuisance variation might be hard in settings where there are a lot of idiosyncratic changes. In this paper, we present a simple machine learning-based approach that can alleviate these issues. In our approach, we train a deep learning model (called PaIRNet, for Pairwise Image Ranking Network) to compare pairs of longitudinal images, with or witho
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#23376;&#38598;&#30340;&#29305;&#24449;&#26469;&#38477;&#20302;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#36890;&#24120;&#20248;&#20110;&#20854;&#20182;&#24050;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#30001;&#25968;&#30334;&#19975;&#25968;&#25454;&#28857;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.02455</link><description>&lt;p&gt;
&#26681;&#25454;&#25968;&#25454;&#30340;&#32500;&#24230;&#40065;&#26834;&#24615;&#36873;&#25321;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Selecting Features by their Resilience to the Curse of Dimensionality. (arXiv:2304.02455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#23376;&#38598;&#30340;&#29305;&#24449;&#26469;&#38477;&#20302;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#36890;&#24120;&#20248;&#20110;&#20854;&#20182;&#24050;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#30001;&#25968;&#30334;&#19975;&#25968;&#25454;&#28857;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#32500;&#24230;&#24456;&#39640;&#65292;&#24182;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#29305;&#24449;&#36873;&#25321;&#26088;&#22312;&#35782;&#21035;&#23545;&#20110;&#23398;&#20064;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;&#30456;&#20851;&#24615;&#21644;&#25104;&#23545;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#36890;&#24120;&#34987;&#20351;&#29992;&#65292;&#20294;&#32500;&#24230;&#35781;&#21650;&#24456;&#23569;&#34987;&#32435;&#20837;&#21040;&#36873;&#25321;&#29305;&#24449;&#30340;&#36807;&#31243;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#23376;&#38598;&#30340;&#29305;&#24449;&#26469;&#38477;&#20302;&#32500;&#24230;&#35781;&#21650;&#12290;&#36890;&#36807;&#35843;&#25972;&#26368;&#36817;&#20851;&#20110;&#35745;&#31639;&#20869;&#22312;&#32500;&#24230;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36873;&#25321;&#33021;&#22815;&#21306;&#20998;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#22240;&#27492;&#20943;&#24369;&#32500;&#24230;&#35781;&#21650;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#36890;&#24120;&#20248;&#20110;&#25104;&#29087;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#30001;&#25968;&#30334;&#19975;&#25968;&#25454;&#28857;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#24449;&#24378;&#24230;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#36873;&#25321;&#30340;&#29305;&#24449;&#26356;&#21152;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world datasets are often of high dimension and effected by the curse of dimensionality. This hinders their comprehensibility and interpretability. To reduce the complexity feature selection aims to identify features that are crucial to learn from said data. While measures of relevance and pairwise similarities are commonly used, the curse of dimensionality is rarely incorporated into the process of selecting features. Here we step in with a novel method that identifies the features that allow to discriminate data subsets of different sizes. By adapting recent work on computing intrinsic dimensionalities, our method is able to select the features that can discriminate data and thus weaken the curse of dimensionality. Our experiments show that our method is competitive and commonly outperforms established feature selection methods. Furthermore, we propose an approximation that allows our method to scale to datasets consisting of millions of data points. Our findings suggest that fea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#30005;&#21147;&#32593;&#32476;&#36816;&#34892;&#26234;&#33021;&#20307;&#24182;&#25552;&#20379;&#20102;&#26032;&#31574;&#30053;&#12290;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#25913;&#36827;&#26159;&#37319;&#29992;N-1&#31574;&#30053;&#65292;&#32771;&#34385;&#21040;&#22312;&#26576;&#26465;&#32447;&#36335;&#26029;&#24320;&#26102;&#30340;&#25299;&#25169;&#25805;&#20316;&#20197;&#20445;&#25345;&#32593;&#32476;&#31283;&#23450;&#12290;&#21478;&#22806;&#65292;&#25299;&#25169;&#21453;&#36716;&#20063;&#26377;&#21033;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;27%&#12290;</title><link>http://arxiv.org/abs/2304.00765</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25805;&#20316;&#31649;&#29702;&#30005;&#21147;&#32593;&#32476;&#65306;&#20808;&#36827;&#22522;&#20110;&#35268;&#21017;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Managing power grids through topology actions: A comparative study between advanced rule-based and reinforcement learning agents. (arXiv:2304.00765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#30005;&#21147;&#32593;&#32476;&#36816;&#34892;&#26234;&#33021;&#20307;&#24182;&#25552;&#20379;&#20102;&#26032;&#31574;&#30053;&#12290;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#25913;&#36827;&#26159;&#37319;&#29992;N-1&#31574;&#30053;&#65292;&#32771;&#34385;&#21040;&#22312;&#26576;&#26465;&#32447;&#36335;&#26029;&#24320;&#26102;&#30340;&#25299;&#25169;&#25805;&#20316;&#20197;&#20445;&#25345;&#32593;&#32476;&#31283;&#23450;&#12290;&#21478;&#22806;&#65292;&#25299;&#25169;&#21453;&#36716;&#20063;&#26377;&#21033;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24403;&#21069;&#24418;&#21183;&#30340;&#21160;&#33633;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#29983;&#20135;&#30340;&#22686;&#21152;&#65292;&#30005;&#21147;&#32593;&#32476;&#36816;&#34892;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#26041;&#27861;&#30340;&#20027;&#21160;&#32593;&#26684;&#31649;&#29702;&#24050;&#32463;&#36798;&#21040;&#20102;&#26497;&#38480;&#12290;&#36890;&#36807;&#8220;&#23398;&#20064;&#36816;&#34892;&#30005;&#21147;&#32593;&#32476;&#8221;&#25361;&#25112;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#32593;&#26684;&#36816;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;Binbinchen&#25552;&#20132;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#26032;&#31574;&#30053;&#12290;&#20027;&#35201;&#30340;&#25913;&#36827;&#26159;N-1&#31574;&#30053;&#65292;&#21363;&#32771;&#34385;&#21040;&#22312;&#26576;&#26465;&#32447;&#36335;&#26029;&#24320;&#26102;&#65292;&#20173;&#21487;&#20445;&#25345;&#32593;&#32476;&#31283;&#23450;&#30340;&#25299;&#25169;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#25299;&#25169;&#32467;&#26500;&#24674;&#22797;&#21040;&#21407;&#22987;&#29366;&#24577;&#30340;&#25299;&#25169;&#21453;&#36716;&#65292;&#35777;&#26126;&#20854;&#26377;&#30410;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#22522;&#20110;&#35268;&#21017;&#30340;&#26234;&#33021;&#20307;&#24615;&#33021;&#25552;&#39640;27&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The operation of electricity grids has become increasingly complex due to the current upheaval and the increase in renewable energy production. As a consequence, active grid management is reaching its limits with conventional approaches. In the context of the Learning to Run a Power Network challenge, it has been shown that Reinforcement Learning (RL) is an efficient and reliable approach with considerable potential for automatic grid operation. In this article, we analyse the submitted agent from Binbinchen and provide novel strategies to improve the agent, both for the RL and the rule-based approach. The main improvement is a N-1 strategy, where we consider topology actions that keep the grid stable, even if one line is disconnected. More, we also propose a topology reversion to the original grid, which proved to be beneficial. The improvements are tested against reference approaches on the challenge test sets and are able to increase the performance of the rule-based agent by 27%. I
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11582</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36866;&#24212;&#24615;&#23454;&#39564;&#65306;&#28789;&#27963;&#25209;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#20551;&#23450;&#25345;&#32493;&#37325;&#26032;&#20998;&#37197;&#27979;&#37327;&#24037;&#20316;&#65292;&#36825;&#22312;&#23454;&#29616;&#36807;&#31243;&#20013;&#23384;&#22312;&#24310;&#36831;&#21453;&#39304;&#21644;&#22522;&#30784;&#35774;&#26045;/&#32452;&#32455;&#38590;&#39064;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20165;&#26377;&#23569;&#25968;&#37325;&#26032;&#20998;&#37197;&#38454;&#27573;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#20854;&#20013;&#27979;&#37327;&#32467;&#26524;&#26159;&#20197;&#25209;&#22788;&#29702;&#24418;&#24335;&#27979;&#37327;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#27491;&#24577;&#36817;&#20284;&#20063;&#21487;&#20197;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#12290;&#36890;&#36807;&#25512;&#23548;&#28176;&#36827;&#39034;&#24207;&#23454;&#39564;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#65292;&#21487;&#20197;&#21033;&#29992;&#24179;&#22343;&#22238;&#25253;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#21160;&#24577;&#35268;&#21010;&#30340;&#29366;&#24577;&#36716;&#31227;&#30456;&#23545;&#20110;&#37319;&#26679;&#20998;&#37197;&#26159;&#21487;&#24494;&#30340;&#65292;&#20801;&#35768;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#35268;&#21010;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36845;&#20195;&#35268;&#21010;&#26041;&#27861;&#65292;&#21363;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#35268;&#21010;&#30446;&#26631;&#26469;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.08133</link><description>&lt;p&gt;
MeshDiffusion&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MeshDiffusion: Score-based Generative 3D Mesh Modeling. (arXiv:2303.08133v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#29289;&#20307;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#21644;&#29289;&#29702;&#20223;&#30495;&#31561;&#22810;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#30456;&#27604;&#20110;&#20307;&#32032;&#21644;&#28857;&#20113;&#31561;&#20854;&#20182;3D&#34920;&#31034;&#65292;&#32593;&#26684;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#20248;&#36234;&#65292;&#22240;&#20026;(1)&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#20219;&#24847;&#22320;&#25805;&#32437;&#24418;&#29366;&#20197;&#20379;&#37325;&#26032;&#29031;&#26126;&#21644;&#20223;&#30495;&#65292;(2)&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;&#29616;&#20195;&#22270;&#24418;&#27969;&#27700;&#32447;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#27969;&#27700;&#32447;&#22823;&#22810;&#25968;&#38024;&#23545;&#32593;&#26684;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#20197;&#24448;&#21487;&#25193;&#23637;&#30340;3D&#32593;&#26684;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#21518;&#22788;&#29702;&#65292;&#24182;&#19988;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#36807;&#20110;&#24179;&#28369;&#25110;&#22024;&#26434;&#30340;&#34920;&#38754;&#65292;&#32570;&#20047;&#31934;&#32454;&#30340;&#20960;&#20309;&#32454;&#33410;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;3D&#32593;&#26684;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#21464;&#24418;&#22235;&#38754;&#20307;&#32593;&#26684;&#26469;&#34920;&#31034;&#32593;&#26684;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#32593;&#26684;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.06484</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#22635;&#34917;&#31070;&#32463;&#22349;&#22604;&#30340;&#27867;&#21270;&#21644;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generalized neural collapse hypothesis that effectively subsumes the original neural collapse and decomposes it into two objectives: minimizing intra-class variability and maximizing inter-class separability. The authors use hyperspherical uniformity as a unified framework to quantify these objectives and propose a general objective, hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22349;&#22604;&#29616;&#35937;&#25551;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24213;&#23618;&#20960;&#20309;&#23545;&#31216;&#24615;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#37117;&#25910;&#25947;&#20110;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22343;&#26041;&#35823;&#24046;&#37117;&#21487;&#20197;&#23548;&#33268;&#31070;&#32463;&#22349;&#22604;&#12290;&#25105;&#20204;&#28040;&#38500;&#20102;&#31070;&#32463;&#22349;&#22604;&#23545;&#29305;&#24449;&#32500;&#24230;&#21644;&#31867;&#21035;&#25968;&#37327;&#30340;&#20851;&#38190;&#20551;&#35774;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#12290;&#21463;&#31070;&#32463;&#22349;&#22604;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30446;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#65288;&#23427;&#25551;&#36848;&#20102;&#21333;&#20301;&#36229;&#29699;&#19978;&#22343;&#21248;&#24615;&#30340;&#31243;&#24230;&#65289;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#36827;&#34892;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20248;&#21270;&#26679;&#26412;&#26631;&#35760;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.05391</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#30340;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Disambiguation of Company names via Deep Recurrent Networks. (arXiv:2303.05391v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#36827;&#34892;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20248;&#21270;&#26679;&#26412;&#26631;&#35760;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#28040;&#27495;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#23545;&#24212;&#20110;&#21516;&#19968;&#21629;&#21517;&#23454;&#20307;&#30340;&#25991;&#26412;&#35760;&#24405;&#12290;&#26412;&#30740;&#31350;&#30340;&#20219;&#21153;&#26159;&#26681;&#25454;&#20844;&#21496;&#30340;&#20070;&#38754;&#21517;&#31216;&#28040;&#38500;&#27495;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;LSTM&#32593;&#32476;&#26041;&#27861;&#26469;&#25552;&#21462;&#20844;&#21496;&#21517;&#31216;&#23383;&#31526;&#20018;&#30340;&#23884;&#20837;&#65292;&#36827;&#32780;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#26469;&#35782;&#21035;&#30495;&#27491;&#34920;&#31034;&#21516;&#19968;&#20844;&#21496;&#65288;&#21363;&#30456;&#21516;&#23454;&#20307;&#65289;&#30340;&#20844;&#21496;&#21517;&#31216;&#23545;&#12290;&#32771;&#34385;&#21040;&#25163;&#21160;&#26631;&#35760;&#23383;&#31526;&#20018;&#23545;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#20174;&#32780;&#20351;&#25972;&#20010;&#23398;&#20064;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#32463;&#23454;&#35777;&#65292;&#25105;&#20204;&#30340;Siamese&#32593;&#32476;&#20248;&#20110;&#22810;&#31181;&#22522;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Name Entity Disambiguation is the Natural Language Processing task of identifying textual records corresponding to the same Named Entity, i.e. real-world entities represented as a list of attributes (names, places, organisations, etc.). In this work, we face the task of disambiguating companies on the basis of their written names. We propose a Siamese LSTM Network approach to extract -- via supervised learning -- an embedding of company name strings in a (relatively) low dimensional vector space and use this representation to identify pairs of company names that actually represent the same company (i.e. the same Entity).  Given that the manual labelling of string pairs is a rather onerous task, we analyse how an Active Learning approach to prioritise the samples to be labelled leads to a more efficient overall learning pipeline.  With empirical investigations, we show that our proposed Siamese Network outperforms several benchmark approaches based on standard string matching algorithms
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#35777;&#25454;&#25903;&#25345;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;AD&#30456;&#20851;&#30340;&#36741;&#21161;&#20219;&#21153;&#26469;&#23398;&#20064;MRI&#25195;&#25551;&#20013;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#35777;&#25454;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.01105</link><description>&lt;p&gt;
&#35777;&#25454;&#25903;&#25345;&#30340;&#36716;&#31227;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evidence-empowered Transfer Learning for Alzheimer's Disease. (arXiv:2303.01105v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#35777;&#25454;&#25903;&#25345;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;AD&#30456;&#20851;&#30340;&#36741;&#21161;&#20219;&#21153;&#26469;&#23398;&#20064;MRI&#25195;&#25551;&#20013;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#35777;&#25454;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20943;&#36731;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36716;&#31227;&#23398;&#20064;&#20381;&#36182;&#20110;&#37325;&#22797;&#20351;&#29992;&#22312;AD&#38750;&#30456;&#20851;&#20219;&#21153;&#65288;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#20998;&#31867;&#65289;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#21307;&#23398;&#26469;&#28304;&#21644;&#30446;&#26631;&#21307;&#23398;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23427;&#32463;&#24120;&#23548;&#33268;&#36127;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#25454;&#25903;&#25345;&#30340;AD&#35786;&#26029;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;AD&#30456;&#20851;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#21363;&#24418;&#24577;&#21464;&#21270;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;MRI&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#36741;&#21161;&#20219;&#21153;&#20013;&#65292;&#35786;&#26029;&#27169;&#22411;&#23398;&#20064;&#26469;&#33258;MRI&#25195;&#25551;&#20013;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#35777;&#25454;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#26080;&#35770;&#27169;&#22411;&#23481;&#37327;&#22914;&#20309;&#65292;&#20063;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has been widely utilized to mitigate the data scarcity problem in the field of Alzheimer's disease (AD). Conventional transfer learning relies on re-using models trained on AD-irrelevant tasks such as natural image classification. However, it often leads to negative transfer due to the discrepancy between the non-medical source and target medical domains. To address this, we present evidence-empowered transfer learning for AD diagnosis. Unlike conventional approaches, we leverage an AD-relevant auxiliary task, namely morphological change prediction, without requiring additional MRI data. In this auxiliary task, the diagnosis model learns the evidential and transferable knowledge from morphological features in MRI scans. Experimental results demonstrate that our framework is not only effective in improving detection performance regardless of model capacity, but also more data-efficient and faithful.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#35299;&#37322;&#21487;&#29702;&#35299;&#24615;&#65292;&#20197;&#21450;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#30340;&#36866;&#24403;&#26041;&#27861;&#33853;&#21518;&#20110;&#25216;&#26415;&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.00934</link><description>&lt;p&gt;
&#26377;&#24110;&#21161;&#12289;&#24341;&#23548;&#36824;&#26159;&#35753;&#20154;&#22256;&#24785;: &#20154;&#20204;&#22914;&#20309;&#30475;&#24453;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Helpful, Misleading or Confusing: How Humans Perceive Fundamental Building Blocks of Artificial Intelligence Explanations. (arXiv:2303.00934v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#35299;&#37322;&#21487;&#29702;&#35299;&#24615;&#65292;&#20197;&#21450;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#30340;&#36866;&#24403;&#26041;&#27861;&#33853;&#21518;&#20110;&#25216;&#26415;&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#21457;&#23637;&#65292;&#20294;&#36866;&#24403;&#30340;&#35780;&#20272;&#26041;&#27861;&#33853;&#21518;&#12290;&#38543;&#30528;&#35299;&#37322;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20197;&#21450;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#25928;&#29992;&#30340;&#20849;&#35782;&#65292;&#35780;&#21028;&#19981;&#21516;&#35299;&#37322;&#30340;&#22909;&#22788;&#21644;&#26377;&#25928;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#30740;&#31350;&#31616;&#21333;&#20915;&#31574;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19981;&#26159;&#22797;&#26434;&#30340;&#39044;&#27979;&#31639;&#27861;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20154;&#20204;&#22914;&#20309;&#29702;&#35299;&#23427;&#20204;&#30340;&#19981;&#21516;&#34920;&#31034;&#24418;&#24335;&#65292;&#20363;&#22914;&#25968;&#23398;&#20844;&#24335;&#12289;&#22270;&#24418;&#34920;&#31034;&#21644;&#25991;&#26412;&#25688;&#35201;&#65288;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#33539;&#22260;&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#21040;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65288;&#24037;&#31243;&#24072;&#12289;&#30740;&#31350;&#32773;&#12289;&#28040;&#36153;&#32773;&#12289;&#30417;&#31649;&#26426;&#26500;&#31561;&#65289;&#22914;&#20309;&#35780;&#21028;&#26356;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#25152;&#26500;&#24314;&#30340;&#22522;&#26412;&#27010;&#24565;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#25105;&#20204;&#20174;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#24314;&#31435;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence techniques are developed at breakneck speed, but suitable evaluation approaches lag behind. With explainers becoming increasingly complex and a lack of consensus on how to assess their utility, it is challenging to judge the benefit and effectiveness of different explanations. To address this gap, we take a step back from sophisticated predictive algorithms and instead look into explainability of simple decision-making models. In this setting, we aim to assess how people perceive comprehensibility of their different representations such as mathematical formulation, graphical representation and textual summarisation (of varying complexity and scope). This allows us to capture how diverse stakeholders -engineers, researchers, consumers, regulators and the like -- judge intelligibility of fundamental concepts that more elaborate artificial intelligence explanations are built from. This position paper charts our approach to establishing appropriate eva
&lt;/p&gt;</description></item><item><title>pyribs&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#29992;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#65292;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#32452;&#21512;&#31639;&#27861;&#65292;&#26159;&#19968;&#20010;&#28789;&#27963;&#26131;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.00191</link><description>&lt;p&gt;
pyribs: &#19968;&#20010;Python&#22522;&#30784;&#24211;&#29992;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
pyribs: A Bare-Bones Python Library for Quality Diversity Optimization. (arXiv:2303.00191v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00191
&lt;/p&gt;
&lt;p&gt;
pyribs&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#29992;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#65292;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#32452;&#21512;&#31639;&#27861;&#65292;&#26159;&#19968;&#20010;&#28789;&#27963;&#26131;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#20248;&#21270;&#22312;&#20248;&#21270;&#30340;&#20998;&#25903;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35813;&#20998;&#25903;&#23547;&#27714;&#25214;&#21040;&#32473;&#23450;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#25105;&#20204;&#35748;&#20026;QD&#31038;&#21306;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#24320;&#21457;&#19968;&#20010;&#20195;&#34920;&#35813;&#39046;&#22495;&#19981;&#26029;&#22686;&#38271;&#30340;&#31639;&#27861;&#25968;&#32452;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#25903;&#25345;&#19968;&#31995;&#21015;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#36719;&#20214;&#20013;&#23454;&#29616;&#35813;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#27010;&#24565;QD&#26694;&#26550;&#30340;&#24211;pyribs&#12290;&#36890;&#36807;&#26367;&#25442;&#27010;&#24565;&#26694;&#26550;&#20013;&#30340;&#32452;&#20214;&#65292;&#22240;&#27492;&#22312;pyribs&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;QD&#25991;&#29486;&#20013;&#32452;&#21512;&#31639;&#27861;&#65292;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#20182;&#20204;&#21487;&#20197;&#35782;&#21035;&#26410;&#25506;&#32034;&#30340;&#31639;&#27861;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;pyribs&#20351;&#36825;&#20010;&#26694;&#26550;&#31616;&#21333;&#65292;&#28789;&#27963;&#21644;&#26131;&#20110;&#35775;&#38382;&#65292;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;API&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#25991;&#26723;&#21644;&#25945;&#31243;&#12290;&#26412;&#31687;&#35770;&#25991;&#37325;&#28857;&#27010;&#36848;&#20102;pyribs&#30340;&#21019;&#24314;&#36807;&#31243;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#27010;&#24565;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a rise in the popularity of quality diversity (QD) optimization, a branch of optimization that seeks to find a collection of diverse, high-performing solutions to a given problem. To grow further, we believe the QD community faces two challenges: developing a framework to represent the field's growing array of algorithms, and implementing that framework in software that supports a range of researchers and practitioners. To address these challenges, we have developed pyribs, a library built on a highly modular conceptual QD framework. By replacing components in the conceptual framework, and hence in pyribs, users can compose algorithms from across the QD literature; equally important, they can identify unexplored algorithm variations. Furthermore, pyribs makes this framework simple, flexible, and accessible, with a user-friendly API supported by extensive documentation and tutorials. This paper overviews the creation of pyribs, focusing on the conceptual framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00085</link><description>&lt;p&gt;
AR3n: &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AR3n&#65288;&#21457;&#38899;&#20026;Aaron&#65289;&#65292;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#65292;&#21487;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#20070;&#20889;&#24247;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#36866;&#24212;&#24615;&#36741;&#21161;&#12290;&#19982;&#20197;&#24448;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24739;&#32773;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#25110;&#29289;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#26469;&#20351;AR3n&#25512;&#24191;&#21040;&#22810;&#20010;&#21463;&#35797;&#32773;&#12290;&#35813;&#31995;&#32479;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#22522;&#20110;&#34987;&#35797;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#36890;&#36807;&#19968;&#32452;&#20223;&#30495;&#23454;&#39564;&#21644;&#20154;&#20307;&#21463;&#35797;&#23454;&#39564;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#25511;&#21046;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#20004;&#31181;&#25511;&#21046;&#22120;&#30340;&#36741;&#21161;&#26426;&#21046;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32858;&#31867;&#20559;&#26012;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#21644;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#37096;&#35757;&#32451;&#27491;&#21017;&#21270;&#26041;&#27861;</title><link>http://arxiv.org/abs/2302.10413</link><description>&lt;p&gt;
CADIS&#65306;&#37319;&#29992;&#32858;&#31867;&#32858;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#27491;&#21017;&#21270;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32858;&#31867;&#20559;&#26012;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization. (arXiv:2302.10413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32858;&#31867;&#20559;&#26012;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#21644;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#37096;&#35757;&#32451;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#21327;&#20316;&#22320;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#23427;&#20204;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;IID&#65289;&#26102;&#65292;&#21363;&#30001;&#36890;&#24120;&#19981;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#30340;&#19968;&#31181;&#26032;&#22411;&#38750;IID&#25968;&#25454;&#65292;&#31216;&#20026;&#32858;&#31867;&#20559;&#26012;&#38750;IID&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#31181;&#25968;&#25454;&#29616;&#35937;&#26159;&#25351;&#23458;&#25143;&#31471;&#21487;&#20197;&#34987;&#20998;&#25104;&#20855;&#26377;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#32676;&#32452;&#12290;&#36890;&#36807;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#27425;&#32423;&#23618;&#34892;&#20026;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#20004;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#19981;&#36829;&#21453;&#20854;&#38544;&#31169;&#26435;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#26696;&#65292;&#30830;&#20445;&#19981;&#21516;&#32676;&#32452;&#20043;&#38388;&#30340;&#24179;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#22411;&#23616;&#37096;&#35757;&#32451;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables edge devices to train a global model collaboratively without exposing their data. Despite achieving outstanding advantages in computing efficiency and privacy protection, federated learning faces a significant challenge when dealing with non-IID data, i.e., data generated by clients that are typically not independent and identically distributed. In this paper, we tackle a new type of Non-IID data, called cluster-skewed non-IID, discovered in actual data sets. The cluster-skewed non-IID is a phenomenon in which clients can be grouped into clusters with similar data distributions. By performing an in-depth analysis of the behavior of a classification model's penultimate layer, we introduce a metric that quantifies the similarity between two clients' data distributions without violating their privacy. We then propose an aggregation scheme that guarantees equality between clusters. In addition, we offer a novel local training regularization based on the knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#20197;&#20811;&#26381;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09664</link><description>&lt;p&gt;
&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#65306;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#35821;&#35328;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. (arXiv:2302.09664v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#20197;&#20811;&#26381;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#20687;&#38382;&#31572;&#20219;&#21153;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20102;&#35299;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#27979;&#37327;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#8212;&#8212;&#19981;&#21516;&#30340;&#21477;&#23376;&#21487;&#20197;&#34920;&#31034;&#30456;&#21516;&#30340;&#24847;&#24605;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#8212;&#8212;&#19968;&#31181;&#21253;&#21547;&#20849;&#20139;&#21547;&#20041;&#25152;&#21019;&#24314;&#30340;&#35821;&#35328;&#19981;&#21464;&#37327;&#30340;&#29109;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#20041;&#29109;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#27604;&#21487;&#27604;&#22522;&#32447;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.08973</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#20013;&#30340;&#24179;&#31561;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#21457;&#23637;&#20102;&#35768;&#22810;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#20294;&#36825;&#20010;&#31038;&#21306;&#20013;&#40092;&#26377;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#20026;&#35841;&#25552;&#20379;&#20445;&#25252;&#21602;&#65311;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#34987;&#19981;&#21516;&#30340;&#23376;&#32676;&#20307;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#23454;&#35777;&#32467;&#26524;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#21487;&#33021;&#20250;&#30452;&#25509;&#36896;&#25104;&#20260;&#23475;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26377;&#20559;&#28431;&#27934;&#21644;&#26377;&#20559;&#25490;&#26021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24378;&#21270;&#35757;&#32451;&#27169;&#22411;&#12289;&#22522;&#20110;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#21644;&#25298;&#32477;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#22312;&#23433;&#20840;&#39044;&#31639;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36866;&#21512;&#20110;&#27979;&#37327;&#38450;&#24481;&#30340;&#24179;&#31561;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#20195;&#38450;&#24481;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#24179;&#31561;&#24615;&#24615;&#33021;&#30340;&#34913;&#37327;&#20215;&#20540;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#21644;&#26041;&#27861;&#33021;&#22815;&#40723;&#21169;&#21644;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21644;&#38450;&#24481;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XploreNAS&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#38454;&#27573;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#38024;&#23545;&#38750;&#29702;&#24819;&#20132;&#21449;&#26639;&#26550;&#26500;&#25506;&#32034;&#23545;&#25239;&#24615;&#24378;&#21644;&#30828;&#20214;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#37319;&#26679;&#20986;&#20855;&#26377;&#23545;&#25239;&#24615;&#24378;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#29702;&#24819;&#24615;&#20132;&#21449;&#26639;&#26550;&#26500;&#19979;&#30340;&#30828;&#20214;&#21644;&#23545;&#25239;&#24615;&#30340;&#22343;&#34913;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.07769</link><description>&lt;p&gt;
XploreNAS&#65306;&#38024;&#23545;&#38750;&#29702;&#24819;&#20132;&#21449;&#26639;&#26550;&#26500;&#25506;&#32034;&#23545;&#25239;&#24615;&#24378;&#21644;&#30828;&#20214;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
XploreNAS: Explore Adversarially Robust &amp; Hardware-efficient Neural Architectures for Non-ideal Xbars. (arXiv:2302.07769v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XploreNAS&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#38454;&#27573;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#38024;&#23545;&#38750;&#29702;&#24819;&#20132;&#21449;&#26639;&#26550;&#26500;&#25506;&#32034;&#23545;&#25239;&#24615;&#24378;&#21644;&#30828;&#20214;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#37319;&#26679;&#20986;&#20855;&#26377;&#23545;&#25239;&#24615;&#24378;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#29702;&#24819;&#24615;&#20132;&#21449;&#26639;&#26550;&#26500;&#19979;&#30340;&#30828;&#20214;&#21644;&#23545;&#25239;&#24615;&#30340;&#22343;&#34913;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#35745;&#31639;&#20869;&#23384;&#24179;&#21488;&#65292;&#22914;&#23384;&#20648;&#22120;&#24615;&#20132;&#21449;&#26639;&#26550;&#65292;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35745;&#31639;&#37327;&#21644;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#20013;&#31934;&#24230;&#30340;&#19981;&#29702;&#24819;&#24615;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#19968;&#23450;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;DNN&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#20250;&#23548;&#33268;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;DNN&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#20026;&#38750;&#29702;&#24819;&#20132;&#21449;&#26639;&#26550;&#26500;&#23547;&#25214;&#23545;&#25239;&#24615;&#24378;&#30340;DNN&#32467;&#26500;&#23545;&#20110;&#22312;&#36793;&#32536;&#19978;&#23433;&#20840;&#22320;&#37096;&#32626;DNN&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XploreNAS&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23547;&#25214;&#36866;&#29992;&#20110;&#38750;&#29702;&#24819;&#20132;&#21449;&#26639;&#26550;&#26500;&#30340;&#30828;&#20214;&#39640;&#25928;&#21644;&#23545;&#25239;&#24615;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#27425;&#24615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#20132;&#21449;&#26639;&#26550;&#26500;&#24863;&#30693;&#30340;&#22823;&#22411;&#36229;&#32593;&#32476;&#65292;&#24182;&#37319;&#26679;&#20986;&#20855;&#26377;&#23545;&#25239;&#24615;&#24378;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#21516;&#26102;&#32500;&#25345;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#30828;&#20214;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20026;&#38750;&#29702;&#24819;&#20132;&#21449;&#26639;&#26550;&#26500;&#30340;DNN&#23454;&#29616;&#30828;&#20214;&#21644;&#23545;&#25239;&#24615;&#30340;&#22343;&#34913;&#21457;&#23637;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute In-Memory platforms such as memristive crossbars are gaining focus as they facilitate acceleration of Deep Neural Networks (DNNs) with high area and compute-efficiencies. However, the intrinsic non-idealities associated with the analog nature of computing in crossbars limits the performance of the deployed DNNs. Furthermore, DNNs are shown to be vulnerable to adversarial attacks leading to severe security threats in their large-scale deployment. Thus, finding adversarially robust DNN architectures for non-ideal crossbars is critical to the safe and secure deployment of DNNs on the edge. This work proposes a two-phase algorithm-hardware co-optimization approach called XploreNAS that searches for hardware-efficient &amp; adversarially robust neural architectures for non-ideal crossbar platforms. We use the one-shot Neural Architecture Search (NAS) approach to train a large Supernet with crossbar-awareness and sample adversarially robust Subnets therefrom, maintaining competitive hard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#25509;&#25910;&#22120;&#21487;&#20197;&#26356;&#31215;&#26497;&#22320;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#35821;&#20041;&#25512;&#29702;&#21644;&#35299;&#30721;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#22522;&#20110;GCN&#30340;&#35821;&#20041;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#24433;&#21709;&#21457;&#23556;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.07727</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#25509;&#25910;&#22120;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Semantic Communication Receiver. (arXiv:2302.07727v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#25509;&#25910;&#22120;&#21487;&#20197;&#26356;&#31215;&#26497;&#22320;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#35821;&#20041;&#25512;&#29702;&#21644;&#35299;&#30721;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#22522;&#20110;GCN&#30340;&#35821;&#20041;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#24433;&#21709;&#21457;&#23556;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35821;&#20041;&#36890;&#20449;&#24050;&#25104;&#20026;&#36890;&#20449;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#20391;&#37325;&#20110;&#21457;&#23556;&#31471;&#30340;&#35821;&#20041;&#32534;&#30721;&#65292;&#32780;&#25105;&#20204;&#35748;&#20026;&#65292;&#25509;&#25910;&#26041;&#30340;&#35821;&#20041;&#35299;&#30721;&#33021;&#21147;&#20063;&#24212;&#35813;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#25509;&#25910;&#22120;&#21487;&#20197;&#26356;&#31215;&#26497;&#22320;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#35821;&#20041;&#25512;&#29702;&#21644;&#35299;&#30721;&#65292;&#32780;&#19981;&#24433;&#21709;&#21457;&#23556;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20165;&#24433;&#21709;&#20854;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;transformer&#30340;&#30693;&#35782;&#25552;&#21462;&#22120;&#65292;&#20197;&#26597;&#25214;&#25509;&#25910;&#21040;&#30340;&#22024;&#26434;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#35821;&#20041;&#35299;&#30721;&#22120;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21333;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, with the rapid development of deep learning and natural language processing technologies, semantic communication has become a topic of great interest in the field of communication. Although existing deep learning-based semantic communication approaches have shown many advantages, they still do not make sufficient use of prior knowledge. Moreover, most existing semantic communication methods focus on the semantic encoding at the transmitter side, while we believe that the semantic decoding capability of the receiver should also be concerned. In this paper, we propose a knowledge enhanced semantic communication framework in which the receiver can more actively utilize the facts in the knowledge base for semantic reasoning and decoding, on the basis of only affecting the parameters rather than the structure of the neural networks at the transmitter side. Specifically, we design a transformer-based knowledge extractor to find relevant factual triples for the received noisy
&lt;/p&gt;</description></item><item><title>CILP&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#20223;&#30495;&#30340;&#27169;&#20223;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#21644;&#39044;&#27979;&#21644;&#20248;&#21270;&#20004;&#20010;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;VM&#35843;&#37197;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#24471;VM&#35843;&#37197;&#35745;&#21010;&#26356;&#21152;&#20934;&#30830;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05630</link><description>&lt;p&gt;
CILP: &#20113;&#35745;&#31639;&#29615;&#22659;&#21160;&#24577;&#36164;&#28304;&#35843;&#37197;&#20013;&#22522;&#20110;&#21327;&#21516;&#20223;&#30495;&#30340;&#27169;&#20223;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
CILP: Co-simulation based Imitation Learner for Dynamic Resource Provisioning in Cloud Computing Environments. (arXiv:2302.05630v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05630
&lt;/p&gt;
&lt;p&gt;
CILP&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#20223;&#30495;&#30340;&#27169;&#20223;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#21644;&#39044;&#27979;&#21644;&#20248;&#21270;&#20004;&#20010;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;VM&#35843;&#37197;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#24471;VM&#35843;&#37197;&#35745;&#21010;&#26356;&#21152;&#20934;&#30830;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#26234;&#33021;&#34394;&#25311;&#26426;&#65288;VM&#65289;&#30340;&#35843;&#37197;&#26159;&#23454;&#29616;&#25104;&#26412;&#21644;&#36164;&#28304;&#26377;&#25928;&#35745;&#31639;&#30340;&#20851;&#38190;&#12290;&#30001;&#20110;VM&#30340;&#24341;&#23548;&#20250;&#28040;&#32791;&#22823;&#37327;&#26102;&#38388;&#65292;&#23545;&#20110;&#24310;&#36831;&#20851;&#38190;&#20219;&#21153;&#26469;&#35828;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#39044;&#27979;&#26410;&#26469;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#20197;&#20027;&#21160;&#35843;&#37197;&#34394;&#25311;&#26426;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#27809;&#26377;&#20840;&#38754;&#32771;&#34385;&#25152;&#26377;&#20851;&#38190;&#26041;&#38754;&#65292;&#20363;&#22914;&#35843;&#37197;&#24320;&#38144;&#12289;&#24322;&#26500;VM&#25104;&#26412;&#20197;&#21450;&#20113;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;CILP&#65292;&#23558;VM&#35843;&#37197;&#38382;&#39064;&#20998;&#35299;&#20026;&#39044;&#27979;&#21644;&#20248;&#21270;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#20854;&#20013;&#26681;&#25454;&#39044;&#27979;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#20248;&#21270;&#35843;&#37197;&#35745;&#21010;&#12290;CILP&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#39044;&#27979;&#26410;&#26469;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#21327;&#21516;&#20223;&#30495;&#25968;&#23383;&#23402;&#29983;&#35774;&#26045;&#26469;&#35745;&#31639;QoS&#35780;&#20998;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20063;&#20316;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#22120;&#65292;&#21160;&#24577;&#20915;&#23450;&#26368;&#20339;&#30340;VM&#35843;&#37197;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent Virtual Machine (VM) provisioning is central to cost and resource efficient computation in cloud computing environments. As bootstrapping VMs is time-consuming, a key challenge for latency-critical tasks is to predict future workload demands to provision VMs proactively. However, existing AI-based solutions tend to not holistically consider all crucial aspects such as provisioning overheads, heterogeneous VM costs and Quality of Service (QoS) of the cloud system. To address this, we propose a novel method, called CILP, that formulates the VM provisioning problem as two sub-problems of prediction and optimization, where the provisioning plan is optimized based on predicted workload demands. CILP leverages a neural network as a surrogate model to predict future workload demands with a co-simulated digital-twin of the infrastructure to compute QoS scores. We extend the neural network to also act as an imitation learner that dynamically decides the optimal VM provisioning plan.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#23545;&#25239;&#26041;&#27861;&#21516;&#26102;&#35757;&#32451;&#27969;&#37327;&#39044;&#27979;&#21644;&#21306;&#20998;&#20004;&#20010;&#39046;&#22495;&#65292;&#21487;&#22312;24&#23567;&#26102;&#27969;&#37327;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#24615;&#33021;&#34920;&#29616;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.05386</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#27700;&#25991;&#29305;&#24449;&#31232;&#32570;&#22320;&#21306;&#30340;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attention-based Domain Adaptation Forecasting of Streamflow in Data-Sparse Regions. (arXiv:2302.05386v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#23545;&#25239;&#26041;&#27861;&#21516;&#26102;&#35757;&#32451;&#27969;&#37327;&#39044;&#27979;&#21644;&#21306;&#20998;&#20004;&#20010;&#39046;&#22495;&#65292;&#21487;&#22312;24&#23567;&#26102;&#27969;&#37327;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#24615;&#33021;&#34920;&#29616;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#37327;&#39044;&#27979;&#23545;&#20110;&#25351;&#23548;&#27700;&#36164;&#28304;&#31649;&#29702;&#12289;&#32531;&#35299;&#27946;&#28061;&#28798;&#23475;&#20197;&#21450;&#24320;&#21457;&#26234;&#24935;&#22522;&#30784;&#35774;&#26045;&#21644;&#27835;&#29702;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20840;&#29699;&#22320;&#21306;&#32570;&#20047;&#36275;&#22815;&#30340;&#27969;&#37327;&#35266;&#27979;&#25968;&#25454;&#20197;&#25351;&#23548;&#22522;&#20110;&#35777;&#25454;&#30340;&#31649;&#29702;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#39046;&#22495;&#30340;&#27700;&#25991;&#29305;&#24449;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#25239;&#26041;&#27861;&#21516;&#26102;&#35757;&#32451;&#27969;&#37327;&#39044;&#27979;&#21644;&#21306;&#20998;&#20004;&#20010;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;24&#23567;&#26102;&#27969;&#37327;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20132;&#21449;&#39046;&#22495;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streamflow forecasts are critical to guide water resource management, mitigate drought and flood effects, and develop climate-smart infrastructure and governance. Many global regions, however, have limited streamflow observations to guide evidence-based management strategies. In this paper, we propose an attention-based domain adaptation streamflow forecaster for data-sparse regions. Our approach leverages the hydrological characteristics of a data-rich source domain to induce effective 24hr lead-time streamflow prediction in a data-constrained target domain. Specifically, we employ a deep-learning framework leveraging domain adaptation techniques to simultaneously train streamflow predictions and discern between both domains using an adversarial method. Experiments against baseline cross-domain forecasting models show improved performance for 24hr lead-time streamflow forecasting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#20215;&#20540;&#20989;&#25968;&#26694;&#26550;EMAX&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#65292;&#20351;&#29992;UCB&#31574;&#30053;&#24341;&#23548;&#25506;&#32034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#20215;&#20540;&#20989;&#25968;&#38598;&#21512;&#20943;&#23569;&#26041;&#24046;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#27604;&#20854;&#20182;&#31574;&#30053;&#26356;&#39640;&#30340;&#23398;&#20064;&#36895;&#24230;&#12289;&#26368;&#32456;&#24615;&#33021;&#21644;&#23545;&#38750;&#38745;&#27490;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03439</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38598;&#25104;&#20215;&#20540;&#20989;&#25968;&#29992;&#20110;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning. (arXiv:2302.03439v5 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#20215;&#20540;&#20989;&#25968;&#26694;&#26550;EMAX&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#65292;&#20351;&#29992;UCB&#31574;&#30053;&#24341;&#23548;&#25506;&#32034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#20215;&#20540;&#20989;&#25968;&#38598;&#21512;&#20943;&#23569;&#26041;&#24046;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#27604;&#20854;&#20182;&#31574;&#30053;&#26356;&#39640;&#30340;&#23398;&#20064;&#36895;&#24230;&#12289;&#26368;&#32456;&#24615;&#33021;&#21644;&#23545;&#38750;&#38745;&#27490;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#26234;&#33021;&#20307;&#36827;&#34892;&#25506;&#32034;&#20197;&#23398;&#20064;&#21512;&#20316;&#30340;&#26041;&#24335;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#38543;&#26426;&#25506;&#32034;&#65292;&#22914;&#949;-&#36138;&#24515;&#31639;&#27861;&#65292;&#36825;&#22312;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#21516;&#26102;&#35757;&#32451;&#65292;MARL&#20013;&#30340;&#29615;&#22659;&#23545;&#20110;&#20219;&#19968;&#21333;&#20010;&#26234;&#33021;&#20307;&#20284;&#20046;&#26159;&#38750;&#38745;&#27490;&#30340;&#65292;&#23548;&#33268;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#30340;&#38598;&#25104;&#20215;&#20540;&#20989;&#25968;&#65288;EMAX&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#25193;&#23637;&#20219;&#20309;&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290; EMAX&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#35757;&#32451;&#20215;&#20540;&#20989;&#25968;&#38598;&#21512;&#65292;&#20197;&#35299;&#20915;&#25506;&#32034;&#21644;&#38750;&#38745;&#27490;&#24615;&#30340;&#20851;&#38190;&#25361;&#25112;&#65306;&#65288;1&#65289;&#21033;&#29992;&#38598;&#21512;&#20013;&#20215;&#20540;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#25506;&#32034;&#38656;&#35201;&#21512;&#20316;&#30340;&#29615;&#22659;&#37096;&#20998;&#30340;UCB&#31574;&#30053;&#12290; &#65288;2&#65289;&#38598;&#21512;&#20013;&#24179;&#22343;&#20540;&#20272;&#35745;&#20316;&#20026;&#30446;&#26631;&#20540;&#12290;&#36825;&#20123;&#30446;&#26631;&#25552;&#20379;&#20102;&#20302;&#26041;&#24046;&#21644;&#31283;&#23450;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#20316;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;EMAX&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#22312;&#23398;&#20064;&#36895;&#24230;&#65292;&#26368;&#32456;&#24615;&#33021;&#21644;&#23545;&#38750;&#38745;&#27490;&#24615;&#30340;&#31283;&#20581;&#24615;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as $\epsilon$-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#26102;&#30340;&#36793;&#30028;&#21160;&#24577;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#24863;&#30693;&#40065;&#26834;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36793;&#30028;&#22686;&#21152;&#36739;&#23567;&#36793;&#30028;&#36317;&#31163;&#20026;&#20248;&#20808;&#30340;&#31227;&#21160;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03015</link><description>&lt;p&gt;
&#25506;&#31350;&#21644;&#21033;&#29992;&#20915;&#31574;&#36793;&#30028;&#21160;&#24577;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness. (arXiv:2302.03015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#26102;&#30340;&#36793;&#30028;&#21160;&#24577;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#24863;&#30693;&#40065;&#26834;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36793;&#30028;&#22686;&#21152;&#36739;&#23567;&#36793;&#30028;&#36317;&#31163;&#20026;&#20248;&#20808;&#30340;&#31227;&#21160;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#30001;&#20854;&#36793;&#30028;&#19982;&#33258;&#28982;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#40065;&#26834;&#35757;&#32451;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#22686;&#21152;&#27599;&#20010;&#26131;&#21463;&#25915;&#20987;&#30340;&#28857;&#30340;&#36793;&#30028;&#36317;&#31163;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#26694;&#26550;&#26469;&#37327;&#21270;&#20915;&#31574;&#36793;&#30028;&#30456;&#23545;&#20110;&#27599;&#20010;&#20010;&#20307;&#28857;&#30340;&#31227;&#21160;&#36895;&#24230;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#20915;&#31574;&#36793;&#30028;&#22312;&#26368;&#26377;&#25928;&#30340;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#20043;&#19968;--&#23545;&#25239;&#35757;&#32451;&#19979;&#30340;&#31227;&#21160;&#36895;&#24230;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#31227;&#21160;&#34892;&#20026;&#65306;&#20915;&#31574;&#36793;&#30028;&#20174;&#26576;&#20123;&#26131;&#21463;&#25915;&#20987;&#28857;&#31227;&#24320;&#65292;&#20294;&#21516;&#26102;&#20063;&#21521;&#20854;&#20182;&#28857;&#38752;&#36817;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#23427;&#20204;&#30340;&#36793;&#30028;&#36317;&#31163;&#12290;&#20026;&#20102;&#32531;&#35299;&#20915;&#31574;&#36793;&#30028;&#30340;&#36825;&#31181;&#20914;&#31361;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#24863;&#30693;&#40065;&#26834;&#35757;&#32451;&#65288;DyART&#65289;&#65292;&#40723;&#21169;&#20915;&#31574;&#36793;&#30028;&#20197;&#22686;&#21152;&#36739;&#23567;&#36793;&#30028;&#36317;&#31163;&#20026;&#20248;&#20808;&#30340;&#31227;&#21160;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DyART&#26159;&#19968;&#20010;&#20851;&#20110;&#20915;&#31574;&#36793;&#30028;&#21160;&#24577;&#30340;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of a deep classifier can be characterized by its margins: the decision boundary's distances to natural data points. However, it is unclear whether existing robust training methods effectively increase the margin for each vulnerable point during training. To understand this, we propose a continuous-time framework for quantifying the relative speed of the decision boundary with respect to each individual point. Through visualizing the moving speed of the decision boundary under Adversarial Training, one of the most effective robust training algorithms, a surprising moving-behavior is revealed: the decision boundary moves away from some vulnerable points but simultaneously moves closer to others, decreasing their margins. To alleviate these conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust Training (DyART), which encourages the decision boundary to engage in movement that prioritizes increasing smaller margins. In contrast to prior works, DyART
&lt;/p&gt;</description></item><item><title>BLiE&#26159;&#19968;&#31181;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#21482;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;BLiE&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#25628;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.01539</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;Lipschitz&#20048;&#35266;&#31574;&#30053;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lipschitz Bandits Approach for Continuous Hyperparameter Optimization. (arXiv:2302.01539v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01539
&lt;/p&gt;
&lt;p&gt;
BLiE&#26159;&#19968;&#31181;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#21482;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;BLiE&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#25628;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26159;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;HPO&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#35201;&#20040;&#38656;&#35201;&#24378;&#30340;&#20551;&#35774;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BLiE&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;Lipschitz&#20048;&#35266;&#31574;&#30053;&#30340;HPO&#31639;&#27861;&#65292;&#23427;&#21482;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;BLiE&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#26223;&#35266;&#20197;&#33258;&#36866;&#24212;&#22320;&#25628;&#32034;&#36229;&#21442;&#25968;&#31354;&#38388;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$(i)$ BLiE&#21457;&#29616;&#20855;&#26377;$O(\frac{1}{\epsilon})^{d_z+\beta}$&#20010;&#24635;&#39044;&#31639;&#30340;$\epsilon$&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20854;&#20013;$d_z$&#21644;$\beta$&#26159;&#38382;&#39064;&#20869;&#22312;&#30340;&#65307;$(ii)$ BLiE&#20855;&#26377;&#39640;&#24230;&#21487;&#24182;&#34892;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BLiE&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;HPO&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;BLiE&#25628;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;&#19982;&#40664;&#35748;&#35843;&#24230;&#30456;&#27604;&#36739;&#65292;BLiE&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most critical problems in machine learning is HyperParameter Optimization (HPO), since choice of hyperparameters has a significant impact on final model performance. Although there are many HPO algorithms, they either have no theoretical guarantees or require strong assumptions. To this end, we introduce BLiE -- a Lipschitz-bandit-based algorithm for HPO that only assumes Lipschitz continuity of the objective function. BLiE exploits the landscape of the objective function to adaptively search over the hyperparameter space. Theoretically, we show that $(i)$ BLiE finds an $\epsilon$-optimal hyperparameter with $O \left( \frac{1}{\epsilon} \right)^{d_z + \beta}$ total budgets, where $d_z$ and $\beta$ are problem intrinsic; $(ii)$ BLiE is highly parallelizable. Empirically, we demonstrate that BLiE outperforms the state-of-the-art HPO algorithms on benchmark tasks. We also apply BLiE to search for noise schedule of diffusion models. Comparison with the default schedule shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#37327;&#23376;&#32435;&#31859;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00100</link><description>&lt;p&gt;
&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#23398;&#20064;&#29992;&#20110;&#37327;&#23376;&#32435;&#31859;&#32467;&#26500;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Reduced-Order Learning from the First Principles for Simulation of Quantum Nanostructures. (arXiv:2302.00100v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#37327;&#23376;&#32435;&#31859;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35774;&#35745;&#21644;&#20998;&#26512;&#22312;&#29983;&#29289;&#12289;&#21307;&#23398;&#12289;&#26448;&#26009;&#12289;&#30005;&#23376;/&#20809;&#23376;&#35774;&#22791;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37327;&#23376;&#32435;&#31859;&#32467;&#26500;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#32500;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;(Schr\"odinger&#26041;&#31243;)&#12290;&#22312;&#22823;&#22411;&#32435;&#31859;&#32467;&#26500;&#20013;&#65292;&#30001;&#20110;&#33258;&#30001;&#24230;(DoF)&#38750;&#24120;&#39640;&#65292;&#36827;&#34892;DNS&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;Schr\"odinger&#26041;&#31243;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#25311;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#20004;&#31181;&#37327;&#23376;&#28857;&#32467;&#26500;&#65307;&#19968;&#31181;&#22312;&#22806;&#37096;&#30005;&#22330;&#19979;&#36816;&#34892;&#65292;&#21478;&#19968;&#31181;&#21463;&#38388;&#26029;&#24615;&#36793;&#30028;&#26465;&#20214;&#19979;&#20869;&#37096;&#21183;&#33021;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#21069;&#32773;&#31867;&#20284;&#20110;&#32435;&#31859;&#30005;&#23376;&#35774;&#22791;&#30340;&#20856;&#22411;&#25805;&#20316;&#65292;&#32780;&#21518;&#32773;&#21017;&#28041;&#21450;&#21040;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#27169;&#25311;&#21644;&#35774;&#35745;&#30340;&#32435;&#31859;&#32467;&#26500;&#21644;&#26448;&#26009;&#24212;&#29992;&#12290;&#20351;&#29992;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#36825;&#20004;&#31181;&#37327;&#23376;&#28857;&#32467;&#26500;&#30340;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional direct numerical simulation (DNS) of the Schr\"odinger equation is needed for design and analysis of quantum nanostructures that offer numerous applications in biology, medicine, materials, electronic/photonic devices, etc. In large-scale nanostructures, extensive computational effort needed in DNS may become prohibitive due to the high degrees of freedom (DoF). This study employs a reduced-order learning algorithm, enabled by the first principles, for simulation of the Schr\"odinger equation to achieve high accuracy and efficiency. The proposed simulation methodology is applied to investigate two quantum-dot structures; one operates under external electric field, and the other is influenced by internal potential variation with periodic boundary conditions. The former is similar to typical operations of nanoelectronic devices, and the latter is of interest to simulation and design of nanostructures and materials, such as applications of density functional theory. Usin
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#35782;&#21035;&#30340;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#28151;&#21512;&#26041;&#27861;&#23398;&#20064;&#26356;&#22909;&#30340;&#36317;&#31163;&#34920;&#31034;&#12290;&#36890;&#36807;&#25511;&#21046;&#20960;&#20309;&#36817;&#37051;&#21644;&#27010;&#29575;&#36817;&#37051;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#30340;&#28151;&#21512;&#30456;&#20284;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.13459</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;&#24191;&#20041;&#28151;&#21512;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Generalized Hybrid Proximity Representation for Image Recognition. (arXiv:2301.13459v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#35782;&#21035;&#30340;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#28151;&#21512;&#26041;&#27861;&#23398;&#20064;&#26356;&#22909;&#30340;&#36317;&#31163;&#34920;&#31034;&#12290;&#36890;&#36807;&#25511;&#21046;&#20960;&#20309;&#36817;&#37051;&#21644;&#27010;&#29575;&#36817;&#37051;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#30340;&#28151;&#21512;&#30456;&#20284;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23398;&#20064;&#21040;&#30340;&#36317;&#31163;&#34920;&#31034;&#21487;&#29992;&#20110;&#25429;&#25417;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20851;&#31995;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#21508;&#31181;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23398;&#20064;&#20960;&#20309;&#21644;&#27010;&#29575;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#12290;&#19982;&#20808;&#21069;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#23398;&#20064;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20197;&#28151;&#21512;&#26041;&#27861;&#23398;&#20064;&#26356;&#22909;&#30340;&#36317;&#31163;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#28151;&#21512;&#24230;&#37327;&#25439;&#22833;&#65288;GHM-Loss&#65289;&#26469;&#36890;&#36807;&#25511;&#21046;&#20960;&#20309;&#36817;&#37051;&#21644;&#27010;&#29575;&#36817;&#37051;&#20043;&#38388;&#30340;&#26435;&#34913;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#30340;&#28151;&#21512;&#30456;&#20284;&#29305;&#24449;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#35777;&#26126;&#65292;&#28982;&#21518;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep metric learning techniques received attention, as the learned distance representations are useful to capture the similarity relationship among samples and further improve the performance of various of supervised or unsupervised learning tasks. We propose a novel supervised metric learning method that can learn the distance metrics in both geometric and probabilistic space for image recognition. In contrast to the previous metric learning methods which usually focus on learning the distance metrics in Euclidean space, our proposed method is able to learn better distance representation in a hybrid approach. To achieve this, we proposed a Generalized Hybrid Metric Loss (GHM-Loss) to learn the general hybrid proximity features from the image data by controlling the trade-off between geometric proximity and probabilistic proximity. To evaluate the effectiveness of our method, we first provide theoretical derivations and proofs of the proposed loss function, then we perform ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#33258;&#36866;&#24212;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#20984;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#26041;&#27861;&#20855;&#26377;&#38750;&#28176;&#36817;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11336</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#22240;&#26524;&#32467;&#26500;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Structural Learning from Time Series: A Convex Optimization Approach. (arXiv:2301.11336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#33258;&#36866;&#24212;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#20984;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#26041;&#27861;&#20855;&#26377;&#38750;&#28176;&#36817;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#26159;&#22240;&#26524;&#25512;&#29702;&#21644;&#31185;&#23398;&#21457;&#29616;&#30340;&#22522;&#30784;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#23558;&#32467;&#26500;&#23398;&#20064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;DAG&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#39640;&#24230;&#38750;&#20984;&#38382;&#39064;&#65292;&#22240;&#27492;&#24456;&#23569;&#26377;&#24037;&#20316;&#21033;&#29992;&#25104;&#29087;&#30340;&#20984;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#32447;&#24615;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#35843;&#31639;&#23376;&#21464;&#20998;&#19981;&#31561;&#24335;(VI)&#20844;&#24335;&#26041;&#20415;&#22320;&#23558;&#20854;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;VI&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32467;&#26500;&#24674;&#22797;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural learning, which aims to learn directed acyclic graphs (DAGs) from observational data, is foundational to causal reasoning and scientific discovery. Recent advancements formulate structural learning into a continuous optimization problem; however, DAG learning remains a highly non-convex problem, and there has not been much work on leveraging well-developed convex optimization techniques for causal structural learning. We fill this gap by proposing a data-adaptive linear approach for causal structural learning from time series data, which can be conveniently cast into a convex optimization problem using a recently developed monotone operator variational inequality (VI) formulation. Furthermore, we establish non-asymptotic recovery guarantee of the VI-based approach and show the superior performance of our proposed method on structure recovery over existing methods via extensive numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;Boltzmann&#23494;&#24230;&#21464;&#24418;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#25554;&#20540;&#33021;&#37327;&#20989;&#25968;&#31561;&#23454;&#29616;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#65292;&#28982;&#21518;&#25214;&#21040;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#34920;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#27604;KL-&#21453;&#25955;&#24230;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.07388</link><description>&lt;p&gt;
&#23398;&#20064;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning Deformation Trajectories of Boltzmann Densities. (arXiv:2301.07388v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;Boltzmann&#23494;&#24230;&#21464;&#24418;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#25554;&#20540;&#33021;&#37327;&#20989;&#25968;&#31561;&#23454;&#29616;Boltzmann&#23494;&#24230;&#30340;&#21464;&#24418;&#65292;&#28982;&#21518;&#25214;&#21040;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#34920;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#27604;KL-&#21453;&#25955;&#24230;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26679;&#26412;&#20294;&#23384;&#22312;&#33021;&#37327;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33021;&#37327;&#20989;&#25968;$f_1$&#21644;&#24191;&#20041;&#39640;&#26031;&#20989;&#25968;$f_0$&#20043;&#38388;&#30340;&#39044;&#23450;&#25110;&#23398;&#20064;&#25554;&#20540;$f_t$&#12290;&#33021;&#37327;&#20989;&#25968;&#30340;&#25554;&#20540;&#24341;&#36215;Boltzmann&#23494;&#24230;$p_t\propto e^{-f_t}$&#30340;&#25554;&#20540;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#27839;&#30528;&#26063;$p_t$&#30340;&#26102;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;$V_t$&#65292;&#23558;&#26679;&#26412;&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#12290;&#23558;&#26679;&#26412;&#27839;&#30528;&#26063;$p_t$&#20174;&#19968;&#20010;&#20998;&#24067;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26465;&#20214;&#21487;&#20197;&#36716;&#21270;&#20026;$V_t$&#21644;$f_t$&#20043;&#38388;&#30340;PDE&#65292;&#25105;&#20204;&#20248;&#21270;$V_t$&#21644;$f_t$&#20197;&#28385;&#36275;&#27492;PDE&#12290;&#25105;&#20204;&#22312;&#39640;&#26031;&#28151;&#21512;&#21644;&#21452;&#20117;&#21183;&#30340;&#37327;&#23376;&#21147;&#23398;&#31890;&#23376;&#30340;Boltzmann&#23494;&#24230;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;KL-&#21453;&#25955;&#24230;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ can be translated to a PDE between $V_t$ and $f_t$ and we optimize $V_t$ and $f_t$ to satisfy this PDE. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.07285</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Sparse Regularizer. (arXiv:2301.07285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07285
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#22522;&#20110; $L_{p}$-norm &#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#21482;&#32771;&#34385;&#27169;&#22411;&#20013;&#21508;&#26435;&#37325;&#20540;&#30340;&#24230;&#37327;&#19981;&#21516;&#65292;&#36825;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#12290;&#35813;&#26041;&#27861;&#30340;&#21152;&#20837;&#39033;&#21487;&#20197;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21487;&#24494;&#20998;&#65292;&#31616;&#21333;&#24555;&#36895;&#35745;&#31639;&#65292;&#19982;&#23610;&#24230;&#26080;&#20851;&#65292;&#20165;&#38656;&#35201;&#24494;&#23567;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#23481;&#26131;&#24182;&#34892;&#21270;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#31934;&#24230;&#27700;&#24179;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
$L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and $L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniques such as weight decay and group LASSO compute a quantity which de pends on model weights considered in isolation from one another. This paper describes a novel regularizer which is not based on an $L_{p}$-norm. In contrast with $L_{p}$-norm-based regularization, this regularizer is concerned with the spatial arrangement of weights within a weight matrix. This regularizer is an additive term for the loss function and is differentiable, simple and fast to compute, scale-invariant, requires a trivial amount of additional memory, and can easily be parallelized. Empirically this method yields approximately a one order-of-magnitude improvement in the number of nonzero model parameters at a given level of accuracy.
&lt;/p&gt;</description></item><item><title>GLIGEN&#26159;&#19968;&#31181;&#24320;&#25918;&#24335;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#27880;&#20837;&#36830;&#32467;&#20449;&#24687;&#65292;&#33021;&#22815;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2301.07093</link><description>&lt;p&gt;
GLIGEN&#65306;&#24320;&#25918;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GLIGEN: Open-Set Grounded Text-to-Image Generation. (arXiv:2301.07093v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07093
&lt;/p&gt;
&lt;p&gt;
GLIGEN&#26159;&#19968;&#31181;&#24320;&#25918;&#24335;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#27880;&#20837;&#36830;&#32467;&#20449;&#24687;&#65292;&#33021;&#22815;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#29366;&#26159;&#20165;&#20351;&#29992;&#25991;&#26412;&#36755;&#20837;&#65292;&#36825;&#21487;&#33021;&#20250;&#38480;&#21046;&#21487;&#25511;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLIGEN&#65292;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#20381;&#36182;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#36755;&#20837;&#12290;&#20026;&#20102;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24191;&#27867;&#27010;&#24565;&#30693;&#35782;&#65292;&#25105;&#20204;&#20923;&#32467;&#25152;&#26377;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#23558;&#36830;&#32467;&#20449;&#24687;&#27880;&#20837;&#21040;&#26032;&#30340;&#21487;&#35757;&#32451;&#23618;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#24320;&#25918;&#24335;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#19988;&#36830;&#32467;&#33021;&#21147;&#22312;&#26032;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#27010;&#24565;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#26222;&#36866;&#24615;&#12290;GLIGEN&#22312;COCO&#21644;LVIS&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#20219;&#21153;&#8212;&#8212;&#39046;&#22495;&#25193;&#23637;&#65292;&#26469;&#27880;&#20837;&#26032;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29616;&#26377;&#30340;&#32467;&#26500;&#21644;&#30693;&#35782;&#12290;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#20855;&#26377;&#28155;&#21152;&#22810;&#20010;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.05225</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#39046;&#22495;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Domain Expansion of Image Generators. (arXiv:2301.05225v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05225
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#20219;&#21153;&#8212;&#8212;&#39046;&#22495;&#25193;&#23637;&#65292;&#26469;&#27880;&#20837;&#26032;&#30340;&#27010;&#24565;&#21040;&#19968;&#20010;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29616;&#26377;&#30340;&#32467;&#26500;&#21644;&#30693;&#35782;&#12290;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#20855;&#26377;&#28155;&#21152;&#22810;&#20010;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27880;&#20837;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29616;&#26377;&#30340;&#32467;&#26500;&#21644;&#30693;&#35782;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#39046;&#22495;&#25193;&#23637;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#21644;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25193;&#23637;&#29983;&#25104;&#22120;&#20197;&#20849;&#21516;&#27169;&#25311;&#25152;&#26377;&#39046;&#22495;&#65292;&#26032;&#26087;&#39046;&#22495;&#21644;&#35856;&#22320;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#22120;&#21253;&#21547;&#26377;&#24847;&#20041;&#30340;&#65292;&#24182;&#32463;&#36807;&#20102;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#33021;&#21542;&#22312;&#26368;&#23567;&#31243;&#24230;&#19978;&#25200;&#21160;&#36825;&#20010;&#36763;&#33510;&#33719;&#24471;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#34920;&#31034;&#26032;&#39046;&#22495;&#65311;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#28508;&#22312;&#31354;&#38388;&#25552;&#20379;&#20102;&#26410;&#20351;&#29992;&#30340;&#8220;&#20241;&#30496;&#8221;&#26041;&#21521;&#65292;&#19981;&#24433;&#21709;&#36755;&#20986;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#65306;&#36890;&#36807;&#8220;&#37325;&#23450;&#21521;&#8221;&#36825;&#20123;&#26041;&#21521;&#65292;&#25105;&#20204;&#21487;&#20197;&#34920;&#31034;&#26032;&#30340;&#39046;&#22495;&#32780;&#19981;&#20250;&#25200;&#20081;&#21407;&#22987;&#34920;&#31034;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#26377;&#33021;&#21147;&#28155;&#21152;&#20960;&#20010;&#8212;&#8212;&#29978;&#33267;&#25968;&#30334;&#20010;&#8212;&#8212;&#26032;&#39046;&#22495;&#65281;&#20351;&#29992;&#25105;&#20204;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#19968;&#20010;&#8220;&#25193;&#23637;&#8221;&#27169;&#22411;&#21487;&#20197;&#21462;&#20195;&#35768;&#22810;&#29305;&#23450;&#39046;&#22495;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can one inject new concepts into an already trained generative model, while respecting its existing structure and knowledge? We propose a new task - domain expansion - to address this. Given a pretrained generator and novel (but related) domains, we expand the generator to jointly model all domains, old and new, harmoniously. First, we note the generator contains a meaningful, pretrained latent space. Is it possible to minimally perturb this hard-earned representation, while maximally representing the new domains? Interestingly, we find that the latent space offers unused, "dormant" directions, which do not affect the output. This provides an opportunity: By "repurposing" these directions, we can represent new domains without perturbing the original representation. In fact, we find that pretrained generators have the capacity to add several - even hundreds - of new domains! Using our expansion method, one "expanded" model can supersede numerous domain-specific models, without expanding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#30740;&#31350;&#20102;&#22312;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#36807;&#31243;&#20013;&#22914;&#20309;&#35782;&#21035;&#20986;&#24178;&#20928;&#26631;&#31614;&#26679;&#26412;&#65292;&#21457;&#29616;&#27599;&#20010;&#23454;&#20363;&#26377;&#33267;&#23569; $2C-1$ &#20010;&#26377;&#22122;&#22768;&#26631;&#31614;&#26102;&#65292;&#35813;&#38382;&#39064;&#25165;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20010;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#22122;&#22768;&#26631;&#31614;&#20998;&#24067;&#33258;&#21160;&#29983;&#25104;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#20197;&#25552;&#39640;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2301.01405</link><description>&lt;p&gt;
&#38754;&#21521;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#21487;&#35782;&#21035;&#24615;&#65306;&#22810;&#39033;&#24335;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Identifiability in Noisy Label Learning: A Multinomial Mixture Approach. (arXiv:2301.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#30740;&#31350;&#20102;&#22312;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#36807;&#31243;&#20013;&#22914;&#20309;&#35782;&#21035;&#20986;&#24178;&#20928;&#26631;&#31614;&#26679;&#26412;&#65292;&#21457;&#29616;&#27599;&#20010;&#23454;&#20363;&#26377;&#33267;&#23569; $2C-1$ &#20010;&#26377;&#22122;&#22768;&#26631;&#31614;&#26102;&#65292;&#35813;&#38382;&#39064;&#25165;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20010;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#22122;&#22768;&#26631;&#31614;&#20998;&#24067;&#33258;&#21160;&#29983;&#25104;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#20197;&#25552;&#39640;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#36827;&#34892;&#23398;&#20064;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26368;&#26377;&#21069;&#36884;&#30340;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#24178;&#20928;&#26631;&#31614;&#26679;&#26412;&#12290;&#36825;&#31181;&#35782;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#20551;&#23450;&#27599;&#20010;&#23454;&#20363;&#21482;&#26377;&#19968;&#20010;&#26377;&#22122;&#22768;&#26631;&#31614;&#65292;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#27809;&#26377;&#38468;&#21152;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29702;&#35770;&#19978;&#26080;&#27861;&#20272;&#35745;&#20986;&#24178;&#20928;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#27491;&#24335;&#35843;&#26597;&#36825;&#20010;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#20197;&#30830;&#23450;&#20351;&#38382;&#39064;&#21487;&#35782;&#21035;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#27599;&#20010;&#23454;&#20363;&#26377;&#33267;&#23569; $2C-1$ &#20010;&#26377;&#22122;&#22768;&#26631;&#31614;&#65292;&#20854;&#20013; C &#26159;&#31867;&#30340;&#25968;&#37327;&#65292;&#21017;&#35813;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#23601;&#21464;&#24471;&#21487;&#35782;&#21035;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20010;&#35201;&#27714;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#27599;&#20010;&#23454;&#20363;&#39069;&#22806;&#30340; $2C-2$ &#25163;&#21160;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22122;&#22768;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#21160;&#29983;&#25104;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#25552;&#39640;&#20102;&#21487;&#35782;&#21035;&#24615;&#65292;&#20351;&#24471;&#21487;&#20197;&#26080;&#38656;&#20219;&#20309;&#20854;&#20182;&#20551;&#35774;&#26469;&#20272;&#35745;&#24178;&#20928;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#21644;&#24212;&#29992;&#31243;&#24207;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels (LNL) plays a crucial role in deep learning. The most promising LNL methods rely on identifying clean-label samples from a dataset with noisy annotations. Such an identification is challenging because the conventional LNL problem, which assumes a single noisy label per instance, is non-identifiable, i.e., clean labels cannot be estimated theoretically without additional heuristics. In this paper, we aim to formally investigate this identifiability issue using multinomial mixture models to determine the constraints that make the problem identifiable. Specifically, we discover that the LNL problem becomes identifiable if there are at least $2C - 1$ noisy labels per instance, where $C$ is the number of classes. To meet this requirement without relying on additional $2C - 2$ manual annotations per instance, we propose a method that automatically generates additional noisy labels by estimating the noisy label distribution based on nearest neighbours. These additio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#33021;&#22815;&#20998;&#21106;25&#31181;&#22120;&#23448;&#21644;6&#31181;&#32959;&#30244;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.00785</link><description>&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#22120;&#23448;&#20998;&#21106;&#21644;&#32959;&#30244;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection. (arXiv:2301.00785v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#33021;&#22815;&#20998;&#21106;25&#31181;&#22120;&#23448;&#21644;6&#31181;&#32959;&#30244;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#22312;&#33258;&#21160;&#21270;&#22120;&#23448;&#20998;&#21106;&#21644;&#32959;&#30244;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#19988;&#37096;&#20998;&#26631;&#27880;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#31867;&#22411;&#32959;&#30244;&#30340;&#26377;&#38480;&#25506;&#31350;&#65292;&#23548;&#33268;&#24471;&#21040;&#30340;&#27169;&#22411;&#36890;&#24120;&#38480;&#20110;&#20998;&#21106;&#29305;&#23450;&#30340;&#22120;&#23448;/&#32959;&#30244;&#65292;&#24182;&#24573;&#30053;&#35299;&#21078;&#32467;&#26500;&#30340;&#35821;&#20041;&#65292;&#20063;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#39537;&#21160;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#23558;&#20174;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451; &#65288;CLIP&#65289;&#20013;&#23398;&#20064;&#21040;&#30340;&#25991;&#26412;&#23884;&#20837;&#32467;&#21512;&#21040;&#20998;&#21106;&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#22522;&#20110;CLIP&#30340;&#26631;&#31614;&#32534;&#30721;&#25429;&#25417;&#20102;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#32467;&#26500;&#21270;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#20998;&#21106;25&#20010;&#22120;&#23448;&#21644;6&#31181;&#31867;&#22411;&#30340;&#32959;&#30244;&#12290;&#35813;&#27169;&#22411;&#30001;14&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#20351;&#29992;3410&#20010;CT&#25195;&#25551;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26469;&#33258;3&#20010;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;6162&#20010;&#22806;&#37096;CT&#25195;&#25551;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#30340;&#22269;&#38469;&#20934;&#30830;&#24615;&#22522;&#20934;&#27979;&#35797;&#65288;MIoU&#65289;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#25628;&#32034;&#38382;&#39064;&#65292;&#23558;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#34920;&#31034;&#20013;&#36827;&#34892;&#36817;&#37051;&#25628;&#32034;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;Minecraft&#29615;&#22659;&#20013;&#22797;&#21046;&#20986;&#26377;&#24847;&#20041;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#21576;&#29616;&#31867;&#20154;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2212.13326</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#39044;&#35757;&#32451;&#28508;&#31354;&#38388;&#25628;&#32034;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Behavioral Cloning via Search in Video PreTraining Latent Space. (arXiv:2212.13326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#25628;&#32034;&#38382;&#39064;&#65292;&#23558;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#34920;&#31034;&#20013;&#36827;&#34892;&#36817;&#37051;&#25628;&#32034;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;Minecraft&#29615;&#22659;&#20013;&#22797;&#21046;&#20986;&#26377;&#24847;&#20041;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#21576;&#29616;&#31867;&#20154;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#33021;&#22815;&#35299;&#20915;&#20687;Minecraft&#36825;&#26679;&#29615;&#22659;&#20013;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#22522;&#20110;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#25628;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20174;&#31867;&#20284;&#20110;&#22270;&#20687;-&#21160;&#20316;&#23545;&#30340;&#28436;&#31034;&#36712;&#36857;&#20013;&#22797;&#21046;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#34920;&#31034;&#20013;&#23545;BASALT MineRL&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36817;&#37051;&#25628;&#32034;&#12290;&#21482;&#35201;&#20195;&#29702;&#21644;&#25968;&#25454;&#38598;&#20013;&#36873;&#23450;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#29366;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#20250;&#20998;&#25955;&#65292;&#20195;&#29702;&#23601;&#20250;&#22797;&#21046;&#19987;&#23478;&#36712;&#36857;&#20013;&#30340;&#21160;&#20316;&#12290;&#28982;&#21518;&#37325;&#26032;&#36827;&#34892;&#36817;&#37051;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28436;&#31034;&#36712;&#36857;&#65292;&#24182;&#22312;Minecraft&#29615;&#22659;&#20013;&#23637;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our aim is to build autonomous agents that can solve tasks in environments like Minecraft. To do so, we used an imitation learning-based approach. We formulate our control problem as a search problem over a dataset of experts' demonstrations, where the agent copies actions from a similar demonstration trajectory of image-action pairs. We perform a proximity search over the BASALT MineRL-dataset in the latent representation of a Video PreTraining model. The agent copies the actions from the expert trajectory as long as the distance between the state representations of the agent and the selected expert trajectory from the dataset do not diverge. Then the proximity search is repeated. Our approach can effectively recover meaningful demonstration trajectories and show human-like behavior of an agent in the Minecraft environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#20540;&#24471;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.09006</link><description>&lt;p&gt;
&#19968;&#31687;&#38024;&#23545;&#22522;&#20110;&#35821;&#38899;&#30340;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Review of Speech-centric Trustworthy Machine Learning: Privacy, Safety, and Fairness. (arXiv:2212.09006v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#20540;&#24471;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24050;&#32463;&#22312;&#20132;&#36890;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#22269;&#38450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#28145;&#21051;&#25913;&#21464;&#20102;&#20154;&#20204;&#30340;&#29983;&#27963;&#12289;&#24037;&#20316;&#21644;&#20114;&#21160;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#35821;&#38899;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#38656;&#35201;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#25165;&#33021;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38544;&#31169;&#27844;&#38706;&#12289;&#27495;&#35270;&#24615;&#33021;&#21644;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#37117;&#24050;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#21644;&#39118;&#38505;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26159;&#21487;&#20449;&#36182;&#30340;&#65292;&#29305;&#21035;&#26159;&#31169;&#23494;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#19982;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#30456;&#20851;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#20027;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#38500;&#20102;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#24635;&#32467;&#25253;&#21578;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#28608;&#21457;&#30740;&#31350;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-centric machine learning systems have revolutionized many leading domains ranging from transportation and healthcare to education and defense, profoundly changing how people live, work, and interact with each other. However, recent studies have demonstrated that many speech-centric ML systems may need to be considered more trustworthy for broader deployment. Specifically, concerns over privacy breaches, discriminating performance, and vulnerability to adversarial attacks have all been discovered in ML research fields. In order to address the above challenges and risks, a significant number of efforts have been made to ensure these ML systems are trustworthy, especially private, safe, and fair. In this paper, we conduct the first comprehensive survey on speech-centric trustworthy ML topics related to privacy, safety, and fairness. In addition to serving as a summary report for the research community, we point out several promising future research directions to inspire the researc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#26089;&#26399;&#27169;&#22411;&#21644;&#21442;&#25968;&#25200;&#21160;&#65292;&#20197;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#26041;&#24335;&#20248;&#21270;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2212.06152</link><description>&lt;p&gt;
&#27169;&#22411;&#22686;&#24378;&#19979;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Dataset Distillation via Model Augmentation. (arXiv:2212.06152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#26089;&#26399;&#27169;&#22411;&#21644;&#21442;&#25968;&#25200;&#21160;&#65292;&#20197;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#26041;&#24335;&#20248;&#21270;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#22823;&#22411;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#20135;&#29983;&#26356;&#23567;&#20294;&#39640;&#25928;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#21305;&#37197;&#30340;DD&#26041;&#27861;&#36798;&#21040;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#65307;&#20294;&#26159;&#65292;&#23427;&#20204;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#22240;&#20026;&#20182;&#20204;&#38656;&#35201;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#20013;&#19981;&#26029;&#20248;&#21270;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#26089;&#26399;&#27169;&#22411;&#21644;&#21442;&#25968;&#25200;&#21160;&#26469;&#23398;&#20064;&#20855;&#26377;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#20449;&#24687;&#21512;&#25104;&#38598;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they require continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20x speedup and comparable performance on par with state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#39034;&#24207;&#33258;&#21160;&#32534;&#30721;&#22120;&#20174;&#31070;&#32463;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#65292;&#21457;&#29616;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20026;&#22522;&#30784;&#30340; SAES &#22312;&#20934;&#30830;&#29575;&#21644;&#32500;&#24230;&#26041;&#38754;&#20248;&#20110;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340; SAES&#12290;</title><link>http://arxiv.org/abs/2212.03771</link><description>&lt;p&gt;
&#34920;&#29616;&#21147;&#26550;&#26500;&#22686;&#24378;&#22522;&#20110;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#20154;&#32676;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expressive architectures enhance interpretability of dynamics-based neural population models. (arXiv:2212.03771v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03771
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#39034;&#24207;&#33258;&#21160;&#32534;&#30721;&#22120;&#20174;&#31070;&#32463;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#65292;&#21457;&#29616;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20026;&#22522;&#30784;&#30340; SAES &#22312;&#20934;&#30830;&#29575;&#21644;&#32500;&#24230;&#26041;&#38754;&#20248;&#20110;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340; SAES&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#35760;&#24405;&#30340;&#31070;&#32463;&#27963;&#21160;&#20013;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20026;&#35782;&#21035;&#21644;&#35299;&#37322;&#29983;&#29289;&#35745;&#31639;&#20013;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#20165;&#20973;&#31070;&#32463;&#26041;&#24046;&#26080;&#27861;&#21807;&#19968;&#30830;&#23450;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#24212;&#35813;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#21644;&#20302;&#32500;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39034;&#24207;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAEs&#65289;&#22312;&#20174;&#27169;&#25311;&#30340;&#31070;&#32463;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37319;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20026;&#21160;&#21147;&#23398;&#22522;&#30784;&#30340;SAEs&#26080;&#27861;&#22312;&#30495;&#23454;&#30340;&#28508;&#22312;&#29366;&#24577;&#32500;&#24230;&#19978;&#25512;&#26029;&#20986;&#20934;&#30830;&#30340;&#21457;&#23556;&#29575;&#65292;&#24182;&#19988;&#26356;&#22823;&#30340;RNNs&#20381;&#36182;&#20110;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#20026;&#22522;&#30784;&#30340;SAEs&#22312;&#30495;&#23454;&#30340;&#28508;&#22312;&#29366;&#24577;&#32500;&#24230;&#19978;&#25512;&#26029;&#20986;&#20934;&#30830;&#30340;&#29575;&#65292;&#21516;&#26102;&#36824;&#24674;&#22797;&#20102;&#28508;&#22312;&#36712;&#36857;&#21644;&#22266;&#23450;&#28857;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate firing rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#20197;&#35299;&#37322;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#35813;&#26041;&#38754;&#30340;&#28431;&#27934;&#12290;&#20854;&#20013;&#19968;&#31181;&#38024;&#23545;&#27599;&#20010;&#34955;&#23376;&#36827;&#34892;&#23450;&#21046;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24433;&#21709;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#34955;&#23376;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.17071</link><description>&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#23545;&#25239;&#25200;&#21160;&#30340;&#28431;&#27934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpreting Vulnerabilities of Multi-Instance Learning to Adversarial Perturbations. (arXiv:2211.17071v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#20197;&#35299;&#37322;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#35813;&#26041;&#38754;&#30340;&#28431;&#27934;&#12290;&#20854;&#20013;&#19968;&#31181;&#38024;&#23545;&#27599;&#20010;&#34955;&#23376;&#36827;&#34892;&#23450;&#21046;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24433;&#21709;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#34955;&#23376;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26159;&#26368;&#36817;&#19968;&#31181;&#38750;&#24120;&#26377;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#12289;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#12289;&#25991;&#26412;&#20998;&#31867;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#37117;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#25915;&#20987;&#12290;&#30001;&#20110;MIL&#26159;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#21482;&#26377;&#19968;&#20010;&#34955;&#23376;&#20013;&#30340;&#23454;&#20363;&#38598;&#21512;&#26159;&#21487;&#35775;&#38382;&#30340;&#65292;&#32780;&#27599;&#20010;&#23454;&#20363;&#37117;&#26080;&#27861;&#33719;&#21462;&#65292;&#23545;&#25239;&#24615;&#25200;&#21160;&#21487;&#33021;&#26159;&#33268;&#21629;&#30340;&#12290;&#20026;&#20102;&#35299;&#37322;MIL&#26041;&#27861;&#30340;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#31181;&#21487;&#20197;&#38024;&#23545;&#27599;&#20010;&#34955;&#23376;&#36827;&#34892;&#23450;&#21046;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24433;&#21709;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#34955;&#23376;&#65292;&#22240;&#27492;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#27450;&#39575;&#26368;&#20808;&#36827;&#30340;MIL&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#20998;&#26512;&#20102;&#27969;&#34892;&#30340;MIL&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Instance Learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instances, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerability of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Through simulations, we have also shown the effectiveness of the proposed algorithms to fool the state-of-the-art (SOTA) MIL methods. Finally, we have
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#20363;&#27169;&#24335;&#32452;&#21512;&#22120;&#23454;&#29616;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#21046;&#26089;&#26399;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#23618;&#20013;&#30340;&#19968;&#23567;&#32452;&#26435;&#37325;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#23398;&#20064;&#27169;&#24335;&#32452;&#21512;&#35268;&#21017;&#20197;&#23454;&#29616;&#23454;&#20363;&#30340;&#20849;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.13223</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#27169;&#24335;&#32452;&#21512;&#22120;&#23454;&#29616;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalizable Implicit Neural Representations via Instance Pattern Composers. (arXiv:2211.13223v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#20363;&#27169;&#24335;&#32452;&#21512;&#22120;&#23454;&#29616;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#21046;&#26089;&#26399;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#23618;&#20013;&#30340;&#19968;&#23567;&#32452;&#26435;&#37325;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#23398;&#20064;&#27169;&#24335;&#32452;&#21512;&#35268;&#21017;&#20197;&#23454;&#29616;&#23454;&#20363;&#30340;&#20849;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#22522;&#20110;&#22352;&#26631;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(INR MLP)&#26469;&#35828;&#65292;&#22312;&#23398;&#20064;&#36328;&#25968;&#25454;&#23454;&#20363;&#30340;&#20849;&#21516;&#34920;&#31034;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#23454;&#20363;&#26041;&#38754;&#20173;&#28982;&#24456;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36890;&#29992;INR&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#22522;&#20110;&#22352;&#26631;&#30340;MLP&#20165;&#36890;&#36807;&#35843;&#21046;&#26089;&#26399;MLP&#23618;&#20013;&#30340;&#19968;&#23567;&#32452;&#26435;&#37325;&#21363;&#23454;&#20363;&#27169;&#24335;&#32452;&#21512;&#22120;&#65292;&#21363;&#33021;&#22815;&#34920;&#31034;&#22797;&#26434;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#32780;&#20854;&#20313;&#30340;MLP&#26435;&#37325;&#21017;&#23398;&#20064;&#36866;&#29992;&#20110;&#23454;&#20363;&#20849;&#21516;&#34920;&#31034;&#30340;&#27169;&#24335;&#32452;&#25104;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;INR&#26694;&#26550;&#19982;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#21644;&#36229;&#32593;&#32476;&#30456;&#20860;&#23481;&#65292;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#30475;&#19981;&#35265;&#23454;&#20363;&#30340;&#35843;&#21046;&#26435;&#37325;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#39046;&#22495;(&#20363;&#22914;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;3D&#29289;&#20307;)&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#32780;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26435;&#37325;&#35843;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in implicit neural representations (INRs), it remains challenging for a coordinate-based multi-layer perceptron (MLP) of INRs to learn a common representation across data instances and generalize it for unseen instances. In this work, we introduce a simple yet effective framework for generalizable INRs that enables a coordinate-based MLP to represent complex data instances by modulating only a small set of weights in an early MLP layer as an instance pattern composer; the remaining MLP weights learn pattern composition rules for common representations across instances. Our generalizable INR framework is fully compatible with existing meta-learning and hypernetworks in learning to predict the modulated weight for unseen instances. Extensive experiments demonstrate that our method achieves high performance on a wide range of domains such as an audio, image, and 3D object, while the ablation study validates our weight modulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#22810;&#31181;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23376;&#32676;&#20581;&#22766;&#24615;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#20855;&#26377;&#26497;&#24378;&#30340;&#20581;&#22766;&#24615;&#65292;&#21487;&#33021;&#27604;&#20854;&#20182;&#31283;&#20581;&#25110;&#32676;&#20307;&#20844;&#24179;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.12703</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23376;&#32676;&#20581;&#22766;&#24615;&#65306;&#19968;&#20010;&#23454;&#35777;&#22522;&#32447;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation. (arXiv:2211.12703v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#22810;&#31181;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23376;&#32676;&#20581;&#22766;&#24615;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#20855;&#26377;&#26497;&#24378;&#30340;&#20581;&#22766;&#24615;&#65292;&#21487;&#33021;&#27604;&#20854;&#20182;&#31283;&#20581;&#25110;&#32676;&#20307;&#20844;&#24179;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35768;&#22810;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#23545;&#23427;&#20204;&#30340;&#23376;&#32676;&#20581;&#22766;&#24615;&#30340;&#20840;&#38754;&#23454;&#35777;&#35780;&#20272;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;&#22522;&#20110;&#29366;&#24577;-of-the-art&#26641;&#27169;&#22411;&#30340;&#22522;&#20934;&#32447;&#27604;&#36739;&#65292;&#20197;&#34920;&#26684;&#25968;&#25454;&#20026;&#32972;&#26223;&#65292;&#20171;&#32461;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#22810;&#31181;&#20844;&#24179;&#21644;&#31283;&#20581;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#35777;&#27604;&#36739;&#12290;&#36890;&#36807;&#22312;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36229;&#36807;340,000&#20010;&#27169;&#22411;&#37197;&#32622;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#20855;&#26377;&#26497;&#24378;&#30340;&#23376;&#32676;&#20581;&#22766;&#24615;&#65292;&#21363;&#20351;&#19982;&#31283;&#20581;&#24615;&#21644;&#20844;&#24179;&#24615;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#26368;&#20339;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#65292;&#32780;&#31283;&#20581;&#25110;&#32676;&#20307;&#20844;&#24179;&#30340;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#25351;&#26631;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have proposed many methods for fair and robust machine learning, but comprehensive empirical evaluation of their subgroup robustness is lacking. In this work, we address this gap in the context of tabular data, where sensitive subgroups are clearly-defined, real-world fairness problems abound, and prior works often do not compare to state-of-the-art tree-based models as baselines. We conduct an empirical comparison of several previously-proposed methods for fair and robust learning alongside state-of-the-art tree-based methods and other baselines. Via experiments with more than $340{,}000$ model configurations on eight datasets, we show that tree-based methods have strong subgroup robustness, even when compared to robustness- and fairness-enhancing methods. Moreover, the best tree-based models tend to show good performance over a range of metrics, while robust or group-fair models can show brittleness, with significant performance differences across different metrics for a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.11656</link><description>&lt;p&gt;
&#39034;&#24207;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65306;&#32852;&#37030;&#20248;&#21270;&#20013;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#65288;MU&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#20174;&#35757;&#32451;&#36807;&#31243;&#20013;&#21024;&#38500;&#32473;&#23450;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#32852;&#37030;&#28040;&#38500;&#65288;FU&#65289;&#26159;&#23558;MU&#25193;&#23637;&#21040;&#20174;&#32852;&#21512;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#36129;&#29486;&#12290;&#24403;&#21069;&#30340;FU&#26041;&#27861;&#36890;&#24120;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;&#28040;&#38500;&#25928;&#26524;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#21512;&#29702;&#30340;&#29702;&#35770;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;(IFU)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;FU&#26041;&#27861;&#12290;&#22312;&#25509;&#25910;&#21040;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#28040;&#38500;&#35831;&#27714;&#21518;&#65292;IFU&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#26426;&#21046;&#30830;&#23450;&#20102;&#37325;&#26032;&#21021;&#22987;&#21270;FL&#25152;&#38656;&#30340;&#26368;&#20339;FL&#36845;&#20195;&#65292;&#21487;&#20197;&#33719;&#24471;&#28040;&#38500;&#20445;&#35777;&#12290;IFU&#30340;&#29702;&#35770;&#20063;&#21487;&#20197;&#25193;&#23637;&#20197;&#35299;&#20915;&#39034;&#24207;&#28040;&#38500;&#35831;&#27714;&#12290;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#26412;&#37325;&#26032;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#30456;&#27604;&#65292;IFU&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#28040;&#38500;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#35745;&#31639;&#30340;&#36951;&#25022;&#19982;&#25152;&#36873;&#31574;&#30053;&#30340;&#38381;&#29615;&#31995;&#32479;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32467;&#26524;&#21457;&#29616;&#32447;&#24615;&#36951;&#25022;&#26263;&#31034;&#30528;&#28176;&#36827;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#26377;&#30028;&#36755;&#20837;&#30340;&#26377;&#30028;&#29366;&#24577;&#31283;&#23450;&#24615;&#21644;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#30340;&#21487;&#21152;&#24615;&#20063;&#24847;&#21619;&#30528;&#32447;&#24615;&#36951;&#25022;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2211.07411</link><description>&lt;p&gt;
&#25253;&#38169;&#23545;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of Regret on Stability of Linear Dynamical Systems. (arXiv:2211.07411v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#35745;&#31639;&#30340;&#36951;&#25022;&#19982;&#25152;&#36873;&#31574;&#30053;&#30340;&#38381;&#29615;&#31995;&#32479;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32467;&#26524;&#21457;&#29616;&#32447;&#24615;&#36951;&#25022;&#26263;&#31034;&#30528;&#28176;&#36827;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#26377;&#30028;&#36755;&#20837;&#30340;&#26377;&#30028;&#29366;&#24577;&#31283;&#23450;&#24615;&#21644;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#30340;&#21487;&#21152;&#24615;&#20063;&#24847;&#21619;&#30528;&#32447;&#24615;&#36951;&#25022;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20248;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#36817;&#26399;&#30340;&#22312;&#32447;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#20195;&#29702;&#26426;&#22120;&#20154;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#38480;&#21046;&#19979;&#36827;&#34892;&#20915;&#31574;&#26159;&#24456;&#24120;&#35265;&#30340;&#12290;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20195;&#29702;&#26426;&#22120;&#20154;&#30340;&#20915;&#31574;&#36136;&#37327;&#36890;&#24120;&#34987;&#36951;&#25022;&#27010;&#24565;&#25152;&#24230;&#37327;&#65292;&#27604;&#36739;&#25152;&#36873;&#31574;&#30053;&#21644;&#26368;&#20248;&#35299;&#21518;&#24724;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#36951;&#25022;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#20294;&#24403;&#32771;&#34385;&#21040;&#21160;&#24577;&#31995;&#32479;&#26102;&#65292;&#35780;&#20272;&#25152;&#36873;&#31574;&#30053;&#30340;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#32447;&#24615;&#29366;&#24577;&#21453;&#39304;&#31574;&#30053;&#21644;&#21463;&#21040;&#23545;&#25239;&#24615;&#24178;&#25200;&#30340;&#32447;&#24615;&#31995;&#32479;&#65292;&#32447;&#24615;&#36951;&#25022;&#27010;&#24565;&#22312;&#26102;&#21464;&#21644;&#26102;&#19981;&#21464;&#35774;&#32622;&#19979;&#37117;&#26263;&#31034;&#28176;&#36827;&#31283;&#23450;&#24615;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#26377;&#30028;&#36755;&#20837;&#30340;&#26377;&#30028;&#29366;&#24577;&#31283;&#23450;&#24615;&#21644;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#30340;&#21487;&#21152;&#24615;&#24847;&#21619;&#30528;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
The setting of an agent making decisions under uncertainty and under dynamic constraints is common for the fields of optimal control, reinforcement learning, and recently also for online learning. In the online learning setting, the quality of an agent's decision is often quantified by the concept of regret, comparing the performance of the chosen decisions to the best possible ones in hindsight. While regret is a useful performance measure, when dynamical systems are concerned, it is important to also assess the stability of the closed-loop system for a chosen policy. In this work, we show that for linear state feedback policies and linear systems subject to adversarial disturbances, linear regret implies asymptotic stability in both time-varying and time-invariant settings. Conversely, we also show that bounded input bounded state stability and summability of the state transition matrices imply linear regret.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26377;&#25928;&#30340;&#20559;&#24046;&#30699;&#27491;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#26512;&#20844;&#24179;&#24615;&#20197;&#21450;&#36879;&#26126;&#25253;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05321</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25233;&#37057;&#30151;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#19982;&#20559;&#24046;&#30699;&#27491;&#65306;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30740;&#31350;&#20154;&#32676;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness and bias correction in machine learning for depression prediction: results from four different study populations. (arXiv:2211.05321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26377;&#25928;&#30340;&#20559;&#24046;&#30699;&#27491;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#26512;&#20844;&#24179;&#24615;&#20197;&#21450;&#36879;&#26126;&#25253;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#29702;&#20445;&#20581;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21463;&#20851;&#27880;&#30340;&#20154;&#32676;&#20013;&#65292;&#23384;&#22312;&#30528;&#30456;&#24403;&#31243;&#24230;&#30340;&#27745;&#21517;&#21270;&#21644;&#19981;&#24179;&#31561;&#65292;&#36825;&#31181;&#19981;&#24179;&#31561;&#20250;&#25193;&#25955;&#21040;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#12290;&#22914;&#26524;&#19981;&#36866;&#24403;&#22320;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#25152;&#23398;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#27169;&#22411;&#23601;&#20250;&#24378;&#21270;&#24050;&#32463;&#23384;&#22312;&#20110;&#31038;&#20250;&#20013;&#30340;&#32467;&#26500;&#24615;&#20559;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;ML&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#26377;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#22269;&#23478;&#21644;&#20154;&#32676;&#12290;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934;&#30340;ML&#26041;&#27861;&#26174;&#31034;&#20986;&#24120;&#35268;&#20559;&#24046;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#32531;&#35299;&#25216;&#26415;&#20197;&#21450;&#25105;&#20204;&#33258;&#24049;&#30340;&#20107;&#21518;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#19981;&#20844;&#24179;&#20559;&#24046;&#30340;&#32423;&#21035;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#24314;&#35758;&#65292;&#20197;&#24320;&#21457;&#39044;&#27979;&#25233;&#37057;&#30151;&#39118;&#38505;&#30340;ML&#27169;&#22411;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#27809;&#26377;&#21333;&#19968;&#26368;&#22909;&#30340;&#39044;&#27979;&#25233;&#37057;&#30151;&#30340;ML&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#32467;&#26524;&#30340;&#24179;&#31561;&#12290;&#36825;&#24378;&#35843;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#20013;&#20998;&#26512;&#20844;&#24179;&#24615;&#20197;&#21450;&#36879;&#26126;&#25253;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations, which spreads through collected data. When not properly accounted for, machine learning (ML) models learned from data can reinforce the structural biases already present in society. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. However, we show that standard mitigation techniques, and our own post-hoc method, can be effective in reducing the level of unfair bias. We provide practical recommendations to develop ML models for depression risk prediction with increased fairness and trust in the real world. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#32452;&#20844;&#24179;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#35889;&#32858;&#31867;&#31639;&#27861; s-FairSC&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#21521;&#37327;&#20056;&#31215;&#26469;&#20805;&#20998;&#21033;&#29992;&#20854;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16435</link><description>&lt;p&gt;
&#24102;&#26377;&#32452;&#20844;&#24179;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Scalable Spectral Clustering with Group Fairness Constraints. (arXiv:2210.16435v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#32452;&#20844;&#24179;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#35889;&#32858;&#31867;&#31639;&#27861; s-FairSC&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#21521;&#37327;&#20056;&#31215;&#26469;&#20805;&#20998;&#21033;&#29992;&#20854;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#24314;&#27169;&#20844;&#24179;&#24615;&#21644;&#32416;&#27491;&#31639;&#27861;&#20559;&#24046;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24037;&#19994;&#23454;&#36341;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32452;&#20844;&#24179;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#35889;&#32858;&#31867;&#65288;SC&#65289;&#31639;&#27861;&#12290;&#32452;&#20844;&#24179;&#20063;&#34987;&#31216;&#20026;&#32479;&#35745;&#24179;&#31561;&#65292;&#21363;&#22312;&#27599;&#20010;&#31751;&#20013;&#65292;&#27599;&#20010;&#21463;&#20445;&#25252;&#30340;&#32452;&#21035;&#20197;&#19982;&#25972;&#20307;&#30456;&#21516;&#30340;&#27604;&#20363;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#38646;&#31354;&#38388;&#25237;&#24433;&#21644;&#38669;&#29305;&#26519;&#30340;&#32553;&#20943;&#30340;&#26032;&#30340;&#35889;&#35745;&#31639;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; s-FairSC &#30340;&#31639;&#27861;&#65292;&#23427;&#21482;&#28041;&#21450;&#31232;&#30095;&#30340;&#30697;&#38453;&#21521;&#37327;&#20056;&#31215;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20844;&#24179; SC &#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are synergies of research interests and industrial efforts in modeling fairness and correcting algorithmic bias in machine learning. In this paper, we present a scalable algorithm for spectral clustering (SC) with group fairness constraints. Group fairness is also known as statistical parity where in each cluster, each protected group is represented with the same proportion as in the entirety. While FairSC algorithm (Kleindessner et al., 2019) is able to find the fairer clustering, it is compromised by high costs due to the kernels of computing nullspaces and the square roots of dense matrices explicitly. We present a new formulation of underlying spectral computation by incorporating nullspace projection and Hotelling's deflation such that the resulting algorithm, called s-FairSC, only involves the sparse matrix-vector products and is able to fully exploit the sparsity of the fair SC model. The experimental results on the modified stochastic block model demonstrate that s-FairSC
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35777;&#26126;&#20256;&#32479;&#30340;&#36890;&#29992;&#23545;&#25239;&#24178;&#25200; (UAPs) &#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#36716;&#31227;&#24615;&#26159;&#27425;&#20248;&#30340;&#65292;&#20026;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#23545;&#25239;&#26041;&#21521; (UADs)&#65292;&#21482;&#22266;&#23450;&#36890;&#29992;&#26041;&#21521;&#65292;&#20197;&#20415;&#20811;&#26381;&#22312;&#36328;DNN&#26550;&#26500;&#19978;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.15997</link><description>&lt;p&gt;
&#36890;&#29992;&#23545;&#25239;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Universal Adversarial Directions. (arXiv:2210.15997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15997
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35777;&#26126;&#20256;&#32479;&#30340;&#36890;&#29992;&#23545;&#25239;&#24178;&#25200; (UAPs) &#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#36716;&#31227;&#24615;&#26159;&#27425;&#20248;&#30340;&#65292;&#20026;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#23545;&#25239;&#26041;&#21521; (UADs)&#65292;&#21482;&#22266;&#23450;&#36890;&#29992;&#26041;&#21521;&#65292;&#20197;&#20415;&#20811;&#26381;&#22312;&#36328;DNN&#26550;&#26500;&#19978;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35266;&#23519;&#21040;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36890;&#29992;&#23545;&#25239;&#24178;&#25200; (UAPs) &#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#24178;&#25200;&#20351;&#29992;&#21333;&#20010;&#25200;&#21160;&#21521;&#37327;&#24178;&#25200;&#25152;&#26377;&#36755;&#20837;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;UAPs&#22312;&#36328;DNN&#26550;&#26500;&#36716;&#31227;&#26102;&#36890;&#24120;&#24456;&#22256;&#38590;&#24182;&#23548;&#33268;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;UAP&#30340;&#21487;Transfer&#24615;&#65292;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#22120;&#21644;UAP&#23545;&#25163;&#29609;&#23478;&#20043;&#38388;&#22312;&#36890;&#29992;&#23545;&#25239;&#31034;&#20363;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#36890;&#29992;&#23545;&#25239;&#31034;&#20363;&#21338;&#24328;&#32570;&#20047;&#19968;&#20010;&#32431;&#32435;&#20160;&#22343;&#34913;&#65292;&#36825;&#34920;&#26126;UAPs&#22312;DNN&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#26159;&#27425;&#20248;&#30340;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#23545;&#25239;&#26041;&#21521; (UADs)&#65292;&#21482;&#22266;&#23450;&#23545;&#25239;&#24178;&#25200;&#30340;&#36890;&#29992;&#26041;&#21521;&#65292;&#20801;&#35768;&#36328;&#26679;&#26412;&#33258;&#30001;&#36873;&#25321;&#24178;&#25200;&#30340;&#24133;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;UAD&#23545;&#25239;&#31034;&#20363;&#21338;&#24328;&#21487;&#20197;&#20855;&#26377;&#32435;&#20160;&#22343;&#34913;&#19988;&#35813;&#22343;&#34913;&#29366;&#24577;&#32431;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their great success in image recognition tasks, deep neural networks (DNNs) have been observed to be susceptible to universal adversarial perturbations (UAPs) which perturb all input samples with a single perturbation vector. However, UAPs often struggle in transferring across DNN architectures and lead to challenging optimization problems. In this work, we study the transferability of UAPs by analyzing equilibrium in the universal adversarial example game between the classifier and UAP adversary players. We show that under mild assumptions the universal adversarial example game lacks a pure Nash equilibrium, indicating UAPs' suboptimal transferability across DNN classifiers. To address this issue, we propose Universal Adversarial Directions (UADs) which only fix a universal direction for adversarial perturbations and allow the perturbations' magnitude to be chosen freely across samples. We prove that the UAD adversarial example game can possess a Nash equilibrium with a pure U
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23545;&#20110;&#21508;&#31181;&#19981;&#21516;&#20998;&#24067;&#30340;&#36755;&#20837;&#29366;&#24577;&#20855;&#26377;&#23567;&#30340;&#24179;&#22343;&#35823;&#24046;&#65292;&#21363;&#20415;&#22312;&#22788;&#29702;&#25351;&#25968;&#38376;&#26102;&#20063;&#20855;&#26377;&#39640;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#26377;&#25928;&#24615;&#65292;&#26159;&#19968;&#31181;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.14894</link><description>&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning to predict arbitrary quantum processes. (arXiv:2210.14894v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23545;&#20110;&#21508;&#31181;&#19981;&#21516;&#20998;&#24067;&#30340;&#36755;&#20837;&#29366;&#24577;&#20855;&#26377;&#23567;&#30340;&#24179;&#22343;&#35823;&#24046;&#65292;&#21363;&#20415;&#22312;&#22788;&#29702;&#25351;&#25968;&#38376;&#26102;&#20063;&#20855;&#26377;&#39640;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#26377;&#25928;&#24615;&#65292;&#26159;&#19968;&#31181;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;n&#20010;&#37327;&#23376;&#27604;&#29305;&#19978;&#20219;&#24847;&#26410;&#30693;&#37327;&#23376;&#36807;&#31243;E&#12290;&#23545;&#20110;&#20219;&#24847;n&#27604;&#29305;&#29366;&#24577;&#19978;&#30340;&#24191;&#27867;&#20998;&#24067;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20174;&#26410;&#30693;&#36807;&#31243;E&#36755;&#20986;&#30340;&#20219;&#20309;&#23616;&#37096;&#24615;&#36136;&#65292;&#24182;&#22312;&#32472;&#21046;&#33258;&#20998;&#24067;&#30340;&#36755;&#20837;&#29366;&#24577;&#19978;&#20855;&#26377;&#23567;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;&#21363;&#20351;&#26410;&#30693;&#36807;&#31243;&#26159;&#20855;&#26377;&#25351;&#25968;&#22810;&#20010;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#35813;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20063;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#23398;&#20064;&#26410;&#30693;&#29366;&#24577;&#23646;&#24615;&#21644;&#23398;&#20064;&#26410;&#30693;&#21487;&#35266;&#23519;&#20302;&#38454;&#36924;&#36817;&#30340;&#26377;&#25928;&#36807;&#31243;&#12290;&#35813;&#20998;&#26512;&#22522;&#20110;&#35777;&#26126;&#26032;&#30340;&#33539;&#25968;&#19981;&#31561;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20248;&#21270;&#23616;&#37096;&#21704;&#23494;&#39039;&#37327;&#30340;&#25913;&#36827;&#31639;&#27861;&#32780;&#23548;&#20986;&#30340;&#32463;&#20856;Bohnenblust-Hille&#19981;&#31561;&#24335;&#30340;&#37327;&#23376;&#31867;&#27604;&#12290;&#22312;&#28041;&#21450;&#21040;10^6&#28436;&#21270;&#26102;&#38388;&#21644;&#39640;&#36798;7&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an efficient machine learning (ML) algorithm for predicting any unknown quantum process $\mathcal{E}$ over $n$ qubits. For a wide range of distributions $\mathcal{D}$ on arbitrary $n$-qubit states, we show that this ML algorithm can learn to predict any local property of the output from the unknown process~$\mathcal{E}$, with a small average error over input states drawn from $\mathcal{D}$. The ML algorithm is computationally efficient even when the unknown process is a quantum circuit with exponentially many gates. Our algorithm combines efficient procedures for learning properties of an unknown state and for learning a low-degree approximation to an unknown observable. The analysis hinges on proving new norm inequalities, including a quantum analogue of the classical Bohnenblust-Hille inequality, which we derive by giving an improved algorithm for optimizing local Hamiltonians. Numerical experiments on predicting quantum dynamics with evolution time up to $10^6$ and system
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#30340;&#32531;&#20914;&#21306;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13545</link><description>&lt;p&gt;
MEET: &#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#33945;&#29305;&#21345;&#32599;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
MEET: A Monte Carlo Exploration-Exploitation Trade-off for Buffer Sampling. (arXiv:2210.13545v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#30340;&#32531;&#20914;&#21306;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36873;&#25321;&#26159;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#25216;&#26415;&#30340;&#20851;&#38190;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#12290;&#38024;&#23545;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26368;&#26032;&#37319;&#26679;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;Q&#20540;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#21033;&#29992;&#20102;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#12290;&#36825;&#26159;&#36890;&#36807;Q&#20540;&#20989;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23454;&#29616;&#30340;&#65292;&#23427;&#25351;&#23548;&#37319;&#26679;&#25506;&#32034;&#26356;&#37325;&#35201;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#24179;&#31283;&#30340;&#32467;&#26524;&#12290;&#23427;&#20204;&#34920;&#26126;&#65292;&#22312;&#23494;&#38598;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#25910;&#25947;&#21644;&#23792;&#20540;&#24615;&#33021;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#26368;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data selection is essential for any data-based optimization technique, such as Reinforcement Learning. State-of-the-art sampling strategies for the experience replay buffer improve the performance of the Reinforcement Learning agent. However, they do not incorporate uncertainty in the Q-Value estimation. Consequently, they cannot adapt the sampling strategies, including exploration and exploitation of transitions, to the complexity of the task. To address this, this paper proposes a new sampling strategy that leverages the exploration-exploitation trade-off. This is enabled by the uncertainty estimation of the Q-Value function, which guides the sampling to explore more significant transitions and, thus, learn a more efficient policy. Experiments on classical control environments demonstrate stable results across various environments. They show that the proposed method outperforms state-of-the-art sampling strategies for dense rewards w.r.t. convergence and peak performance by 26% on av
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;RibSeg&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#21152;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#65292;&#20849;&#21253;&#21547;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#12289;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#12289;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.09309</link><description>&lt;p&gt;
RibSeg v2&#65306;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction. (arXiv:2210.09309v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;RibSeg&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#21152;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#65292;&#20849;&#21253;&#21547;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#12289;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#12289;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#26159;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#30340;&#24120;&#35265;&#21069;&#25552;&#26465;&#20214;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35201;&#20040;&#20351;&#29992;&#20869;&#37096;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#20026;&#31038;&#32676;&#25152;&#20849;&#20139;&#65292;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#32907;&#39592;&#20998;&#21106;&#32780;&#24573;&#30053;&#20102;&#32907;&#39592;&#26631;&#35760;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#20043;&#21069;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#32907;&#39592;&#20998;&#21106;&#30340;RibSeg&#25968;&#25454;&#38598;&#25193;&#23637;&#20026;&#32508;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#24182;&#21152;&#20837;&#20102;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#65292;&#20849;&#21253;&#21547;&#20102;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#12290;&#22522;&#20110;RibSeg v2&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21253;&#21547;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#65292;&#20197;&#21450;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#30340;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#21644;&#20998;&#26512;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#65292;&#8230;
&lt;/p&gt;
&lt;p&gt;
Automatic rib labeling and anatomical centerline extraction are common prerequisites for various clinical applications. Prior studies either use in-house datasets that are inaccessible to communities, or focus on rib segmentation that neglects the clinical significance of rib labeling. To address these issues, we extend our prior dataset (RibSeg) on the binary rib segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT scans (15,466 individual ribs in total) and annotations manually inspected by experts for rib labeling and anatomical centerline extraction. Based on the RibSeg v2, we develop a pipeline including deep learning-based methods for rib labeling, and a skeletonization-based method for centerline extraction. To improve computational efficiency, we propose a sparse point cloud representation of CT scans and compare it with standard dense voxel grids. Moreover, we design and analyze evaluation metrics to address the key challenges of each task. Our dataset,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#35299;&#20915;&#25152;&#26377;&#22266;&#23450;&#24418;&#29366;&#28216;&#25103;&#31354;&#38388;&#30340;NEs&#12289;CEs&#21644;CCEs&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#28789;&#27963;&#30340;&#24179;&#34913;&#36873;&#25321;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#21152;&#24378;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.09257</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#35299;&#20915;NEs&#12289;CEs&#21644;CCEs&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Turbocharging Solution Concepts: Solving NEs, CEs and CCEs with Neural Equilibrium Solvers. (arXiv:2210.09257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#35299;&#20915;&#25152;&#26377;&#22266;&#23450;&#24418;&#29366;&#28216;&#25103;&#31354;&#38388;&#30340;NEs&#12289;CEs&#21644;CCEs&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#28789;&#27963;&#30340;&#24179;&#34913;&#36873;&#25321;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#21152;&#24378;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#21338;&#24328;&#35770;&#20013;&#30340;Nash Equilibria&#12289;Correlated Equilibria&#21644;Coarse Correlated Equilibria&#31561;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#27491;&#21017;&#24418;&#24335;&#30340;&#21338;&#24328;&#21487;&#33021;&#38656;&#35201;&#31105;&#27490;&#25110;&#38750;&#30830;&#23450;&#24615;&#26102;&#38388;&#25910;&#25947;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#29305;&#27530;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#36817;&#20284;&#35299;&#20915;&#25152;&#26377;&#22266;&#23450;&#24418;&#29366;&#30340;&#28216;&#25103;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#36895;&#24230;&#21644;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#24179;&#34913;&#36873;&#25321;&#26694;&#26550;&#65292;&#33021;&#22815;&#21807;&#19968;&#36873;&#25321;&#26368;&#23567;&#21270;&#30456;&#23545;&#29109;&#25110;&#26368;&#22823;&#21270;&#31119;&#21033;&#30340;&#24179;&#34913;&#12290;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29983;&#25104;&#20219;&#20309;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#32593;&#32476;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#28216;&#25103;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#32593;&#32476;&#26159;&#35768;&#22810;&#21487;&#33021;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#30340;&#24378;&#22823;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solution concepts such as Nash Equilibria, Correlated Equilibria, and Coarse Correlated Equilibria are useful components for many multiagent machine learning algorithms. Unfortunately, solving a normal-form game could take prohibitive or non-deterministic time to converge, and could fail. We introduce the Neural Equilibrium Solver which utilizes a special equivariant neural network architecture to approximately solve the space of all games of fixed shape, buying speed and determinism. We define a flexible equilibrium selection framework, that is capable of uniquely selecting an equilibrium that minimizes relative entropy, or maximizes welfare. The network is trained without needing to generate any supervised training data. We show remarkable zero-shot generalization to larger games. We argue that such a network is a powerful component for many possible multiagent algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#26925;&#22278;&#30028;&#38754;&#38382;&#39064;&#30340;&#23574;&#28857;&#25429;&#33719;PINN&#65292;&#23427;&#20351;&#29992;&#23574;&#28857;&#24378;&#21046;&#27700;&#24179;&#38598;&#20989;&#25968;&#20316;&#20026;&#39069;&#22806;&#29305;&#24449;&#36755;&#20837;&#65292;&#20197;&#38160;&#21033;&#22320;&#25429;&#33719;&#35299;&#23574;&#28857;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#32593;&#26684;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2210.08424</link><description>&lt;p&gt;
&#29992;&#20110;&#26925;&#22278;&#30028;&#38754;&#38382;&#39064;&#30340;&#23574;&#28857;&#25429;&#33719;PINN
&lt;/p&gt;
&lt;p&gt;
A cusp-capturing PINN for elliptic interface problems. (arXiv:2210.08424v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#26925;&#22278;&#30028;&#38754;&#38382;&#39064;&#30340;&#23574;&#28857;&#25429;&#33719;PINN&#65292;&#23427;&#20351;&#29992;&#23574;&#28857;&#24378;&#21046;&#27700;&#24179;&#38598;&#20989;&#25968;&#20316;&#20026;&#39069;&#22806;&#29305;&#24449;&#36755;&#20837;&#65292;&#20197;&#38160;&#21033;&#22320;&#25429;&#33719;&#35299;&#23574;&#28857;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#32593;&#26684;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23574;&#28857;&#25429;&#33719;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#65292;&#29992;&#20110;&#35299;&#20915;&#35299;&#22312;&#30028;&#38754;&#19978;&#20855;&#26377;&#19981;&#36830;&#32493;&#19968;&#38454;&#23548;&#25968;&#30340;&#38388;&#26029;&#31995;&#25968;&#26925;&#22278;&#30028;&#38754;&#38382;&#39064;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#26679;&#30340;&#35299;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23574;&#28857;&#24378;&#21046;&#27700;&#24179;&#38598;&#20989;&#25968;&#20316;&#20026;&#32593;&#32476;&#30340;&#39069;&#22806;&#29305;&#24449;&#36755;&#20837;&#65292;&#20197;&#20445;&#25345;&#22266;&#26377;&#30340;&#35299;&#24615;&#36136;&#65292;&#21363;&#38160;&#21033;&#22320;&#25429;&#33719;&#35299;&#23574;&#28857;(&#20854;&#20013;&#23548;&#25968;&#19981;&#36830;&#32493;)&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26080;&#32593;&#26684;&#30340;&#20248;&#28857;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#19981;&#35268;&#21017;&#22495;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#35757;&#32451;&#32593;&#32476;&#65292;&#20854;&#20013;&#25439;&#22833;&#20989;&#25968;&#21253;&#25324;&#24494;&#20998;&#26041;&#31243;&#30340;&#27531;&#24046;&#20197;&#21450;&#26576;&#20123;&#30028;&#38754;&#21644;&#36793;&#30028;&#26465;&#20214;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#23574;&#28857;&#25429;&#33719;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#24403;&#21069;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a cusp-capturing physics-informed neural network (PINN) to solve discontinuous-coefficient elliptic interface problems whose solution is continuous but has discontinuous first derivatives on the interface. To find such a solution using neural network representation, we introduce a cusp-enforced level set function as an additional feature input to the network to retain the inherent solution properties; that is, capturing the solution cusps (where the derivatives are discontinuous) sharply. In addition, the proposed neural network has the advantage of being mesh-free, so it can easily handle problems in irregular domains. We train the network using the physics-informed framework in which the loss function comprises the residual of the differential equation together with certain interface and boundary conditions. We conduct a series of numerical experiments to demonstrate the effectiveness of the cusp-capturing technique and the accuracy of the present network mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#22823;&#35268;&#27169;&#22797;&#26434;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.05301</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20960;&#20309;&#23398;&#20064;&#30340;&#20869;&#22312;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension for Large-Scale Geometric Learning. (arXiv:2210.05301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#22823;&#35268;&#27169;&#22797;&#26434;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#30340;&#27010;&#24565;&#23545;&#20110;&#29702;&#35299;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#32500;&#24230;&#30340;&#19968;&#31181;&#22825;&#30495;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#23646;&#24615;&#30340;&#25968;&#37327;&#12290;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#25512;&#23548;&#20986;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#20363;&#22914;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#32463;&#39564;&#35266;&#23519;&#65292;&#26080;&#27861;&#24212;&#23545;&#24403;&#20195;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#20844;&#29702;&#22522;&#30784;&#12290;V. Pestov&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#22312;&#32500;&#24230;&#20844;&#29702;&#22320;&#19982;&#25968;&#23398;&#38598;&#20013;&#24230;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#35745;&#31639;&#36825;&#20123;&#20869;&#22312;&#32500;&#24230;&#30340;&#26041;&#27861;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36825;&#20123;&#20844;&#29702;&#30340;&#20869;&#22312;&#32500;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of dimension is essential to grasp the complexity of data. A naive approach to determine the dimension of a dataset is based on the number of attributes. More sophisticated methods derive a notion of intrinsic dimension (ID) that employs more complex feature functions, e.g., distances between data points. Yet, many of these approaches are based on empirical observations, cannot cope with the geometric character of contemporary datasets, and do lack an axiomatic foundation. A different approach was proposed by V. Pestov, who links the intrinsic dimension axiomatically to the mathematical concentration of measure phenomenon. First methods to compute this and related notions for ID were computationally intractable for large-scale real-world datasets. In the present work, we derive a computationally feasible method for determining said axiomatic ID functions. Moreover, we demonstrate how the geometric properties of complex data are accounted for in our modeling. In particular, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#25193;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23548;&#33322;&#36712;&#36857;&#24182;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#22120;Marky&#29983;&#25104;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#20196;&#65292;&#20197;&#21450;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;GAN&#22312;&#26032;&#30340;&#35270;&#35282;&#19978;&#21512;&#25104;&#22270;&#20687;&#35266;&#23519;&#12290;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#20102;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.03112</link><description>&lt;p&gt;
&#19968;&#26465;&#26032;&#36335;: &#29992;&#21512;&#25104;&#25351;&#20196;&#21644;&#27169;&#20223;&#23398;&#20064;&#25193;&#23637;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning. (arXiv:2210.03112v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#25193;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23548;&#33322;&#36712;&#36857;&#24182;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#22120;Marky&#29983;&#25104;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#20196;&#65292;&#20197;&#21450;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;GAN&#22312;&#26032;&#30340;&#35270;&#35282;&#19978;&#21512;&#25104;&#22270;&#20687;&#35266;&#23519;&#12290;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#20102;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#30340;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#30001;&#20110;&#20154;&#31867;&#25351;&#20196;&#25968;&#25454;&#31232;&#32570;&#19988;&#35757;&#32451;&#29615;&#22659;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#20195;&#29702;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#31354;&#38388;&#35821;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#21512;&#25104;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#25193;&#20805;&#12290;&#25105;&#20204;&#21033;&#29992;Marky&#65292;&#19968;&#31181;&#39640;&#21697;&#36136;&#30340;&#22810;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#21019;&#24314;&#20102;500&#22810;&#20010;&#23460;&#20869;&#29615;&#22659;&#65292;&#36890;&#36807;&#36825;&#20123;&#20840;&#26223;&#22270;&#26500;&#24314;&#23548;&#33322;&#36712;&#36857;&#65292;&#24182;&#20026;&#27599;&#20010;&#36712;&#36857;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;GAN&#22312;&#26032;&#30340;&#35270;&#35282;&#19978;&#21512;&#25104;&#22270;&#20687;&#35266;&#23519;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#20102;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navigation instructions in photorealistic environments, as a step towards robots that can follow human instructions. However, given the scarcity of human instruction data and limited diversity in the training environments, these agents still struggle with complex language grounding and spatial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investigate large-scale augmentation with synthetic instructions. We take 500+ indoor environments captured in densely-sampled 360 degree panoramas, construct navigation trajectories through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky, a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. The resulting d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20449;&#24687;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37051;&#25509;&#30697;&#38453;&#26356;&#26032;&#25216;&#26415;&#39044;&#35757;&#32451;&#22270;&#21367;&#31215;&#32593;&#32476;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#32454;&#32990;&#22312;&#21453;&#20107;&#23454;&#24178;&#25200;&#19979;&#30340;&#22522;&#22240;&#34920;&#36798;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20272;&#35745;&#22120;&#26469;&#39640;&#25928;&#20272;&#35745;&#36793;&#32536;&#24178;&#25200;&#25928;&#24212;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00116</link><description>&lt;p&gt;
&#21033;&#29992;&#21464;&#20998;&#22240;&#26524;&#25512;&#26029;&#21644;&#31934;&#32454;&#20851;&#31995;&#20449;&#24687;&#39044;&#27979;&#32454;&#32990;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information. (arXiv:2210.00116v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20449;&#24687;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37051;&#25509;&#30697;&#38453;&#26356;&#26032;&#25216;&#26415;&#39044;&#35757;&#32451;&#22270;&#21367;&#31215;&#32593;&#32476;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#32454;&#32990;&#22312;&#21453;&#20107;&#23454;&#24178;&#25200;&#19979;&#30340;&#22522;&#22240;&#34920;&#36798;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20272;&#35745;&#22120;&#26469;&#39640;&#25928;&#20272;&#35745;&#36793;&#32536;&#24178;&#25200;&#25928;&#24212;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32454;&#32990;&#22312;&#24178;&#25200;&#19979;&#30340;&#21709;&#24212;&#21487;&#33021;&#20026;&#33647;&#29289;&#30740;&#21457;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#24102;&#26469;&#37325;&#35201;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#21464;&#20998;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#39044;&#27979;&#32454;&#32990;&#22312;&#21453;&#20107;&#23454;&#24178;&#25200;&#19979;&#65288;&#21363;&#32454;&#32990;&#26410;&#30495;&#23454;&#25509;&#25910;&#30340;&#24178;&#25200;&#65289;&#30340;&#22522;&#22240;&#34920;&#36798;&#65292;&#21033;&#29992;&#20195;&#34920;&#29983;&#29289;&#23398;&#30693;&#35782;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#20449;&#24687;&#26469;&#36741;&#21161;&#20010;&#24615;&#21270;&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#25968;&#25454;&#33258;&#36866;&#24212;GRN&#24320;&#21457;&#20102;&#37051;&#25509;&#30697;&#38453;&#26356;&#26032;&#25216;&#26415;&#29992;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#65292;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#22522;&#22240;&#20851;&#31995;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the responses of a cell under perturbations may bring important benefits to drug discovery and personalized therapeutics. In this work, we propose a novel graph variational Bayesian causal inference framework to predict a cell's gene expressions under counterfactual perturbations (perturbations that this cell did not factually receive), leveraging information representing biological knowledge in the form of gene regulatory networks (GRNs) to aid individualized cellular response predictions. Aiming at a data-adaptive GRN, we also developed an adjacency matrix updating technique for graph convolutional networks and used it to refine GRNs during pre-training, which generated more insights on gene relations and enhanced model performance. Additionally, we propose a robust estimator within our framework for the asymptotically efficient estimation of marginal perturbation effect, which is yet to be carried out in previous works. With extensive experiments, we exhibited the advanta
&lt;/p&gt;</description></item><item><title>DecAF &#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#65307;&#21516;&#26102;&#65292;&#23427;&#36824;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.00063</link><description>&lt;p&gt;
DecAF&#65306;&#38024;&#23545;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#31572;&#26696;&#21644;&#36923;&#36753;&#24418;&#24335;&#32852;&#21512;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. (arXiv:2210.00063v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00063
&lt;/p&gt;
&lt;p&gt;
DecAF &#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#65307;&#21516;&#26102;&#65292;&#23427;&#36824;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#26088;&#22312;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#31561;&#20107;&#23454;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#29983;&#25104;&#21487;&#22312;&#30693;&#35782;&#24211;&#19978;&#25191;&#34892;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#35201;&#20040;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21069;&#32773;&#36890;&#24120;&#33021;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#20294;&#30001;&#20110;&#29983;&#25104;&#30340;&#36923;&#36753;&#24418;&#24335;&#21487;&#33021;&#23384;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#22240;&#27492;&#23384;&#22312;&#26080;&#27861;&#25191;&#34892;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; DecAF&#65292;&#23427;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DecAF &#22522;&#20110;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23454;&#20307;&#38142;&#25509;&#24037;&#20855;--&#36825;&#31181;&#31616;&#21270;&#20351;&#20854;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#23481;&#26131;&#12290;DecAF &#22312; WebQSP&#12289;FreebaseQA &#21644; GrailQA &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22312; CommonsenseQA &#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases (KBs) aims to answer natural language questions with factual information such as entities and relations in KBs. Previous methods either generate logical forms that can be executed over KBs to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework DecAF that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, DecAF is based on simple free-text retrieval without relying on any entity linking tools -- this simplification eases its adaptation to different datasets. DecAF achieves new state-of-the-art accuracy on WebQSP, FreebaseQA, and GrailQA benchmarks, while getting competitive results on the Com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#27491;&#30830;&#37325;&#26500;&#21306;&#22495;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27668;&#20505;&#27169;&#25311;&#36755;&#20986;&#30340;&#36136;&#37327;&#19982;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.12433</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#36229;&#20998;&#36776;&#29575;&#22810;&#21306;&#22495;&#27668;&#20505;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deep generative model super-resolves spatially correlated multiregional climate data. (arXiv:2209.12433v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#27491;&#30830;&#37325;&#26500;&#21306;&#22495;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27668;&#20505;&#27169;&#25311;&#36755;&#20986;&#30340;&#36136;&#37327;&#19982;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20840;&#29699;&#27668;&#20505;&#27169;&#25311;&#30340;&#31895;&#30053;&#36755;&#20986;&#36827;&#34892;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702; (downscaling)&#26159;&#21046;&#23450;&#38656;&#35201;&#38271;&#26399;&#27668;&#20505;&#21464;&#21270;&#39044;&#27979;&#30340;&#25919;&#27835;&#21644;&#31038;&#20250;&#20915;&#31574;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24555;&#36895;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#23578;&#26410;&#20445;&#30041;&#27668;&#20505;&#25968;&#25454;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#32780;&#24403;&#25105;&#20204;&#22788;&#29702;&#20855;&#26377;&#31354;&#38388;&#24191;&#24230;&#30340;&#31995;&#32479;&#22914;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27491;&#30830;&#37325;&#26500;&#36229;&#20998;&#36776;&#29575;&#19979;&#38477;&#26102;&#30340;&#21306;&#22495;&#38388;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#20445;&#25345;&#20687;&#32032;&#24179;&#22343;&#32479;&#35745;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36798;50&#20493;&#30340;&#25918;&#22823;&#12290;&#36890;&#36807;&#30452;&#25509;&#27604;&#36739;&#28201;&#24230;&#21644;&#38477;&#27700;&#20998;&#24067;&#30340;&#27979;&#37327;&#27668;&#35937;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#25972;&#21512;&#27668;&#20505;&#23398;&#24847;&#20041;&#19978;&#30340;&#29289;&#29702;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#24615;&#33021;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026; $\pi$SRGAN (&#29289;&#29702;&#20449;&#24687;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Super-resolving the coarse outputs of global climate simulations, termed downscaling, is crucial in making political and social decisions on systems requiring long-term climate change projections. Existing fast super-resolution techniques, however, have yet to preserve the spatially correlated nature of climatological data, which is particularly important when we address systems with spatial expanse, such as the development of transportation infrastructure. Herein, we show an adversarial network-based machine learning enables us to correctly reconstruct the inter-regional spatial correlations in downscaling with high magnification of up to fifty while maintaining pixel-wise statistical consistency. Direct comparison with the measured meteorological data of temperature and precipitation distributions reveals that integrating climatologically important physical information improves the downscaling performance, which prompts us to call this approach $\pi$SRGAN (Physics Informed Super-Reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#20998;&#31867;&#22120;&#30340;&#26550;&#26500;&#65292;&#31216;&#20316;SPRITZ-1.5C&#65292;&#23427;&#21516;&#26102;&#32467;&#21512;&#20102;1&#31867;&#20998;&#31867;&#30340;&#23433;&#20840;&#24615;&#21644;&#20256;&#32479;2&#31867;&#20998;&#31867;&#30340;&#39640;&#24615;&#33021;&#65292;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#23433;&#20840;&#22411;&#24212;&#29992;&#20013;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12195</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#25552;&#21319;&#35745;&#31639;&#26426;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Employing Deep Ensemble Learning for Improving the Security of Computer Networks against Adversarial Attacks. (arXiv:2209.12195v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#20998;&#31867;&#22120;&#30340;&#26550;&#26500;&#65292;&#31216;&#20316;SPRITZ-1.5C&#65292;&#23427;&#21516;&#26102;&#32467;&#21512;&#20102;1&#31867;&#20998;&#31867;&#30340;&#23433;&#20840;&#24615;&#21644;&#20256;&#32479;2&#31867;&#20998;&#31867;&#30340;&#39640;&#24615;&#33021;&#65292;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#23433;&#20840;&#22411;&#24212;&#29992;&#20013;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#32593;&#32476;&#21644;&#22810;&#23186;&#20307;&#23433;&#20840;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32593;&#32476;&#32467;&#26500;&#30340;&#33030;&#24369;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#19981;&#22826;&#36866;&#29992;&#20110;&#35832;&#22914;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#30340;&#23433;&#20840;&#22411;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#26550;&#26500;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#65292;&#38656;&#35201;&#20351;&#29992;&#20855;&#26377;&#25361;&#25112;&#25915;&#20987;&#33021;&#21147;&#30340;&#23433;&#20840;&#22411;&#26550;&#26500;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#20998;&#31867;&#22120;&#30340;&#21019;&#26032;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#38754;&#20020;&#25915;&#20987;&#26102;&#21516;&#26102;&#32467;&#21512;&#20102;&#22686;&#24378;&#30340;&#19968;&#31867;&#20998;&#31867;&#21644;&#20256;&#32479;&#30340;&#20108;&#31867;&#20998;&#31867;&#30340;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#34987;&#31216;&#20026;1.5&#31867;&#20998;&#31867;&#22120;&#65288;SPRITZ-1.5C&#65289;&#65292;&#30001;&#32456;&#23494;&#38598;&#20998;&#31867;&#22120;&#12289;&#19968;&#20010;&#20108;&#31867;&#20998;&#31867;&#22120;&#65288;&#21363;CNN&#65289;&#21644;&#20004;&#20010;&#24182;&#34892;&#30340;&#19968;&#31867;&#20998;&#31867;&#22120;&#65288;&#21363;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#26500;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Convolutional Neural Networks (CNN) have demonstrated promising performance in various real-world cybersecurity applications, such as network and multimedia security. However, the underlying fragility of CNN structures poses major security problems, making them inappropriate for use in security-oriented applications including such computer networks. Protecting these architectures from adversarial attacks necessitates using security-wise architectures that are challenging to attack.  In this study, we present a novel architecture based on an ensemble classifier that combines the enhanced security of 1-Class classification (known as 1C) with the high performance of conventional 2-Class classification (known as 2C) in the absence of attacks.Our architecture is referred to as the 1.5-Class (SPRITZ-1.5C) classifier and constructed using a final dense classifier, one 2C classifier (i.e., CNNs), and two parallel 1C classifiers (i.e., auto-encoders). In our experiments, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24471;&#20998;&#24335;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20219;&#20309;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#19981;&#38656;&#35201;&#25490;&#38500;&#23454;&#36136;&#24615;&#38750;&#23545;&#25968;&#20985;&#24615;&#30340;&#38480;&#21046;&#24615;&#20989;&#25968;&#19981;&#31561;&#24335;&#26465;&#20214;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#39033;&#24335;&#30340;&#35268;&#27169;&#12290;&#36825;&#26159;&#24471;&#20998;&#24335;&#29983;&#25104;&#27169;&#22411;&#23454;&#35777;&#25104;&#21151;&#30340;&#24378;&#26377;&#21147;&#29702;&#35770;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.11215</link><description>&lt;p&gt;
&#37319;&#26679;&#19982;&#23398;&#20064;&#20998;&#25968;&#21516;&#26679;&#31616;&#21333;&#65306;&#23545;&#20551;&#35774;&#25968;&#25454;&#26368;&#23567;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. (arXiv:2209.11215v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24471;&#20998;&#24335;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20219;&#20309;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#19981;&#38656;&#35201;&#25490;&#38500;&#23454;&#36136;&#24615;&#38750;&#23545;&#25968;&#20985;&#24615;&#30340;&#38480;&#21046;&#24615;&#20989;&#25968;&#19981;&#31561;&#24335;&#26465;&#20214;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#39033;&#24335;&#30340;&#35268;&#27169;&#12290;&#36825;&#26159;&#24471;&#20998;&#24335;&#29983;&#25104;&#27169;&#22411;&#23454;&#35777;&#25104;&#21151;&#30340;&#24378;&#26377;&#21147;&#29702;&#35770;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24471;&#20998;&#24335;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20363;&#22914;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#23427;&#26159;&#22823;&#35268;&#27169;&#23454;&#38469;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DALL&#183;E 2&#65289;&#30340;&#25903;&#26609;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#22312;&#20551;&#35774;&#24471;&#20998;&#20272;&#35745;&#20934;&#30830;&#26102;&#65292;&#36825;&#26679;&#30340;SGM&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20219;&#20309;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#65306;&#65288;1&#65289;&#36866;&#29992;&#20110;$L^2$&#20934;&#30830;&#30340;&#20998;&#25968;&#20272;&#35745;&#65288;&#32780;&#19981;&#26159;$L^\infty$&#20934;&#30830;&#30340;&#65289;&#65307;(2) &#19981;&#38656;&#35201;&#25490;&#38500;&#23454;&#36136;&#24615;&#38750;&#23545;&#25968;&#20985;&#24615;&#30340;&#38480;&#21046;&#24615;&#20989;&#25968;&#19981;&#31561;&#24335;&#26465;&#20214;&#65307;&#65288;3&#65289;&#22312;&#25152;&#26377;&#30456;&#20851;&#30340;&#38382;&#39064;&#21442;&#25968;&#20013;&#20855;&#26377;&#22810;&#39033;&#24335;&#30340;&#35268;&#27169;&#65307;&#65288;4&#65289;&#21305;&#37197;&#26368;&#20808;&#36827;&#30340;Langevin&#25193;&#25955;&#30340;&#31163;&#25955;&#21270;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#21069;&#25552;&#26159;&#35780;&#20998;&#35823;&#24046;&#36275;&#22815;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;SGM&#23454;&#35777;&#25104;&#21151;&#30340;&#24378;&#26377;&#21147;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#22522;&#20110;&#20020;&#30028;&#38459;&#23612;Langevin&#25193;&#25955;&#65288;CLD&#65289;&#30340;SGM&#65292;&#19982;&#20197;&#24448;&#30740;&#31350;&#32467;&#35770;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23398;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#20248;&#20110;&#30446;&#21069;&#26368;&#20248;&#26041;&#27861;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.08907</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#31526;&#21495;&#27169;&#22411;&#26080;&#20851;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Symbolic Model-Agnostic Loss Functions via Meta-Learning. (arXiv:2209.08907v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23398;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#20248;&#20110;&#30446;&#21069;&#26368;&#20248;&#26041;&#27861;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#25628;&#32034;&#26041;&#27861;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#36827;&#21270;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;&#25968;&#23398;&#25805;&#20316;&#31354;&#38388;&#20013;&#25628;&#32034;&#31526;&#21495;&#25439;&#22833;&#20989;&#25968;&#30340;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#25439;&#22833;&#20989;&#25968;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#26799;&#24230;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#32463;&#39564;&#35777;&#23454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#25552;&#20986;&#30340;&#26041;&#27861;&#21457;&#29616;&#30340;&#20803;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop upon the emerging topic of loss function learning, which aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for learning model-agnostic loss functions via a hybrid neuro-symbolic search approach. The framework first uses evolution-based methods to search the space of primitive mathematical operations to find a set of symbolic loss functions. Second, the set of learned loss functions are subsequently parameterized and optimized via an end-to-end gradient-based training procedure. The versatility of the proposed framework is empirically validated on a diverse set of supervised learning tasks. Results show that the meta-learned loss functions discovered by the newly proposed method outperform both the cross-entropy loss and state-of-the-art loss function learning methods on a diverse range of neural network architectures and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;&#22855;&#24322;&#24615;&#20998;&#31163;&#28145;&#24230;Ritz&#26041;&#27861;&#27714;&#35299;&#24102;&#22855;&#24322;&#28304;&#30340;&#26925;&#22278;&#38382;&#39064;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;&#19968;&#33324;&#28857;&#28304;&#12289;&#32447;&#28304;&#20197;&#21450;&#28857;&#32447;&#28304;&#30340;&#32452;&#21512;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2209.02931</link><description>&lt;p&gt;
&#37319;&#29992;&#22855;&#24322;&#24615;&#20998;&#31163;&#28145;&#24230;Ritz&#26041;&#27861;&#27714;&#35299;&#24102;&#22855;&#24322;&#28304;&#30340;&#26925;&#22278;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Elliptic Problems with Singular Sources using Singularity Splitting Deep Ritz Method. (arXiv:2209.02931v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;&#22855;&#24322;&#24615;&#20998;&#31163;&#28145;&#24230;Ritz&#26041;&#27861;&#27714;&#35299;&#24102;&#22855;&#24322;&#28304;&#30340;&#26925;&#22278;&#38382;&#39064;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;&#19968;&#33324;&#28857;&#28304;&#12289;&#32447;&#28304;&#20197;&#21450;&#28857;&#32447;&#28304;&#30340;&#32452;&#21512;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#21644;&#22855;&#24322;&#28304;&#30340;&#20108;&#38454;&#26925;&#22278;&#26041;&#31243;&#12290;&#35813;&#31867;&#38382;&#39064;&#28085;&#30422;&#20102;&#19968;&#33324;&#28857;&#28304;&#12289;&#32447;&#28304;&#20197;&#21450;&#28857;&#32447;&#28304;&#30340;&#32452;&#21512;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#23558;&#30495;&#23454;&#35299;&#20998;&#35299;&#20026;&#24050;&#30693;&#35299;&#26512;&#22855;&#24322;&#37096;&#20998;&#21644;&#28385;&#36275;&#20855;&#26377;&#24179;&#28369;&#28304;&#30340;&#36866;&#24403;&#20462;&#25913;&#26925;&#22278;PDE&#30340;&#24120;&#35268;&#37096;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;Ritz&#26041;&#27861;&#27714;&#35299;&#24120;&#35268;&#37096;&#20998;&#12290;&#24314;&#35758;&#37319;&#29992;&#36335;&#24452;&#36319;&#36394;&#31574;&#30053;&#36873;&#25321;&#24809;&#32602;&#21442;&#25968;&#20197;&#24378;&#21046;&#23454;&#26045;Dirichlet&#36793;&#30028;&#26465;&#20214;&#12290;&#22312;&#20108;&#32500;&#21644;&#22810;&#32500;&#31354;&#38388;&#20013;&#23637;&#31034;&#20102;&#22823;&#37327;&#20855;&#26377;&#28857;&#28304;&#12289;&#32447;&#28304;&#25110;&#23427;&#20204;&#32452;&#21512;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35828;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#20197;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop an efficient solver based on neural networks for second-order elliptic equations with variable coefficients and singular sources. This class of problems covers general point sources, line sources and the combination of point-line sources, and has a broad range of practical applications. The proposed approach is based on decomposing the true solution into a singular part that is known analytically using the fundamental solution of the Laplace equation and a regular part that satisfies a suitable modified elliptic PDE with a smoother source, and then solving for the regular part using the deep Ritz method. A path-following strategy is suggested to select the penalty parameter for enforcing the Dirichlet boundary condition. Extensive numerical experiments in two- and multi-dimensional spaces with point sources, line sources or their combinations are presented to illustrate the efficiency of the proposed approach, and a comparative study with several existing appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniCon&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#37319;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2208.11435</link><description>&lt;p&gt;
UniCon: &#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#21333;&#21521;&#20998;&#27495;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniCon&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#37319;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#21161;&#20110;&#29616;&#23454;&#24212;&#29992;&#65292;&#22914;&#23478;&#24237;&#26426;&#22120;&#20154;&#21644;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20026;&#21508;&#31181;&#23458;&#25143;&#20219;&#21153;&#35774;&#35745;&#24378;&#22823;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#20445;&#38544;&#31169;&#65292;&#22240;&#20026;&#30001;&#20110;&#20445;&#23494;&#38382;&#39064;&#65292;&#23458;&#25143;&#25968;&#25454;&#20849;&#20139;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#22810;&#23458;&#25143;VQA&#20219;&#21153;&#30340;&#20445;&#23494;&#24615;&#32422;&#26463;&#21644;&#23458;&#25143;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#21333;&#21521;&#20998;&#27495;&#23398;&#20064;&#65288;UniCon&#65289;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#23458;&#25143;&#30340;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#22411;&#20849;&#20139;&#23398;&#20064;&#31934;&#32454;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#35777;&#65292;&#21033;&#29992;&#20998;&#35010;&#23398;&#20064;&#26550;&#26500;&#30830;&#20445;&#38544;&#31169;&#65292;&#20854;&#20013;&#23436;&#25972;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#32452;&#20214;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#19982;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#24230;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) using multi-modal data facilitates real-life applications, such as home robots and medical diagnoses. However, one significant challenge is to design a robust learning method for various client tasks. One critical aspect is to ensure privacy, as client data sharing is limited due to confidentiality concerns. This work focuses on addressing the issue of confidentiality constraints in multi-client VQA tasks and limited labeled training data of clients. We propose the Unidirectional Split Learning with Contrastive Loss (UniCon) method to overcome these limitations. The proposed method trains a global model on the entire data distribution of different clients, learning refined cross-modal representations through model sharing. Privacy is ensured by utilizing a split learning architecture in which a complete model is partitioned into two components for independent training. Moreover, recent self-supervised learning techniques were found to be highly compatibl
&lt;/p&gt;</description></item><item><title>FORBID&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#24418;&#21487;&#35270;&#21270;&#20013;&#22240;&#33410;&#28857;&#24418;&#29366;&#32780;&#24341;&#36215;&#30340;&#37325;&#21472;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#32500;&#25345;&#33410;&#28857;&#25299;&#25169;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#31227;&#38500;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2208.10334</link><description>&lt;p&gt;
FORBID: &#19968;&#31181;&#24555;&#36895;&#31227;&#38500;&#22270;&#32472;&#21046;&#20013;&#37325;&#21472;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FORBID: Fast Overlap Removal By stochastic gradIent Descent for Graph Drawing. (arXiv:2208.10334v2 [cs.CG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10334
&lt;/p&gt;
&lt;p&gt;
FORBID&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#24418;&#21487;&#35270;&#21270;&#20013;&#22240;&#33410;&#28857;&#24418;&#29366;&#32780;&#24341;&#36215;&#30340;&#37325;&#21472;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#32500;&#25345;&#33410;&#28857;&#25299;&#25169;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#31227;&#38500;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#32472;&#21046;&#31639;&#27861;&#23558;&#33410;&#28857;&#35270;&#20026;&#28857;&#65292;&#32780;&#22270;&#24418;&#21487;&#35270;&#21270;&#24037;&#20855;&#32463;&#24120;&#23558;&#23427;&#20204;&#34920;&#31034;&#20026;&#24418;&#29366;&#12290;&#36825;&#20123;&#24418;&#29366;&#25903;&#25345;&#26174;&#31034;&#26631;&#31614;&#25110;&#20351;&#29992;&#22823;&#23567;&#25110;&#39068;&#33394;&#32534;&#30721;&#21508;&#31181;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20250;&#20135;&#29983;&#33410;&#28857;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#36825;&#20250;&#36890;&#36807;&#38544;&#34255;&#37096;&#20998;&#20449;&#24687;&#26469;&#38459;&#30861;&#25506;&#32034;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#28040;&#38500;&#36825;&#20123;&#37325;&#21472;&#20197;&#25552;&#39640;&#22270;&#24418;&#21487;&#35270;&#21270;&#30340;&#21487;&#35835;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#23558;&#37325;&#21472;&#31227;&#38500;&#30475;&#20316;&#26159;&#19968;&#20010;&#32852;&#21512;&#24212;&#21147;&#21644;&#32553;&#25918;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39640;&#25928;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#35777;&#26126;&#20102;&#23427;&#24555;&#36895;&#31227;&#38500;&#37325;&#21472;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22270;&#24418;&#24067;&#23616;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many graph drawing algorithms consider nodes as points, graph visualization tools often represent them as shapes. These shapes support the display of information such as labels or encode various data with size or color. However, they can create overlaps between nodes which hinder the exploration process by hiding parts of the information. It is therefore of utmost importance to remove these overlaps to improve graph visualization readability. If not handled by the layout process, Overlap Removal (OR) algorithms have been proposed as layout post-processing. As graph layouts usually convey information about their topology, it is important that OR algorithms preserve them as much as possible. We propose a novel algorithm that models OR as a joint stress and scaling optimization problem, and leverages efficient stochastic gradient descent. This approach is compared with state-of-the-art algorithms, and several quality metrics demonstrate its efficiency to quickly remove overlaps whil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#25193;&#23637;&#20026;&#20801;&#35768;&#39532;&#23572;&#21487;&#22827;&#38142;&#35266;&#27979;&#65292;&#30740;&#31350;&#20102;&#30456;&#24212;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#31867;&#27604;&#31639;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#24212;&#30340;&#28388;&#27874;&#21644;Viterbi&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.06368</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Markov Observation Models. (arXiv:2208.06368v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#25193;&#23637;&#20026;&#20801;&#35768;&#39532;&#23572;&#21487;&#22827;&#38142;&#35266;&#27979;&#65292;&#30740;&#31350;&#20102;&#30456;&#24212;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#31867;&#27604;&#31639;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#24212;&#30340;&#28388;&#27874;&#21644;Viterbi&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#25193;&#23637;&#20026;&#20801;&#35768;&#39532;&#23572;&#21487;&#22827;&#38142;&#35266;&#27979;&#12290;&#29305;&#21035;&#22320;&#65292;&#20551;&#35774;&#35266;&#27979;&#20540;&#26159;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#38142;&#65292;&#20854;&#19968;&#27493;&#36716;&#31227;&#27010;&#29575;&#20381;&#36182;&#20110;&#38544;&#34255;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#38024;&#23545;&#36825;&#31181;&#26356;&#21152;&#26222;&#36941;&#30340;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#19982;Baum-Welch&#31639;&#27861;&#30340;&#31867;&#27604;&#31639;&#27861;&#65292;&#20197;&#20272;&#35745;&#38544;&#34255;&#29366;&#24577;&#21644;&#35266;&#27979;&#24207;&#21015;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#20197;&#21450;&#20272;&#35745;&#21021;&#22987;&#32852;&#21512;&#38544;&#34255;&#29366;&#24577;-&#35266;&#27979;&#20998;&#24067;&#30340;&#27010;&#29575;&#12290;&#20174;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#30340;&#35745;&#31639;&#20013;&#24471;&#20986;&#20102;&#20449;&#24565;&#29366;&#24577;&#25110;&#28388;&#27874;&#24615;&#36882;&#25512;&#26469;&#36319;&#36394;&#38544;&#34255;&#30340;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#21160;&#24577;&#35268;&#21010;&#31867;&#27604;&#30340;Viterbi&#31639;&#27861;&#65292;&#20197;&#20272;&#35745;&#32473;&#23450;&#35266;&#27979;&#24207;&#21015;&#30340;&#26368;&#21487;&#33021;&#30340;&#38544;&#34255;&#29366;&#24577;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Herein, the Hidden Markov Model is expanded to allow for Markov chain observations. In particular, the observations are assumed to be a Markov chain whose one step transition probabilities depend upon the hidden Markov chain. An Expectation-Maximization analog to the Baum-Welch algorithm is developed for this more general model to estimate the transition probabilities for both the hidden state and for the observations as well as to estimate the probabilities for the initial joint hidden-state-observation distribution. A believe state or filter recursion to track the hidden state then arises from the calculations of this Expectation-Maximization algorithm. A dynamic programming analog to the Viterbi algorithm is also developed to estimate the most likely sequence of hidden states given the sequence of observations.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#35777;&#26126;&#26426;&#21046;PoL&#23384;&#22312;&#19981;&#23569;&#38382;&#39064;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#27450;&#39575;&#31574;&#30053;&#24456;&#23481;&#26131;&#34987;&#25171;&#36133;&#25110;&#26080;&#27861;&#37325;&#29616;&#65292;&#22240;&#27492;&#23545;&#23545;&#25163;&#30340;&#23433;&#20840;&#20445;&#38556;&#19981;&#31283;&#20581;&#12290;&#26032;&#30340;&#27450;&#39575;&#31574;&#30053;&#24341;&#20837;&#21487;&#20197;&#25171;&#30772;PoL&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#25104;&#26412;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2208.03567</link><description>&lt;p&gt;
&#23398;&#20064;&#35777;&#26126;&#26426;&#21046;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Proof-of-Learning is Currently More Broken Than You Think. (arXiv:2208.03567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03567
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35777;&#26126;&#26426;&#21046;PoL&#23384;&#22312;&#19981;&#23569;&#38382;&#39064;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#27450;&#39575;&#31574;&#30053;&#24456;&#23481;&#26131;&#34987;&#25171;&#36133;&#25110;&#26080;&#27861;&#37325;&#29616;&#65292;&#22240;&#27492;&#23545;&#23545;&#25163;&#30340;&#23433;&#20840;&#20445;&#38556;&#19981;&#31283;&#20581;&#12290;&#26032;&#30340;&#27450;&#39575;&#31574;&#30053;&#24341;&#20837;&#21487;&#20197;&#25171;&#30772;PoL&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35777;&#26126;&#65288;PoL&#65289;&#25552;&#20986;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#35760;&#24405;&#35757;&#32451;&#26816;&#26597;&#28857;&#65292;&#20197;&#24314;&#31435;&#20026;&#35757;&#32451;&#32791;&#36153;&#30340;&#35745;&#31639;&#25552;&#20379;&#35777;&#26126;&#12290; PoL&#30340;&#20316;&#32773;&#25918;&#24323;&#20102;&#21152;&#23494;&#26041;&#27861;&#65292;&#20197;&#25442;&#21462;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#25442;&#21462;&#20102;&#20005;&#26684;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#20182;&#20204;&#36890;&#36807;&#23637;&#31034;&#30423;&#29992;&#27169;&#22411;&#30340;&#35745;&#31639;&#35777;&#26126;--&#35745;&#31639;&#20599;&#26469;&#30340;&#27169;&#22411;&#30340;&#35777;&#26126;&#65292;&#21644;&#30495;&#27491;&#22320;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#35201;&#30340;&#35777;&#26126;&#19968;&#26679;&#26114;&#36149;&#26469;&#23454;&#35777;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#20363;&#65292;&#20174;&#32780;&#20351;&#36825;&#20010;&#35266;&#23519;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;PoL&#39564;&#35777;&#23545;&#20110;&#23545;&#25163;&#26469;&#35828;&#19981;&#31283;&#20581;&#26159;&#30495;&#23454;&#30340;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;&#24037;&#20316;&#22823;&#22823;&#20302;&#20272;&#20102;&#36825;&#31181;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#30340;&#27450;&#39575;&#31574;&#30053;&#35201;&#20040;&#19981;&#21487;&#37325;&#29616;&#65292;&#35201;&#20040;&#38024;&#23545;PoL&#30340;&#21066;&#24369;&#24418;&#24335;--&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#26356;&#25913;&#39564;&#35777;&#30340;&#36229;&#21442;&#25968;&#26469;&#25387;&#36133;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#25209;&#27450;&#39575;&#31574;&#30053;&#65292;&#23427;&#20204;&#21487;&#20197;&#25171;&#30772;&#36866;&#29992;&#20110;PoL&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#20195;&#20215;&#24456;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proof-of-Learning (PoL) proposes that a model owner logs training checkpoints to establish a proof of having expended the computation necessary for training. The authors of PoL forego cryptographic approaches and trade rigorous security guarantees for scalability to deep learning. They empirically argued the benefit of this approach by showing how spoofing--computing a proof for a stolen model--is as expensive as obtaining the proof honestly by training the model. However, recent work has provided a counter-example and thus has invalidated this observation.  In this work we demonstrate, first, that while it is true that current PoL verification is not robust to adversaries, recent work has largely underestimated this lack of robustness. This is because existing spoofing strategies are either unreproducible or target weakened instantiations of PoL--meaning they are easily thwarted by changing hyperparameters of the verification. Instead, we introduce the first spoofing strategies that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;</title><link>http://arxiv.org/abs/2208.00884</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#21147;&#20998;&#24067;&#20998;&#26512;&#30340;&#23156;&#20799;&#36816;&#21160;&#20998;&#31867;&#8212;&#8212;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#30340;&#38468;&#21152;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation. (arXiv:2208.00884v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#35774;&#22791;&#26469;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26089;&#26399;&#30340;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#26469;&#21306;&#20998;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#65288;&#21363;&#22352;&#31435;&#19981;&#23433;&#36816;&#21160;&#65289;&#19982;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#65288;&#21363;&#25197;&#21160;&#36816;&#21160;&#65289;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#27599;&#20010;&#23156;&#20799;&#22312;&#20986;&#29983;&#21518; 4-16 &#21608;&#30340;&#38388;&#38548;&#26399;&#20869;&#36830;&#32493;&#19971;&#20010;&#23454;&#39564;&#23460;&#20250;&#35805;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21253;&#25324;&#26469;&#33258;&#19968;&#20010; 32x32 &#32593;&#26684;&#21387;&#21147;&#20256;&#24863;&#22443;&#21450;&#20854; 1024 &#20010;&#20256;&#24863;&#22120;&#30340;&#21387;&#21147;&#25968;&#25454;&#12290;&#20026;&#20102;&#39564;&#35777;&#27010;&#24565;&#65292;&#20174;&#20004;&#20010;&#30446;&#26631;&#24180;&#40836;&#27573;&#20013;&#65292;&#27599;&#20010;&#25345;&#32493; 5 &#31186;&#30340; 1776 &#20010;&#21387;&#21147;&#25968;&#25454;&#29255;&#27573;&#34987;&#29992;&#20110;&#36816;&#21160;&#20998;&#31867;&#12290;&#27599;&#20010;&#29255;&#27573;&#37117;&#26159;&#26681;&#25454;&#30456;&#24212;&#30340;&#21516;&#27493;&#35270;&#39057;&#25968;&#25454;&#30001;&#20154;&#24037;&#35780;&#20272;&#21592;&#36827;&#34892;&#39044;&#27880;&#37322;&#30340;&#65292;&#26631;&#35760;&#20026;&#22352;&#31435;&#19981;&#23433;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at objective early detection of neuromotor disorders such as cerebral palsy, we proposed an innovative non-intrusive approach using a pressure sensing device to classify infant general movements (GMs). Here, we tested the feasibility of using pressure data to differentiate typical GM patterns of the ''fidgety period'' (i.e., fidgety movements) vs. the ''pre-fidgety period'' (i.e., writhing movements). Participants (N = 45) were sampled from a typically-developing infant cohort. Multi-modal sensor data, including pressure data from a 32x32-grid pressure sensing mat with 1024 sensors, were prospectively recorded for each infant in seven succeeding laboratory sessions in biweekly intervals from 4-16 weeks of post-term age. For proof-of-concept, 1776 pressure data snippets, each 5s long, from the two targeted age periods were taken for movement classification. Each snippet was pre-annotated based on corresponding synchronised video data by human assessors as either fidgety present (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20998;&#26512;&#21644;&#25968;&#20540;&#26041;&#27861;&#30740;&#31350;&#20102;&#38543;&#26426;&#34892;&#36208;Metroplolis&#26041;&#26696;&#20013;&#21521;&#31283;&#23450;&#29366;&#24577;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#21457;&#29616;&#38543;&#26426;&#28459;&#27493;&#20013;&#30340;&#23581;&#35797;&#36339;&#21464;&#37327;&#29305;&#24449;&#38271;&#24230;&#21487;&#20197;&#21576;&#29616;&#20026;&#23616;&#37096;&#21270;&#36716;&#21464;&#30340;&#24418;&#24335;&#65292;&#25910;&#25947;&#21518;&#21450;&#23616;&#37096;&#21270;&#36716;&#21464;&#21069;&#30340;&#24347;&#35947;&#20998;&#21035;&#21463;&#25193;&#25955;&#21644;&#8230;&#65288;&#35770;&#25991;&#37325;&#28857;&#65289;</title><link>http://arxiv.org/abs/2207.10488</link><description>&lt;p&gt;
Metroplolis Monte Carlo&#37319;&#26679;: &#25910;&#25947;&#24615;&#12289;&#23616;&#22495;&#21270;&#36716;&#21464;&#21450;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Metropolis Monte Carlo sampling: convergence, localization transition and optimality. (arXiv:2207.10488v4 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20998;&#26512;&#21644;&#25968;&#20540;&#26041;&#27861;&#30740;&#31350;&#20102;&#38543;&#26426;&#34892;&#36208;Metroplolis&#26041;&#26696;&#20013;&#21521;&#31283;&#23450;&#29366;&#24577;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#21457;&#29616;&#38543;&#26426;&#28459;&#27493;&#20013;&#30340;&#23581;&#35797;&#36339;&#21464;&#37327;&#29305;&#24449;&#38271;&#24230;&#21487;&#20197;&#21576;&#29616;&#20026;&#23616;&#37096;&#21270;&#36716;&#21464;&#30340;&#24418;&#24335;&#65292;&#25910;&#25947;&#21518;&#21450;&#23616;&#37096;&#21270;&#36716;&#21464;&#21069;&#30340;&#24347;&#35947;&#20998;&#21035;&#21463;&#25193;&#25955;&#21644;&#8230;&#65288;&#35770;&#25991;&#37325;&#28857;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;&#20351;&#29992;&#20998;&#26512;&#21644;&#25968;&#20540;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#34892;&#36208;Metroplolis&#26041;&#26696;&#20013;&#21521;&#31283;&#23450;&#29366;&#24577;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#20998;&#26512;&#19968;&#20123;&#36275;&#22815;&#31616;&#21333;&#20197;&#23454;&#29616;&#20998;&#26512;&#36827;&#23637;&#30340;&#27169;&#22411;&#31639;&#27861;&#30340;&#24347;&#35947;&#24615;&#36136;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20174;&#30446;&#26631;&#31283;&#24577;&#20998;&#24067;&#20559;&#24046;&#30340;&#29305;&#24449;&#38271;&#24230;&#20316;&#20026;&#23450;&#20041;&#38543;&#26426;&#28459;&#27493;&#30340;&#23581;&#35797;&#36339;&#21464;&#37327;&#21487;&#20197;&#21576;&#29616;&#20026;&#23616;&#37096;&#21270;&#36716;&#21464;&#30340;&#24418;&#24335;&#12290;&#34429;&#28982;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#36845;&#20195;&#23545;&#20110;&#25152;&#26377;&#36339;&#21464;&#37327;&#30340;&#36873;&#25321;&#37117;&#20250;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#65292;&#20294;&#23616;&#37096;&#21270;&#36716;&#21464;&#20250;&#26174;&#33879;&#25913;&#21464;&#22312;&#31639;&#27861;&#26377;&#38480;&#27493;&#21518;&#21040;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#19982;&#30446;&#26631;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#28176;&#36817;&#24418;&#29366;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23616;&#37096;&#21270;&#36716;&#21464;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#24347;&#35947;&#20998;&#21035;&#21463;&#25193;&#25955;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Among random sampling methods, Markov Chain Monte Carlo algorithms are foremost. Using a combination of analytical and numerical approaches, we study their convergence properties towards the steady state, within a random walk Metropolis scheme. Analysing the relaxation properties of some model algorithms sufficiently simple to enable analytic progress, we show that the deviations from the target steady-state distribution can feature a localization transition as a function of the characteristic length of the attempted jumps defining the random walk. While the iteration of the Monte Carlo algorithm converges to equilibrium for all choices of jump parameters, the localization transition changes drastically the asymptotic shape of the difference between the probability distribution reached after a finite number of steps of the algorithm and the target equilibrium distribution. We argue that the relaxation before and after the localisation transition is respectively limited by diffusion and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#29992;&#25143;&#20043;&#38388;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36172;&#21338;&#26426;&#21327;&#21516;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#39640;&#26031;&#36827;&#31243;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#33719;&#24471;&#27425;&#20248;&#36951;&#25022;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.07948</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#30340;&#36172;&#21338;&#26426;&#21327;&#21516;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning in Kernel-based Bandits for Distributed Users. (arXiv:2207.07948v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#29992;&#25143;&#20043;&#38388;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36172;&#21338;&#26426;&#21327;&#21516;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#39640;&#26031;&#36827;&#31243;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#33719;&#24471;&#27425;&#20248;&#36951;&#25022;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#35843;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#24076;&#26395;&#26368;&#22823;&#21270;&#20854;&#20010;&#24615;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26159;&#20854;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#21644;&#20840;&#23616;&#30446;&#26631;&#20989;&#25968;&#30340;&#21152;&#26435;&#21644;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#30452;&#25509;&#35775;&#38382;&#20854;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#30340;&#38543;&#26426;&#36172;&#21338;&#21453;&#39304;&#65292;&#20294;&#21482;&#26377;&#23545;&#20840;&#23616;&#30446;&#26631;&#20989;&#25968;&#30340;&#37096;&#20998;&#35270;&#22270;&#65292;&#24182;&#23545;&#20854;&#20182;&#23458;&#25143;&#31471;&#36827;&#34892;&#20449;&#24687;&#20132;&#27969;&#20197;&#36827;&#34892;&#21327;&#21516;&#23398;&#20064;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20869;&#26680;&#30340;&#36172;&#21338;&#26426;&#26694;&#26550;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#23646;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#39640;&#26031;&#36827;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#65288;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#20869;&#65289;&#30340;&#27425;&#20248;&#36951;&#25022;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21487;&#20197;&#37319;&#29992;GP&#27169;&#22411;&#30340;&#31232;&#30095;&#36924;&#36817;&#26469;&#20943;&#23569;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study collaborative learning among distributed clients facilitated by a central server. Each client is interested in maximizing a personalized objective function that is a weighted sum of its local objective and a global objective. Each client has direct access to random bandit feedback on its local objective, but only has a partial view of the global objective and relies on information exchange with other clients for collaborative learning. We adopt the kernel-based bandit framework where the objective functions belong to a reproducing kernel Hilbert space. We propose an algorithm based on surrogate Gaussian process (GP) models and establish its order-optimal regret performance (up to polylogarithmic factors). We also show that the sparse approximations of the GP models can be employed to reduce the communication overhead across clients.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31867;&#20998;&#31867;&#27861;&#30340;&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;(COCA)&#65292;&#33021;&#22815;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.01472</link><description>&lt;p&gt;
&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Contrastive One-Class Time Series Anomaly Detection. (arXiv:2207.01472v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31867;&#20998;&#31867;&#27861;&#30340;&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;(COCA)&#65292;&#33021;&#22815;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32047;&#31215;&#21644;&#26631;&#31614;&#30340;&#32570;&#22833;&#65292;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31867;&#20998;&#31867;&#27861;&#30340;&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;(COCA)&#65292;&#23427;&#36890;&#36807;&#25152;&#35859;&#30340;"&#24207;&#21015;&#23545;&#27604;"&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accumulation of time-series data and the absence of labels make time-series Anomaly Detection (AD) a self-supervised deep learning task. Single-normality-assumption-based methods, which reveal only a certain aspect of the whole normality, are incapable of tasks involved with a large number of anomalies. Specifically, Contrastive Learning (CL) methods distance negative pairs, many of which consist of both normal samples, thus reducing the AD performance. Existing multi-normality-assumption-based methods are usually two-staged, firstly pre-training through certain tasks whose target may differ from AD, limiting their performance. To overcome the shortcomings, a deep Contrastive One-Class Anomaly detection method of time series (COCA) is proposed by authors, following the normality assumptions of CL and one-class classification. It treats the original and reconstructed representations as the positive pair of negative-sample-free CL, namely "sequence contrast". Next, invariance terms a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#21482;&#26356;&#26032;&#19968;&#20010;&#22352;&#26631;&#22359;&#65292;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#26377;&#30028;&#27425;&#26799;&#24230;&#20551;&#35774;&#65292;&#24182;&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20026;$\widetilde{\mathcal{O}}(1/\sqrt{k})$&#21644;$\mathcal{O}(1/k)$&#12290;</title><link>http://arxiv.org/abs/2206.14981</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#20248;&#21270;&#30340;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Coordinate Subgradient Method for Nonsmooth Optimization. (arXiv:2206.14981v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#21482;&#26356;&#26032;&#19968;&#20010;&#22352;&#26631;&#22359;&#65292;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#26377;&#30028;&#27425;&#26799;&#24230;&#20551;&#35774;&#65292;&#24182;&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20026;$\widetilde{\mathcal{O}}(1/\sqrt{k})$&#21644;$\mathcal{O}(1/k)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#38750;&#20984;&#24615;&#65288;&#38750;&#20809;&#28369;&#24369;&#20984;&#24615;&#65289;&#20248;&#21270;&#38382;&#39064;&#30340;&#8220;&#38543;&#26426;&#22352;&#26631;&#21322;&#26799;&#24230;&#27861;&#8221;&#65288;RCS&#65289;&#12290;RCS&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#22352;&#26631;&#22359;&#36827;&#34892;&#26356;&#26032;&#65292;&#27604;&#26356;&#26032;&#25152;&#26377;&#22352;&#26631;&#26356;&#23454;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#26377;&#30028;&#27425;&#26799;&#24230;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#27604;&#20256;&#32479;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#26356;&#20026;&#36890;&#29992;&#65292;&#20197;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#31181;&#24191;&#20041;&#30340;Lipschitz&#31867;&#22411;&#20551;&#35774;&#23545;RCS&#22312;&#20984;&#21644;&#38750;&#20984;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;$f$&#20026;&#38750;&#20809;&#28369;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#22312;&#26399;&#26395;&#19978;&#24314;&#31435;&#20102;$\widetilde{\mathcal{O}}(1/\sqrt{k})$&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22312;&#27425;&#20248;&#38388;&#38553;&#26041;&#38754;&#30340;$\tilde o(1/\sqrt{k})$&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#22914;&#26524;$f$&#36827;&#19968;&#27493;&#28385;&#36275;&#20840;&#23616;&#20108;&#27425;&#22686;&#38271;&#26465;&#20214;&#65292;&#21017;&#26174;&#31034;&#20102;&#25913;&#36827;&#30340;$\mathcal{O}(1/k)$&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose the {Randomized Coordinate Subgradient method} (RCS) for solving nonsmooth convex and nonsmooth nonconvex (nonsmooth weakly convex) optimization problems. RCS randomly selects one block coordinate to update at each iteration, making it more practical than updating all coordinates. We consider the linearly bounded subgradients assumption for the objective function, which is more general than the traditional Lipschitz continuity assumption, to account for practical scenarios. We then conduct thorough convergence analysis for RCS in both convex and nonconvex cases based on this generalized Lipschitz-type assumption. Specifically, we establish the $\widetilde{\mathcal{O}}(1/\sqrt{k})$ convergence rate in expectation and the $\tilde o(1/\sqrt{k})$ almost sure asymptotic convergence rate in terms of suboptimality gap when $f$ is nonsmooth convex. If $f$ further satisfies the global quadratic growth condition, the improved $\mathcal{O}(1/k)$ rate is shown in terms of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;&#20854;&#20013;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#20027;&#35201;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.10210</link><description>&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#30340;&#34701;&#21512;&#65306;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Integration of Machine Learning into Automated Test Generation: A Systematic Mapping Study. (arXiv:2206.10210v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;&#20854;&#20013;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#20027;&#35201;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21487;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30740;&#31350;&#27979;&#35797;&#23454;&#36341;&#12289;&#30740;&#31350;&#32773;&#30446;&#26631;&#12289;&#24212;&#29992;&#30340;ML&#25216;&#26415;&#12289;&#35780;&#20272;&#21644;&#25361;&#25112;&#31561;&#26041;&#38754;&#30340;&#20852;&#36215;&#30740;&#31350;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#22312;102&#31687;&#35770;&#25991;&#20013;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#26144;&#23556;&#12290;&#32467;&#26524;&#65306;ML&#29983;&#25104;&#31995;&#32479;&#12289;GUI&#12289;&#21333;&#20803;&#12289;&#24615;&#33021;&#21644;&#32452;&#21512;&#27979;&#35797;&#30340;&#36755;&#20837;&#25110;&#25913;&#36827;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;ML&#36824;&#29992;&#20110;&#29983;&#25104;&#27979;&#35797;&#35009;&#20915;&#12289;&#22522;&#20110;&#23646;&#24615;&#30340;&#21644;&#26399;&#26395;&#36755;&#20986;oracle&#12290;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#24120;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#24120;&#22522;&#20110;Q-learning&#65292;&#26159;&#24120;&#35265;&#30340;&#65292;&#26377;&#20123;&#20986;&#29256;&#29289;&#36824;&#37319;&#29992;&#20102;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;(&#21322;/&#26080;)&#30417;&#30563;&#26041;&#27861;&#20351;&#29992;&#20256;&#32479;&#27979;&#35797;&#25351;&#26631;&#21644;ML&#30456;&#20851;&#25351;&#26631;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#19982;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#30340;&#27979;&#35797;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#35770;&#65306;&#36804;&#20170;&#20026;&#27490;&#30340;&#24037;&#20316;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36824;&#25581;&#31034;&#20102;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Machine learning (ML) may enable effective automated test generation.  Objective: We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges.  Methods: We perform a systematic mapping on a sample of 102 publications.  Results: ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property-based, and expected output oracles. Supervised learning - often based on neural networks and reinforcement learning - often based on Q-learning - are common, and some publications also employ unsupervised or semi-supervised learning. (Semi-/Un-)Supervised approaches are evaluated using both traditional testing metrics and ML-related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function.  Conclusion: Work-to-date shows grea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#28145;&#23618;CNN&#20013;&#25552;&#21462;&#8220;&#29305;&#24449;&#20445;&#30041;&#30005;&#36335;&#8221;&#65292;&#36825;&#20123;&#26159;&#33021;&#22815;&#20445;&#30041;&#30446;&#26631;&#29305;&#24449;&#30340;&#23376;&#20989;&#25968;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#36807;&#28388;&#36807;&#31243;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#29992;&#20110;&#21487;&#35270;&#21270;&#30005;&#36335;&#22270;&#20687;&#36807;&#28388;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2206.01627</link><description>&lt;p&gt;
CNN&#20013;&#29992;&#20110;&#29305;&#24449;&#20445;&#30041;&#30005;&#36335;&#30340;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Pruning for Feature-Preserving Circuits in CNNs. (arXiv:2206.01627v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#28145;&#23618;CNN&#20013;&#25552;&#21462;&#8220;&#29305;&#24449;&#20445;&#30041;&#30005;&#36335;&#8221;&#65292;&#36825;&#20123;&#26159;&#33021;&#22815;&#20445;&#30041;&#30446;&#26631;&#29305;&#24449;&#30340;&#23376;&#20989;&#25968;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#36807;&#28388;&#36807;&#31243;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#29992;&#20110;&#21487;&#35270;&#21270;&#30005;&#36335;&#22270;&#20687;&#36807;&#28388;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#24378;&#21147;&#27169;&#22411;&#31867;&#65292;&#20294;&#26159;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#24222;&#22823;&#65292;&#38590;&#20197;&#35299;&#37322;&#23427;&#20204;&#23454;&#29616;&#30340;&#22270;&#20687;&#36807;&#28388;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#28145;&#23618;CNN&#20013;&#25552;&#21462;&#8220;&#29305;&#24449;&#20445;&#30041;&#30005;&#36335;&#8221;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#32593;&#32476;&#20013;&#30340;&#27169;&#22359;&#21270;&#23376;&#20989;&#25968;&#65292;&#20165;&#21253;&#21547;&#19982;&#30446;&#26631;&#29305;&#24449;&#30456;&#20851;&#30340;&#37096;&#20998;&#21367;&#31215;&#26680;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#26631;&#20934;&#25552;&#21462;&#36825;&#20123;&#31232;&#30095;&#30005;&#36335;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#21462;"&#23376;&#29305;&#24449;"&#30005;&#36335;&#65292;&#23427;&#20204;&#33021;&#20445;&#30041;&#29305;&#23450;&#22270;&#20687;&#23545;&#26576;&#20010;&#29305;&#24449;&#30340;&#21453;&#24212;&#65292;&#23558;&#35813;&#29305;&#24449;&#20998;&#25104;&#26356;&#31232;&#30095;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#8220;&#30005;&#36335;&#22270;&#8221;&#24037;&#20855;&#65292;&#20197;&#21487;&#35299;&#26512;&#30340;&#26684;&#24335;&#21576;&#29616;&#30005;&#36335;&#23454;&#29616;&#30340;&#25972;&#20010;&#22270;&#20687;&#36807;&#28388;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural networks are a powerful model class for a range of computer vision problems, but it is difficult to interpret the image filtering process they implement, given their sheer size. In this work, we introduce a method for extracting 'feature-preserving circuits' from deep CNNs, leveraging methods from saliency-based neural network pruning. These circuits are modular sub-functions, embedded within the network, containing only a subset of convolutional kernels relevant to a target feature. We compare the efficacy of 3 saliency-criteria for extracting these sparse circuits. Further, we show how 'sub-feature' circuits can be extracted, that preserve a feature's responses to particular images, dividing the feature into even sparser filtering processes. We also develop a tool for visualizing 'circuit diagrams', which render the entire image filtering process implemented by circuits in a parsable format.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.13589</link><description>&lt;p&gt;
&#38754;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#24754;&#35266;&#24773;&#32490;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35777;&#26126;&#26377;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#30001;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#33021;&#21462;&#20915;&#20110;&#28508;&#22312;&#29366;&#24577;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#28151;&#28102;&#24847;&#20041;&#19978;&#21516;&#26102;&#24433;&#21709;&#34892;&#21160;&#21644;&#35266;&#27979;&#20540;&#65292;&#36825;&#23545;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#30340;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#30340;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#65288;P3O&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24191;&#20041;&#20989;&#25968;&#36924;&#36817;&#30340;&#19978;&#19979;&#25991;&#20013;&#35299;&#20915;&#20102;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28151;&#28102;&#25968;&#25454;&#38598;&#30340;&#37096;&#20998;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;P3O&#21487;&#20197;&#23454;&#29616;n^{-1/2}&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#39550;&#39542;&#21592;&#23545;&#20110;&#32473;&#23450;&#34892;&#31243;&#20013;&#33021;&#28304;&#21487;&#29992;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#27169;&#30005;&#27744;&#33021;&#37327;&#28040;&#32791;&#65292;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25552;&#39640;&#23545;&#36710;&#36742;&#24615;&#33021;&#30340;&#20449;&#20219;&#24182;&#25552;&#21319;&#20854;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12825</link><description>&lt;p&gt;
&#38754;&#21521;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#30005;&#27744;&#33021;&#37327;&#28040;&#32791;&#30340;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Prediction of Battery Energy Consumption for Hybrid Electric Vehicles. (arXiv:2204.12825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12825
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#39550;&#39542;&#21592;&#23545;&#20110;&#32473;&#23450;&#34892;&#31243;&#20013;&#33021;&#28304;&#21487;&#29992;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#27169;&#30005;&#27744;&#33021;&#37327;&#28040;&#32791;&#65292;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25552;&#39640;&#23545;&#36710;&#36742;&#24615;&#33021;&#30340;&#20449;&#20219;&#24182;&#25552;&#21319;&#20854;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#30340;&#21487;&#29992;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#33021;&#32791;&#12290;&#23588;&#20854;&#26159;&#65292;&#22952;&#30861;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#12289;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#65288;HEV&#65289;&#21644;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#65288;PHEV&#65289;&#22823;&#35268;&#27169;&#37319;&#29992;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#26159;&#39550;&#39542;&#21592;&#23545;&#20110;&#32473;&#23450;&#34892;&#31243;&#20013;&#33021;&#28304;&#21487;&#29992;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#27169;&#30005;&#27744;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#36710;&#36742;&#24615;&#33021;&#30340;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#21319;&#20854;&#21487;&#29992;&#24615;&#12290;&#22823;&#22810;&#25968;&#30456;&#20851;&#24037;&#20316;&#20391;&#37325;&#20110;&#24433;&#21709;&#33021;&#37327;&#28040;&#32791;&#30340;&#30005;&#27744;&#30340;&#29289;&#29702;&#21644;/&#25110;&#21270;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21253;&#25324;&#30005;&#27744;&#30456;&#20851;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usability of vehicles is highly dependent on their energy consumption. In particular, one of the main factors hindering the mass adoption of electric (EV), hybrid (HEV), and plug-in hybrid (PHEV) vehicles is range anxiety, which occurs when a driver is uncertain about the availability of energy for a given trip. To tackle this problem, we propose a machine learning approach for modeling the battery energy consumption. By reducing predictive uncertainty, this method can help increase trust in the vehicle's performance and thus boost its usability. Most related work focuses on physical and/or chemical models of the battery that affect the energy consumption. We propose a data-driven approach which relies on real-world datasets including battery related attributes. Our approach showed an improvement in terms of predictive uncertainty as well as in accuracy compared to traditional methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11740</link><description>&lt;p&gt;
&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#23545;&#20851;&#38190;&#26399;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#23454;&#29616;&#31361;&#35302;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#12290;&#65288;arXiv: 2203.11740v12 [cs.NE] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#31361;&#35302;&#20849;&#20139;&#36830;&#25509;&#26435;&#37325;&#20043;&#22806;&#65292;PNN&#36824;&#21253;&#25324;&#31361;&#35302;&#26377;&#25928;&#33539;&#22260;&#30340;&#26435;&#37325;[14-25]&#12290;PNN&#32771;&#34385;&#31361;&#35302;&#24378;&#24230;&#24179;&#34913;&#22312;&#31361;&#35302;&#21534;&#22124;&#30340;&#21160;&#24577;&#21644;&#38271;&#24230;&#24120;&#25968;&#20043;&#21644;&#30340;&#38745;&#24577;&#20013;[14]&#65292;&#24182;&#21253;&#21547;&#20102;&#40060;&#32676;&#34892;&#20026;&#30340;&#20808;&#23548;&#34892;&#20026;&#12290;&#31361;&#35302;&#24418;&#25104;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#20250;&#25233;&#21046;&#26641;&#31361;&#29983;&#25104;[15]&#12290;&#31867;&#20284;&#20110;Spring Boot&#20013;&#30340;&#24378;&#21046;&#38887;&#24615;&#65292;&#21453;&#21521;&#22238;&#36335;&#30340;&#35760;&#24518;&#25345;&#20037;&#24230;&#26799;&#24230;&#20063;&#23384;&#22312;&#12290;&#30456;&#23545;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#26799;&#24230;&#20449;&#24687;&#23384;&#20648;&#22312;&#31867;&#20284;&#20110;&#33041;&#35126;&#30340;&#35760;&#24518;&#30165;&#36857;&#32454;&#32990;&#20013;&#65292;&#22312;&#21453;&#21521;&#22238;&#36335;&#30340;&#31361;&#35302;&#24418;&#25104;&#20013;&#12290;&#20105;&#35758;&#35748;&#20026;&#20154;&#31867;&#28023;&#39532;&#31070;&#32463;&#20803;&#30340;&#20877;&#29983;&#33021;&#21147;&#26159;&#21542;&#25345;&#32493;&#21040;&#32769;&#24180;&#65292;&#24182;&#21487;&#33021;&#22312;&#21518;&#26399;&#36845;&#20195;&#20013;&#24418;&#25104;&#26032;&#30340;&#26356;&#38271;&#30340;&#22238;&#36335;[17,18]&#12290;&#20851;&#38381;&#20851;&#38190;&#26399;&#20250;&#23548;&#33268;&#31070;&#32463;&#32010;&#20081;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;[19]&#12290;&#32771;&#34385;&#21040;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28608;&#27963;&#31361;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;von Neumann&#29109;&#30340;&#26032;&#25351;&#26631;&#26469;&#37325;&#26032;&#23457;&#35270;GNNs&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#20174;&#25972;&#20010;&#37051;&#23621;&#21487;&#35782;&#21035;&#30340;&#35282;&#24230;&#30740;&#31350;&#36328;&#31867;&#36793;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;Conv-Agnostic GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#23621;&#25928;&#24212;&#65292;&#22312;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#22823;&#22810;&#25968;GNN&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.11200</link><description>&lt;p&gt;
&#21033;&#29992;&#37051;&#23621;&#25928;&#24212;&#65306;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#30340;Conv-Agnostic GNNs&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting Neighbor Effect: Conv-Agnostic GNNs Framework for Graphs with Heterophily. (arXiv:2203.11200v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;von Neumann&#29109;&#30340;&#26032;&#25351;&#26631;&#26469;&#37325;&#26032;&#23457;&#35270;GNNs&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#20174;&#25972;&#20010;&#37051;&#23621;&#21487;&#35782;&#21035;&#30340;&#35282;&#24230;&#30740;&#31350;&#36328;&#31867;&#36793;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;Conv-Agnostic GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#23621;&#25928;&#24212;&#65292;&#22312;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#22823;&#22810;&#25968;GNN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#23384;&#22312;&#65292;&#22270;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20849;&#35782;&#26159;GNNs&#22312;&#21516;&#36136;&#24615;&#22270;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20855;&#26377;&#35768;&#22810;&#36328;&#31867;&#36793;&#30340;&#24322;&#36136;&#24615;&#22270;&#19978;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#36328;&#31867;&#36793;&#35266;&#28857;&#21644;&#30456;&#20851;&#30340;&#21516;&#36136;&#27604;&#25351;&#26631;&#19981;&#33021;&#24456;&#22909;&#22320;&#35299;&#37322;GNNs&#22312;&#26576;&#20123;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19979;&#30340;&#34920;&#29616;&#65292;&#36825;&#24847;&#21619;&#30528;&#24182;&#38750;&#25152;&#26377;&#36328;&#31867;&#36793;&#23545;GNNs&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;von Neumann&#29109;&#30340;&#26032;&#25351;&#26631;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;GNNs&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#20174;&#25972;&#20010;&#37051;&#23621;&#21487;&#35782;&#21035;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36328;&#31867;&#36793;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Conv-Agnostic GNN&#26694;&#26550;&#65288;CAGNNs&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#33410;&#28857;&#23398;&#20064;&#37051;&#23621;&#25928;&#24212;&#65292;&#22312;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#22823;&#22810;&#25968;GNN&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#33410;&#28857;&#30340;&#29305;&#24449;&#20998;&#35299;&#20026;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#21028;&#21035;&#29305;&#24449;&#21644;&#37051;&#23621;&#29305;&#24449;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#23621;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65292;&#22312;CAGNNs&#26694;&#26550;&#20013;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#37051;&#23621;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the homophily assumption in graph convolution networks (GNNs), a common consensus in the graph node classification task is that GNNs perform well on homophilic graphs but may fail on heterophilic graphs with many inter-class edges. However, the previous inter-class edges perspective and related homo-ratio metrics cannot well explain the GNNs performance under some heterophilic datasets, which implies that not all the inter-class edges are harmful to GNNs. In this work, we propose a new metric based on von Neumann entropy to re-examine the heterophily problem of GNNs and investigate the feature aggregation of inter-class edges from an entire neighbor identifiable perspective. Moreover, we propose a simple yet effective Conv-Agnostic GNN framework (CAGNNs) to enhance the performance of most GNNs on heterophily datasets by learning the neighbor effect for each node. Specifically, we first decouple the feature of each node into the discriminative feature for downstream tasks and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#22238;&#24402;&#26694;&#26550;&#20013;&#30340;&#38477;&#32500;&#19982;Wasserstein&#31283;&#23450;&#24615;&#24212;&#29992;&#65292;&#38024;&#23545;&#22312;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#25311;&#21512;&#22238;&#24402;&#20989;&#25968;&#26102;&#20986;&#29616;&#30340;&#35823;&#24046;&#25512;&#23548;&#20102;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26680;&#22238;&#24402;&#25991;&#29486;&#20013;&#30340;&#20272;&#35745;&#65292;&#25512;&#23548;&#20102;&#20004;&#27493;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2203.09347</link><description>&lt;p&gt;
&#38477;&#32500;&#19982;Wasserstein&#31283;&#23450;&#24615;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#22238;&#24402;&#26694;&#26550;&#20013;&#30340;&#38477;&#32500;&#19982;Wasserstein&#31283;&#23450;&#24615;&#24212;&#29992;&#65292;&#38024;&#23545;&#22312;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#25311;&#21512;&#22238;&#24402;&#20989;&#25968;&#26102;&#20986;&#29616;&#30340;&#35823;&#24046;&#25512;&#23548;&#20102;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26680;&#22238;&#24402;&#25991;&#29486;&#20013;&#30340;&#20272;&#35745;&#65292;&#25512;&#23548;&#20102;&#20004;&#27493;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#22238;&#24402;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26420;&#32032;&#30340;&#20004;&#27493;&#27861;&#65292;&#39318;&#20808;&#38477;&#20302;&#36755;&#20837;&#21464;&#37327;&#30340;&#32500;&#25968;&#65292;&#20877;&#20351;&#29992;&#26680;&#22238;&#24402;&#26469;&#39044;&#27979;&#36755;&#20986;&#21464;&#37327;&#12290;&#20026;&#20102;&#20998;&#26512;&#30001;&#27492;&#20135;&#29983;&#30340;&#22238;&#24402;&#35823;&#24046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#38024;&#23545;Wasserstein&#36317;&#31163;&#30340;&#26032;&#30340;&#26680;&#22238;&#24402;&#31283;&#23450;&#24615;&#32467;&#26524;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#38480;&#21046;&#24403;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#25311;&#21512;&#22238;&#24402;&#20989;&#25968;&#26102;&#20986;&#29616;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#36890;&#29992;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#24212;&#29992;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26680;&#22238;&#24402;&#25991;&#29486;&#20013;&#30340;&#20272;&#35745;&#65292;&#25512;&#23548;&#20986;&#20102;&#20004;&#27493;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21518;&#32773;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable with kernel regression. In order to analyze the resulting regression errors, a novel stability result for kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit the regression function. We apply the general stability result to principal component analysis (PCA). Exploiting known estimates from the literature on both principal component analysis and kernel regression, we deduce convergence rates for the two-step procedure. The latter turns out to be particularly useful in a semi-supervised setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;CIDER&#65292;&#21487;&#20197;&#21033;&#29992;&#36229;&#29699;&#23884;&#20837;&#23454;&#29616;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20248;&#21270;&#20004;&#20010;&#25439;&#22833;&#20197;&#20419;&#36827;&#24378;&#20869;&#37096;-&#22806;&#37096;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2203.04450</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#36229;&#29699;&#23884;&#20837;&#23454;&#29616;&#23884;&#20837;&#31354;&#38388;&#30340;OOV&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?. (arXiv:2203.04450v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;CIDER&#65292;&#21487;&#20197;&#21033;&#29992;&#36229;&#29699;&#23884;&#20837;&#23454;&#29616;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20248;&#21270;&#20004;&#20010;&#25439;&#22833;&#20197;&#20419;&#36827;&#24378;&#20869;&#37096;-&#22806;&#37096;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#22522;&#20110;&#36317;&#31163;&#30340;OOD&#26816;&#27979;&#65292;&#20854;&#20013;&#22914;&#26524;&#27979;&#35797;&#26679;&#26412;&#19982;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#31867;&#30340;&#36136;&#24515;&#25110;&#21407;&#22411;&#30456;&#23545;&#36739;&#36828;&#65292;&#21017;&#26816;&#27979;&#21040;&#36825;&#20123;&#26679;&#26412;&#20026;OOD&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#29616;&#25104;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20998;&#31867;ID&#26679;&#26412;&#65292;&#20294;&#22312;&#27979;&#35797;&#36755;&#20837;&#21253;&#21547;OOD&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20855;&#26377;&#26368;&#20248;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CIDER&#65292;&#19968;&#31181;&#21033;&#29992;&#36229;&#29699;&#23884;&#20837;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;CIDER&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#25439;&#22833;&#20197;&#20419;&#36827;&#24378;ID-OOD&#21487;&#20998;&#24615;&#65306;&#19968;&#31181;&#20998;&#25955;&#25439;&#22833;&#65292;&#20419;&#36827;&#19981;&#21516;&#31867;&#21035;&#21407;&#22411;&#20043;&#38388;&#30340;&#22823;&#35282;&#36317;&#31163;&#65292;&#20197;&#21450;&#19968;&#31181;&#32039;&#20945;&#25439;&#22833;&#65292;&#40723;&#21169;&#26679;&#26412;&#38752;&#36817;&#20854;&#31867;&#21035;&#21407;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#24182;&#24314;&#31435;&#20102;&#36229;&#29699;&#31354;&#38388;&#23884;&#20837;&#23646;&#24615;&#19982;OOD&#26816;&#27979;&#24615;&#33021;&#20043;&#38388;&#23578;&#26410;&#24320;&#21457;&#30340;&#20851;&#31995;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CIDER&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;OOD&#26816;&#27979;&#24615;&#33021;&#30340;&#26032;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.06891</link><description>&lt;p&gt;
&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference for sequential experiments. (arXiv:2202.06891v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38024;&#23545;&#36830;&#32493;&#35774;&#35745;&#23454;&#39564;&#36827;&#34892;&#30340;&#20107;&#21518;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#27492;&#23454;&#39564;&#20013;&#65292;&#22810;&#20010;&#21333;&#20301;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#20998;&#37197;&#27835;&#30103;&#65292;&#24182;&#20351;&#29992;&#38543;&#26102;&#38388;&#32780;&#36866;&#24212;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23545;&#36866;&#24212;&#24615;&#27835;&#30103;&#31574;&#30053;&#20570;&#20986;&#26368;&#23569;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#26368;&#23567;&#21487;&#33021;&#35268;&#27169;&#30340;&#21453;&#20107;&#23454;&#22343;&#20540;&#25552;&#20379;&#25512;&#26029;&#20445;&#35777;&#65292;&#21363;&#22312;&#27599;&#20010;&#21333;&#20301;&#21644;&#27599;&#20010;&#26102;&#38388;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#27835;&#30103;&#30340;&#24179;&#22343;&#32467;&#26524;&#12290;&#22312;&#27809;&#26377;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#26410;&#30693;&#21464;&#37327;&#27604;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#28857;&#36824;&#22810;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#21453;&#20107;&#23454;&#22343;&#20540;&#19978;&#65292;&#35813;&#27169;&#22411;&#20316;&#20026;&#38750;&#21442;&#25968;&#24418;&#24335;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#20197;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#30340;&#21452;&#32447;&#24615;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#20272;&#35745;&#65292;&#21363;&#26368;&#36817;&#37051;&#30340;&#21464;&#20307;&#65292;&#24182;&#20026;&#27599;&#20010;&#21333;&#20301;&#21644;&#27599;&#20010;&#26102;&#38388;&#30340;&#21453;&#20107;&#23454;&#22343;&#20540;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#39640;&#27010;&#29575;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider after-study statistical inference for sequentially designed experiments wherein multiple units are assigned treatments for multiple time points using treatment policies that adapt over time. Our goal is to provide inference guarantees for the counterfactual mean at the smallest possible scale -- mean outcome under different treatments for each unit and each time -- with minimal assumptions on the adaptive treatment policy. Without any structural assumptions on the counterfactual means, this challenging task is infeasible due to more unknowns than observed data points. To make progress, we introduce a latent factor model over the counterfactual means that serves as a non-parametric generalization of the non-linear mixed effects model and the bilinear latent factor model considered in prior works. For estimation, we use a non-parametric method, namely a variant of nearest neighbors, and establish a non-asymptotic high probability error bound for the counterfactual mean for ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21892;&#24847;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#39044;&#27979;&#22120;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20559;&#21521;&#20110;&#19981;&#19968;&#33268;&#35299;&#30340;&#35777;&#26126;&#65292;&#20174;&#32780;&#35828;&#26126;&#21892;&#24847;&#36807;&#25311;&#21512;&#19981;&#20250;&#21457;&#29983;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#26631;&#20934;&#32447;&#24615;&#22238;&#24402;&#20197;&#22806;&#12290;</title><link>http://arxiv.org/abs/2201.11489</link><description>&lt;p&gt;
&#21892;&#24847;&#36807;&#25311;&#21512;&#30340;&#38544;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Benign Overfitting. (arXiv:2201.11489v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21892;&#24847;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#39044;&#27979;&#22120;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20559;&#21521;&#20110;&#19981;&#19968;&#33268;&#35299;&#30340;&#35777;&#26126;&#65292;&#20174;&#32780;&#35828;&#26126;&#21892;&#24847;&#36807;&#25311;&#21512;&#19981;&#20250;&#21457;&#29983;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#26631;&#20934;&#32447;&#24615;&#22238;&#24402;&#20197;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#29616;&#35937;&#20013;&#30340;&#21892;&#24847;&#36807;&#25311;&#21512;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#23436;&#32654;&#22320;&#25311;&#21512;&#20102;&#24102;&#26377;&#22122;&#22768;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#19968;&#29616;&#35937;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#38500;&#20102;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#22806;&#65292;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20309;&#26102;&#21487;&#20197;&#25110;&#19981;&#33021;&#26399;&#26395;&#21892;&#24847;&#36807;&#25311;&#21512;&#21457;&#29983;&#30340;&#33509;&#24178;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20856;&#22411;&#19988;&#30456;&#24403;&#36890;&#29992;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#21892;&#24847;&#36807;&#25311;&#21512;&#25968;&#25454;&#27169;&#22411;&#65292;&#20854;&#20013;&#23558;&#26576;&#20010;&#22266;&#23450;&#32500;&#24230; $k$ &#30340;&#20219;&#24847;&#36755;&#20837;&#20998;&#24067;&#19982;&#39640;&#32500;&#20998;&#24067;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#23545;&#20110;&#38750;&#24517;&#39035;&#32463;&#36807;&#33391;&#22909;&#35268;&#23450;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25105;&#20204;&#35777;&#26126;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#39044;&#27979;&#22120;&#65288;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#25152;&#25910;&#25947;&#21040;&#30340;&#65289;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#20559;&#21521;&#20110;&#19981;&#19968;&#33268;&#30340;&#35299;&#30340;&#65292;&#22240;&#27492;&#36890;&#24120;&#19981;&#20250;&#21457;&#29983;&#21892;&#24847;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;&#26631;&#20934;&#32447;&#24615;&#22238;&#24402;&#20197;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining near-optimal expected loss, has received much attention in recent years, but still remains not fully understood beyond well-specified linear regression setups. In this paper, we provide several new results on when one can or cannot expect benign overfitting to occur, for both regression and classification tasks. We consider a prototypical and rather generic data model for benign overfitting of linear predictors, where an arbitrary input distribution of some fixed dimension $k$ is concatenated with a high-dimensional distribution. For linear regression which is not necessarily well-specified, we show that the minimum-norm interpolating predictor (that standard training methods converge to) is biased towards an inconsistent solution in general, hence benign overfitting will generally not occur. Moreover, we show how this can be extended beyond standard linear regression, by an argum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340; IP &#22320;&#29702;&#23450;&#20301;&#26694;&#26550; GNN-Geo&#65292;&#23427;&#36890;&#36807;&#23545;&#36830;&#25509;&#36827;&#34892;&#24314;&#27169;&#26469;&#32454;&#21270;&#33410;&#28857;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.10767</link><description>&lt;p&gt;
GNN-Geo: &#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32454;&#31890;&#24230;IP&#22320;&#29702;&#23450;&#20301;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNN-Geo: A Graph Neural Network-based Fine-grained IP geolocation Framework. (arXiv:2112.10767v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340; IP &#22320;&#29702;&#23450;&#20301;&#26694;&#26550; GNN-Geo&#65292;&#23427;&#36890;&#36807;&#23545;&#36830;&#25509;&#36827;&#34892;&#24314;&#27169;&#26469;&#32454;&#21270;&#33410;&#28857;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#32454;&#31890;&#24230;IP&#22320;&#29702;&#23450;&#20301;&#26041;&#27861;&#22312;&#19981;&#36981;&#24490;&#20551;&#35774;&#35268;&#21017;&#30340;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#24456;&#38590;&#27010;&#25324;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#34987;&#23581;&#35797;&#29992;&#20110;&#22686;&#21152;&#27010;&#25324;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;MLP &#19981;&#36866;&#21512;&#20110;&#22788;&#29702;&#31867;&#20284;&#20110;&#32593;&#32476;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;MLP &#25226; IP &#22320;&#22336;&#35270;&#20026;&#23396;&#31435;&#30340;&#23454;&#20363;&#65292;&#24573;&#30053;&#36830;&#25509;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#22320;&#29702;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#19968;&#31181;&#26032;&#20852;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#22686;&#21152;&#27010;&#25324;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#23558; IP &#22320;&#29702;&#23450;&#20301;&#37325;&#26032;&#26500;&#36896;&#20026;&#23646;&#24615;&#22270;&#33410;&#28857;&#22238;&#24402;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; GNN &#30340; IP &#22320;&#29702;&#23450;&#20301;&#26694;&#26550;&#65292;&#31216;&#20026; GNN-Geo&#12290;GNN-Geo &#30001;&#39044;&#22788;&#29702;&#22120;&#12289;&#32534;&#30721;&#22120;&#12289;&#28040;&#24687;&#20256;&#36882;&#65288;MP&#65289;&#23618;&#21644;&#35299;&#30721;&#22120;&#32452;&#25104;&#12290;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#23558;&#27979;&#37327;&#25968;&#25454;&#36716;&#25442;&#20026;&#21021;&#22987;&#33410;&#28857;&#23884;&#20837;&#12290;MP &#23618;&#36890;&#36807;&#23545;&#36830;&#25509;&#36827;&#34892;&#24314;&#27169;&#26469;&#32454;&#21270;&#21021;&#22987;&#33410;&#28857;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based fine-grained IP geolocation methods are hard to generalize in computer networks which do not follow hypothetical rules. Recently, deep learning methods, like multi-layer perceptron (MLP), are tried to increase generalization capabilities. However, MLP is not so suitable for graph-structured data like networks. MLP treats IP addresses as isolated instances and ignores the connection information, which limits geolocation accuracy. In this work, we research how to increase the generalization capability with an emerging graph deep learning method -- Graph Neural Network (GNN). First, IP geolocation is re-formulated as an attributed graph node regression problem. Then, we propose a GNN-based IP geolocation framework named GNN-Geo. GNN-Geo consists of a preprocessor, an encoder, messaging passing (MP) layers and a decoder. The preprocessor and encoder transform measurement data into the initial node embeddings. MP layers refine the initial node embeddings by modeling the connectio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#65292;&#22312;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#25913;&#21892;&#21335;&#32654;&#21335;&#37096;&#23433;&#31532;&#26031;&#22320;&#21306;&#21476;&#22478;&#21270;&#37492;&#23450;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#26631;&#27880;&#21644;&#26410;&#26631;&#27880;&#26679;&#26412;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#32553;&#30701;&#23545;&#20559;&#36828;&#22320;&#21306;&#32771;&#21476;&#36951;&#23384;&#37492;&#23450;&#30340;&#24037;&#26102;&#21644;&#36153;&#29992;&#12290;</title><link>http://arxiv.org/abs/2112.06437</link><description>&lt;p&gt;
&#36965;&#24863;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65306;&#37492;&#23450;&#21335;&#32654;&#21335;&#37096;&#23433;&#31532;&#26031;&#21476;&#22478;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Contrastive Learning for Remote Sensing: Identifying Ancient Urbanization in the South Central Andes. (arXiv:2112.06437v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#65292;&#22312;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#25913;&#21892;&#21335;&#32654;&#21335;&#37096;&#23433;&#31532;&#26031;&#22320;&#21306;&#21476;&#22478;&#21270;&#37492;&#23450;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#26631;&#27880;&#21644;&#26410;&#26631;&#27880;&#26679;&#26412;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#32553;&#30701;&#23545;&#20559;&#36828;&#22320;&#21306;&#32771;&#21476;&#36951;&#23384;&#37492;&#23450;&#30340;&#24037;&#26102;&#21644;&#36153;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#21476;&#23398;&#22312;&#37319;&#26679;&#21644;&#27604;&#20363;&#34920;&#31034;&#19978;&#23384;&#22312;&#26681;&#26412;&#38382;&#39064;&#65292;&#24314;&#31435;&#23450;&#23621;&#27169;&#24335;&#30340;&#20174;&#22320;&#21306;&#21040;&#21306;&#22495;&#30340;&#35270;&#22270;&#36890;&#24120;&#38656;&#31995;&#32479;&#27493;&#34892;&#21208;&#27979;&#12290;&#32780;&#21355;&#26143;&#21644;&#33322;&#31354;&#24433;&#20687;&#30340;&#31995;&#32479;&#25163;&#21160;&#21208;&#27979;&#65292;&#22312;&#36328;&#22320;&#21306;&#23610;&#24230;&#19979;&#21487;&#20197;&#21576;&#29616;&#21382;&#21490;&#36951;&#23384;&#29616;&#35937;&#30340;&#20998;&#24067;&#35270;&#22270;&#65292;&#20294;&#36825;&#31181;&#31895;&#26292;&#26041;&#27861;&#21364;&#24448;&#24448;&#32791;&#26102;&#12289;&#32791;&#21147;&#19988;&#23545;&#25935;&#24863;&#24230;&#21644;&#29305;&#24322;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#27880;&#21355;&#26143;&#21644;&#21382;&#21490;&#31354;&#20013;&#24433;&#20687;&#23450;&#20301;&#32771;&#21476;&#36951;&#23384;&#30340;&#21487;&#25193;&#23637;&#23398;&#20064;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#32771;&#21476;&#36951;&#23384;&#30456;&#23545;&#20110;&#26223;&#35266;&#25972;&#20307;&#32780;&#35328;&#26497;&#20026;&#24494;&#23567;&#65292;&#30446;&#21069;&#30340;&#23545;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#25913;&#21892;&#23545;&#21335;&#32654;&#21335;&#37096;&#23433;&#31532;&#26031;&#21476;&#22478;&#21270;&#30340;&#37492;&#23450;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26631;&#27880;&#21644;&#26410;&#26631;&#27880;&#26679;&#26412;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#21487;&#26497;&#22823;&#22320;&#32553;&#30701;&#23545;&#20559;&#36828;&#22320;&#21306;&#32771;&#21476;&#36951;&#23384;&#37492;&#23450;&#30340;&#24037;&#26102;&#21644;&#36153;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Archaeology has long faced fundamental issues of sampling and scalar representation. Traditionally, the local-to-regional-scale views of settlement patterns are produced through systematic pedestrian surveys. Recently, systematic manual survey of satellite and aerial imagery has enabled continuous distributional views of archaeological phenomena at interregional scales. However, such 'brute force' manual imagery survey methods are both time- and labor-intensive, as well as prone to inter-observer differences in sensitivity and specificity. The development of self-supervised learning methods offers a scalable learning scheme for locating archaeological features using unlabeled satellite and historical aerial images. However, archaeological features are generally only visible in a very small proportion relative to the landscape, while the modern contrastive-supervised learning approach typically yields an inferior performance on highly imbalanced datasets. In this work, we propose a fram
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TraVLR&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;V+L&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#25968;&#25454;&#38598;&#21512;&#25104;&#65292;&#21253;&#25324;&#22235;&#20010;V+L&#25512;&#29702;&#20219;&#21153;&#65292;&#21516;&#26102;&#20351;&#29992;&#21452;&#27169;&#24335;&#20887;&#20313;&#32534;&#30721;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2111.10756</link><description>&lt;p&gt;
TraVLR: &#29616;&#22312;&#20320;&#30475;&#21040;&#23427;&#20102;&#65292;&#29616;&#22312;&#20320;&#27809;&#30475;&#21040;&#23427;&#20102;&#65281;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#30340;&#21452;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning. (arXiv:2111.10756v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10756
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TraVLR&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;V+L&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#25968;&#25454;&#38598;&#21512;&#25104;&#65292;&#21253;&#25324;&#22235;&#20010;V+L&#25512;&#29702;&#20219;&#21153;&#65292;&#21516;&#26102;&#20351;&#29992;&#21452;&#27169;&#24335;&#20887;&#20313;&#32534;&#30721;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#35270;&#35273;&#35821;&#35328;&#65288;V+L&#65289;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19981;&#33021;&#20805;&#20998;&#35780;&#20272;&#23427;&#20204;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#34920;&#31034;&#35270;&#35273;&#21644;&#35821;&#35328;&#27010;&#24565;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#38024;&#23545;V+L&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;V+L&#22522;&#20934;&#32463;&#24120;&#25253;&#21578;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#24471;&#20998;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450;&#27169;&#22411;&#22833;&#36133;&#21644;&#25104;&#21151;&#30340;&#20855;&#20307;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TraVLR&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;V+L&#25512;&#29702;&#20219;&#21153;&#12290;TraVLR&#30340;&#21512;&#25104;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#20219;&#21153;&#30456;&#20851;&#32500;&#24230;&#38480;&#21046;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#65292;&#20174;&#32780;&#35780;&#20272;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;TraVLR&#20013;&#30340;&#27599;&#20010;&#31034;&#20363;&#37117;&#20197;&#20004;&#31181;&#27169;&#24577;&#20887;&#20313;&#32534;&#30721;&#22330;&#26223;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#26399;&#38388;&#21487;&#20197;&#21024;&#38500;&#25110;&#28155;&#21152;&#20854;&#20013;&#30340;&#20219;&#19968;&#27169;&#24577;&#32780;&#19981;&#20250;&#22833;&#21435;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;V+L&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not adequately evaluate the extent to which they represent visual and linguistic concepts in a unified space. We propose several novel evaluation settings for V+L models, including cross-modal transfer. Furthermore, existing V+L benchmarks often report global accuracy scores on the entire dataset, making it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows us to constrain its training and testing distributions along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. Each example in TraVLR redundantly encodes the scene in two modalities, allowing either to be dropped or added during training or testing without losing relevant information. We compare the performance of four state-of-the-art V+L models,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20004;&#25903;&#22242;&#38431;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#35299;&#65292;&#35777;&#26126;&#20102;&#35745;&#31639;&#36825;&#31867;&#21338;&#24328;&#30340;NE&#26159;&#22256;&#38590;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#31616;&#21333;&#20294;&#38750;&#24179;&#20961;&#30340;&#28216;&#25103;&#22522;&#20934;&#29992;&#20110;&#26816;&#39564;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2111.04178</link><description>&lt;p&gt;
&#20004;&#25903;&#38431;&#20237;&#38646;&#21644;&#21338;&#24328;&#20013;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards convergence to Nash equilibria in two-team zero-sum games. (arXiv:2111.04178v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04178
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20004;&#25903;&#22242;&#38431;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#35299;&#65292;&#35777;&#26126;&#20102;&#35745;&#31639;&#36825;&#31867;&#21338;&#24328;&#30340;NE&#26159;&#22256;&#38590;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#31616;&#21333;&#20294;&#38750;&#24179;&#20961;&#30340;&#28216;&#25103;&#22522;&#20934;&#29992;&#20110;&#26816;&#39564;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20004;&#25903;&#22242;&#38431;&#30005;&#23376;&#31454;&#25216;&#20013;&#30340;&#24212;&#29992;&#21644;&#22810;&#26234;&#33021;&#20307;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#20986;&#33394;&#34920;&#29616;&#25552;&#20986;&#20102;&#26377;&#20851;&#20004;&#25903;&#22242;&#38431;&#21338;&#24328;&#20248;&#21270;&#30340;&#37325;&#35201;&#19988;&#34987;&#24573;&#35270;&#30340;&#29702;&#35770;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#35299;&#37322;&#27010;&#24565;&#12290;&#39318;&#20808;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#36825;&#19968;&#31867;&#21338;&#24328;&#65292;&#35745;&#31639;NE&#22312;&#22797;&#26434;&#24230;&#31867; ${\mathrm{CLS}}$ &#20013;&#26159; $\textit{hard}$&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#26816;&#39564;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#31616;&#21333;&#20294;&#38750;&#24179;&#20961;&#30340;&#36825;&#26679;&#30340;&#28216;&#25103;&#22522;&#20934;&#12290;&#36825;&#20123;&#28216;&#25103;&#19981;&#20855;&#22791;&#35777;&#26126;&#30456;&#20851;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#26469;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Contemporary applications of machine learning in two-team e-sports and the superior expressivity of multi-agent generative adversarial networks raise important and overlooked theoretical questions regarding optimization in two-team games. Formally, two-team zero-sum games are defined as multi-player games where players are split into two competing sets of agents, each experiencing a utility identical to that of their teammates and opposite to that of the opposing team. We focus on the solution concept of Nash equilibria (NE). We first show that computing NE for this class of games is $\textit{hard}$ for the complexity class ${\mathrm{CLS}}$. To further examine the capabilities of online learning algorithms in games with full-information feedback, we propose a benchmark of a simple -- yet nontrivial -- family of such games. These games do not enjoy the properties used to prove convergence for relevant algorithms. In particular, we use a dynamical systems perspective to demonstrate that 
&lt;/p&gt;</description></item><item><title>RL4RS&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20351;&#29992;&#20154;&#36896;&#25968;&#25454;&#38598;&#21644;&#21322;&#20223;&#30495;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2110.11073</link><description>&lt;p&gt;
RL4RS&#65306;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender System. (arXiv:2110.11073v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11073
&lt;/p&gt;
&lt;p&gt;
RL4RS&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20351;&#29992;&#20154;&#36896;&#25968;&#25454;&#38598;&#21644;&#21322;&#20223;&#30495;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30446;&#26631;&#22312;&#20110;&#20174;&#19968;&#25209;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#23558;&#25512;&#33616;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#36890;&#24120;&#23384;&#22312;&#24040;&#22823;&#30340;&#29616;&#23454;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#27425;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#8212;&#8212;RL4RS&#65292;&#26088;&#22312;&#21462;&#20195;&#20043;&#21069;RL-based RS&#39046;&#22495;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#32780;&#20351;&#29992;&#30340;&#20154;&#24037;&#21644;&#21322;&#20223;&#30495;RS&#25968;&#25454;&#38598;&#12290;&#19982;&#23398;&#26415;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;RL-based RS&#38754;&#20020;&#30528;&#37096;&#32626;&#21069;&#38656;&#35201;&#36827;&#34892;&#33391;&#22909;&#39564;&#35777;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#23581;&#35797;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#29615;&#22659;&#27169;&#25311;&#35780;&#20272;&#12289;&#29615;&#22659;&#35780;&#20272;&#12289;&#21453;&#20107;&#23454;&#31574;&#30053;&#35780;&#20272;&#20197;&#21450;&#24314;&#31435;&#20110;&#27979;&#35797;&#38598;&#30340;&#29615;&#22659;&#35780;&#20272;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#36164;&#28304;RL4RS&#65288;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#24182;&#23545;&#29616;&#23454;&#24046;&#36317;&#31561;&#29305;&#27530;&#38382;&#39064;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning based recommender systems (RL-based RS) aim at learning a good policy from a batch of collected data, by casting recommendations to multi-step decision-making tasks. However, current RL-based RS research commonly has a large reality gap. In this paper, we introduce the first open-source real-world dataset, RL4RS, hoping to replace the artificial datasets and semi-simulated RS datasets previous studies used due to the resource limitation of the RL-based RS domain. Unlike academic RL research, RL-based RS suffers from the difficulties of being well-validated before deployment. We attempt to propose a new systematic evaluation framework, including evaluation of environment simulation, evaluation on environments, counterfactual policy evaluation, and evaluation on environments built from test set. In summary, the RL4RS (Reinforcement Learning for Recommender Systems), a new resource with special concerns on the reality gaps, contains two real-world datasets, data und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;ML4C&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#20998;&#31867;&#26410;&#23631;&#34109;&#19977;&#20803;&#32452;&#26159;&#21542;&#26159;v-&#32467;&#26500;&#65292;&#24182;&#26500;&#24314;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2110.00637</link><description>&lt;p&gt;
ML4C: &#36890;&#36807;&#28508;&#22312;&#37051;&#22495;&#35266;&#23519;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
ML4C: Seeing Causality Through Latent Vicinity. (arXiv:2110.00637v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;ML4C&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#20998;&#31867;&#26410;&#23631;&#34109;&#19977;&#20803;&#32452;&#26159;&#21542;&#26159;v-&#32467;&#26500;&#65292;&#24182;&#26500;&#24314;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#22240;&#26524;&#23398;&#20064;&#65288;SCL&#65289;&#26088;&#22312;&#36890;&#36807;&#35775;&#38382;&#19982;&#22320;&#38754;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#30456;&#20851;&#30340;&#20808;&#21069;&#30475;&#21040;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#30417;&#30563;&#30340;&#22909;&#22788;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#22914;&#20309;&#21463;&#30410;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22240;&#26524;&#23398;&#20064;&#30340;&#21452;&#38454;&#27573;&#33539;&#20363;&#65292;&#36890;&#36807;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#26469;&#35299;&#20915;SCL&#38382;&#39064;&#12290;&#25353;&#29031;&#36825;&#20010;&#33539;&#20363;&#65292;&#25105;&#20204;&#38024;&#23545;&#31163;&#25955;&#25968;&#25454;&#30340;SCL&#38382;&#39064;&#25552;&#20986;&#20102;ML4C&#12290;ML4C&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#20854;&#26032;&#39062;&#30340;&#23398;&#20064;&#30446;&#26631;&#26159;&#20998;&#31867;&#26410;&#23631;&#34109;&#19977;&#20803;&#32452;&#65288;UT&#65289;&#26159;&#21542;&#26159;v-&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#25552;&#20379;&#20102;&#30456;&#24212;&#39592;&#26550;&#30340;&#36755;&#20837;&#25968;&#25454;&#38598;&#24320;&#22987;&#65292;ML4C&#22312;&#23558;UT&#20998;&#31867;&#20026;v-&#32467;&#26500;&#21518;&#23545;&#20854;&#36827;&#34892;&#21462;&#21521;&#12290;&#36825;&#20123;v-&#32467;&#26500;&#19968;&#36215;&#29992;&#20110;&#26500;&#24314;&#26368;&#32456;&#36755;&#20986;&#12290;&#20026;&#35299;&#20915;&#22522;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#28508;&#22312;&#37051;&#22495;&#35782;&#21035;&#65292;&#36890;&#36807;&#23545;UT&#36827;&#34892;&#20998;&#31867;&#65292;&#26368;&#32456;&#33719;&#24471;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Causal Learning (SCL) aims to learn causal relations from observational data by accessing previously seen datasets associated with ground truth causal relations. This paper presents a first attempt at addressing a fundamental question: What are the benefits from supervision and how does it benefit? Starting from seeing that SCL is not better than random guessing if the learning target is non-identifiable a priori, we propose a two-phase paradigm for SCL by explicitly considering structure identifiability. Following this paradigm, we tackle the problem of SCL on discrete data and propose ML4C. The core of ML4C is a binary classifier with a novel learning target: it classifies whether an Unshielded Triple (UT) is a v-structure or not. Specifically, starting from an input dataset with the corresponding skeleton provided, ML4C orients each UT once it is classified as a v-structure. These v-structures are together used to construct the final output. To address the fundamental que
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;Shamir&#23494;&#38053;&#20998;&#20139;&#30340;&#23433;&#20840;PAC Bayesian&#22238;&#24402;&#21327;&#35758;&#65292;&#36890;&#36807;&#35299;&#20915;&#22810;&#26041;&#35745;&#31639;&#21644;&#25968;&#25454;&#20849;&#20139;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#19981;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#22320;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2109.11200</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;Shamir&#23494;&#38053;&#20998;&#20139;&#30340;&#23433;&#20840;PAC Bayesian&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Secure PAC Bayesian Regression via Real Shamir Secret Sharing. (arXiv:2109.11200v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.11200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;Shamir&#23494;&#38053;&#20998;&#20139;&#30340;&#23433;&#20840;PAC Bayesian&#22238;&#24402;&#21327;&#35758;&#65292;&#36890;&#36807;&#35299;&#20915;&#22810;&#26041;&#35745;&#31639;&#21644;&#25968;&#25454;&#20849;&#20139;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#19981;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#22320;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35782;&#21035;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#24471;&#21040;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#21327;&#35758;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#27169;&#22411;&#65292;&#20381;&#38752;&#26368;&#36817;&#25551;&#36848;&#30340;&#25216;&#26415;&#31216;&#20026;&#23454;&#25968;&#31192;&#23494;&#20849;&#20139;&#12290;&#25105;&#20204;&#20174;PAC Bayesian&#30028;&#24320;&#22987;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#35813;&#21442;&#25968;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;PAC Bayesian&#30028;&#32473;&#20986;&#30340;&#20808;&#39564;&#12290;&#35201;&#33719;&#24471;&#27169;&#22411;&#21442;&#25968;&#65292;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#32447;&#24615;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#22810;&#20010;&#26041;&#25345;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#19988;&#19981;&#24895;&#24847;&#25918;&#24323;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23454;&#25968;&#31192;&#23494;&#20849;&#20139;&#21644;&#22810;&#26041;&#35745;&#31639;&#26469;&#20849;&#20139;&#25968;&#25454;&#24182;&#22312;&#19981;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#22320;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65307;&#19968;&#20010;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
A common approach of system identification and machine learning is to generate a model by using training data to predict the test data instances as accurate as possible. Nonetheless, concerns about data privacy are increasingly raised, but not always addressed. We present a secure protocol for learning a linear model relying on recently described technique called real number secret sharing. We take as our starting point the PAC Bayesian bounds and deduce a closed form for the model parameters which depends on the data and the prior from the PAC Bayesian bounds. To obtain the model parameters one needs to solve a linear system. However, we consider the situation where several parties hold different data instances and they are not willing to give up the privacy of the data. Hence, we suggest to use real number secret sharing and multiparty computation to share the data and solve the linear regression in a secure way without violating the privacy of data. We suggest two methods; a secure 
&lt;/p&gt;</description></item><item><title>FedChain&#26159;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#26041;&#27861;&#21644;&#20840;&#23616;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#25968;&#26041;&#38754;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#21033;&#29992;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.06869</link><description>&lt;p&gt;
FedChain&#65306;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#36817;&#20284;&#26368;&#20248;&#36890;&#20449;&#25104;&#26412;&#30340;&#38142;&#25509;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedChain: Chained Algorithms for Near-Optimal Communication Cost in Federated Learning. (arXiv:2108.06869v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06869
&lt;/p&gt;
&lt;p&gt;
FedChain&#26159;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#26041;&#27861;&#21644;&#20840;&#23616;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#25968;&#26041;&#38754;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#21033;&#29992;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22312;&#20998;&#24067;&#22312;&#35768;&#22810;&#23458;&#25143;&#31471;&#19978;&#30340;&#24322;&#26500;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#26412;&#22320;&#26041;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#22312;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#20043;&#21069;&#23545;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#22810;&#20010;&#20248;&#21270;&#27493;&#39588;&#65288;&#20363;&#22914;&#65292;FedAvg&#65289;&#12290;&#26412;&#22320;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#20998;&#26512;&#20013;&#65292;&#36825;&#26159;&#20197;&#20381;&#36182;&#20110;&#36890;&#20449;&#22238;&#21512;&#25968;R&#30340;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#20026;&#20195;&#20215;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20840;&#23616;&#26041;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20165;&#22312;&#27599;&#20010;&#22238;&#21512;&#36820;&#22238;&#26799;&#24230;&#21521;&#37327;&#65288;&#20363;&#22914;&#65292;SGD&#65289;&#65292;&#22312;R&#26041;&#38754;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#26041;&#38754;&#26356;&#24555;&#65292;&#20294;&#21363;&#20351;&#23458;&#25143;&#31471;&#26159;&#21516;&#26500;&#30340;&#65292;&#20063;&#26080;&#27861;&#21033;&#29992;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;FedChain&#65292;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#26041;&#27861;&#21644;&#20840;&#23616;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#23454;&#29616;&#20102;&#22312;R&#26041;&#38754;&#30340;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#21033;&#29992;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20351;&#29992;FedChain&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#31639;&#27861;&#65292;&#25913;&#21892;&#20102;&#20960;&#20010;&#26631;&#20934;FL&#22522;&#20934;&#27979;&#35797;&#30340;&#24050;&#30693;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FedChain&#22312;&#23458;&#25143;&#31471;&#24322;&#26500;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#24403;&#23458;&#25143;&#31471;&#26159;&#21516;&#26500;&#30340;&#26102;&#20173;&#28982;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg). Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R. On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous. We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while leveraging the similarity between clients. Using FedChain, we instantiate algorithms that improve upon previously kno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#8212;&#8212;&#24046;&#20998;&#35780;&#35770;&#23478;GAN&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#37096;&#20998;&#25968;&#25454;&#38598;&#21253;&#21547;&#25152;&#38656;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#26412;&#22320;&#20449;&#24687;&#21644;&#25104;&#23545;&#20559;&#22909;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#26399;&#26395;&#30340;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2107.06700</link><description>&lt;p&gt;
&#24046;&#20998;&#35780;&#35770;&#23478;GAN:&#36890;&#36807;&#20559;&#22909;&#25552;&#31034;&#29983;&#25104;&#29992;&#25143;&#38656;&#27714;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Differential-Critic GAN: Generating What You Want by a Cue of Preferences. (arXiv:2107.06700v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#8212;&#8212;&#24046;&#20998;&#35780;&#35770;&#23478;GAN&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#37096;&#20998;&#25968;&#25454;&#38598;&#21253;&#21547;&#25152;&#38656;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#26412;&#22320;&#20449;&#24687;&#21644;&#25104;&#23545;&#20559;&#22909;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#26399;&#26395;&#30340;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#35780;&#35770;&#23478;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DiCGAN&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#25152;&#38656;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#24403;&#21482;&#26377;&#37096;&#20998;&#25968;&#25454;&#38598;&#20855;&#26377;&#25152;&#38656;&#29305;&#24615;&#26102;&#65292;DiCGAN&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#26399;&#26395;&#30340;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#24110;&#21161;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#29289;&#20135;&#21697;&#12290;&#29616;&#26377;&#26041;&#27861;&#39318;&#20808;&#36873;&#25321;&#25152;&#38656;&#26679;&#26412;&#65292;&#28982;&#21518;&#22312;&#36873;&#23450;&#30340;&#26679;&#26412;&#19978;&#35757;&#32451;&#24120;&#35268;GAN&#20197;&#33719;&#21462;&#29992;&#25143;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#25152;&#38656;&#25968;&#25454;&#30340;&#36873;&#25321;&#20381;&#36182;&#20110;&#20840;&#23616;&#30693;&#35782;&#21644;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#12290;DiCGAN&#24341;&#20837;&#20102;&#24046;&#20998;&#35780;&#35770;&#23478;&#65292;&#35813;&#35780;&#35770;&#23478;&#20174;&#25104;&#23545;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#36825;&#26159;&#26412;&#22320;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#23450;&#20041;&#22312;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#19978;&#12290;&#35780;&#35770;&#23478;&#36890;&#36807;&#22312;Wasserstein GAN&#30340;&#35780;&#35770;&#23478;&#19978;&#23450;&#20041;&#39069;&#22806;&#30340;&#25490;&#21517;&#25439;&#22833;&#26469;&#26500;&#24314;&#12290;&#23427;&#36171;&#20104;&#27599;&#23545;&#26679;&#26412;&#38388;&#35780;&#35770;&#23478;&#20540;&#30340;&#24046;&#24322;&#20197;&#29992;&#25143;&#20559;&#22909;&#65292;&#24341;&#23548;&#25152;&#38656;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;&#20811;&#38534;&#20840;&#23616;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Differential-Critic Generative Adversarial Network (DiCGAN) to learn the distribution of user-desired data when only partial instead of the entire dataset possesses the desired property. DiCGAN generates desired data that meets the user's expectations and can assist in designing biological products with desired properties. Existing approaches select the desired samples first and train regular GANs on the selected samples to derive the user-desired data distribution. However, the selection of the desired data relies on global knowledge and supervision over the entire dataset. DiCGAN introduces a differential critic that learns from pairwise preferences, which are local knowledge and can be defined on a part of training data. The critic is built by defining an additional ranking loss over the Wasserstein GAN's critic. It endows the difference of critic values between each pair of samples with the user preference and guides the generation of the desired data instead of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;TAP&#26041;&#27861;&#36827;&#34892;&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#20197;Z2&#21516;&#27493;&#20026;&#20363;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TAP&#33258;&#30001;&#33021;&#27867;&#20989;&#22312;Bayes&#21518;&#39564;&#20998;&#24067;&#22343;&#20540;&#38468;&#36817;&#23384;&#22312;&#21807;&#19968;&#23616;&#37096;&#26497;&#23567;&#20540;&#19988;&#20855;&#26377;&#24378;&#20984;&#24615;&#12290;&#36890;&#36807;&#33258;&#28982;&#26799;&#24230;/&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#19968;&#23450;&#30340;AMP&#36845;&#20195;&#20013;&#33719;&#24471;&#35813;&#26497;&#23567;&#20540;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#36825;&#20026;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2106.11428</link><description>&lt;p&gt;
TAP&#33258;&#30001;&#33021;&#20989;&#25968;&#30340;&#23616;&#37096;&#20984;&#24615;&#21644;Z2&#21516;&#27493;&#30340;AMP&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Local convexity of the TAP free energy and AMP convergence for Z2-synchronization. (arXiv:2106.11428v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;TAP&#26041;&#27861;&#36827;&#34892;&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#20197;Z2&#21516;&#27493;&#20026;&#20363;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TAP&#33258;&#30001;&#33021;&#27867;&#20989;&#22312;Bayes&#21518;&#39564;&#20998;&#24067;&#22343;&#20540;&#38468;&#36817;&#23384;&#22312;&#21807;&#19968;&#23616;&#37096;&#26497;&#23567;&#20540;&#19988;&#20855;&#26377;&#24378;&#20984;&#24615;&#12290;&#36890;&#36807;&#33258;&#28982;&#26799;&#24230;/&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#19968;&#23450;&#30340;AMP&#36845;&#20195;&#20013;&#33719;&#24471;&#35813;&#26497;&#23567;&#20540;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#36825;&#20026;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Z2&#21516;&#27493;&#20026;&#39640;&#32500;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#30740;&#31350;&#20102;&#21033;&#29992;TAP&#26041;&#27861;&#36827;&#34892;&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#20449;&#21495;&#24378;&#24230;$\lambda &gt; 1$&#65288;&#24369;&#24674;&#22797;&#38408;&#20540;&#65289;&#65292;TAP&#33258;&#30001;&#33021;&#27867;&#20989;&#22312;Bayes&#21518;&#39564;&#20998;&#24067;&#22343;&#20540;&#38468;&#36817;&#23384;&#22312;&#21807;&#19968;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#22312;&#35813;&#26497;&#23567;&#20540;&#30340;&#23616;&#37096;&#37051;&#22495;&#20869;&#65292;TAP&#33258;&#30001;&#33021;&#26159;&#24378;&#20984;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#26412;&#22320;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#33258;&#28982;&#26799;&#24230;/&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#21487;&#20197;&#20174;&#19968;&#23450;&#30340;Approximate Message Passing&#65288;AMP&#65289;&#36845;&#20195;&#20013;&#33719;&#24471;&#35813;&#26497;&#23567;&#20540;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#36825;&#20026;&#36890;&#36807;&#26368;&#23567;&#21270;TAP&#33258;&#30001;&#33021;&#36827;&#34892;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;AMP&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;$\lambda &gt;1$&#65292;AMP&#22312;TAP&#26497;&#23567;&#28857;&#22788;&#26159;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20174;&#35889;&#21021;&#22987;&#21270;&#24320;&#22987;&#32447;&#24615;&#25910;&#25947;&#21040;&#35813;&#26497;&#23567;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study mean-field variational Bayesian inference using the TAP approach, for Z2-synchronization as a prototypical example of a high-dimensional Bayesian model. We show that for any signal strength $\lambda &gt; 1$ (the weak-recovery threshold), there exists a unique local minimizer of the TAP free energy functional near the mean of the Bayes posterior law. Furthermore, the TAP free energy in a local neighborhood of this minimizer is strongly convex. Consequently, a natural-gradient/mirror-descent algorithm achieves linear convergence to this minimizer from a local initialization, which may be obtained by a constant number of iterates of Approximate Message Passing (AMP). This provides a rigorous foundation for variational inference in high dimensions via minimization of the TAP free energy.  We also analyze the finite-sample convergence of AMP, showing that AMP is asymptotically stable at the TAP minimizer for any $\lambda &gt; 1$, and is linearly convergent to this minimizer from a spectr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#24674;&#22797;&#22240;&#26524;DAGs&#65292;&#24182;&#24314;&#31435;&#20102;&#32622;&#20449;&#21306;&#38388;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#24674;&#22797;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;DAGs&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.02600</link><description>&lt;p&gt;
&#26469;&#33258;&#33258;&#28608;&#21644;&#30456;&#20114;&#28608;&#21457;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Discovery from Self and Mutually Exciting Time Series. (arXiv:2106.02600v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#24674;&#22797;&#22240;&#26524;DAGs&#65292;&#24182;&#24314;&#31435;&#20102;&#32622;&#20449;&#21306;&#38388;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#24674;&#22797;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;DAGs&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#32447;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#24674;&#22797;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#38543;&#26426;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;(VI)&#24418;&#24335;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#33324;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#38750;&#28176;&#36827;&#24615;&#30340;&#24674;&#22797;&#20445;&#35777;&#21644;&#21487;&#37327;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#30830;&#23450;&#19968;&#31995;&#21015;&#38750;&#32447;&#24615;&#21333;&#35843;&#36830;&#25509;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#21644;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;DAGs&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;Sepsis&#30456;&#20851;&#32010;&#20081;(SADs)&#26041;&#38754;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#24378;&#22823;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#65288;&#22914;XGBoost&#65289;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26410;&#26469;&#36827;&#34892;&#26465;&#20214;&#38543;&#26426;&#22330;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a generalized linear structural causal model, coupled with a novel data-adaptive linear regularization, to recover causal directed acyclic graphs (DAGs) from time series. By leveraging a recently developed stochastic monotone Variational Inequality (VI) formulation, we cast the causal discovery problem as a general convex optimization. Furthermore, we develop a non-asymptotic recovery guarantee and quantifiable uncertainty by solving a linear program to establish confidence intervals for a wide range of non-linear monotone link functions. We validate our theoretical results and show the competitive performance of our method via extensive numerical experiments. Most importantly, we demonstrate the effectiveness of our approach in recovering highly interpretable causal DAGs over Sepsis Associated Derangements (SADs) while achieving comparable prediction performance to powerful ``black-box'' models such as XGBoost. Thus, the future adoption of our proposed method to conduct con
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#35299;&#21078;&#33041;&#37096; MRI &#30340;CNN&#27169;&#22411;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#32467;&#26500;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#28145;&#24230;&#38598;&#25104;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;&#22823;&#22411;&#22810;&#31449;&#28857;&#30340;&#33041;&#37096;&#35299;&#21078;MRI&#25968;&#25454;&#38598;&#65292;&#35813;&#27979;&#35797;&#20026;&#19977;&#32500;&#33041;&#37096;&#35299;&#21078;MRI&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2106.01132</link><description>&lt;p&gt;
&#22522;&#20110;&#19977;&#32500;&#35299;&#21078;&#33041;&#37096; MRI &#30340; CNN &#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#32467;&#26500;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning. (arXiv:2106.01132v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01132
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#35299;&#21078;&#33041;&#37096; MRI &#30340;CNN&#27169;&#22411;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#32467;&#26500;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#28145;&#24230;&#38598;&#25104;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;&#22823;&#22411;&#22810;&#31449;&#28857;&#30340;&#33041;&#37096;&#35299;&#21078;MRI&#25968;&#25454;&#38598;&#65292;&#35813;&#27979;&#35797;&#20026;&#19977;&#32500;&#33041;&#37096;&#35299;&#21078;MRI&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#29305;&#21035;&#26159;CNN&#27169;&#22411;&#24050;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24341;&#36215;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#29305;&#21035;&#26159;&#25104;&#22411;&#39044;&#27979;&#25110;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#24448;&#24448;&#28041;&#21450;&#23567;&#22411;&#21333;&#19968;&#31449;&#28857;&#38431;&#21015;&#65292;&#20197;&#21450;&#29305;&#23450;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#21644;&#33258;&#23450;&#20041;CNN&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#36817;&#30340;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;3D CNN&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#36824;&#35780;&#20272;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#22312;&#22522;&#20110;&#20307;&#32032;&#30340;&#24418;&#24577;&#23398;&#65288;VBM&#65289;&#39044;&#22788;&#29702;&#21644;&#20934;&#21407;&#22987;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#26159;&#22312;&#21253;&#21547;N = 10k&#25195;&#25551;&#30340;&#22823;&#22411;&#22810;&#31449;&#28857;3D&#33041;&#37096;&#35299;&#21078;MRI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#28041;&#21450;3&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65306;&#24180;&#40836;&#39044;&#27979;&#65292;&#24615;&#21035;&#20998;&#31867;&#21644;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#35786;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#21487;&#20197;&#22312;VBM&#39044;&#22788;&#29702;&#26041;&#38754;&#25552;&#20379;&#26174;&#30528;&#26356;&#22909;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#22312;&#20934;&#21407;&#22987;&#22270;&#20687;&#19978;&#65292;&#36825;&#20984;&#26174;&#20102;&#39044;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30001;&#21516;&#19968;&#32467;&#26500;&#30340;6&#20010;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#25104;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20248;&#20110;&#25152;&#26377;&#20010;&#20307;&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#19977;&#32500;&#33041;&#37096;&#35299;&#21078;MRI&#25968;&#25454;&#20998;&#26512;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) and specifically CNN models have become a de facto method for a wide range of vision tasks, outperforming traditional machine learning (ML) methods. Consequently, they drew a lot of attention in the neuroimaging field in particular for phenotype prediction or computer-aided diagnosis. However, most of the current studies often deal with small single-site cohorts, along with a specific pre-processing pipeline and custom CNN architectures, which make them difficult to compare to. We propose an extensive benchmark of recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM) pre-processing and quasi-raw images. Experiments were conducted on a large multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3 challenging tasks: age prediction, sex classification, and schizophrenia diagnosis. We found that all models provide significantly better predictions with VBM 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#26041;&#27861;Balanced Entropy Acquisition&#65288;BalEntAcq&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#28508;&#22312;softmax&#27010;&#29575;&#21644;&#26631;&#31614;&#21464;&#37327;&#30340;&#20449;&#24687;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#24179;&#34913;&#29109;&#23398;&#20064;&#20934;&#21017;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2105.14559</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#34913;&#29109;&#23398;&#20064;&#20934;&#21017;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle. (arXiv:2105.14559v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#26041;&#27861;Balanced Entropy Acquisition&#65288;BalEntAcq&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#28508;&#22312;softmax&#27010;&#29575;&#21644;&#26631;&#31614;&#21464;&#37327;&#30340;&#20449;&#24687;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#24179;&#34913;&#29109;&#23398;&#20064;&#20934;&#21017;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20855;&#26377;&#26377;&#38480;&#39044;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#24182;&#36890;&#36807;&#20943;&#23569;&#26631;&#35760;&#25104;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#36807;&#31243;&#12290;&#20449;&#24687;&#26368;&#22823;&#21270;&#23398;&#20064;&#21407;&#21017;&#65288;&#20363;&#22914; BALD&#65289;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#24050;&#32463;&#22312;&#21508;&#31181;&#20027;&#21160;&#23398;&#20064;&#24212;&#29992;&#20013;&#25104;&#21151;&#22320;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#20110;&#27744;&#30340;&#30446;&#26631;&#26412;&#36136;&#19978;&#24341;&#20837;&#20102;&#20887;&#20313;&#36873;&#25321;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#36827;&#34892;&#25209;&#22788;&#29702;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#26041;&#27861;Balanced Entropy Acquisition&#65288;BalEntAcq&#65289;&#65292;&#23427;&#25429;&#25417;&#20102;&#28508;&#22312;softmax&#27010;&#29575;&#21644;&#26631;&#31614;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20449;&#24687;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;Beta&#20998;&#24067;&#36924;&#36817;&#27599;&#20010;&#36793;&#32536;&#20998;&#24067;&#12290;Beta&#36924;&#36817;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;BalEntAcq&#21046;&#23450;&#20026;&#22686;&#24378;&#29109;&#21644;&#36793;&#32536;&#32852;&#21512;&#29109;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#25152;&#24471;&#21040;&#30340;BalEntAcq&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#65292;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#22312;&#20869;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring labeled data is challenging in many machine learning applications with limited budgets. Active learning gives a procedure to select the most informative data points and improve data efficiency by reducing the cost of labeling. The info-max learning principle maximizing mutual information such as BALD has been successful and widely adapted in various active learning applications. However, this pool-based specific objective inherently introduces a redundant selection and further requires a high computational cost for batch selection. In this paper, we design and propose a new uncertainty measure, Balanced Entropy Acquisition (BalEntAcq), which captures the information balance between the uncertainty of underlying softmax probability and the label variable. To do this, we approximate each marginal distribution by Beta distribution. Beta approximation enables us to formulate BalEntAcq as a ratio between an augmented entropy and the marginalized joint entropy. The closed-form expr
&lt;/p&gt;</description></item><item><title>CogDL&#26159;&#19968;&#20010;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#24211;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#21644;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#35774;&#35745;&#21644;&#22810;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#21253;&#25324;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#21644;&#23454;&#29992;&#24037;&#20855;&#65292;&#26159;&#36827;&#34892;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2103.00959</link><description>&lt;p&gt;
CogDL&#65306;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#24211;
&lt;/p&gt;
&lt;p&gt;
CogDL: A Comprehensive Library for Graph Deep Learning. (arXiv:2103.00959v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00959
&lt;/p&gt;
&lt;p&gt;
CogDL&#26159;&#19968;&#20010;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#24211;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#21644;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#35774;&#35745;&#21644;&#22810;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#21253;&#25324;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#21644;&#23454;&#29992;&#24037;&#20855;&#65292;&#26159;&#36827;&#34892;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#22270;&#23398;&#20064;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#22270;&#12290;&#28982;&#32780;&#65292;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#23384;&#22312;&#19968;&#20123;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22270;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;GNN&#22797;&#26434;&#30340;&#35757;&#32451;&#21644;&#22270;&#20219;&#21153;&#30340;&#38750;&#26631;&#20934;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CogDL&#65292;&#19968;&#20010;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#24211;&#65292;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#36731;&#26494;&#39640;&#25928;&#22320;&#36827;&#34892;&#23454;&#39564;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#26500;&#24314;&#24212;&#29992;&#12290;&#22312;CogDL&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#21508;&#31181;&#22270;&#20219;&#21153;&#30340;GNN&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#32479;&#19968;&#35774;&#35745;&#65292;&#20351;&#20854;&#22312;&#29616;&#26377;&#30340;&#22270;&#23398;&#20064;&#24211;&#20013;&#29420;&#26641;&#19968;&#24092;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#22120;&#65292;CogDL&#21487;&#20197;&#20248;&#21270;GNN&#35757;&#32451;&#24490;&#29615;&#65292;&#37319;&#29992;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#31561;&#22810;&#31181;&#35757;&#32451;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#21508;&#31181;GNN&#27169;&#22411;&#24320;&#21457;&#20102;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#21253;&#25324;&#32463;&#20856;&#27169;&#22411;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CogDL&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#29992;&#24037;&#20855;&#21644;&#24037;&#20855;&#65292;&#25903;&#25345;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#24320;&#21457;&#65292;&#22914;&#25968;&#25454;&#21152;&#36733;&#22120;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;&#27969;&#34892;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;CogDL&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#21508;&#31181;&#22270;&#20219;&#21153;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have attracted tremendous attention from the graph learning community in recent years. It has been widely adopted in various real-world applications from diverse domains, such as social networks and biological graphs. The research and applications of graph deep learning present new challenges, including the sparse nature of graph data, complicated training of GNNs, and non-standard evaluation of graph tasks. To tackle the issues, we present CogDL, a comprehensive library for graph deep learning that allows researchers and practitioners to conduct experiments, compare methods, and build applications with ease and efficiency. In CogDL, we propose a unified design for the training and evaluation of GNN models for various graph tasks, making it unique among existing graph learning libraries. By utilizing this unified trainer, CogDL can optimize the GNN training loop with several training techniques, such as mixed precision training. Moreover, we develop efficie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32858;&#21512;&#26465;&#20214;&#20998;&#20301;&#25968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33021;&#22815;&#24212;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#21253;&#30340;&#22810;&#31181;&#27169;&#22411;&#65292;&#23545;&#35768;&#22810;&#20174;&#19994;&#32773;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.00083</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Flexible Model Aggregation for Quantile Regression. (arXiv:2103.00083v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32858;&#21512;&#26465;&#20214;&#20998;&#20301;&#25968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33021;&#22815;&#24212;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#21253;&#30340;&#22810;&#31181;&#27169;&#22411;&#65292;&#23545;&#35768;&#22810;&#20174;&#19994;&#32773;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20301;&#25968;&#22238;&#24402;&#26159;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#23398;&#20064;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#26088;&#22312;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25110;&#22312;&#19981;&#36807;&#24230;&#31616;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;&#22810;&#26679;&#21270;&#20154;&#32676;&#24314;&#27169;&#12290;&#26412;&#25991;&#30740;&#31350;&#32858;&#21512;&#20219;&#24847;&#25968;&#37327;&#30340;&#26465;&#20214;&#20998;&#20301;&#25968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#32771;&#34385;&#26435;&#37325;&#38598;&#25104;&#65292;&#26435;&#37325;&#19981;&#20165;&#21487;&#20197;&#21464;&#21270;&#20110;&#21333;&#20010;&#27169;&#22411;&#65292;&#36824;&#21487;&#20197;&#21464;&#21270;&#20110;&#20998;&#20301;&#25968;&#27700;&#24179;&#21644;&#29305;&#24449;&#20540;&#12290;&#26412;&#25991;&#25152;&#32771;&#34385;&#30340;&#25152;&#26377;&#27169;&#22411;&#22343;&#21487;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#21253;&#25311;&#21512;&#65292;&#22240;&#27492;&#23545;&#35768;&#22810;&#20174;&#19994;&#32773;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65288;&#20174;&#23454;&#29616;&#30340;&#35282;&#24230;&#26469;&#30475;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantile regression is a fundamental problem in statistical learning motivated by a need to quantify uncertainty in predictions, or to model a diverse population without being overly reductive. For instance, epidemiological forecasts, cost estimates, and revenue predictions all benefit from being able to quantify the range of possible values accurately. As such, many models have been developed for this problem over many years of research in statistics, machine learning, and related fields. Rather than proposing yet another (new) algorithm for quantile regression we adopt a meta viewpoint: we investigate methods for aggregating any number of conditional quantile models, in order to improve accuracy and robustness. We consider weighted ensembles where weights may vary over not only individual models, but also over quantile levels, and feature values. All of the models we consider in this paper can be fit using modern deep learning toolkits, and hence are widely accessible (from an implem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdvSim&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#21508;&#31181;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2101.06549</link><description>&lt;p&gt;
AdvSim: &#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.06549
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdvSim&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#21508;&#31181;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#27169;&#25311;&#21487;&#33021;&#20986;&#29616;&#33258;&#20027;&#22534;&#26632;&#22833;&#36133;&#30340;&#24773;&#20917;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#22330;&#26223;&#21482;&#20026;&#35268;&#21010;&#27169;&#22359;&#29983;&#25104;&#20102;&#19968;&#20123;&#30456;&#23545;&#36739;&#23569;&#30340;&#22330;&#26223;&#65292;&#20854;&#36755;&#20837;&#20026;&#22320;&#38754;&#30495;&#23454;&#36710;&#36742;&#29366;&#24577;&#12290;&#36825;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#35782;&#21035;&#25152;&#26377;&#21487;&#33021;&#30340;&#33258;&#20027;&#22833;&#36133;&#65292;&#20363;&#22914;&#30001;&#20110;&#36974;&#25377;&#23548;&#33268;&#30340;&#24863;&#30693;&#25925;&#38556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AdvSim&#65292;&#19968;&#20010;&#23545;&#20110;&#20219;&#20309;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#20027;&#31995;&#32479;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#20132;&#36890;&#22330;&#26223;&#65292;AdvSim&#20197;&#29289;&#29702;&#21487;&#34892;&#30340;&#26041;&#24335;&#20462;&#25913;&#21442;&#19982;&#32773;&#30340;&#36712;&#36857;&#24182;&#26356;&#26032;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21305;&#37197;&#21463;&#25200;&#21160;&#30340;&#19990;&#30028;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#27169;&#25311;&#65292;&#25105;&#20204;&#33719;&#24471;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#36866;&#29992;&#20110;&#23436;&#25972;&#30340;&#33258;&#20027;&#22534;&#26632;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#22823;&#37327;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#36866;&#29992;&#20110;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
As self-driving systems become better, simulating scenarios where the autonomy stack may fail becomes more important. Traditionally, those scenarios are generated for a few scenes with respect to the planning module that takes ground-truth actor states as input. This does not scale and cannot identify all possible autonomy failures, such as perception failures due to occlusion. In this paper, we propose AdvSim, an adversarial framework to generate safety-critical scenarios for any LiDAR-based autonomy system. Given an initial traffic scenario, AdvSim modifies the actors' trajectories in a physically plausible manner and updates the LiDAR sensor data to match the perturbed world. Importantly, by simulating directly from sensor data, we obtain adversarial scenarios that are safety-critical for the full autonomy stack. Our experiments show that our approach is general and can identify thousands of semantically meaningful safety-critical scenarios for a wide range of modern self-driving sy
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#32467;&#26500;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#20849;&#20139;&#19968;&#31181;&#24120;&#35265;&#32467;&#26500;&#32780;&#37096;&#20998;&#30830;&#23450;&#20102;&#30495;&#27491;&#30340;&#20301;&#32622;&#28151;&#21512;&#29289;&#30340;&#31751;&#20013;&#24515;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#28385;&#36275;&#26576;&#31181;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#20063;&#36866;&#29992;&#20110;&#25104;&#20998;&#25968;&#37327;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2009.13040</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#26497;&#23567;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Local Minima Structures in Gaussian Mixture Models. (arXiv:2009.13040v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.13040
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#32467;&#26500;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#20849;&#20139;&#19968;&#31181;&#24120;&#35265;&#32467;&#26500;&#32780;&#37096;&#20998;&#30830;&#23450;&#20102;&#30495;&#27491;&#30340;&#20301;&#32622;&#28151;&#21512;&#29289;&#30340;&#31751;&#20013;&#24515;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#28385;&#36275;&#26576;&#31181;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#20063;&#36866;&#29992;&#20110;&#25104;&#20998;&#25968;&#37327;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20154;&#21475;&#26497;&#38480;&#30340;&#24773;&#20917;&#19979;&#35843;&#26597;&#20102;&#28151;&#21512;&#25104;&#20998;&#27169;&#22411;&#65288;GMM&#65289;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#20855;&#26377;&#19968;&#33324;&#25104;&#20998;&#25968;&#37327;&#30340;GMM&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#32467;&#26500;&#12290;&#30001;&#20110;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#20998;&#31163;&#33391;&#22909;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#20063;&#21487;&#33021;&#23384;&#22312;&#19981;&#26159;&#20840;&#23616;&#26368;&#20248;&#30340;&#22810;&#20010;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25152;&#26377;&#23616;&#37096;&#26497;&#23567;&#20540;&#37117;&#20849;&#20139;&#19968;&#31181;&#24120;&#35265;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#37096;&#20998;&#30830;&#23450;&#20102;&#30495;&#27491;&#30340;&#20301;&#32622;&#28151;&#21512;&#29289;&#65288;&#21363;&#39640;&#26031;&#25104;&#20998;&#30340;&#22343;&#20540;&#65289;&#30340;&#31751;&#20013;&#24515;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#23616;&#37096;&#26497;&#23567;&#20540;&#21487;&#20197;&#34920;&#31034;&#20026;&#20004;&#31181;&#31867;&#22411;&#23376;&#37197;&#32622;&#30340;&#38750;&#37325;&#21472;&#32452;&#21512;&#65306;&#23558;&#21333;&#20010;&#22343;&#20540;&#20272;&#35745;&#19982;&#22810;&#20010;&#39640;&#26031;&#20998;&#37327;&#25311;&#21512;&#25110;&#23558;&#22810;&#20010;&#20272;&#35745;&#25311;&#21512;&#21040;&#21333;&#20010;&#30495;&#23454;&#20998;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#28385;&#36275;&#26576;&#31181;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#25104;&#20998;&#25968;&#37327;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#19968;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#35774;&#32622;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#26500;&#35745;&#25968;&#35770;&#35777;&#23548;&#20986;&#20102;&#36825;&#20123;&#38750;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#31934;&#30830;&#25968;&#37327;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the landscape of the negative log-likelihood function of Gaussian Mixture Models (GMMs) with a general number of components in the population limit. As the objective function is non-convex, there can be multiple local minima that are not globally optimal, even for well-separated mixture models. Our study reveals that all local minima share a common structure that partially identifies the cluster centers (i.e., means of the Gaussian components) of the true location mixture. Specifically, each local minimum can be represented as a non-overlapping combination of two types of sub-configurations: fitting a single mean estimate to multiple Gaussian components or fitting multiple estimates to a single true component. These results apply to settings where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is overor under-specified. We also present a more fine-grained analysis for the setting of one-dimensional G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#26679;&#26412;&#22823;&#23567;&#19979;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2005.12900</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#31361;&#30772;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22823;&#23567;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model. (arXiv:2005.12900v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#26679;&#26412;&#22823;&#23567;&#19979;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30528;&#30524;&#20110;&#22312;&#26377;&#29983;&#25104;&#27169;&#22411;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#24378;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#24102;&#26377;&#25240;&#25187;&#30340;&#26080;&#38480;&#26102;&#38388;&#27493;&#38271;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#29366;&#24577;&#31354;&#38388;&#20026;$\mathcal{S}$&#65292;&#21160;&#20316;&#31354;&#38388;&#20026;$\mathcal{A}$&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32479;&#35745;&#31934;&#24230;&#20043;&#38388;&#26435;&#34913;&#30340;&#23436;&#25972;&#22270;&#26223;&#23578;&#26410;&#30830;&#23450;&#12290;&#29305;&#21035;&#26159;&#65292;&#25152;&#26377;&#30340;&#20808;&#21069;&#32467;&#26524;&#37117;&#21463;&#21040;&#20005;&#37325;&#30340;&#26679;&#26412;&#22823;&#23567;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22768;&#31216;&#30340;&#32479;&#35745;&#20445;&#35777;&#20165;&#22312;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;&#33267;&#23569;$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$&#26102;&#25165;&#25104;&#31435;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#8212;&#8212;&#22312;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;$\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$&#30340;&#24773;&#20917;&#19979;&#23601;&#33021;&#35777;&#26126;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#65288;&#20960;&#20046;&#31526;&#21512;&#19968;&#20123;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#38500;&#20102;&#26080;&#38480;&#26102;&#38388;&#27493;&#38271;M&#35299;&#20915;&#26041;&#26696;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26377;&#38480;&#26679;&#26412;&#21644;&#36817;&#20284;&#20215;&#20540;&#36845;&#20195;&#38382;&#39064;&#65292;&#20197;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the sample efficiency of reinforcement learning, assuming access to a generative model (or simulator). We first consider $\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with state space $\mathcal{S}$ and action space $\mathcal{A}$. Despite a number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, all prior results suffer from a severe sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$. The current paper overcomes this barrier by certifying the minimax optimality of two algorithms -- a perturbed model-based algorithm and a conservative model-based algorithm -- as soon as the sample size exceeds the order of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). Moving beyond infinite-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#22522;&#30784;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#31574;&#30053;&#32593;&#32476;&#32467;&#21512;&#65292;&#20351;&#31574;&#30053;&#32593;&#32476;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#32452;&#21512;&#19978;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;RL/IL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2004.07200</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#22522;&#30784;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Compositional Policy Learning via Language Grounding. (arXiv:2004.07200v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#22522;&#30784;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#31574;&#30053;&#32593;&#32476;&#32467;&#21512;&#65292;&#20351;&#31574;&#30053;&#32593;&#32476;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#32452;&#21512;&#19978;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;RL/IL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#22312;&#26368;&#36817;&#37117;&#26377;&#20102;&#31361;&#30772;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#22312;&#35757;&#32451;&#29615;&#22659;&#20043;&#22806;&#36827;&#34892;&#25512;&#24191;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#20851;&#20110;&#19990;&#30028;&#65288;&#22914;&#35821;&#35328;&#25551;&#36848;&#65289;&#30340;&#30693;&#35782;&#26469;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#24102;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#35821;&#35328;&#24341;&#23548;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20013;&#29615;&#22659;&#34987;&#25551;&#36848;&#20026;&#19981;&#21516;&#23646;&#24615;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#27809;&#26377;&#20844;&#20849;&#29615;&#22659;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#24179;&#21488;BabyAI++&#65292;&#20854;&#20013;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#19982;&#35270;&#35273;&#22806;&#35266;&#35299;&#32806;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;BabyAI++&#25552;&#20379;&#20102;&#21508;&#31181;&#35270;&#35273;&#21160;&#21147;&#23398;&#32452;&#21512;&#20197;&#21450;&#30456;&#24212;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#23398;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#33021;&#21147;&#65292;&#19968;&#32452;&#35270;&#35273;&#21160;&#21147;&#23398;&#37197;&#23545;&#34987;&#20445;&#30041;&#22312;BabyAI++&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#24341;&#23548;RL/IL&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#24341;&#23548;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#31574;&#30053;&#32593;&#32476;&#32467;&#21512;&#65292;&#20351;&#31574;&#30053;&#32593;&#32476;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#32452;&#21512;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#22522;&#30784;&#27169;&#22359;&#65292;&#23558;&#31526;&#21495;&#23646;&#24615;&#34920;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#38598;&#25104;&#21040;&#31574;&#30053;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;RL/IL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent breakthroughs in reinforcement learning (RL) and imitation learning (IL), existing algorithms fail to generalize beyond the training environments. In reality, humans can adapt to new tasks quickly by leveraging prior knowledge about the world such as language descriptions. To facilitate the research on language-guided agents with domain adaption, we propose a novel zero-shot compositional policy learning task, where the environments are characterized as a composition of different attributes. Since there are no public environments supporting this study, we introduce a new research platform BabyAI++ in which the dynamics of environments are disentangled from visual appearance. At each episode, BabyAI++ provides varied vision-dynamics combinations along with corresponding descriptive texts. To evaluate the adaption capability of learned agents, a set of vision-dynamics pairings are held-out for testing on BabyAI++. Unsurprisingly, we find that current language-guided RL/IL 
&lt;/p&gt;</description></item><item><title>Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2002.01368</link><description>&lt;p&gt;
&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#31867;&#21035;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#65288;Open-LACU&#65289;
&lt;/p&gt;
&lt;p&gt;
Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01368
&lt;/p&gt;
&lt;p&gt;
Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#24335;&#35782;&#21035;&#65288;OSR&#65289;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#23581;&#35797;&#20197;&#21512;&#25104;&#21333;&#20010;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27599;&#27425;&#23581;&#35797;&#37117;&#36829;&#21453;&#20102;&#24320;&#25918;&#38598;&#23450;&#20041;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#22312;&#26410;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26032;&#39062;&#30340;&#31867;&#21035;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#25512;&#24191;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#32972;&#26223;&#31867;&#21035;&#21644;&#26410;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#26410;&#30693;&#31867;&#21035;&#12290;&#36890;&#36807;&#20998;&#31867;&#36825;&#20004;&#31181;&#26032;&#39062;&#31867;&#21035;&#30340;&#26041;&#24335;&#65292;Open-LACU&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#24182;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36127;&#26680;&#22238;&#24402;&#30340;&#31639;&#27861;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#37051;&#22495;&#21644;&#22270;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20854;&#20248;&#36234;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/1910.09383</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#36127;&#26680;&#22238;&#24402;&#26500;&#24314;&#37051;&#22495;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Neighborhood and Graph Constructions using Non-Negative Kernel Regression. (arXiv:1910.09383v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36127;&#26680;&#22238;&#24402;&#30340;&#31639;&#27861;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#37051;&#22495;&#21644;&#22270;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20854;&#20248;&#36234;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#37051;&#22495;&#23450;&#20041;&#21644;&#22270;&#26500;&#24314;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#32463;&#24120;&#20351;&#29992;&#12290;k&#36817;&#37051;&#65288;kNN&#65289;&#21644; $\epsilon$-&#37051;&#22495;&#26041;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#37051;&#22495;&#36873;&#25321;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#31616;&#21333;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#28041;&#21450;&#30340;&#21442;&#25968;&#36873;&#25321;&#65292;&#22914; k &#21644; $\epsilon$&#65292;&#20173;&#28982;&#26159;&#20020;&#26102;&#30340;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#36873;&#25321;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#34920;&#26126;&#37051;&#22495;&#26500;&#36896;&#31561;&#21516;&#20110;&#19968;&#20010;&#31232;&#30095;&#20449;&#21495;&#36924;&#36817;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#38750;&#36127;&#26680;&#22238;&#24402;&#65288;NNK&#65289;&#65292;&#29992;&#20110;&#33719;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#34920;&#31034;&#30340;&#37051;&#22495;&#12290;NNK&#19982;&#20449;&#21495;&#34920;&#31034;&#30340;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#26041;&#27861;&#30456;&#20284;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#20960;&#20309;&#21644;&#29702;&#35770;&#24615;&#36136;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#65288;i&#65289;NNK&#31639;&#27861;&#22312;&#37051;&#22495;&#21644;&#22270;&#26500;&#24314;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#65288;ii&#65289;NNK&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#37051;&#22495;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#21450;&#65288;iii&#65289;NNK&#22312;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven neighborhood definitions and graph constructions are often used in machine learning and signal processing applications. k-nearest neighbor~(kNN) and $\epsilon$-neighborhood methods are among the most common methods used for neighborhood selection, due to their computational simplicity. However, the choice of parameters associated with these methods, such as k and $\epsilon$, is still ad hoc. We make two main contributions in this paper. First, we present an alternative view of neighborhood selection, where we show that neighborhood construction is equivalent to a sparse signal approximation problem. Second, we propose an algorithm, non-negative kernel regression~(NNK), for obtaining neighborhoods that lead to better sparse representation. NNK draws similarities to the orthogonal matching pursuit approach to signal representation and possesses desirable geometric and theoretical properties. Experiments demonstrate (i) the robustness of the NNK algorithm for neighborhood and 
&lt;/p&gt;</description></item></channel></rss>