<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11925</link><description>&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#32780;&#26080;&#38656;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#65306;&#22522;&#20110;&#22810;&#32423;Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11925
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#28151;&#21512;&#26102;&#38388;&#30340;&#39044;&#27979;&#30340;oracle&#30693;&#35782;&#35201;&#27714;&#65292;&#21363;&#24230;&#37327;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#22266;&#23450;&#31574;&#30053;&#19979;&#36798;&#21040;&#20854;&#31283;&#24577;&#20998;&#24067;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#23545;&#20110;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#29699;&#25910;&#25947;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#32423;Actor-Critic&#65288;MAC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#26102;&#38388;&#30693;&#35782;&#30340;&#20381;&#36182;&#24615;&#30340;&#26377;&#25928;&#20943;&#36731;&#65292;&#36825;&#26159;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;$\mathcal{O}$&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.01671</link><description>&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#65306;&#32479;&#35745;&#26816;&#39564;&#12289;&#24230;&#37327;&#29109;&#20013;&#30340;&#38477;&#32500;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#20197;&#21033;&#29992;&#26469;&#31616;&#21270;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#20851;&#20110;&#26500;&#24314;&#25490;&#21015;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#27963;&#21160;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#30340;&#21464;&#37327;&#22914;&#20309;&#32479;&#35745;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#20854;&#20013;&#26679;&#26412;&#37327;&#20801;&#35768;&#38543;&#30528;&#32500;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#22312;&#32479;&#35745;&#29702;&#35770;&#26041;&#38754;&#65292;&#20851;&#20110;&#25490;&#21015;&#19981;&#21464;&#24615;&#22914;&#20309;&#24110;&#21161;&#20272;&#35745;&#20013;&#38477;&#32500;&#30340;&#30693;&#35782;&#29978;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20960;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22238;&#39038;&#24182;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27979;&#35797;&#22810;&#20803;&#20998;&#24067;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65307;&#65288;ii&#65289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#65307;&#65288;iii&#65289;&#20998;&#26512;&#20809;&#28369;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#24182;&#23558;&#20854;&#19982;&#26410;&#24378;&#21152;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#23545;&#24212;&#20989;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04550</link><description>&lt;p&gt;
Riemann-Lebesgue Forest&#22238;&#24402;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemann-Lebesgue Forest for Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Forest (RLF)&#12290;RLF&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#20989;&#25968;&#30340;&#20540;&#22495;&#21010;&#20998;&#20026;&#20960;&#20010;&#21306;&#38388;&#26469;&#27169;&#25311;&#21487;&#27979;&#20989;&#25968;&#30340;&#36924;&#36817;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Tree&#65292;&#23427;&#22312;&#27599;&#20010;&#38750;&#21494;&#33410;&#28857;&#19978;&#26377;&#26426;&#20250;&#20174;&#21709;&#24212;Y&#25110;&#29305;&#24449;&#31354;&#38388;X&#20013;&#30340;&#26041;&#21521;&#36827;&#34892;&#20999;&#21106;&#12290;&#25105;&#20204;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#26469;&#25512;&#23548;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;RLF&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#24403;&#24213;&#23618;&#20989;&#25968;Y=f(X)&#36981;&#24490;&#21152;&#27861;&#22238;&#24402;&#27169;&#22411;&#26102;&#65292;RLF&#19982;Scornet&#31561;&#20154;&#30340;&#35770;&#35777;&#65288;2014&#24180;&#65289;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08426</link><description>&lt;p&gt;
GD&#26080;&#27861;&#32988;&#20219;&#65306;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19977;&#31181;&#24433;&#21709;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GD doesn't make the cut: Three ways that non-differentiability affects neural network training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#38750;&#21487;&#24494;&#20989;&#25968;&#65288;NGDMs&#65289;&#21644;&#24212;&#29992;&#20110;&#21487;&#24494;&#20989;&#25968;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#65288;GDs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NGDMs&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;GDs&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;$L$-&#20809;&#28369;&#24615;&#30340;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#25991;&#29486;&#23545;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NGDM&#35299;&#20915;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#65292;&#34920;&#26126;&#22686;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#20250;&#23548;&#33268;NGDMs&#20013;&#26368;&#20248;&#35299;&#30340;$L_1$&#33539;&#25968;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20110;$L_1$&#24809;&#32602;&#30340;&#32593;&#32476;&#20462;&#21098;&#25216;&#26415;&#24182;&#26410;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65288;Edge of Stability&#65289;&#65292;&#25351;&#20986;&#21363;&#20351;&#23545;&#20110;Lipschitz&#36830;&#32493;&#20984;&#21487;&#24494;&#20989;&#25968;&#65292;&#23427;&#20063;&#19981;&#36866;&#29992;&#20110;&#38750;&#20984;&#38750;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.07751</link><description>&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65306;&#38656;&#27714;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Human Language Models: A Need and the Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07751
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#23558;&#20154;&#31867;&#21644;&#31038;&#20250;&#22240;&#32032;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;LLM&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23545;&#20316;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#30495;&#27491;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#23558;&#20154;&#31867;&#32972;&#26223;&#25972;&#21512;&#21040;LLM&#20013;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35774;&#35745;&#32771;&#34385;&#21644;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#35201;&#25429;&#25417;&#21738;&#20123;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#23427;&#20204;&#20197;&#21450;&#35201;&#37319;&#29992;&#20309;&#31181;&#24314;&#27169;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#20174;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#31185;&#23398;&#30340;&#27010;&#24565;&#20986;&#21457;&#65292;&#25903;&#25345;&#19977;&#20010;&#31435;&#22330;&#26469;&#21019;&#24314;&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65288;LHLMs&#65289;&#65306;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24212;&#21253;&#25324;&#20154;&#31867;&#32972;&#26223;&#12290;&#20854;&#27425;&#65292;LHLMs&#24212;&#35813;&#24847;&#35782;&#21040;&#20154;&#19981;&#20165;&#20165;&#26159;&#20182;&#20204;&#25152;&#23646;&#30340;&#32676;&#20307;&#12290;&#31532;&#19977;&#65292;LHLMs&#24212;&#35813;&#33021;&#22815;&#32771;&#34385;&#21040;&#20154;&#31867;&#32972;&#26223;&#30340;&#21160;&#24577;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#20999;&#29255;&#20998;&#24067;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#26399;&#26395;&#20272;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#24046;&#24322;&#26500;&#24314;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38543;&#26426;&#36335;&#24452;&#20999;&#29255;&#20998;&#24067;&#21644;&#20004;&#20010;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#30340;&#21464;&#31181;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25299;&#25169;&#12289;&#32479;&#35745;&#21644;&#35745;&#31639;&#24615;&#36136;&#19978;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.15889</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#30340;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sliced Wasserstein with Random-Path Projecting Directions. (arXiv:2401.15889v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#20999;&#29255;&#20998;&#24067;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#26399;&#26395;&#20272;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#24046;&#24322;&#26500;&#24314;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38543;&#26426;&#36335;&#24452;&#20999;&#29255;&#20998;&#24067;&#21644;&#20004;&#20010;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#30340;&#21464;&#31181;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25299;&#25169;&#12289;&#32479;&#35745;&#21644;&#35745;&#31639;&#24615;&#36136;&#19978;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20013;&#65292;&#20999;&#29255;&#20998;&#24067;&#36873;&#25321;&#24050;&#34987;&#29992;&#20316;&#25552;&#39640;&#22522;&#20110;&#26368;&#23567;&#21270;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#30340;&#21442;&#25968;&#20272;&#35745;&#22120;&#24615;&#33021;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#21033;&#29992;&#26114;&#36149;&#30340;&#20248;&#21270;&#26469;&#36873;&#25321;&#20999;&#29255;&#20998;&#24067;&#65292;&#35201;&#20040;&#20351;&#29992;&#38656;&#35201;&#26114;&#36149;&#30340;&#25277;&#26679;&#26041;&#27861;&#30340;&#20999;&#29255;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#20999;&#29255;&#20998;&#24067;&#65292;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#26399;&#26395;&#20272;&#35745;&#30340;&#25277;&#26679;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#65288;RPD&#65289;&#65292;&#23427;&#26159;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#36755;&#20837;&#27979;&#37327;&#20013;&#20004;&#20010;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#24046;&#24322;&#26500;&#24314;&#30340;&#12290;&#20174;RPD&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#38543;&#26426;&#36335;&#24452;&#20999;&#29255;&#20998;&#24067;&#65288;RPSD&#65289;&#21644;&#20004;&#20010;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#30340;&#21464;&#31181;&#65292;&#21363;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;RPSW&#65289;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;IWRPSW&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#25299;&#25169;&#12289;&#32479;&#35745;&#21644;&#35745;&#31639;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced Wasserstein distance in applications. Previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. In this work, we propose an optimization-free slicing distribution that provides a fast sampling for the Monte Carlo estimation of expectation. In particular, we introduce the random-path projecting direction (RPD) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. From the RPD, we derive the random-path slicing distribution (RPSD) and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW). We then discuss the topological, statistical, and computational propert
&lt;/p&gt;</description></item><item><title>TIM &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#20132;&#20114;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11687</link><description>&lt;p&gt;
TIM: &#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#33033;&#20914;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
TIM: An Efficient Temporal Interaction Module for Spiking Transformer. (arXiv:2401.11687v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11687
&lt;/p&gt;
&lt;p&gt;
TIM &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#20132;&#20114;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#20316;&#20026;&#31532;&#19977;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#22240;&#20854;&#29983;&#29289;&#21512;&#29702;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26041;&#38754;&#12290;&#21463;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25972;&#21512;&#23548;&#33268;&#20102;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#21464;&#21387;&#22120;&#22312;&#22686;&#24378; SNNs &#30340;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20102;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#33033;&#20914;&#33258;&#27880;&#24847; (SSA) &#26426;&#21046;&#22312;&#21033;&#29992; SNNs &#30340;&#26102;&#38388;&#22788;&#29702;&#28508;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; Temporal Interaction Module (TIM) &#30340;&#26032;&#22411;&#21367;&#31215;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378; SNN &#26550;&#26500;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;TIM &#30340;&#25972;&#21512;&#19982;&#29616;&#26377;&#30340; SNN &#26694;&#26550;&#26080;&#32541;&#34900;&#25509;&#65292;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#39069;&#22806;&#30340;&#26368;&#23569;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while sign
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#65292;&#24182;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#30446;&#26631;&#29289;&#20307;&#25235;&#21462;&#34987;&#29615;&#22659;&#36974;&#25377;&#26102;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17785</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#23398;&#20064;&#22806;&#22312;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Extrinsic Dexterity with Parameterized Manipulation Primitives. (arXiv:2310.17785v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17785
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#65292;&#24182;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#30446;&#26631;&#29289;&#20307;&#25235;&#21462;&#34987;&#29615;&#22659;&#36974;&#25377;&#26102;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#38382;&#39064;&#37117;&#28041;&#21450;&#21040;&#30446;&#26631;&#29289;&#20307;&#65292;&#20854;&#25152;&#26377;&#25235;&#21462;&#37117;&#34987;&#29615;&#22659;&#36974;&#25377;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21333;&#27425;&#25235;&#21462;&#35745;&#21010;&#24635;&#26159;&#22833;&#36133;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39318;&#20808;&#23558;&#29289;&#20307;&#25805;&#20316;&#21040;&#19968;&#20010;&#36866;&#21512;&#36827;&#34892;&#25235;&#21462;&#30340;&#37197;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#30340;&#21160;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26469;&#32467;&#21512;&#19968;&#31995;&#21015;&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#12290;&#36890;&#36807;&#23398;&#20064;&#20302;&#32423;&#25805;&#20316;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#12289;&#22841;&#20855;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#25511;&#21046;&#29289;&#20307;&#30340;&#29366;&#24577;&#12290;&#22312;&#26080;&#25511;&#21046;&#26465;&#20214;&#19979;&#20998;&#26512;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#22797;&#26434;&#34892;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#20934;&#30830;&#24314;&#27169;&#20114;&#21160;&#21644;&#25509;&#35302;&#21160;&#21147;&#23398;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22312;&#28145;&#24230;&#22270;&#20687;&#19978;&#30452;&#25509;&#25805;&#20316;&#30340;&#20998;&#23618;&#31574;&#30053;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#34701;&#21512;&#20855;&#26377;&#20114;&#34917;&#19987;&#38271;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.01542</link><description>&lt;p&gt;
&#34701;&#21512;&#20855;&#26377;&#20114;&#34917;&#19987;&#38271;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fusing Models with Complementary Expertise. (arXiv:2310.01542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#34701;&#21512;&#20855;&#26377;&#20114;&#34917;&#19987;&#38271;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33021;&#22815;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#30340;AI&#27169;&#22411;&#19968;&#30452;&#26159;&#25512;&#21160;AI&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#33719;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#23478;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#65292;&#20294;&#26159;&#22312;&#27979;&#35797;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#30340;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#24847;&#21619;&#30528;&#20219;&#20309;&#21333;&#20010;&#19987;&#23478;&#37117;&#19981;&#36275;&#22815;&#12290;&#25105;&#20204;&#32771;&#34385;&#34701;&#21512;&#19987;&#23478;&#27169;&#22411;&#36755;&#20986;&#30340;Fusion of Experts&#65288;FoE&#65289;&#38382;&#39064;&#65292;&#36825;&#20123;&#19987;&#23478;&#27169;&#22411;&#20855;&#26377;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20114;&#34917;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#21644;&#29983;&#25104;&#25991;&#26412;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#8220;&#33410;&#20461;&#8221;&#35774;&#32622;&#65292;&#21363;&#24076;&#26395;&#22312;&#27979;&#35797;&#26102;&#20943;&#23569;&#19987;&#23478;&#27169;&#22411;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the "frugal" setting where it is desired to reduce the number of expert model evaluations at test time.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#19982;&#38543;&#26426;&#37327;&#23376;&#21270;&#30456;&#36830;&#25509;&#65292;&#35777;&#26126;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#38543;&#26426;&#37327;&#23376;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20840;&#23616;&#37319;&#26679;&#22120;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#26684;&#28857;&#22330;&#37197;&#32622;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#33258;&#30456;&#20851;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.17082</link><description>&lt;p&gt;
&#22312;&#26684;&#28857;&#22330;&#35770;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38543;&#26426;&#37327;&#23376;&#21270;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Stochastic Quantization in Lattice Field Theory. (arXiv:2309.17082v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#19982;&#38543;&#26426;&#37327;&#23376;&#21270;&#30456;&#36830;&#25509;&#65292;&#35777;&#26126;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#38543;&#26426;&#37327;&#23376;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20840;&#23616;&#37319;&#26679;&#22120;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#26684;&#28857;&#22330;&#37197;&#32622;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#33258;&#30456;&#20851;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#21644;&#38543;&#26426;&#37327;&#23376;&#21270;&#65288;SQ&#65289;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;&#36890;&#36807;&#36817;&#20284;&#28385;&#36275;Langevin&#26041;&#31243;&#30340;&#38543;&#26426;&#36807;&#31243;&#30340;&#21453;&#28436;&#65292;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#26469;&#23454;&#29616;DM&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;DM&#21487;&#20197;&#20316;&#20026;&#20840;&#23616;&#37319;&#26679;&#22120;&#26469;&#29983;&#25104;&#20108;&#32500;$\phi^4$&#29702;&#35770;&#20013;&#30340;&#37327;&#23376;&#26684;&#28857;&#22330;&#37197;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;DM&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#33258;&#30456;&#20851;&#26102;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#31639;&#27861;&#22312;&#20020;&#30028;&#21306;&#22495;&#36935;&#21040;&#20020;&#30028;&#20943;&#24930;&#26102;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#26395;&#22312;&#26684;&#28857;&#22330;&#35770;&#27169;&#25311;&#20013;&#28608;&#21457;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#22823;&#22411;&#25968;&#25454;&#38598;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we establish a direct connection between generative diffusion models (DMs) and stochastic quantization (SQ). The DM is realized by approximating the reversal of a stochastic process dictated by the Langevin equation, generating samples from a prior distribution to effectively mimic the target distribution. Using numerical simulations, we demonstrate that the DM can serve as a global sampler for generating quantum lattice field configurations in two-dimensional $\phi^4$ theory. We demonstrate that DMs can notably reduce autocorrelation times in the Markov chain, especially in the critical region where standard Markov Chain Monte-Carlo (MCMC) algorithms experience critical slowing down. The findings can potentially inspire further advancements in lattice field theory simulations, in particular in cases where it is expensive to generate large ensembles.
&lt;/p&gt;</description></item><item><title>ScatterUQ&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04588</link><description>&lt;p&gt;
ScatterUQ: &#29992;&#20110;&#22810;&#31867;&#21035;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#20132;&#20114;&#24335;&#19981;&#30830;&#23450;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems. (arXiv:2308.04588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04588
&lt;/p&gt;
&lt;p&gt;
ScatterUQ&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22810;&#31867;&#21035;&#26631;&#31614;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26657;&#20934;&#30340;&#31867;&#21035;&#39044;&#27979;&#27010;&#29575;&#21644;&#36229;&#20986;&#20998;&#24067;&#25351;&#31034;&#22120;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#32773;&#21644;&#24037;&#31243;&#24072;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20449;&#24687;&#22312;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#19979;&#23545;&#20219;&#24847;&#25968;&#25454;&#28304;&#36827;&#34892;&#21487;&#35270;&#21270;&#20256;&#36798;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ScatterUQ&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#21487;&#35270;&#21270;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model's confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribu
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06276</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#30340;&#22522;&#22240;&#34920;&#36798;&#20540;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#20102;&#22810;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#20316;&#20026;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65292;&#20197;&#22522;&#20110;&#32959;&#30244;&#36716;&#24405;&#32452;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26377;&#35268;&#21017;&#21270;&#30340;Cox&#22238;&#24402;&#26174;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#21644;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;ANN&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#33391;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#32959;&#30244;&#22522;&#22240;&#34920;&#36798;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#20197;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26469;&#35757;&#32451;Cox&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#20351;&#29992;&#26469;&#33258;The Cancer Genome Atlas&#65288;TCGA&#65289;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;Cox&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#24449;&#65292;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05001</link><description>&lt;p&gt;
COURIER: &#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#29305;&#24449;&#39044;&#35757;&#32451;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#36824;&#21407;
&lt;/p&gt;
&lt;p&gt;
COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features. (arXiv:2306.05001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20114;&#32852;&#32593;&#30340;&#21457;&#23637;&#65292;&#35270;&#35273;&#29305;&#24449;&#24050;&#25104;&#20026;&#24433;&#21709;&#29992;&#25143;&#20852;&#36259;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#34701;&#20837;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#26041;&#21521;&#26159;&#39047;&#20855;&#21069;&#26223;&#30340;&#12290;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#23558;&#24050;&#26377;&#39044;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;&#24471;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#27880;&#20837;&#21040;&#27169;&#22411;&#20013;&#20165;&#33021;&#20135;&#29983;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;&#20197;&#19979;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#26041;&#27861;&#26159;&#20026;&#20102;&#26126;&#30830;&#30340;&#12289;&#37325;&#28857;&#25918;&#22312;&#35821;&#20041;&#29305;&#24449;&#19978;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#65292;&#26080;&#27861;&#23398;&#20064;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#30340;&#20010;&#20154;&#20852;&#36259;; &#20854;&#27425;&#65292;&#39044;&#35757;&#32451;&#30340;&#21482;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;&#20687;&#23884;&#20837;&#19982;&#25105;&#20204;&#22312;CTR&#39044;&#27979;&#20219;&#21153;&#20013;&#25152;&#24050;&#26377;&#30340;&#31867;&#21035;&#21644;&#29289;&#21697;&#26631;&#39064;&#31561;&#35821;&#20041;&#29305;&#24449;&#30456;&#27604;&#26377;&#36739;&#23567;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22810;&#25913;&#36827;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of the multi-media internet, visual characteristics have become an important factor affecting user interests. Thus, incorporating visual features is a promising direction for further performance improvements in click-through rate (CTR) prediction. However, we found that simply injecting the image embeddings trained with established pre-training methods only has marginal improvements. We attribute the failure to two reasons: First, The pre-training methods are designed for well-defined computer vision tasks concentrating on semantic features, and they cannot learn personalized interest in recommendations. Secondly, pre-trained image embeddings only containing semantic information have little information gain, considering we already have semantic features such as categories and item titles as inputs in the CTR prediction task. We argue that a pre-training method tailored for recommendation is necessary for further improvements. To this end, we propose a recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;</title><link>http://arxiv.org/abs/2306.04642</link><description>&lt;p&gt;
DiffusionShield&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;(GDMs)&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#30340;GDMs&#31038;&#21306;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#24182;&#20419;&#36827;&#20102;GDMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26080;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#36215;&#20102;&#26377;&#20851;&#29256;&#26435;&#20445;&#25252;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#33402;&#26415;&#23478;(&#21253;&#25324;&#30011;&#23478;&#21644;&#25668;&#24433;&#24072;)&#36234;&#26469;&#36234;&#25285;&#24515;GDMs&#21487;&#20197;&#27627;&#19981;&#36153;&#21147;&#22320;&#22797;&#21046;&#20182;&#20204;&#29420;&#29305;&#30340;&#21019;&#24847;&#20316;&#21697;&#32780;&#26080;&#38656;&#25480;&#26435;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;GDMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#26696;&#8212;&#8212;DiffusionShield&#12290;&#36890;&#36807;&#23558;&#25152;&#26377;&#26435;&#20449;&#24687;&#32534;&#30721;&#25104;&#19968;&#20010;&#19981;&#21487;&#23519;&#35273;&#30340;&#27700;&#21360;&#24182;&#23558;&#20854;&#27880;&#20837;&#22270;&#20687;&#20013;&#65292;DiffusionShield&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;GDMs&#20405;&#26435;&#12290;&#23427;&#30340;&#27700;&#21360;&#21487;&#20197;&#34987;GDMs&#36731;&#26494;&#22320;&#23398;&#20064;&#24182;&#37325;&#29616;&#22312;&#23427;&#20204;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#12290;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#27700;&#21360;&#65292;&#21487;&#20197;&#25581;&#38706;&#29256;&#26435;&#20405;&#26435;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03311</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#23398;&#20064;&#24207;&#21015;&#20219;&#21153;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Embeddings for Sequential Tasks Using Population of Agents. (arXiv:2306.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22914;&#26524;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20943;&#23569;&#20102;&#25105;&#20204;&#20851;&#20110;&#20182;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37027;&#20040;&#20004;&#20010;&#20219;&#21153;&#23601;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#29702;&#35770;&#20934;&#21017;&#25429;&#25417;&#20102;&#36825;&#31181;&#30452;&#35273;&#65292;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#20195;&#29702;&#20154;&#32676;&#20307;&#26469;&#27979;&#37327;&#24207;&#21015;&#20915;&#31574;&#29615;&#22659;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#38500;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20004;&#20010;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#37327;&#21270;&#27604;&#36739;&#65292;&#22522;&#20110;&#20219;&#21153;&#23884;&#20837;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65306;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an information-theoretic framework to learn fixed-dimensional embeddings for tasks in reinforcement learning. We leverage the idea that two tasks are similar to each other if observing an agent's performance on one task reduces our uncertainty about its performance on the other. This intuition is captured by our information-theoretic criterion which uses a diverse population of agents to measure similarity between tasks in sequential decision-making settings. In addition to qualitative assessment, we empirically demonstrate the effectiveness of our techniques based on task embeddings by quantitative comparisons against strong baselines on two application scenarios: predicting an agent's performance on a test task by observing its performance on a small quiz of tasks, and selecting tasks with desired characteristics from a given set of options.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;</title><link>http://arxiv.org/abs/2306.01213</link><description>&lt;p&gt;
&#22522;&#20110;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#21407;&#21017;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms. (arXiv:2306.01213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35299;&#32544;&#32469;&#30340;&#22240;&#26524;&#34920;&#31034;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22240;&#20854;&#23545;&#25552;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#35299;&#32544;&#32469;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#36890;&#36807;&#22240;&#22240;&#26524;&#20851;&#31995;&#35266;&#23519;&#26631;&#31614;&#26469;&#30417;&#30563;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#27969;&#30340;&#24494;&#20998;&#21516;&#32986;&#20989;&#25968;&#23558;&#22122;&#22768;&#21464;&#37327;&#26144;&#23556;&#21040;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#20013;&#26469;&#24314;&#27169;&#22240;&#26524;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#22240;&#26524;&#35201;&#32032;&#30340;&#35299;&#32544;&#32469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#32469;&#20808;&#39564;&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#22240;&#26524;&#32467;&#26500;&#26469;&#40723;&#21169;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#22240;&#26524;&#20998;&#35299;&#20998;&#24067;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#22240;&#26524;&#35201;&#32032;&#21644;&#26426;&#21046;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#30452;&#21040;&#25490;&#21015;&#21644;&#36880;&#20803;&#37325;&#21442;&#25968;&#21270;&#30340;&#38480;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Learning disentangled causal representations is a challenging problem that has gained significant attention recently due to its implications for extracting meaningful information for downstream tasks. In this work, we define a new notion of causal disentanglement from the perspective of independent causal mechanisms. We propose ICM-VAE, a framework for learning causally disentangled representations supervised by causally related observed labels. We model causal mechanisms using learnable flow-based diffeomorphic functions to map noise variables to latent causal variables. Further, to promote the disentanglement of causal factors, we propose a causal disentanglement prior that utilizes the known causal structure to encourage learning a causally factorized distribution in the latent space. Under relatively mild conditions, we provide theoretical results showing the identifiability of causal factors and mechanisms up to permutation and elementwise reparameterization. We empirically demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19968;&#33324;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#29942;&#39048;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25311;&#21512;&#21644;&#21387;&#32553;&#38454;&#27573;&#12290;&#36890;&#36807;&#35813;&#20998;&#26512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.08013</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#21387;&#32553;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#29942;&#39048;(arXiv:2305.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression. (arXiv:2305.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19968;&#33324;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#29942;&#39048;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25311;&#21512;&#21644;&#21387;&#32553;&#38454;&#27573;&#12290;&#36890;&#36807;&#35813;&#20998;&#26512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#20854;&#26680;&#24515;&#22312;&#20110;&#36319;&#36394;&#38544;&#34255;&#23618;&#19982;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20540;&#21644;&#38544;&#34255;&#23618;&#19982;DNN&#36755;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20540;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#25454;Shwartz-Ziv&#21644;Tishby(2017)&#25552;&#20986;&#30340;&#20551;&#35828;&#65292;&#35757;&#32451;&#36807;&#31243;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#32452;&#25104;:&#25311;&#21512;&#21644;&#21387;&#32553;&#12290;&#21518;&#32773;&#34987;&#35748;&#20026;&#26159;DNNs&#34920;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#23545;&#19968;&#33324;NNs&#30340;IB&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Goldfeld&#31561;&#20154;(2019)&#25552;&#20986;&#30340;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: one between the hidden layer and the class label, and the other between the hidden layer and the DNN input. According to the hypothesis put forth by Shwartz-Ziv and Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis has only been verified for toy NNs or specific types of NNs, such as quantized NNs and dropout NNs. In this paper, we introduce a comprehensive framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compres
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;</title><link>http://arxiv.org/abs/2304.01910</link><description>&lt;p&gt;
&#26657;&#20934;&#28151;&#20081;&#65306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#26080;&#24847;&#20013;&#19988;&#26080;&#23475;
&lt;/p&gt;
&lt;p&gt;
Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable. (arXiv:2304.01910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01910
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36816;&#34892;&#21464;&#21270;&#22312;&#23454;&#38469;&#20013;&#26356;&#23569;&#36935;&#21040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#24182;&#35777;&#26126;&#26041;&#24046;&#20027;&#35201;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#25152;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22312;&#37325;&#22797;&#27979;&#35797;&#26102;&#20250;&#26377;&#26174;&#33879;&#30340;&#27979;&#35797;&#38598;&#24615;&#33021;&#24046;&#24322;&#65292;&#24433;&#21709;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#27604;&#36739;&#21644;&#35757;&#32451;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#27604;&#36739;&#26469;&#35299;&#37322;&#36825;&#31181;&#21464;&#21270;&#65306;&#65288;1&#65289;&#23613;&#31649;&#22312;&#27979;&#35797;&#38598;&#19978;&#26377;&#26174;&#33879;&#30340;&#26041;&#24046;&#65292;&#20294;&#26631;&#20934;&#30340;CIFAR-10&#19982;ImageNet&#35757;&#32451;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#34920;&#29616;&#21364;&#38750;&#24120;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#26041;&#24046;&#19981;&#20687;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#20040;&#20005;&#37325;&#12290;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#20197;&#32039;&#23494;&#36817;&#20284;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#20998;&#24067;&#32467;&#26500;&#12290;&#65288;3&#65289;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20197;&#19979;&#20004;&#20010;&#24847;&#20041;&#19978;&#65292;&#27979;&#35797;&#38598;&#26041;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#39640;&#25935;&#24863;&#24615;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#38543;&#26426;&#28304;&#65288;&#22914;&#25968;&#25454;&#25490;&#24207;&#21644;&#25193;&#20805;&#65289;&#25152;&#23548;&#33268;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#26159;&#39057;&#29575;&#26497;&#38480;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#26469;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which those test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We argue that test-set variance is inevitable in the following two senses. First, we show that variance is largely caused by high sensitivity of the training process to initial conditions, rather than by specific sources of randomness like the data order and augmentations. Second, we prove that variance is u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#20013;&#25152;&#26377;&#35760;&#24405;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#29575;&#21644;&#21361;&#23475;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10053</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#32447;&#24615;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data. (arXiv:2301.10053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#20013;&#25152;&#26377;&#35760;&#24405;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#29575;&#21644;&#21361;&#23475;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65288;SDG&#65289;&#30340;&#21457;&#23637;&#34987;&#35465;&#20026;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#38590;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SDG&#26088;&#22312;&#23398;&#20064;&#30495;&#23454;&#25968;&#25454;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#29983;&#25104;&#32467;&#26500;&#21644;&#32479;&#35745;&#23398;&#30456;&#20284;&#30340;&#8220;&#20154;&#36896;&#8221;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#25512;&#35770;&#25915;&#20987;&#21487;&#33021;&#20250;&#30772;&#22351;&#38544;&#31169;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#24322;&#24120;&#35760;&#24405;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12290;&#35813;&#25915;&#20987;&#22522;&#20110;&#32447;&#24615;&#37325;&#26500;&#26041;&#27861;&#29992;&#20110;&#32858;&#21512;&#32479;&#35745;&#25968;&#25454;&#65292;&#25915;&#20987;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#24322;&#24120;&#20540;&#65292;&#32780;&#26159;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#35760;&#24405;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#26368;&#20808;&#36827;&#30340;SDG&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#21644;&#26368;&#36817;&#30340;&#24046;&#20998;&#38544;&#31169;SDG&#26426;&#21046;&#12290;&#36890;&#36807;&#23450;&#20041;&#24418;&#24335;&#21270;&#30340;&#38544;&#31169;&#21338;&#24328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#29978;&#33267;&#23545;&#20219;&#24847;&#35760;&#24405;&#37117;&#21487;&#20197;&#39640;&#24230;&#31934;&#30830;&#65292;&#24182;&#19988;&#36825;&#26159;&#25915;&#20987;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in synthetic data generation (SDG) have been hailed as a solution to the difficult problem of sharing sensitive data while protecting privacy. SDG aims to learn statistical properties of real data in order to generate "artificial" data that are structurally and statistically similar to sensitive data. However, prior research suggests that inference attacks on synthetic data can undermine privacy, but only for specific outlier records. In this work, we introduce a new attribute inference attack against synthetic data. The attack is based on linear reconstruction methods for aggregate statistics, which target all records in the dataset, not only outliers. We evaluate our attack on state-of-the-art SDG algorithms, including Probabilistic Graphical Models, Generative Adversarial Networks, and recent differentially private SDG mechanisms. By defining a formal privacy game, we show that our attack can be highly accurate even on arbitrary records, and that this is the result o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#30340;&#36830;&#32493;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.00936</link><description>&lt;p&gt;
&#24102;&#22806;&#37096;&#35760;&#24518;&#30340;&#22810;&#27169;&#24577;&#21160;&#24577;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Multi-modal Dynamics with External Memory. (arXiv:2203.00936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#30340;&#36830;&#32493;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#36830;&#32493;&#20986;&#29616;&#26102;&#65292;&#22914;&#20309;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#21160;&#24577;&#29615;&#22659;&#20013;&#12290;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#24847;&#35782;&#21040;&#26032;&#30340;&#27169;&#24335;&#20986;&#29616;&#65292;&#20294;&#23427;&#27809;&#26377;&#35775;&#38382;&#21333;&#20010;&#35757;&#32451;&#24207;&#21015;&#30340;&#30495;&#23454;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#65292;&#22240;&#20026;&#21442;&#25968;&#20256;&#36882;&#21463;&#21040;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#24433;&#21709;&#65292;&#32780;&#24773;&#33410;&#35760;&#24518;&#35774;&#35745;&#38656;&#35201;&#30693;&#36947;&#24207;&#21015;&#30340;&#30495;&#23454;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#24773;&#33410;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#30340;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#20811;&#26381;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#35760;&#24518;&#30340;&#27880;&#24847;&#26435;&#37325;&#19978;&#20351;&#29992;Dirichlet&#36807;&#31243;&#20808;&#39564;&#65292;&#20197;&#20419;&#36827;&#27169;&#24335;&#25551;&#36848;&#31526;&#30340;&#26377;&#25928;&#23384;&#20648;&#12290;&#36890;&#36807;&#26816;&#32034;&#20808;&#21069;&#20219;&#21153;&#30456;&#20284;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#27492;&#25551;&#36848;&#31526;&#39304;&#20837;&#20854;&#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#26469;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. The state-of-the-art continual learning approaches cannot handle this setup, because parameter transfer suffers from catastrophic interference and episodic memory design requires the knowledge of the ground-truth modes of sequences. We devise a novel continual learning method that overcomes both limitations by maintaining a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transitio
&lt;/p&gt;</description></item></channel></rss>