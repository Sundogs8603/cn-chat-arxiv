<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.13651</link><description>&lt;p&gt;
&#33258;&#24102;&#25968;&#25454;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLMs&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#20197;&#21450;&#23427;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#19978;&#30340;&#21464;&#25442;&#23545;LLMs&#30340;&#28789;&#25935;&#24230;&#25110;&#19981;&#21464;&#24615;&#65292;&#30452;&#25509;&#30417;&#25511;LLM&#22312;&#37326;&#22806;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#25110;&#22312;&#27169;&#22411;&#37096;&#32626;&#26399;&#38388;&#36827;&#34892;&#30340;&#27969;&#25968;&#25454;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, tox
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#22270;&#65288;OSG&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#27010;&#25324;&#20986;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13630</link><description>&lt;p&gt;
&#31163;&#32447;&#25216;&#33021;&#22270;&#65288;OSG&#65289;&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#33021;&#36827;&#34892;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Skill Graph (OSG): A Framework for Learning and Planning using Offline Reinforcement Learning Skills. (arXiv:2306.13630v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#22270;&#65288;OSG&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#27010;&#25324;&#20986;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#22312;&#31454;&#25216;&#28216;&#25103;&#20013;&#30340;&#25104;&#21151;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26085;&#24120;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#21463;&#21040;&#38480;&#21046;&#65288;&#20363;&#22914;&#24037;&#19994;&#12289;&#23478;&#24237;&#12289;&#21307;&#30103;&#31561;&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#35268;&#21010;&#31163;&#32447;&#25216;&#33021;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#20849;&#21516;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#27010;&#25324;&#20986;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26426;&#26800;&#33218;&#36827;&#34892;&#27979;&#35797;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning has received wide interest due to its success in competitive games. Yet, its adoption in everyday applications is limited (e.g. industrial, home, healthcare, etc.). In this paper, we address this limitation by presenting a framework for planning over offline skills and solving complex tasks in real-world environments. Our framework is comprised of three modules that together enable the agent to learn from previously collected data and generalize over it to solve long-horizon tasks. We demonstrate our approach by testing it on a robotic arm that is required to solve complex tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#35206;&#30422;&#38382;&#39064;&#65292;&#20854;&#20013;&#21338;&#24328;&#29702;&#35770;&#31639;&#27861;CovGame&#33021;&#35299;&#20915;&#19981;&#21516;PAC RL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.13601</link><description>&lt;p&gt;
PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Active Coverage for PAC Reinforcement Learning. (arXiv:2306.13601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#35206;&#30422;&#38382;&#39064;&#65292;&#20854;&#20013;&#21338;&#24328;&#29702;&#35770;&#31639;&#27861;CovGame&#33021;&#35299;&#20915;&#19981;&#21516;PAC RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#21644;&#21033;&#29992;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#29305;&#24615;&#30340;&#25968;&#25454;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#19981;&#21516;&#26041;&#38754;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#26080;&#22870;&#21169;&#25506;&#32034;&#21644;&#31163;&#32447;&#23398;&#20064;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#21608;&#26399;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#30340;&#20027;&#21160;&#35206;&#30422;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#30446;&#26631;&#26159;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#20197;&#28385;&#36275;&#32473;&#23450;&#30340;&#37319;&#26679;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20027;&#21160;&#35206;&#30422;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;&#19982;&#31034;&#20363;&#26377;&#20851;&#30340;&#19979;&#30028;&#21644;&#19968;&#20010;&#31616;&#21333;&#30340;&#21338;&#24328;&#29702;&#35770;&#31639;&#27861;CovGame&#65292;&#35813;&#31639;&#27861;&#20960;&#20046;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CovGame&#21487;&#29992;&#20316;&#35299;&#20915;&#19981;&#21516;PAC RL&#20219;&#21153;&#30340;&#26500;&#24314;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting and leveraging data with good coverage properties plays a crucial role in different aspects of reinforcement learning (RL), including reward-free exploration and offline learning. However, the notion of "good coverage" really depends on the application at hand, as data suitable for one context may not be so for another. In this paper, we formalize the problem of active coverage in episodic Markov decision processes (MDPs), where the goal is to interact with the environment so as to fulfill given sampling requirements. This framework is sufficiently flexible to specify any desired coverage property, making it applicable to any problem that involves online exploration. Our main contribution is an instance-dependent lower bound on the sample complexity of active coverage and a simple game-theoretic algorithm, CovGame, that nearly matches it. We then show that CovGame can be used as a building block to solve different PAC RL tasks. In particular, we obtain a simple algorithm for
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>TACOformer &#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#35270;&#35282;&#65292;&#21033;&#29992; Token-chAnnel COmpound&#65288;TACO&#65289;Cross Attention &#27169;&#22359;&#65292;&#21516;&#26102;&#24314;&#27169;&#36890;&#36947;&#32423;&#21035;&#21644;&#20196;&#29260;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13592</link><description>&lt;p&gt;
TACOformer: &#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#20196;&#29260;&#36890;&#36947;&#22797;&#21512;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition. (arXiv:2306.13592v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13592
&lt;/p&gt;
&lt;p&gt;
TACOformer &#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#35270;&#35282;&#65292;&#21033;&#29992; Token-chAnnel COmpound&#65288;TACO&#65289;Cross Attention &#27169;&#22359;&#65292;&#21516;&#26102;&#24314;&#27169;&#36890;&#36947;&#32423;&#21035;&#21644;&#20196;&#29260;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#35782;&#21035;&#25104;&#20026;&#20102;&#19968;&#20010;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#27169;&#24577;&#12289;&#22810;&#36890;&#36947;&#30340;&#29983;&#29702;&#20449;&#21495;&#26174;&#33879;&#25552;&#39640;&#20102;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#24182;&#25429;&#33719;&#36328;&#27169;&#24577;&#30340;&#20381;&#36182;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#24573;&#30053;&#20102;&#22810;&#20010;&#36890;&#36947;&#20449;&#21495;&#20043;&#38388;&#30340;&#20196;&#29260;&#21040;&#20196;&#29260;&#25110;&#36890;&#36947;&#21040;&#36890;&#36947;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#35270;&#35282;&#65292;&#23427;&#25972;&#21512;&#20102;&#36890;&#36947;&#32423;&#21035;&#21644;&#20196;&#29260;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#37319;&#29992;&#22797;&#21512;&#26426;&#21046;&#36827;&#34892;&#36328;&#36890;&#36947;&#22788;&#29702;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#31216;&#20026; Token-chAnnel COmpound&#65288;TACO&#65289;Cross Attention&#65292;&#29992;&#20110;&#25191;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TACOformer &#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multi-channel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel correlations of multichannel signals from different modalities, which limits the classification capability of the models to some extent. In this paper, we propose a comprehensive perspective of multimodal fusion that integrates channel-level and token-level cross-modal interactions. Specifically, we introduce a unified cross attention module called Token-chAnnel COmpound (TACO) Cross Attention to perform multimodal fusion, which simultaneously models channel-
&lt;/p&gt;</description></item><item><title>NetBooster&#26159;&#19968;&#31181;&#22686;&#24378;&#24494;&#23567;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36171;&#33021;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.13586</link><description>&lt;p&gt;
NetBooster&#65306;&#31449;&#22312;&#28145;&#24230;&#24040;&#20154;&#30340;&#32937;&#33152;&#19978;&#65292;&#36171;&#33021;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants. (arXiv:2306.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13586
&lt;/p&gt;
&lt;p&gt;
NetBooster&#26159;&#19968;&#31181;&#22686;&#24378;&#24494;&#23567;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36171;&#33021;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#22240;&#22312;&#20247;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#38656;&#27714;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24494;&#23567;&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#30340;&#26377;&#38480;&#27169;&#22411;&#23481;&#37327;&#65292;&#20351;&#24471;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#37322;&#25918;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27492;&#20250;&#24341;&#36215;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NetBooster&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#25193;&#23637;&#65292;&#28982;&#21518;&#32553;&#23567;&#30340;&#31574;&#30053;&#65292;&#22686;&#24378;TNN&#30340;&#26550;&#26500;&#65292;&#20174;&#32780;&#36171;&#33021;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;NetBooster&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24494;&#23567;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy. Extensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#26032;&#26041;&#27861;&#20165;&#23545;&#37492;&#21035;&#22120;&#20989;&#25968;&#26045;&#21152;&#32602;&#20998;&#26799;&#24230;&#33539;&#25968;&#32422;&#26463;&#65292;&#25552;&#39640;&#20102;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;GAN&#20307;&#31995;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#36827;&#34892;&#35757;&#32451;&#30340;GAN&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;Frechet Inception&#36317;&#31163;&#36824;&#26159;Inception Score&#19978;&#37117;&#26377;&#21516;&#26679;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13576</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Penalty Gradient Normalization for Generative Adversarial Networks. (arXiv:2306.13576v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#26032;&#26041;&#27861;&#20165;&#23545;&#37492;&#21035;&#22120;&#20989;&#25968;&#26045;&#21152;&#32602;&#20998;&#26799;&#24230;&#33539;&#25968;&#32422;&#26463;&#65292;&#25552;&#39640;&#20102;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;GAN&#20307;&#31995;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#36827;&#34892;&#35757;&#32451;&#30340;GAN&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;Frechet Inception&#36317;&#31163;&#36824;&#26159;Inception Score&#19978;&#37117;&#26377;&#21516;&#26679;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#65288;PGN&#65289;&#30340;&#26032;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26159;&#30001;&#23574;&#38160;&#30340;&#26799;&#24230;&#31354;&#38388;&#24341;&#36215;&#30340;&#12290;&#19982;&#26799;&#24230;&#24809;&#32602;&#21644;&#35889;&#24402;&#19968;&#21270;&#31561;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;PGN&#20165;&#23545;&#37492;&#21035;&#22120;&#20989;&#25968;&#26045;&#21152;&#32602;&#20998;&#26799;&#24230;&#33539;&#25968;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#37492;&#21035;&#22120;&#30340;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;GAN&#20307;&#31995;&#32467;&#26500;&#65292;&#21482;&#38656;&#36827;&#34892;&#23569;&#37327;&#20462;&#25913;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#32602;&#20998;&#26799;&#24230;&#24402;&#19968;&#21270;&#35757;&#32451;&#30340;GAN&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;Frechet Inception&#36317;&#31163;&#36824;&#26159;Inception Score&#19978;&#37117;&#26377;&#21516;&#26679;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel normalization method called penalty gradient normalization (PGN) to tackle the training instability of Generative Adversarial Networks (GANs) caused by the sharp gradient space. Unlike existing work such as gradient penalty and spectral normalization, the proposed PGN only imposes a penalty gradient norm constraint on the discriminator function, which increases the capacity of the discriminator. Moreover, the proposed penalty gradient normalization can be applied to different GAN architectures with little modification. Extensive experiments on three datasets show that GANs trained with penalty gradient normalization outperform existing methods in terms of both Frechet Inception and Distance and Inception Score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;MLP&#30456;&#36739;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#26088;&#22312;&#25512;&#36827;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.13575</link><description>&lt;p&gt;
MLP&#30340;&#35268;&#27169;&#21270;&#65306;&#24402;&#32435;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;MLP&#30456;&#36739;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#26088;&#22312;&#25512;&#36827;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#26500;&#24314;&#22359;&#8212;&#8212;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#12290;MLP&#30340;&#23454;&#39564;&#24615;&#27934;&#35265;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative "less inductive bias is better", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#23433;&#20840;&#27169;&#24335;&#20462;&#21098;&#8221;&#26041;&#27861;&#35299;&#20915;&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#20013;&#27169;&#24335;&#25968;&#37327;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13561</link><description>&lt;p&gt;
&#23433;&#20840;&#27169;&#24335;&#20462;&#21098;&#19979;&#30340;&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#27169;&#22411;&#39640;&#25928;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Selection for Predictive Pattern Mining Model by Safe Pattern Pruning. (arXiv:2306.13561v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#23433;&#20840;&#27169;&#24335;&#20462;&#21098;&#8221;&#26041;&#27861;&#35299;&#20915;&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#20013;&#27169;&#24335;&#25968;&#37327;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#26159;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24403;&#36755;&#20837;&#34987;&#34920;&#31034;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#38598;&#21512;&#12289;&#22270;&#21644;&#24207;&#21015;&#65289;&#26102;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#12290;&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#32771;&#34385;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#23376;&#32467;&#26500;&#65288;&#20363;&#22914;&#23376;&#38598;&#12289;&#23376;&#22270;&#21644;&#23376;&#24207;&#21015;&#65292;&#31216;&#20026;&#27169;&#24335;&#65289;&#20316;&#20026;&#27169;&#22411;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#12290;&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#38543;&#30528;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#27169;&#24335;&#20462;&#21098;&#65288;SPP&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#39044;&#27979;&#27169;&#24335;&#25366;&#25496;&#20013;&#27169;&#24335;&#25968;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#30340;&#25972;&#20010;&#27169;&#22411;&#26500;&#24314;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#24212;&#29992;&#23427;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#28041;&#21450;&#38598;&#21512;&#12289;&#22270;&#21644;&#24207;&#21015;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive pattern mining is an approach used to construct prediction models when the input is represented by structured data, such as sets, graphs, and sequences. The main idea behind predictive pattern mining is to build a prediction model by considering substructures, such as subsets, subgraphs, and subsequences (referred to as patterns), present in the structured data as features of the model. The primary challenge in predictive pattern mining lies in the exponential growth of the number of patterns with the complexity of the structured data. In this study, we propose the Safe Pattern Pruning (SPP) method to address the explosion of pattern numbers in predictive pattern mining. We also discuss how it can be effectively employed throughout the entire model building process in practical data analysis. To demonstrate the effectiveness of the proposed method, we conduct numerical experiments on regression and classification problems involving sets, graphs, and sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26446;&#32676;&#31639;&#23376;&#24314;&#27169;&#27969;&#24418;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#20419;&#36827;&#20808;&#39564;&#26469;&#21442;&#25968;&#21270;&#31995;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#21487;&#24212;&#29992;&#30340;&#29305;&#24449;&#22686;&#24378;&#26469;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.13544</link><description>&lt;p&gt;
&#24102;&#21464;&#20998;&#26446;&#32676;&#31639;&#23376;&#30340;&#22810;&#27010;&#24565;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Manifold Contrastive Learning with Variational Lie Group Operators. (arXiv:2306.13544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26446;&#32676;&#31639;&#23376;&#24314;&#27169;&#27969;&#24418;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#20419;&#36827;&#20808;&#39564;&#26469;&#21442;&#25968;&#21270;&#31995;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#21487;&#24212;&#29992;&#30340;&#29305;&#24449;&#22686;&#24378;&#26469;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21487;&#36716;&#31227;&#34920;&#31034;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#19982;&#29983;&#29289;&#35270;&#35273;&#30340;&#33145;&#20391;&#36890;&#36335;&#27169;&#22411;&#31867;&#20284;&#65292;&#35266;&#23519;&#21040;&#36825;&#20123;&#32593;&#32476;&#23548;&#33268;&#20102;&#20498;&#25968;&#31532;&#20108;&#23618;&#34920;&#31034;&#20013;&#31867;&#21035;&#27969;&#24418;&#30340;&#20998;&#31163;&#12290;&#23613;&#31649;&#36825;&#31181;&#35266;&#23519;&#31526;&#21512;&#34920;&#31034;&#23398;&#20064;&#30340;&#27969;&#24418;&#20551;&#35774;&#65292;&#20294;&#30446;&#21069;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#22312;&#26126;&#30830;&#24314;&#27169;&#36825;&#20010;&#27969;&#24418;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24448;&#24448;&#20165;&#24212;&#29992;&#26469;&#33258;&#39044;&#20808;&#25351;&#23450;&#30340;&#8220;&#27491;&#23545;&#27604;&#23545;&#8221;&#38598;&#21512;&#30340;&#22686;&#24378;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#30001;&#31232;&#30095;&#20419;&#36827;&#20808;&#39564;&#30340;&#31995;&#25968;&#21442;&#25968;&#21270;&#30340;&#26446;&#32676;&#31639;&#23376;&#26469;&#30452;&#25509;&#24314;&#27169;&#28508;&#22312;&#27969;&#24418;&#12290;&#36825;&#20123;&#31995;&#25968;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#20102;&#27969;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#26679;&#26412;&#25552;&#20379;&#20102;&#21487;&#24212;&#29992;&#30340;&#29305;&#24449;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of deep neural networks has become a prevalent paradigm for learning representations that transfer to a variety of downstream tasks. Similar to proposed models of the ventral stream of biological vision, it is observed that these networks lead to a separation of category manifolds in the representations of the penultimate layer. Although this observation matches the manifold hypothesis of representation learning, current self-supervised approaches are limited in their ability to explicitly model this manifold. Indeed, current approaches often only apply augmentations from a pre-specified set of "positive pairs" during learning. In this work, we propose a contrastive learning approach that directly models the latent manifold using Lie group operators parameterized by coefficients with a sparsity-promoting prior. A variational distribution over these coefficients provides a generative model of the manifold, with samples which provide feature augmentations applica
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TorGNN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#36816;&#29992;&#35299;&#26512;&#25197;&#26354;&#24230;&#37327;&#21270;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#24182;&#22312;16&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.13541</link><description>&lt;p&gt;
&#25197;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Torsion Graph Neural Networks. (arXiv:2306.13541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TorGNN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#36816;&#29992;&#35299;&#26512;&#25197;&#26354;&#24230;&#37327;&#21270;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#24182;&#22312;16&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23427;&#20204;&#34987;&#24320;&#21457;&#20986;&#26469;&#23558;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#20449;&#24687;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;&#21463;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#31163;&#25955;&#40654;&#26364;&#26354;&#29575;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TorGNN&#65292;&#19968;&#31181;&#22686;&#24378;&#20102;&#35299;&#26512;&#25197;&#26354;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#29992;&#22522;&#20110;&#35299;&#26512;&#25197;&#26354;&#24230;&#30340;&#26435;&#37325;&#20844;&#24335;&#26469;&#34920;&#24449;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#25968;&#23398;&#19978;&#65292;&#35299;&#26512;&#25197;&#26354;&#24230;&#26159;&#19968;&#31181;&#25299;&#25169;&#19981;&#21464;&#37327;&#65292;&#21487;&#20197;&#21306;&#20998;&#21516;&#20262;&#20294;&#19981;&#21516;&#32986;&#30340;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#30340;TorGNN&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#36793;&#65292;&#25105;&#20204;&#37117;&#21487;&#20197;&#25214;&#21040;&#30456;&#24212;&#30340;&#23616;&#37096;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#28982;&#21518;&#35745;&#31639;&#20854;&#35299;&#26512;&#25197;&#26354;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#20854;&#29992;&#20316;&#20449;&#24687;&#20256;&#36882;&#36807;&#31243;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;TorGNN&#27169;&#22411;&#24050;&#22312;&#26469;&#33258;16&#31181;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) models have demonstrated a great potential for the analysis of non-Euclidian data. They are developed to incorporate the geometric and topological information of non-Euclidian data into the end-to-end deep learning architectures. Motivated by the recent success of discrete Ricci curvature in graph neural network (GNNs), we propose TorGNN, an analytic Torsion enhanced Graph Neural Network model. The essential idea is to characterize graph local structures with an analytic torsion based weight formula. Mathematically, analytic torsion is a topological invariant that can distinguish spaces which are homotopy equivalent but not homeomorphic. In our TorGNN, for each edge, a corresponding local simplicial complex is identified, then the analytic torsion (for this local simplicial complex) is calculated, and further used as a weight (for this edge) in message-passing process. Our TorGNN model is validated on link prediction tasks from sixteen different types of n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; PathMLP&#65292;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#36335;&#24452;&#37319;&#26679;&#31574;&#30053;&#30340;&#36731;&#37327;&#21270;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479; GNNs &#20013;&#22312;&#33719;&#24471;&#39640;&#38454;&#20449;&#24687;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#21363;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12289;&#39640;&#38454;&#20449;&#24687;&#26410;&#20805;&#20998;&#21033;&#29992;&#20197;&#21450;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#39640;&#38454;&#20449;&#24687;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.13532</link><description>&lt;p&gt;
PathMLP&#65306;&#39640;&#38454;&#21516;&#36136;&#24615;&#30340;&#24179;&#28369;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
PathMLP: Smooth Path Towards High-order Homophily. (arXiv:2306.13532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; PathMLP&#65292;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#36335;&#24452;&#37319;&#26679;&#31574;&#30053;&#30340;&#36731;&#37327;&#21270;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479; GNNs &#20013;&#22312;&#33719;&#24471;&#39640;&#38454;&#20449;&#24687;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#21363;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12289;&#39640;&#38454;&#20449;&#24687;&#26410;&#20805;&#20998;&#21033;&#29992;&#20197;&#21450;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#39640;&#38454;&#20449;&#24687;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#22270;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#24322;&#36136;&#24615;&#65292;&#33410;&#28857;&#19981;&#20877;&#20542;&#21521;&#20110;&#36830;&#25509;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#65292;&#25361;&#25112;&#20102;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#24182;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26576;&#20123;&#24322;&#36136;&#25968;&#25454;&#30340;&#39640;&#38454;&#20449;&#24687;&#34920;&#29616;&#20986;&#39640;&#21516;&#36136;&#24615;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20013;&#28041;&#21450;&#39640;&#38454;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;GNNs&#20013;&#33719;&#24471;&#39640;&#38454;&#20449;&#24687;&#30340;&#24120;&#35265;&#20570;&#27861;&#20027;&#35201;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#21644;&#25913;&#21464;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#34429;&#28982;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19977;&#20010;&#32570;&#28857;&#65306;1&#65289;&#30001;&#20110;&#36807;&#24230;&#30340;&#27169;&#22411;&#28145;&#24230;&#21644;&#20256;&#25773;&#26102;&#38388;&#32780;&#36807;&#24230;&#24179;&#28369;; 2&#65289;&#39640;&#38454;&#20449;&#24687;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;; 3&#65289;&#35745;&#31639;&#25928;&#29575;&#20302;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#36335;&#24452;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#25429;&#33719;&#21253;&#21547;&#39640;&#38454;&#21516;&#36136;&#24615;&#30340;&#24179;&#28369;&#36335;&#24452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;(Multi-layer Perceptrons, MLP)&#30340;&#36731;&#37327;&#21270;&#27169;&#22411;&#65292;&#31216;&#20043;&#20026;PathMLP&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs exhibit increasing heterophily, where nodes no longer tend to be connected to nodes with the same label, challenging the homophily assumption of classical graph neural networks (GNNs) and impeding their performance. Intriguingly, we observe that certain high-order information on heterophilous data exhibits high homophily, which motivates us to involve high-order information in node representation learning. However, common practices in GNNs to acquire high-order information mainly through increasing model depth and altering message-passing mechanisms, which, albeit effective to a certain extent, suffer from three shortcomings: 1) over-smoothing due to excessive model depth and propagation times; 2) high-order information is not fully utilized; 3) low computational efficiency. In this regard, we design a similarity-based path sampling strategy to capture smooth paths containing high-order homophily. Then we propose a lightweight model based on multi-layer perceptrons (M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#21457;&#29616;&#39640;&#26031;&#21270;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#23545;&#20110;&#32500;&#25968;&#22686;&#21152;&#21576;&#29616;&#20986;&#32447;&#24615;&#20851;&#31995;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#32500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13520</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26059;&#36716;&#39640;&#26031;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of Gaussianization with Random Rotations. (arXiv:2306.13520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#21457;&#29616;&#39640;&#26031;&#21270;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#23545;&#20110;&#32500;&#25968;&#22686;&#21152;&#21576;&#29616;&#20986;&#32447;&#24615;&#20851;&#31995;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#32500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#35266;&#23519;&#21040;&#20854;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23545;&#20110;&#39640;&#26031;&#36755;&#20837;&#65292;&#25152;&#38656;&#30340;&#23618;&#25968;&#19982;&#32500;&#25968;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35813;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#32500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussianization is a simple generative model that can be trained without backpropagation. It has shown compelling performance on low dimensional data. As the dimension increases, however, it has been observed that the convergence speed slows down. We show analytically that the number of required layers scales linearly with the dimension for Gaussian input. We argue that this is because the model is unable to capture dependencies between dimensions. Empirically, we find the same linear increase in cost for arbitrary input $p(x)$, but observe favorable scaling for some distributions. We explore potential speed-ups and formulate challenges for further research.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26356;&#19968;&#33324;&#30340;&#20108;&#36827;&#21046;&#22495;&#65292;&#26356;&#33021;&#25269;&#24481;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#25913;&#36827;&#30340;&#21387;&#32553;&#21644;&#36991;&#20813;&#20102;&#20005;&#37325;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.13515</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#22495;&#24191;&#20041;&#21270;&#65306;&#29992;&#20110; BNN &#31232;&#30095;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Binary domain generalization for sparsifying binary neural networks. (arXiv:2306.13515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26356;&#19968;&#33324;&#30340;&#20108;&#36827;&#21046;&#22495;&#65292;&#26356;&#33021;&#25269;&#24481;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#25913;&#36827;&#30340;&#21387;&#32553;&#21644;&#36991;&#20813;&#20102;&#20005;&#37325;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476; (BNN) &#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#20854;&#25104;&#21151;&#65292;&#20294; BNN &#20173;&#28982;&#21463;&#21040;&#22266;&#23450;&#21644;&#26377;&#38480;&#30340;&#21387;&#32553;&#22240;&#23376;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#30340;&#20840;&#31934;&#24230; DNN &#31232;&#30095;&#21270;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110; BNN&#12290;&#20107;&#23454;&#19978;&#65292;&#23545; BNN &#36827;&#34892;&#26435;&#37325;&#31232;&#30095;&#21270;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126; BNN &#30340;&#26631;&#20934;&#20108;&#36827;&#21046;&#21270;&#22495;&#24182;&#19981;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26356;&#19968;&#33324;&#30340;&#20108;&#36827;&#21046;&#22495;&#65292;&#25193;&#23637;&#20102;&#26631;&#20934;&#20108;&#36827;&#21046;&#22495;&#65292;&#26356;&#33021;&#25269;&#24481;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#25913;&#36827;&#30340;&#21387;&#32553;&#21644;&#36991;&#20813;&#20102;&#20005;&#37325;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#20010;&#23558;&#20840;&#31934;&#24230;&#32593;&#32476;&#30340;&#26435;&#37325;&#37327;&#21270;&#21040;&#25152;&#25552;&#20986;&#30340;&#20108;&#36827;&#21046;&#22495;&#30340;&#38381;&#24335;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#31232;&#30095;&#21270;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#22312; CIFAR-10 &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary neural networks (BNNs) are an attractive solution for developing and deploying deep neural network (DNN)-based applications in resource constrained devices. Despite their success, BNNs still suffer from a fixed and limited compression factor that may be explained by the fact that existing pruning methods for full-precision DNNs cannot be directly applied to BNNs. In fact, weight pruning of BNNs leads to performance degradation, which suggests that the standard binarization domain of BNNs is not well adapted for the task. This work proposes a novel more general binary domain that extends the standard binary one that is more robust to pruning techniques, thus guaranteeing improved compression and avoiding severe performance losses. We demonstrate a closed-form solution for quantizing the weights of a full-precision network into the proposed binary domain. Finally, we show the flexibility of our method, which can be combined with other pruning strategies. Experiments over CIFAR-10 
&lt;/p&gt;</description></item><item><title>DISCO-10M&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20854;&#35268;&#27169;&#36229;&#36807;&#20102;&#20043;&#21069;&#26368;&#22823;&#30340;&#21487;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#36164;&#28304;&#21644;CLAP&#23884;&#20837;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13512</link><description>&lt;p&gt;
DISCO-10M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DISCO-10M: A Large-Scale Music Dataset. (arXiv:2306.13512v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13512
&lt;/p&gt;
&lt;p&gt;
DISCO-10M&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20854;&#35268;&#27169;&#36229;&#36807;&#20102;&#20043;&#21069;&#26368;&#22823;&#30340;&#21487;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#36164;&#28304;&#21644;CLAP&#23884;&#20837;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#25968;&#25454;&#38598;&#22312;&#25512;&#21160;&#38899;&#20048;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#38899;&#20048;&#25968;&#25454;&#38598;&#23384;&#22312;&#35268;&#27169;&#26377;&#38480;&#12289;&#21487;&#35775;&#38382;&#24615;&#24046;&#21644;&#32570;&#20047;&#38899;&#39057;&#36164;&#28304;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DISCO-10M&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20854;&#35268;&#27169;&#36229;&#36807;&#20102;&#20043;&#21069;&#26368;&#22823;&#30340;&#21487;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20026;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36807;&#28388;&#27969;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#21644;&#38899;&#39057;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39044;&#35745;&#31639;&#30340;CLAP&#23884;&#20837;&#21644;DISCO-10M&#65292;&#20415;&#20110;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#23884;&#20837;&#20351;&#24471;&#23545;&#25152;&#25552;&#20379;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#39640;&#25928;&#25506;&#32034;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;DISCO-10M&#65292;&#25512;&#21160;&#38899;&#20048;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#26032;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#27665;&#20027;&#21270;&#21644;&#20419;&#36827;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#36890;&#36807;&#26041;&#24046;&#26368;&#22823;&#21270;&#21407;&#29702;&#21644;&#37325;&#26500;&#35823;&#24046;&#26368;&#23567;&#21270;&#20004;&#31181;&#26041;&#27861;&#25512;&#23548;&#24471;&#38381;&#21512;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.13503</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#20004;&#31181;&#25512;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two derivations of Principal Component Analysis on datasets of distributions. (arXiv:2306.13503v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#36890;&#36807;&#26041;&#24046;&#26368;&#22823;&#21270;&#21407;&#29702;&#21644;&#37325;&#26500;&#35823;&#24046;&#26368;&#23567;&#21270;&#20004;&#31181;&#26041;&#27861;&#25512;&#23548;&#24471;&#38381;&#21512;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#24212;&#29992;&#20110;&#30001;&#20854;&#20301;&#32622;&#21644;&#21327;&#26041;&#24046;&#21051;&#30011;&#30340;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#32780;&#38750;&#28857;&#25968;&#25454;&#38598;&#12290;&#19982;&#28857;&#25968;&#25454;&#38598;&#19978;&#30340;&#24120;&#35268;PCA&#21487;&#20197;&#31561;&#25928;&#22320;&#36890;&#36807;&#26041;&#24046;&#26368;&#22823;&#21270;&#21407;&#29702;&#21644;&#37325;&#26500;error&#26368;&#23567;&#21270;&#20004;&#31181;&#26041;&#27861;&#25512;&#23548;&#20986;&#19968;&#26679;&#65292;&#25105;&#20204;&#20174;&#36825;&#20004;&#20010;&#26041;&#38754;&#25512;&#23548;&#20986;&#20102;&#20998;&#24067;PCA&#30340;&#38381;&#21512;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this brief note, we formulate Principal Component Analysis (PCA) over datasets consisting not of points but of distributions, characterized by their location and covariance. Just like the usual PCA on points can be equivalently derived via a variance-maximization principle and via a minimization of reconstruction error, we derive a closed-form solution for distributional PCA from both of these perspectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#23376;&#31354;&#38388;&#32858;&#31867;&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#22810;&#20010;&#24369;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#32452;&#21512;&#25104;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.13500</link><description>&lt;p&gt;
&#22522;&#20110;&#32423;&#32852;&#23376;&#31354;&#38388;&#32858;&#31867;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cascade Subspace Clustering for Outlier Detection. (arXiv:2306.13500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#23376;&#31354;&#38388;&#32858;&#31867;&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#22810;&#20010;&#24369;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#32452;&#21512;&#25104;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#31232;&#30095;&#21644;&#20302;&#31209;&#34920;&#31034;&#30340;&#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#65292;&#20197;&#20445;&#35777;&#27491;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#33258;&#34920;&#31034;&#34920;&#26126;&#65292;&#23376;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#28857;&#24635;&#21487;&#20197;&#34920;&#31034;&#20026;&#23376;&#31354;&#38388;&#20013;&#20854;&#20182;&#28857;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#21487;&#20197;&#22312;&#33258;&#34920;&#31034;&#19978;&#23450;&#20041;&#19968;&#20010;&#36866;&#24403;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20869;&#28857;&#21644;&#22806;&#28857;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#33258;&#34920;&#31034;&#30340;&#37325;&#26500;&#35823;&#24046;&#20173;&#28982;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#65292;&#20294;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#21463;&#26799;&#24230;&#25552;&#21319;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#27425;&#33258;&#34920;&#31034;&#30340;&#36845;&#20195;&#26041;&#24335;&#65292;&#23558;&#19968;&#31995;&#21015;&#24369;&#8220;&#24322;&#24120;&#26816;&#27979;&#22120;&#8221;&#32452;&#21512;&#25104;&#21333;&#20010;&#24378;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#22522;&#20110;&#24377;&#24615;&#32593;&#26500;&#36896;&#33258;&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#23450;&#20041;&#36866;&#24403;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#33258;&#34920;&#31034;&#30340;&#27531;&#24046;&#29992;&#20110;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#19979;&#19968;&#20010;&#36739;&#24369;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection. Self-representation states that a point in a subspace can always be expressed as a linear combination of other points in the subspace. A suitable Markov Chain can be defined on the self-representation and it allows us to recognize the difference between inliers and outliers. However, the reconstruction error of self-representation that is still informative to detect outlier detection, is neglected.Inspired by the gradient boosting, in this paper, we propose a new outlier detection framework that combines a series of weak "outlier detectors" into a single strong one in an iterative fashion by constructing multi-pass self-representation. At each stage, we construct a self-representation based on elastic-net and define a suitable Markov Chain on it to detect outliers. The residual of the self-representation is used for the next stage to learn the next weaker outlier 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;Boost&#19981;&#21464;&#22810;&#39033;&#24335;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#20998;&#26512;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#26377;&#25928;&#20998;&#31867;&#65292;&#21516;&#26102;&#21152;&#36895;&#31639;&#27861;&#25191;&#34892;&#65292;&#24615;&#33021;&#25509;&#36817;&#20110;&#20351;&#29992;&#23436;&#25972;&#20449;&#24687;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13496</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;Boost invariance&#31526;&#21495;&#35266;&#27979;&#37327;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieval of Boost Invariant Symbolic Observables via Feature Importance. (arXiv:2306.13496v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;Boost&#19981;&#21464;&#22810;&#39033;&#24335;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#20998;&#26512;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#26377;&#25928;&#20998;&#31867;&#65292;&#21516;&#26102;&#21152;&#36895;&#31639;&#27861;&#25191;&#34892;&#65292;&#24615;&#33021;&#25509;&#36817;&#20110;&#20351;&#29992;&#23436;&#25972;&#20449;&#24687;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#33021;&#29289;&#29702;&#30340;&#21943;&#27880;&#26631;&#31614;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34987;&#25551;&#36848;&#20026;&#40657;&#30418;&#23376;&#65292;&#23427;&#20204;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#65292;&#24456;&#38590;&#25552;&#21462;&#20851;&#38190;&#30340;&#21306;&#21035;&#35266;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Boost&#19981;&#21464;&#22810;&#39033;&#24335;&#26041;&#27861;&#65292;&#23427;&#33021;&#30452;&#25509;&#20998;&#26512;&#34920;&#31034;&#32473;&#23450;&#20219;&#21153;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#31616;&#21333;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#25552;&#20379;&#19968;&#20010;&#26497;&#20302;&#32500;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#20195;&#34920;&#26377;&#25928;&#21306;&#21035;&#29289;&#29702;&#30456;&#20851;&#35266;&#27979;&#37327;&#30340;&#26368;&#23567;&#29305;&#24449;&#38598;&#65292;&#24182;&#22914;&#20309;&#22240;&#27492;&#21152;&#36895;&#31639;&#27861;&#25191;&#34892;&#65292;&#30456;&#23545;&#25509;&#36817;&#20110;&#20351;&#29992;&#23436;&#25972;&#20449;&#24687;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning approaches for jet tagging in high-energy physics are characterized as black boxes that process a large amount of information from which it is difficult to extract key distinctive observables. In this proceeding, we present an alternative to deep learning approaches, Boost Invariant Polynomials, which enables direct analysis of simple analytic expressions representing the most important features in a given task. Further, we show how this approach provides an extremely low dimensional classifier with a minimum set of features representing %effective discriminating physically relevant observables and how it consequently speeds up the algorithm execution, with relatively close performance to the algorithm using the full information.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#30005;&#36335;&#39564;&#35777;&#65292;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#20223;&#30495;&#27425;&#25968;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#30005;&#36335;&#21709;&#24212;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#26368;&#22351;&#24773;&#20917;&#21644;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2306.13484</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#30005;&#36335;&#39564;&#35777;&#30340;&#33258;&#36866;&#24212;&#35268;&#21010;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Planning Search Algorithm for Analog Circuit Verification. (arXiv:2306.13484v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13484
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#30005;&#36335;&#39564;&#35777;&#65292;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#20223;&#30495;&#27425;&#25968;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#30005;&#36335;&#21709;&#24212;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#26368;&#22351;&#24773;&#20917;&#21644;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#30005;&#36335;&#39564;&#35777;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#36825;&#20123;&#30005;&#36335;&#27599;&#24180;&#37117;&#22312;&#19981;&#26029;&#22686;&#38271;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;Si&#21069;&#39564;&#35777;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#27491;&#24120;&#21151;&#33021;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20943;&#23569;&#25163;&#21160;&#39564;&#35777;IC&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36739;&#23569;&#30340;&#20223;&#30495;&#27425;&#25968;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#25805;&#20316;&#26465;&#20214;&#37197;&#32622;&#65288;OCC&#65289;&#65292;&#20197;&#35757;&#32451;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26367;&#20195;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20986;&#36827;&#19968;&#27493;&#26356;&#22256;&#38590;&#30340;OCC&#12290;&#23545;&#20110;&#20960;&#20010;&#36845;&#20195;&#37325;&#22797;&#36825;&#20010;&#36807;&#31243;&#24050;&#32463;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30005;&#36335;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;GP&#20272;&#35745;&#30005;&#36335;&#21709;&#24212;&#65292;&#25552;&#39640;&#20102;&#25214;&#21040;&#26576;&#20123;&#30005;&#36335;&#21709;&#24212;&#30340;&#26368;&#22351;&#24773;&#20917;&#29978;&#33267;&#20351;&#20043;&#22833;&#36133;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20026;&#25152;&#26377;&#30005;&#36335;&#25552;&#20379;&#26356;&#25509;&#36817;&#35268;&#26684;&#30340;OCC&#65292;&#24182;&#30830;&#23450;&#19968;&#31181;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrated circuit verification has gathered considerable interest in recent times. Since these circuits keep growing in complexity year by year, pre-Silicon (pre-SI) verification becomes ever more important, in order to ensure proper functionality. Thus, in order to reduce the time needed for manually verifying ICs, we propose a machine learning (ML) approach, which uses less simulations. This method relies on an initial evaluation set of operating condition configurations (OCCs), in order to train Gaussian process (GP) surrogate models. By using surrogate models, we can propose further, more difficult OCCs. Repeating this procedure for several iterations has shown better GP estimation of the circuit's responses, on both synthetic and real circuits, resulting in a better chance of finding the worst case, or even failures, for certain circuit responses. Thus, we show that the proposed approach is able to provide OCCs closer to the specifications for all circuits and identify a failure 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36830;&#32493;&#25512;&#26029;&#32593;&#32476;&#65288;CINs&#65289;&#30340;&#27010;&#24565;&#65292;&#37325;&#28857;&#35752;&#35770;&#22312;&#32447;&#25512;&#29702;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13474</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#32447;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Processing with Deep Neural Networks. (arXiv:2306.13474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36830;&#32493;&#25512;&#26029;&#32593;&#32476;&#65288;CINs&#65289;&#30340;&#27010;&#24565;&#65292;&#37325;&#28857;&#35752;&#35770;&#22312;&#32447;&#25512;&#29702;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#24212;&#29992;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#22686;&#38271;&#65306;&#35270;&#35273;&#27169;&#22411;&#20934;&#30830;&#20998;&#31867;&#20154;&#31867;&#22312;&#35270;&#39057;&#20013;&#30340;&#21160;&#20316;&#65292;&#24182;&#20687;&#20154;&#31867;&#19987;&#23478;&#19968;&#26679;&#20934;&#30830;&#22320;&#35782;&#21035;&#21307;&#23398;&#25195;&#25551;&#20013;&#30340;&#30284;&#32454;&#32990;&#32452;&#32455;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#20889;&#20316;&#65292;&#25104;&#20026;&#26085;&#24120;&#39184;&#26700;&#35848;&#35770;&#30340;&#20027;&#39064;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#29992;&#36884;&#20196;&#20154;&#25391;&#22859;&#65292;&#20294;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20063;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#35757;&#32451;&#21644;&#26381;&#21153;&#27169;&#22411;&#30340;&#32463;&#27982;&#25104;&#26412;&#21644;&#36127;&#38754;&#29615;&#22659;&#22806;&#37096;&#24615;&#19982;&#36130;&#21153;&#21487;&#34892;&#24615;&#21644;&#27668;&#20505;&#34892;&#21160;&#30446;&#26631;&#19981;&#30456;&#21327;&#35843;&#12290;&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#32780;&#19981;&#26159;&#36861;&#27714;&#39044;&#27979;&#24615;&#33021;&#30340;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20855;&#20307;&#22320;&#65292;&#26412;&#35770;&#25991;&#30340;&#19968;&#20010;&#26680;&#24515;&#36129;&#29486;&#26159;&#35299;&#20915;&#20102;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36830;&#32493;&#25512;&#26029;&#32593;&#32476;&#65288;CINs&#65289;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities and adoption of deep neural networks (DNNs) grow at an exhilarating pace: Vision models accurately classify human actions in videos and identify cancerous tissue in medical scans as precisely than human experts; large language models answer wide-ranging questions, generate code, and write prose, becoming the topic of everyday dinner-table conversations. Even though their uses are exhilarating, the continually increasing model sizes and computational complexities have a dark side. The economic cost and negative environmental externalities of training and serving models is in evident disharmony with financial viability and climate action goals.  Instead of pursuing yet another increase in predictive performance, this dissertation is dedicated to the improvement of neural network efficiency. Specifically, a core contribution addresses the efficiency aspects during online inference. Here, the concept of Continual Inference Networks (CINs) is proposed and explored across fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;RPM&#22312;&#22270;&#20687;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#20302;&#32500;&#31163;&#25955;&#38544;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#28508;&#22312;&#21464;&#37327;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36866;&#24403;&#22320;&#39044;&#27979;&#30446;&#26631;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13472</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#30340;&#28508;&#22312;&#23376;&#32676;&#36716;&#25442;&#19979;&#30340;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prediction under Latent Subgroup Shifts with High-Dimensional Observations. (arXiv:2306.13472v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;RPM&#22312;&#22270;&#20687;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#20302;&#32500;&#31163;&#25955;&#38544;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#28508;&#22312;&#21464;&#37327;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36866;&#24403;&#22320;&#39044;&#27979;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#24418;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#28508;&#22312;&#21464;&#37327;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#36716;&#25442;&#65292;&#21363;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#20998;&#24067;&#19981;&#21516;&#12290;&#26412;&#30740;&#31350;&#36816;&#29992;&#35782;&#21035;&#21442;&#25968;&#27169;&#22411;&#65288;RPM&#65289;&#22312;&#22270;&#20687;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#20302;&#32500;&#31163;&#25955;&#38544;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#20855;&#20307;&#38382;&#39064;&#20013;&#36866;&#24403;&#22320;&#39044;&#27979;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new approach to prediction in graphical models with latent-shift adaptation, i.e., where source and target environments differ in the distribution of an unobserved confounding latent variable. Previous work has shown that as long as "concept" and "proxy" variables with appropriate dependence are observed in the source environment, the latent-associated distributional changes can be identified, and target predictions adapted accurately. However, practical estimation methods do not scale well when the observations are complex and high-dimensional, even if the confounding latent is categorical. Here we build upon a recently proposed probabilistic unsupervised learning framework, the recognition-parametrised model (RPM), to recover low-dimensional, discrete latents from image observations. Applied to the problem of latent shifts, our novel form of RPM identifies causal latent structure in the source environment, and adapts properly to predict in the target. We demonstrate re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35748;&#20026;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#25104;&#21151;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#25311;&#21512;&#38543;&#26426;&#29366;&#24577;&#21450;&#38543;&#26426;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26426;&#25968;&#25454;&#30340;&#33021;&#21147;&#36829;&#21453;&#20102;&#24403;&#21069;&#23567;&#27867;&#21270;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#26500;&#24314;&#34917;&#20805;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21487;&#23558;&#20219;&#24847;&#26631;&#35760;&#25311;&#21512;&#21040;&#37327;&#23376;&#29366;&#24577;&#19978;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#36825;&#20123;&#32467;&#26524;&#25490;&#38500;&#20102;&#21333;&#21333;&#22522;&#20110;&#32463;&#20856;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#25152;&#26377;&#21487;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.13461</link><description>&lt;p&gt;
&#29702;&#35299;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding quantum machine learning also requires rethinking generalization. (arXiv:2306.13461v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35748;&#20026;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#25104;&#21151;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#25311;&#21512;&#38543;&#26426;&#29366;&#24577;&#21450;&#38543;&#26426;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26426;&#25968;&#25454;&#30340;&#33021;&#21147;&#36829;&#21453;&#20102;&#24403;&#21069;&#23567;&#27867;&#21270;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#26500;&#24314;&#34917;&#20805;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21487;&#23558;&#20219;&#24847;&#26631;&#35760;&#25311;&#21512;&#21040;&#37327;&#23376;&#29366;&#24577;&#19978;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#36825;&#20123;&#32467;&#26524;&#25490;&#38500;&#20102;&#21333;&#21333;&#22522;&#20110;&#32463;&#20856;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#25152;&#26377;&#21487;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21482;&#29992;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#25104;&#21151;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30340;&#38543;&#26426;&#21270;&#23454;&#39564;&#65292;&#23637;&#31034;&#20256;&#32479;&#30340;&#29702;&#35299;&#27867;&#21270;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#36825;&#20123;&#37327;&#23376;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20934;&#30830;&#22320;&#25311;&#21512;&#38543;&#26426;&#29366;&#24577;&#21644;&#38543;&#26426;&#35757;&#32451;&#25968;&#25454;&#30340;&#26631;&#35760;&#12290;&#36825;&#31181;&#35760;&#24518;&#38543;&#26426;&#25968;&#25454;&#30340;&#33021;&#21147;&#36829;&#21453;&#20102;&#24403;&#21069;&#23567;&#27867;&#21270;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#24314;&#31435;&#22312;VC&#32500;&#12289;Rademacher&#22797;&#26434;&#24230;&#21644;&#25152;&#26377;&#22343;&#21248;&#30456;&#20851;&#24615;&#24230;&#37327;&#22522;&#30784;&#19978;&#30340;&#26041;&#27861;&#26377;&#20123;&#26840;&#25163;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#26500;&#24314;&#34917;&#20805;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23558;&#20219;&#24847;&#26631;&#35760;&#25311;&#21512;&#21040;&#37327;&#23376;&#29366;&#24577;&#19978;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#19981;&#25490;&#38500;&#21482;&#29992;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#23601;&#33021;&#33719;&#24471;&#33391;&#22909;&#27867;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#26159;&#25490;&#38500;&#20102;&#21333;&#21333;&#22522;&#20110;&#32463;&#20856;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#25152;&#26377;&#21487;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning models have shown successful generalization performance even when trained with few data. In this work, through systematic randomization experiments, we show that traditional approaches to understanding generalization fail to explain the behavior of such quantum models. Our experiments reveal that state-of-the-art quantum neural networks accurately fit random states and random labeling of training data. This ability to memorize random data defies current notions of small generalization error, problematizing approaches that build on complexity measures such as the VC dimension, the Rademacher complexity, and all their uniform relatives. We complement our empirical results with a theoretical construction showing that quantum neural networks can fit arbitrary labels to quantum states, hinting at their memorization ability. Our results do not preclude the possibility of good generalization with few training data but rather rule out any possible guarantees based only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27668;&#20505;&#21644;&#23186;&#20171;&#29289;&#24188;&#34411;&#25351;&#25968;&#23545;&#27888;&#31859;&#23572;&#32435;&#24503;&#37030;&#30331;&#38761;&#28909;&#30123;&#24773;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#34442;&#23376;&#24188;&#34411;&#25351;&#25968;&#25552;&#39640;&#20102;&#30123;&#24773;&#39044;&#27979;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.13456</link><description>&lt;p&gt;
&#21033;&#29992;&#27668;&#35937;&#21644;&#26118;&#34411;&#23398;&#25968;&#25454;&#25913;&#21892;&#27888;&#31859;&#23572;&#32435;&#24503;&#37030;&#30331;&#38761;&#28909;&#30123;&#24773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhanced Dengue Outbreak Prediction in Tamilnadu using Meteorological and Entomological data. (arXiv:2306.13456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27668;&#20505;&#21644;&#23186;&#20171;&#29289;&#24188;&#34411;&#25351;&#25968;&#23545;&#27888;&#31859;&#23572;&#32435;&#24503;&#37030;&#30331;&#38761;&#28909;&#30123;&#24773;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#34442;&#23376;&#24188;&#34411;&#25351;&#25968;&#25552;&#39640;&#20102;&#30123;&#24773;&#39044;&#27979;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27668;&#20505;&#25968;&#25454;&#21644;&#23186;&#20171;&#29289;&#24188;&#34411;&#25351;&#25968;&#23545;&#30331;&#38761;&#28909;&#30123;&#24773;&#29190;&#21457;&#30340;&#24433;&#21709;&#12290;&#22312;&#27604;&#36739;&#20102;&#21508;&#31181;LSTM&#27169;&#22411;&#21518;&#65292;&#36873;&#25321;&#21452;&#21521;&#22534;&#21472;LSTM&#32593;&#32476;&#65292;&#20998;&#26512;&#25910;&#38598;&#33258;2014&#24180;&#33267;2020&#24180;&#21360;&#24230;&#27888;&#31859;&#23572;&#32435;&#24503;&#37030;&#30340;&#26102;&#38388;&#24207;&#21015;&#27668;&#20505;&#25968;&#25454;&#21644;&#21355;&#29983;&#25968;&#25454;&#12290;&#36890;&#36807;&#21253;&#21547;&#34442;&#23376;&#24188;&#34411;&#25351;&#25968;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#34913;&#37327;&#30142;&#30149;&#38450;&#25511;&#25514;&#26045;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on studying the impact of climate data and vector larval indices on dengue outbreak. After a comparative study of the various LSTM models, Bidirectional Stacked LSTM network is selected to analyze the time series climate data and health data collected for the state of Tamil Nadu (India), for the period 2014 to 2020. Prediction accuracy of the model is significantly improved by including the mosquito larval index, an indication of VBD control measure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36712;&#36857;&#37319;&#26679;&#19979;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;MNIST&#25968;&#25454;&#38598;&#23454;&#39564;&#65292;&#21457;&#29616;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#39640;&#20102;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13442</link><description>&lt;p&gt;
&#36712;&#36857;&#37319;&#26679;&#19979;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#23567;&#25209;&#37327;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Minibatch training of neural network ensembles via trajectory sampling. (arXiv:2306.13442v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36712;&#36857;&#37319;&#26679;&#19979;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;MNIST&#25968;&#25454;&#38598;&#23454;&#39564;&#65292;&#21457;&#29616;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#39640;&#20102;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#30340;&#23567;&#38543;&#26426;&#23376;&#38598;&#65288;&#25110;&#23567;&#25209;&#37327;&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#20272;&#35745;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#22312;&#35757;&#32451;&#26102;&#38388;&#19982;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#20043;&#38388;&#35299;&#32806;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23567;&#25209;&#37327;&#26041;&#27861;&#21487;&#20197;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#36890;&#36807;&#36712;&#36857;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;(NNEs)&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;NNE&#26469;&#20998;&#31867;MNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#26102;&#38388;&#65292;&#20351;&#20854;&#33021;&#22815;&#32553;&#25918;&#20026;&#25968;&#25454;&#38598;&#22823;&#23567;&#19982;&#24179;&#22343;&#23567;&#25209;&#37327;&#22823;&#23567;&#20043;&#27604;&#65292;&#23545;&#20110;MNIST&#26469;&#35828;&#65292;&#35745;&#31639;&#25928;&#29575;&#36890;&#24120;&#25552;&#39640;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#24378;&#35843;&#20351;&#29992;&#36739;&#38271;&#30340;&#36712;&#36857;&#26469;&#34920;&#31034;NNE&#30340;&#20248;&#28857;&#65292;&#26082;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;&#21448;&#21487;&#20197;&#22312;&#23567;&#25209;&#37327;&#26356;&#26032;&#25152;&#38656;&#30340;&#26679;&#26412;&#26041;&#38754;&#38477;&#20302;&#26356;&#26032;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most iterative neural network training methods use estimates of the loss function over small random subsets (or minibatches) of the data to update the parameters, which aid in decoupling the training time from the (often very large) size of the training datasets. Here, we show that a minibatch approach can also be used to train neural network ensembles (NNEs) via trajectory methods in a highly efficent manner. We illustrate this approach by training NNEs to classify images in the MNIST datasets. This method gives an improvement to the training times, allowing it to scale as the ratio of the size of the dataset to that of the average minibatch size which, in the case of MNIST, gives a computational improvement typically of two orders of magnitude. We highlight the advantage of using longer trajectories to represent NNEs, both for improved accuracy in inference and reduced update cost in terms of the samples needed in minibatch updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26435;&#34913;&#20102;&#20215;&#26684;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20351;&#24471;&#23545;&#20110;&#28304;&#30340;&#36873;&#25321;&#65292;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#20195;&#20215;&#21487;&#20197;&#20943;&#23569;&#20844;&#24179;&#24809;&#32602;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.13440</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#36136;&#37327;&#21644;&#20844;&#24179;&#20043;&#38388;&#26435;&#34913;&#20215;&#26684;&#65292;&#23454;&#29616;&#22312;&#32447;&#20998;&#37197;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Trading-off price for data quality to achieve fair online allocation. (arXiv:2306.13440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26435;&#34913;&#20102;&#20215;&#26684;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20351;&#24471;&#23545;&#20110;&#28304;&#30340;&#36873;&#25321;&#65292;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#20195;&#20215;&#21487;&#20197;&#20943;&#23569;&#20844;&#24179;&#24809;&#32602;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38271;&#26399;&#30340;&#20844;&#24179;&#24809;&#32602;&#12290;&#20294;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#20915;&#31574;&#32773;&#21487;&#20197;&#35266;&#23519;&#21040;&#21463;&#20445;&#25252;&#30340;&#23646;&#24615;&#8212;&#8212;&#36825;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#21487;&#20197;&#36141;&#20080;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#26469;&#28304;&#30340;&#25968;&#25454;&#20197;&#24110;&#21161;&#20272;&#35745;&#23427;&#20204;&#65292;&#20174;&#32780;&#20197;&#19968;&#23450;&#30340;&#20195;&#20215;&#20943;&#23569;&#20844;&#24179;&#24809;&#32602;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#33218;&#23545;&#24212;&#20110;&#25968;&#25454;&#28304;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#21253;&#21547;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#23427;&#20855;&#26377; $\mathcal{O}(\sqrt{T})$ &#30340;&#36951;&#25022;&#30028;&#12290;&#19968;&#20010;&#20851;&#38190;&#22256;&#38590;&#26159;&#36873;&#25321;&#28304;&#25152;&#33719;&#24471;&#30340;&#22238;&#25253;&#30001;&#20844;&#24179;&#24809;&#32602;&#30456;&#20851;&#65292;&#36825;&#23548;&#33268;&#23613;&#31649;&#26159;&#38543;&#26426;&#35774;&#32622;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32771;&#34385;&#21040;&#28304;&#36873;&#25321;&#20043;&#21069;&#21487;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#35768;&#22810;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23454;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online allocation subject to a long-term fairness penalty. Contrary to existing works, however, we do not assume that the decision-maker observes the protected attributes -- which is often unrealistic in practice. Instead they can purchase data that help estimate them from sources of different quality; and hence reduce the fairness penalty at some cost. We model this problem as a multi-armed bandit problem where each arm corresponds to the choice of a data source, coupled with the online allocation problem. We propose an algorithm that jointly solves both problems and show that it has a regret bounded by $\mathcal{O}(\sqrt{T})$. A key difficulty is that the rewards received by selecting a source are correlated by the fairness penalty, which leads to a need for randomization (despite a stochastic setting). Our algorithm takes into account contextual information available before the source selection, and can adapt to many different fairness notions. We also sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36319;&#36394;&#26102;&#38388;&#21464;&#21270;&#36793;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32447;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#39044;&#27979;&#26377;&#36793;&#30028;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13428</link><description>&lt;p&gt;
&#20851;&#20110;&#36319;&#36394;&#39044;&#27979;&#26377;&#36793;&#30028;&#26102;&#38388;&#24207;&#21015;&#26102;&#30340;&#21487;&#21464;&#36793;&#30028;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On tracking varying bounds when forecasting bounded time series. (arXiv:2306.13428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36319;&#36394;&#26102;&#38388;&#21464;&#21270;&#36793;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32447;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#39044;&#27979;&#26377;&#36793;&#30028;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#36830;&#32493;&#20294;&#26377;&#30028;&#30340;&#38543;&#26426;&#21464;&#37327;&#20855;&#26377;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26410;&#35266;&#27979;&#36793;&#30028;&#12290;&#22312;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36793;&#30028;&#35270;&#20026;&#26377;&#30028;&#38543;&#26426;&#21464;&#37327;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#23637;&#30340;&#23545;&#25968;&#20284;&#28982;&#20272;&#35745;&#65292;&#24182;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#36890;&#36807;&#22312;&#32447;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#36319;&#36394;&#36793;&#30028;&#12290;&#30001;&#20110;&#24471;&#21040;&#30340;&#20248;&#21270;&#38382;&#39064;&#19981;&#26159;&#20984;&#30340;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#20851;&#20110;&#20934;&#20984;&#20248;&#21270;&#30340;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#26368;&#32456;&#24471;&#21040;&#20102;&#19968;&#31181;&#22312;&#32447;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#22522;&#20110;&#27169;&#25311;&#30740;&#31350;&#21644;&#19968;&#20010;&#30495;&#23454;&#30340;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#38382;&#39064;&#26469;&#35828;&#26126;&#21644;&#35752;&#35770;&#25105;&#20204;&#26041;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a new framework where a continuous, though bounded, random variable has unobserved bounds that vary over time. In the context of univariate time series, we look at the bounds as parameters of the distribution of the bounded random variable. We introduce an extended log-likelihood estimation and design algorithms to track the bound through online maximum likelihood estimation. Since the resulting optimization problem is not convex, we make use of recent theoretical results on Normalized Gradient Descent (NGD) for quasiconvex optimization, to eventually derive an Online Normalized Gradient Descent algorithm. We illustrate and discuss the workings of our approach based on both simulation studies and a real-world wind power forecasting problem.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#20013;&#24341;&#20837;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19979;&#34892;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#26143;&#24231;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#35843;&#25972;&#26143;&#24231;&#35774;&#35745;&#20174;&#32780;&#24179;&#34913;&#19981;&#21516;&#29992;&#25143;&#30340;&#35823;&#30721;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13423</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;&#33258;&#32534;&#30721;&#22120;&#30340;&#19979;&#34892;NOMA&#26143;&#24231;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Weighted Autoencoder-Based Approach to Downlink NOMA Constellation Design. (arXiv:2306.13423v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13423
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#20013;&#24341;&#20837;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19979;&#34892;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#26143;&#24231;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#35843;&#25972;&#26143;&#24231;&#35774;&#35745;&#20174;&#32780;&#24179;&#34913;&#19981;&#21516;&#29992;&#25143;&#30340;&#35823;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;(AE)&#36827;&#34892;&#36890;&#20449;&#31995;&#32479;&#31471;&#21040;&#31471;&#35774;&#35745;&#22240;&#20854;&#28789;&#27963;&#24615;&#21644;&#21331;&#36234;&#24615;&#33021;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#38500;&#20102;&#21333;&#29992;&#25143;&#20256;&#36755;&#20043;&#22806;&#65292;&#26368;&#36817;&#36824;&#22312;&#22810;&#29992;&#25143;&#35774;&#32622;&#20013;&#25506;&#32034;&#20102;AE-based&#35774;&#35745;&#65292;&#20363;&#22914;&#65292;&#29992;&#20110;&#35774;&#35745;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;(NOMA)&#30340;&#26143;&#24231;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;AE&#35757;&#32451;&#20013;&#24341;&#20837;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#22522;&#20110;AE&#30340;&#19979;&#34892;NOMA&#35774;&#35745;&#12290;&#36890;&#36807;&#25913;&#21464;&#26435;&#37325;&#31995;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#35843;&#25972;&#26143;&#24231;&#35774;&#35745;&#65292;&#24179;&#34913;&#19981;&#21516;&#29992;&#25143;&#30340;&#35823;&#30721;&#29575;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#26174;&#24335;&#30340;&#26377;&#20851;&#20854;&#20449;&#36947;&#36136;&#37327;&#30340;&#20449;&#24687;&#12290;&#32467;&#21512;SICNet&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#21152;&#26435;AE&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#21644;&#19981;&#21516;&#29992;&#25143;&#35823;&#30721;&#29575;&#30340;&#28789;&#27963;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end design of communication systems using deep autoencoders (AEs) is gaining attention due to its flexibility and excellent performance. Besides single-user transmission, AE-based design is recently explored in multi-user setup, e.g., for designing constellations for non-orthogonal multiple access (NOMA). In this paper, we further advance the design of AE-based downlink NOMA by introducing weighted loss function in the AE training. By changing the weight coefficients, one can flexibly tune the constellation design to balance error probability of different users, without relying on explicit information about their channel quality. Combined with the SICNet decoder, we demonstrate a significant improvement in achievable levels and flexible control of error probability of different users using the proposed weighted AE-based framework.
&lt;/p&gt;</description></item><item><title>CLUE&#20351;&#29992;&#26465;&#20214;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#19987;&#23478;&#25968;&#25454;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20854;&#23427;&#32321;&#37325;&#30340;&#22806;&#22312;&#22870;&#21169;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.13412</link><description>&lt;p&gt;
CLUE: &#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26657;&#20934;&#28508;&#22312;&#23548;&#21521;
&lt;/p&gt;
&lt;p&gt;
CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning. (arXiv:2306.13412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13412
&lt;/p&gt;
&lt;p&gt;
CLUE&#20351;&#29992;&#26465;&#20214;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#19987;&#23478;&#25968;&#25454;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20854;&#23427;&#32321;&#37325;&#30340;&#22806;&#22312;&#22870;&#21169;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#39044;&#20808;&#25910;&#38598;&#21644;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;&#20294;&#26159;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38656;&#35201;&#30830;&#23450;&#21644;&#21046;&#23450;&#27599;&#20010;&#25968;&#25454;&#36716;&#25442;&#30340;&#22806;&#22312;&#22870;&#21169;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#32321;&#37325;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLUE&#65306;&#36890;&#36807;&#20351;&#29992;&#19968;&#20123;&#19987;&#23478;&#25968;&#25454;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#26469;&#28040;&#38500;&#22806;&#22312;&#22870;&#21169;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#30452;&#25509;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;CLUE&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#19987;&#23478;&#25968;&#25454;&#30340;&#23884;&#20837;&#24378;&#21046;&#36716;&#25442;&#20026;&#26657;&#20934;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20351;&#20869;&#22312;&#22870;&#21169;&#19982;&#19987;&#23478;&#24847;&#22270;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#19987;&#23478;&#39537;&#21160;&#30340;&#20869;&#22312;&#22870;&#21169;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce \textbf{C}alibrated \textbf{L}atent g\textbf{U}idanc\textbf{E} (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE's key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26368;&#36817;&#30340;&#21464;&#38761;&#28857;&#26159;&#36880;&#27493;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#35268;&#33539;&#27169;&#22411;&#30340;&#20013;&#38388;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.13411</link><description>&lt;p&gt;
&#27809;&#26377;&#20013;&#38388;&#30417;&#30563;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning Without Intermediate Supervision. (arXiv:2306.13411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13411
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26368;&#36817;&#30340;&#21464;&#38761;&#28857;&#26159;&#36880;&#27493;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#35268;&#33539;&#27169;&#22411;&#30340;&#20013;&#38388;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#26500;&#24314;&#33021;&#22815;&#27169;&#20223;&#32463;&#20856;&#31639;&#27861;&#65288;&#22914;&#25490;&#24207;&#12289;&#26368;&#30701;&#36335;&#24452;&#31561;&#65289;&#25191;&#34892;&#30340;&#27169;&#22411;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23398;&#20064;&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#19988;&#36755;&#20837;&#35268;&#27169;&#26174;&#33879;&#26356;&#22823;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36880;&#27493;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#35775;&#38382;&#21407;&#22987;&#31639;&#27861;&#30340;&#25152;&#26377;&#20013;&#38388;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20351;&#29992;&#20013;&#38388;&#30417;&#30563;&#19987;&#27880;&#20110;&#20165;&#20174;&#36755;&#20837;&#36755;&#20986;&#23545;&#23398;&#20064;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32467;&#26500;&#25913;&#36827;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65292;&#21487;&#20197;&#35268;&#33539;&#27169;&#22411;&#30340;&#20013;&#38388;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#31639;&#27861;&#36712;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;CLRS&#31639;&#27861;&#30340;&#20219;&#21153;&#19978;&#19982;&#20854;&#36712;&#36857;&#30417;&#30563;&#23545;&#24212;&#29289;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning is an emerging area of machine learning focusing on building models which can imitate the execution of classic algorithms, such as sorting, shortest paths, etc. One of the main challenges is to learn algorithms that are able to generalize to out-of-distribution data, in particular with significantly larger input sizes. Recent work on this problem has demonstrated the advantages of learning algorithms step-by-step, giving models access to all intermediate steps of the original algorithm. In this work, we instead focus on learning neural algorithmic reasoning only from the input-output pairs without appealing to the intermediate supervision. We propose simple but effective architectural improvements and also build a self-supervised objective that can regularise intermediate computations of the model without access to the algorithm trajectory. We demonstrate that our approach is competitive to its trajectory-supervised counterpart on tasks from the CLRS Algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;ExLL&#65292;&#23427;&#20855;&#26377;&#20174;&#31232;&#32570;&#25968;&#25454;&#23398;&#20064;&#12289;&#33258;&#32452;&#32455;&#30340;&#21407;&#22411;&#26550;&#26500;&#12289;&#21487;&#35299;&#37322;&#30340;IF-THEN&#35268;&#21017;&#21644;&#25104;&#23545;&#34701;&#21512;&#25512;&#29702;&#31561;&#29305;&#28857;&#65292;&#36866;&#29992;&#20110;&#35843;&#25972;&#65292;&#32858;&#31867;&#21644;&#20445;&#25252;&#27969;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.13410</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#20840;&#23616;&#8221;&#37197;&#23545;&#34701;&#21512;&#30340;&#21487;&#35299;&#37322;&#32456;&#36523;&#27969;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Lifelong Stream Learning Based on "Glocal" Pairwise Fusion. (arXiv:2306.13410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;ExLL&#65292;&#23427;&#20855;&#26377;&#20174;&#31232;&#32570;&#25968;&#25454;&#23398;&#20064;&#12289;&#33258;&#32452;&#32455;&#30340;&#21407;&#22411;&#26550;&#26500;&#12289;&#21487;&#35299;&#37322;&#30340;IF-THEN&#35268;&#21017;&#21644;&#25104;&#23545;&#34701;&#21512;&#25512;&#29702;&#31561;&#29305;&#28857;&#65292;&#36866;&#29992;&#20110;&#35843;&#25972;&#65292;&#32858;&#31867;&#21644;&#20445;&#25252;&#27969;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#30005;&#35805;&#65292;&#28040;&#36153;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#23478;&#30005;&#19978;&#20351;&#29992;&#23454;&#26102;&#35774;&#22791;&#25345;&#32493;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#20123;&#35774;&#22791;&#20855;&#26377;&#26377;&#38480;&#30340;&#22788;&#29702;&#21644;&#20869;&#23384;&#23384;&#20648;&#33021;&#21147;&#65292;&#32780;&#32456;&#36523;&#23398;&#20064;&#21017;&#22312;&#36739;&#38271;&#26102;&#38388;&#20869;&#33719;&#21462;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#32456;&#36523;&#23398;&#20064;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#22312;&#36825;&#20123;&#32422;&#26463;&#19979;&#24037;&#20316;&#65292;&#21516;&#26102;&#25552;&#20379;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#32456;&#36523;&#23398;&#20064;&#65288;ExLL&#65289;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20960;&#20010;&#37325;&#35201;&#29305;&#28857;&#65306;1&#65289;&#20174;&#31232;&#32570;&#30340;&#31034;&#20363;&#21644;&#36164;&#28304;&#30340;&#27969;&#25968;&#25454;&#20013;&#21333;&#27425;&#23398;&#20064;&#23398;&#20064;&#65307;2&#65289;&#33258;&#32452;&#32455;&#21407;&#22411;&#26550;&#26500;&#65292;&#26681;&#25454;&#30456;&#20284;&#24615;&#23558;&#27969;&#25968;&#25454;&#32858;&#31867;&#21040;&#21487;&#20998;&#31163;&#30340;&#32452;&#20013;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65307;3&#65289;&#21487;&#35299;&#37322;&#30340;&#26550;&#26500;&#65292;&#23558;&#32858;&#31867;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;IF-THEN&#35268;&#21017;&#65292;&#24182;&#26681;&#25454;&#25512;&#29702;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#26469;&#35777;&#26126;&#27169;&#22411;&#39044;&#27979;&#65307;&#20197;&#21450;4&#65289;&#22312;&#20840;&#23616;&#21644;&#26412;&#22320;&#32423;&#21035;&#36827;&#34892;&#25104;&#23545;&#34701;&#21512;&#30340;&#25512;&#29702;&#65292;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time on-device continual learning applications are used on mobile phones, consumer robots, and smart appliances. Such devices have limited processing and memory storage capabilities, whereas continual learning acquires data over a long period of time. By necessity, lifelong learning algorithms have to be able to operate under such constraints while delivering good performance. This study presents the Explainable Lifelong Learning (ExLL) model, which incorporates several important traits: 1) learning to learn, in a single pass, from streaming data with scarce examples and resources; 2) a self-organizing prototype-based architecture that expands as needed and clusters streaming data into separable groups by similarity and preserves data against catastrophic forgetting; 3) an interpretable architecture to convert the clusters into explainable IF-THEN rules as well as to justify model predictions in terms of what is similar and dissimilar to the inference; and 4) inferences at the glo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#39640;&#38454;&#27169;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30830;&#23450;&#24378;&#21046;&#25391;&#33633;&#28304;&#30340;&#20301;&#32622;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;FO&#24773;&#20917;&#65292;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#21333;&#20010;&#21644;&#22810;&#28304;FO&#24773;&#20917;&#22343;&#21487;&#36866;&#29992;&#12290;&#22312;&#33521;&#22269;&#39640;&#21387;&#36755;&#30005;&#32593;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13397</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#38454;&#27169;&#24335;&#30340;&#30005;&#32593;&#24378;&#21046;&#25391;&#33633;&#28304;&#20301;&#32622;&#35782;&#21035;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Higher-order Motif-based Time Series Classification for Forced Oscillation Source Location in Power Grids. (arXiv:2306.13397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#39640;&#38454;&#27169;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30830;&#23450;&#24378;&#21046;&#25391;&#33633;&#28304;&#30340;&#20301;&#32622;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;FO&#24773;&#20917;&#65292;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#21333;&#20010;&#21644;&#22810;&#28304;FO&#24773;&#20917;&#22343;&#21487;&#36866;&#29992;&#12290;&#22312;&#33521;&#22269;&#39640;&#21387;&#36755;&#30005;&#32593;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#29992;&#20110;&#21457;&#29616;&#26102;&#24207;&#25968;&#25454;&#30340;&#39640;&#38454;&#32467;&#26500;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#27169;&#24335;&#23884;&#20837;&#30456;&#20851;&#22330;&#65288;MECF&#65289;&#26469;&#21051;&#30011;&#21160;&#21147;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#38454;&#26102;&#38388;&#32467;&#26500;&#12290;&#24212;&#29992;&#22522;&#20110;MECF&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#30830;&#23450;&#24378;&#21046;&#25391;&#33633;&#65288;FO&#65289;&#28304;&#30340;&#20301;&#32622;&#65292;&#35813;&#21608;&#26399;&#24615;&#24178;&#25200;&#23545;&#30005;&#32593;&#30340;&#31283;&#23450;&#24615;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#30830;&#23450;FO&#28304;&#20301;&#32622;&#23545;&#30005;&#32593;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20613;&#37324;&#21494;&#20998;&#26512;&#30456;&#27604;&#65292;&#22522;&#20110;MECF&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#36866;&#29992;&#20110;&#21508;&#31181;FO&#24773;&#20917;&#65292;&#21253;&#25324;&#21333;&#20010;FO&#65292;&#20849;&#25391;FO&#21644;&#22810;&#28304;FO&#12290; &#22522;&#20110;MECF&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#31995;&#32479;&#27169;&#22411;&#25110;&#25299;&#25169;&#32467;&#26500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#33521;&#22269;&#39640;&#21387;&#36755;&#30005;&#32593;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#22522;&#20110;MECF&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#32806;&#21512;&#24378;&#24230;&#21644;&#27979;&#37327;&#22122;&#22768;&#23545;&#20301;&#32622;&#21028;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series motifs are used for discovering higher-order structures of time series data. Based on time series motifs, the motif embedding correlation field (MECF) is proposed to characterize higher-order temporal structures of dynamical system time series. A MECF-based unsupervised learning approach is applied in locating the source of the forced oscillation (FO), a periodic disturbance that detrimentally impacts power grids. Locating the FO source is imperative for system stability. Compared with the Fourier analysis, the MECF-based unsupervised learning is applicable under various FO situations, including the single FO, FO with resonance, and multiple sources FOs. The MECF-based unsupervised learning is a data-driven approach without any prior knowledge requirement of system models or typologies. Tests on the UK high-voltage transmission grid illustrate the effectiveness of MECF-based unsupervised learning. In addition, the impacts of coupling strength and measurement noise on locati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#28024;&#20837;&#36793;&#30028;&#31995;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19982;&#31934;&#32454;IBM&#27169;&#25311;&#30456;&#24403;&#30340;&#30636;&#24577;&#29305;&#24449;&#65292;&#24182;&#21487;&#29992;&#20110;&#23454;&#26102;&#27969;&#25511;&#21644;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#21487;&#25193;&#23637;&#21040;&#28041;&#21450;&#22810;&#20010;&#31227;&#21160;&#20307;&#21644;&#27969;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26356;&#22797;&#26434;&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.13395</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#31227;&#21160;&#28024;&#20837;&#36793;&#30028;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#27874;&#28010;&#24335;&#32764;&#36807;&#28193;&#27969;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks modeling for systems with moving immersed boundaries: application to an unsteady flow past a plunging foil. (arXiv:2306.13395v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#28024;&#20837;&#36793;&#30028;&#31995;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19982;&#31934;&#32454;IBM&#27169;&#25311;&#30456;&#24403;&#30340;&#30636;&#24577;&#29305;&#24449;&#65292;&#24182;&#21487;&#29992;&#20110;&#23454;&#26102;&#27969;&#25511;&#21644;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#21487;&#25193;&#23637;&#21040;&#28041;&#21450;&#22810;&#20010;&#31227;&#21160;&#20307;&#21644;&#27969;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26356;&#22797;&#26434;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20197;&#21450;&#22312;&#27969;&#20307;&#21147;&#23398;&#24212;&#29992;&#20013;&#20419;&#36827;&#26597;&#35810;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#26497;&#23569;&#25968;&#20851;&#20110;PINNs&#29992;&#20110;&#36807;&#28193;&#27969;&#36890;&#36807;&#21160;&#24577;&#20307;&#65292;&#20363;&#22914;&#25391;&#21160;&#32764;&#30340;&#24037;&#20316;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#36716;&#25442;&#20026;&#38468;&#22312;&#20307;&#19978;&#30340;&#21442;&#32771;&#31995;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#22810;&#20010;&#31227;&#21160;&#20307;&#25110;&#21464;&#24418;&#32467;&#26500;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#28024;&#20837;&#24335;&#36793;&#30028;&#24863;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#36807;&#28193;&#27969;&#36890;&#36807;&#31227;&#21160;&#29289;&#20307;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20102;&#20174;&#28024;&#20837;&#36793;&#30028;&#26041;&#27861;&#65288;IBM&#65289;&#27169;&#25311;&#25968;&#25454;&#20013;&#21516;&#26102;&#24674;&#22797;&#21387;&#21147;&#21644;&#36895;&#24230;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#65292;&#36895;&#24230;&#37325;&#24314;&#30340;&#26377;&#25928;&#24615;&#24050;&#38024;&#23545;&#31934;&#32454;&#20998;&#36776;&#29575;IBM&#25968;&#25454;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20294;&#20316;&#20026;&#36827;&#19968;&#27493;&#30340;&#27493;&#39588;&#65292;&#24674;&#22797;&#30340;&#21387;&#21147;&#19982;&#20219;&#24847;&#25289;&#26684;&#26391;&#26085;-&#27431;&#25289;&#65288;ALE&#65289;&#27714;&#35299;&#22120;&#30340;&#21387;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;PINN&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19982;&#31934;&#32454;&#20998;&#36776;&#29575;IBM&#27169;&#25311;&#30456;&#24403;&#30340;&#30636;&#24577;&#29305;&#24449;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#23454;&#26102;&#27969;&#25511;&#21644;&#20248;&#21270;&#65292;&#24182;&#19988;&#26412;&#30740;&#31350;&#21487;&#25193;&#23637;&#21040;&#28041;&#21450;&#22810;&#20010;&#31227;&#21160;&#20307;&#21644;&#27969;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26356;&#22797;&#26434;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, physics informed neural networks (PINNs) have been explored extensively for solving various forward and inverse problems and facilitating querying applications in fluid mechanics applications. However, work on PINNs for unsteady flows past moving bodies, such as flapping wings is scarce. Earlier studies mostly relied on transferring to a body attached frame of reference which is restrictive towards handling multiple moving bodies or deforming structures. Hence, in the present work, an immersed boundary aware framework has been explored for developing surrogate models for unsteady flows past moving bodies. Specifically, simultaneous pressure recovery and velocity reconstruction from Immersed boundary method (IBM) simulation data has been investigated. While, efficacy of velocity reconstruction has been tested against the fine resolution IBM data, as a step further, the pressure recovered was compared with that of an arbitrary Lagrange Eulerian (ALE) based solver. Under this fr
&lt;/p&gt;</description></item><item><title>DiffInfinite &#26159;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#22823;&#30340;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#20351;&#29992;&#23567;&#34917;&#19969;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#24182;&#34892;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20197;&#21450;&#20445;&#25252;&#24615;&#25968;&#25454;&#22788;&#29702;&#31561;&#29420;&#29305;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.13384</link><description>&lt;p&gt;
DiffInfinite: &#36890;&#36807;&#24182;&#34892;&#38543;&#26426;&#34917;&#19969;&#25193;&#25955;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#23454;&#29616;&#22823;&#22411;&#33945;&#29256;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology. (arXiv:2306.13384v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13384
&lt;/p&gt;
&lt;p&gt;
DiffInfinite &#26159;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#22823;&#30340;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#20351;&#29992;&#23567;&#34917;&#19969;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#24182;&#34892;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20197;&#21450;&#20445;&#25252;&#24615;&#25968;&#25454;&#22788;&#29702;&#31561;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; DiffInfinite&#65292;&#36825;&#26159;&#19968;&#31181;&#23618;&#27425;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#22823;&#30340;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#38271;&#31243;&#30456;&#20851;&#24615;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20808;&#29983;&#25104;&#21512;&#25104;&#20998;&#21106;&#25513;&#27169;&#65292;&#38543;&#21518;&#29992;&#20316;&#39640;&#20445;&#30495;&#24230;&#29983;&#25104;&#25193;&#25955;&#36807;&#31243;&#30340;&#26465;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#37319;&#26679;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#24847;&#25152;&#38656;&#22270;&#20687;&#23610;&#23544;&#65292;&#32780;&#21482;&#38656;&#35201;&#23567;&#34917;&#19969;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#22823;&#22411;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#24182;&#34892;&#21270;&#65292;&#21516;&#26102;&#36991;&#20813;&#24179;&#38138;&#21453;&#23556;&#24335;&#20266;&#24433;&#12290;&#35757;&#32451;&#21033;&#29992;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#25193;&#20805;&#23567;&#22411;&#12289;&#31232;&#30095;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#22823;&#35268;&#27169;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20197;&#21450;&#20445;&#25252;&#24615;&#25968;&#25454;&#22788;&#29702;&#12290;DiffInfinite &#25968;&#25454;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#30001;&#21313;&#21517;&#26377;&#32463;&#39564;&#30340;&#30149;&#29702;&#23398;&#23478;&#36827;&#34892;&#35843;&#26597;&#39564;&#35777;&#65292;&#20197;&#21450;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artefacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is validated in a survey by ten experienced pathologists as well as a downstream segmentation task. Fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20849;&#21019;&#32852;&#21512;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.13381</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21512;&#20316;&#20849;&#21019;&#20840;&#29699;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Co-creating a globally interpretable model with human input. (arXiv:2306.13381v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20849;&#21019;&#32852;&#21512;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#32852;&#21512;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#24067;&#23572;&#20915;&#31574;&#35268;&#21017;&#30340;&#24418;&#24335;&#65292;&#20154;&#31867;&#36755;&#20837;&#20197;&#36923;&#36753;&#26465;&#20214;&#25110;&#37096;&#20998;&#27169;&#26495;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#36825;&#31181;&#32858;&#21512;&#30340;&#27169;&#22411;&#26500;&#24314;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#32852;&#21512;&#20915;&#31574;&#35270;&#35282;&#12290;&#20197;&#24448;&#30340;&#21162;&#21147;&#36890;&#24120;&#38598;&#20013;&#20110;&#32858;&#21512;&#32467;&#26524;&#32780;&#19981;&#26159;&#20915;&#31574;&#36923;&#36753;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an aggregated human-AI collaboration aimed at generating a joint interpretable model. The model takes the form of Boolean decision rules, where human input is provided in the form of logical conditions or as partial templates. This focus on the combined construction of a model offers a different perspective on joint decision making. Previous efforts have typically focused on aggregating outcomes rather than decisions logic. We demonstrate the proposed approach through two examples and highlight the usefulness and challenges of the approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#28237;&#27969;&#27169;&#22411;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#25968;&#25454;&#31232;&#32570;&#26102;&#33021;&#22815;&#23454;&#29616;&#21069;&#32622;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.13370</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#38543;&#26426;&#26862;&#26519;&#29992;&#20110;&#28237;&#27969;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation. (arXiv:2306.13370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#28237;&#27969;&#27169;&#22411;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#25968;&#25454;&#31232;&#32570;&#26102;&#33021;&#22815;&#23454;&#29616;&#21069;&#32622;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#29616;&#24037;&#19994;&#35774;&#35745;&#30340;&#34394;&#25311;&#35748;&#35777;&#36807;&#31243;&#20013;&#65292;&#23545;&#20110;&#27169;&#25311;&#39537;&#21160;&#36807;&#31243;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#26041;&#27861;&#26469;&#32771;&#34385;&#28237;&#27969;&#27169;&#22411;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#28040;&#38500;&#29992;&#25143;&#36755;&#20837;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22312;&#20934;&#30830;&#25968;&#25454;&#31232;&#32570;&#26102;&#24320;&#21457;&#20808;&#39564;&#20272;&#35745;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve virtual certification for industrial design, quantifying the uncertainties in simulation-driven processes is crucial. We discuss a physics-constrained approach to account for epistemic uncertainty of turbulence models. In order to eliminate user input, we incorporate a data-driven machine learning strategy. In addition to it, our study focuses on developing an a priori estimation of prediction confidence when accurate data is scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26816;&#32034;&#24615;&#33021;&#30340;&#25351;&#26631;&#26469;&#35299;&#20915;&#24191;&#21463;&#27426;&#36814;&#30340;Recall@K&#24230;&#37327;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#25351;&#26631;&#21487;&#20197;&#20272;&#35745;&#27867;&#21270;&#33021;&#21147;&#24182;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#27867;&#21270;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.13357</link><description>&lt;p&gt;
&#25429;&#25417;&#22270;&#20687;&#26816;&#32034;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Catching Image Retrieval Generalization. (arXiv:2306.13357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26816;&#32034;&#24615;&#33021;&#30340;&#25351;&#26631;&#26469;&#35299;&#20915;&#24191;&#21463;&#27426;&#36814;&#30340;Recall@K&#24230;&#37327;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#25351;&#26631;&#21487;&#20197;&#20272;&#35745;&#27867;&#21270;&#33021;&#21147;&#24182;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#27867;&#21270;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#27010;&#24565;&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#24191;&#21463;&#27426;&#36814;&#30340;Recall@K&#24230;&#37327;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26816;&#32034;&#24615;&#33021;&#30340;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#19982;Recall@K&#19981;&#21516;&#65292;&#21487;&#20197;&#20272;&#35745;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25351;&#26631;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#27867;&#21270;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concepts of overfitting and generalization are vital for evaluating machine learning models. In this work, we show that the popular Recall@K metric depends on the number of classes in the dataset, which limits its ability to estimate generalization. To fix this issue, we propose a new metric, which measures retrieval performance, and, unlike Recall@K, estimates generalization. We apply the proposed metric to popular image retrieval methods and provide new insights about deep metric learning generalization.
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13292</link><description>&lt;p&gt;
&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variance-Covariance Regularization Improves Representation Learning. (arXiv:2306.13292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#20174;&#19968;&#20010;&#39046;&#22495;&#33719;&#24471;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20851;&#20110;&#36825;&#20123;&#21518;&#32493;&#20219;&#21153;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24378;&#26377;&#21147;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#35201;&#27714;&#22312;&#21021;&#22987;&#39044;&#35757;&#32451;&#38454;&#27573;&#25429;&#33719;&#21508;&#31181;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#36275;&#22815;&#30340;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#38598;&#20013;&#20110;&#20027;&#35201;&#20943;&#23569;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#36235;&#21183;&#21487;&#33021;&#23548;&#33268;&#19981;&#20805;&#20998;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#25439;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#65288;VCR&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#12290;&#20511;&#37492;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#34920;&#29616;&#20986;&#39640;&#26041;&#24046;&#21644;&#39640;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has emerged as a key approach in the machine learning domain, enabling the application of knowledge derived from one domain to improve performance on subsequent tasks. Given the often limited information about these subsequent tasks, a strong transfer learning approach calls for the model to capture a diverse range of features during the initial pretraining stage. However, recent research suggests that, without sufficient regularization, the network tends to concentrate on features that primarily reduce the pretraining loss function. This tendency can result in inadequate feature learning and impaired generalization capability for target tasks. To address this issue, we propose Variance-Covariance Regularization (VCR), a regularization technique aimed at fostering diversity in the learned network features. Drawing inspiration from recent advancements in the self-supervised learning approach, our approach promotes learned representations that exhibit high variance and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#25240;&#25187;&#22240;&#23376;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36991;&#20813;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13284</link><description>&lt;p&gt;
&#25913;&#27491;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#25240;&#25187;&#22240;&#23376;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Correcting discount-factor mismatch in on-policy policy gradient methods. (arXiv:2306.13284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#25240;&#25187;&#22240;&#23376;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36991;&#20813;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#31574;&#30053;&#26799;&#24230;&#24418;&#24335;&#65292;&#21253;&#25324;&#19977;&#20010;&#22240;&#32032;: &#21160;&#20316;&#20540;&#12289;&#21160;&#20316;&#20284;&#28982;&#26799;&#24230;&#21644;&#25240;&#25187;&#21033;&#28070;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#24120;&#29992;&#30340;&#36827;&#31574;&#30053;&#26041;&#27861;&#24573;&#30053;&#20102;&#29366;&#24577;&#20998;&#24067;&#20013;&#30340;&#25240;&#25187;&#22240;&#23376;&#65292;&#36825;&#26159;&#25216;&#26415;&#19978;&#30340;&#38169;&#35823;&#65292;&#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#29978;&#33267;&#21487;&#33021;&#24341;&#21457;&#36864;&#21270;&#30340;&#23398;&#20064;&#34892;&#20026;&#12290;&#26082;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#22312;&#26799;&#24230;&#20272;&#35745;&#20013;&#20351;&#29992; $\gamma^t$ &#20316;&#20026;&#22240;&#23376;&#26469;&#32416;&#27491;&#27492; discrepency&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#24182;&#19988;&#22312;&#21518;&#32493;&#29366;&#24577;&#31867;&#20284;&#20110;&#21069;&#38754;&#29366;&#24577;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#25240;&#25187;&#31283;&#24577;&#20998;&#24067;&#38382;&#39064;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#35768;&#22810;&#29616;&#26377;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#20013;&#12290;&#25105;&#20204;&#30340;&#26657;&#27491;&#26041;&#27861;&#22312;&#26041;&#24046;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#19982; $\gamma^t$ &#26657;&#27491;&#30456;&#20851;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The policy gradient theorem gives a convenient form of the policy gradient in terms of three factors: an action value, a gradient of the action likelihood, and a state distribution involving discounting called the \emph{discounted stationary distribution}. But commonly used on-policy methods based on the policy gradient theorem ignores the discount factor in the state distribution, which is technically incorrect and may even cause degenerate learning behavior in some environments. An existing solution corrects this discrepancy by using $\gamma^t$ as a factor in the gradient estimate. However, this solution is not widely adopted and does not work well in tasks where the later states are similar to earlier states. We introduce a novel distribution correction to account for the discounted stationary distribution that can be plugged into many existing gradient estimators. Our correction circumvents the performance degradation associated with the $\gamma^t$ correction with a lower variance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#21160;&#30913;&#20849;&#25391;&#24433;&#20687;&#35786;&#26029;&#20013;&#24402;&#19968;&#21270;&#26041;&#26696;&#23545;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#30340;&#28789;&#25935;&#24230;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;MRI&#31649;&#36947;&#20869;&#30340;&#22270;&#20687;&#37325;&#24314;&#36807;&#31243;&#23545;&#21508;&#31181;&#24418;&#24335;&#30340;&#22122;&#22768;&#39640;&#24230;&#25935;&#24863;&#65292;&#23548;&#33268;&#22270;&#20687;&#20013;&#20986;&#29616;&#20219;&#24847;&#30340;&#20266;&#24433;&#65292;&#22122;&#22768;&#20998;&#24067;&#19981;&#26159;&#22266;&#23450;&#30340;&#65292;&#32780;&#19988;&#22240;&#20026;DL&#27169;&#22411;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20266;&#24433;&#38750;&#24120;&#25935;&#24863;&#65292;&#25152;&#20197;&#26412;&#30740;&#31350;&#26159;&#26497;&#20854;&#26377;&#24847;&#20041;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.13276</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#21160;&#30913;&#20849;&#25391;&#24433;&#20687;&#35786;&#26029;&#20013;&#24402;&#19968;&#21270;&#26041;&#26696;&#23545;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#30340;&#28789;&#25935;&#24230;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity and Robustness of Normalization Schemes to Input Distribution Shifts in Automatic MR Image Diagnosis. (arXiv:2306.13276v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#21160;&#30913;&#20849;&#25391;&#24433;&#20687;&#35786;&#26029;&#20013;&#24402;&#19968;&#21270;&#26041;&#26696;&#23545;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#30340;&#28789;&#25935;&#24230;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;MRI&#31649;&#36947;&#20869;&#30340;&#22270;&#20687;&#37325;&#24314;&#36807;&#31243;&#23545;&#21508;&#31181;&#24418;&#24335;&#30340;&#22122;&#22768;&#39640;&#24230;&#25935;&#24863;&#65292;&#23548;&#33268;&#22270;&#20687;&#20013;&#20986;&#29616;&#20219;&#24847;&#30340;&#20266;&#24433;&#65292;&#22122;&#22768;&#20998;&#24067;&#19981;&#26159;&#22266;&#23450;&#30340;&#65292;&#32780;&#19988;&#22240;&#20026;DL&#27169;&#22411;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20266;&#24433;&#38750;&#24120;&#25935;&#24863;&#65292;&#25152;&#20197;&#26412;&#30740;&#31350;&#26159;&#26497;&#20854;&#26377;&#24847;&#20041;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30001;&#20110;&#26174;&#31034;&#20986;&#20248;&#31168;&#30340;&#36719;&#32452;&#32455;&#23545;&#27604;&#24230;&#65292;&#34987;&#35748;&#20026;&#26159;&#21307;&#23398;&#25104;&#20687;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#36825;&#20351;&#24471;&#20154;&#31867;&#25918;&#23556;&#23398;&#23478;&#33021;&#22815;&#36731;&#26494;&#22320;&#35782;&#21035;&#35768;&#22810;&#30149;&#21464;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20063;&#20351;&#29992;&#36825;&#20123;&#37325;&#24314;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#35786;&#26029;&#22810;&#31181;&#30142;&#30149;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MRI&#31649;&#36947;&#20869;&#30340;&#22270;&#20687;&#37325;&#24314;&#36807;&#31243;&#38656;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#30828;&#20214;&#65292;&#24182;&#35843;&#25972;&#22823;&#37327;&#30340;&#25195;&#25551;&#20202;&#21442;&#25968;&#65292;&#22240;&#27492;&#23545;&#21508;&#31181;&#24418;&#24335;&#30340;&#22122;&#22768;&#39640;&#24230;&#25935;&#24863;&#65292;&#23548;&#33268;&#22270;&#20687;&#20013;&#20986;&#29616;&#20219;&#24847;&#30340;&#20266;&#24433;&#12290;&#27492;&#22806;&#65292;&#22122;&#22768;&#20998;&#24067;&#19981;&#26159;&#22266;&#23450;&#30340;&#65292;&#22312;&#26426;&#22120;&#20869;&#12289;&#26426;&#22120;&#20043;&#38388;&#21644;&#24739;&#32773;&#20043;&#38388;&#21464;&#21270;&#65292;&#23548;&#33268;&#22270;&#20687;&#20013;&#30340;&#20266;&#24433;&#20063;&#38543;&#20043;&#19981;&#21516;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;DL&#27169;&#22411;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20266;&#24433;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#20026;&#23427;&#23548;&#33268;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic Resonance Imaging (MRI) is considered the gold standard of medical imaging because of the excellent soft-tissue contrast exhibited in the images reconstructed by the MRI pipeline, which in-turn enables the human radiologist to discern many pathologies easily. More recently, Deep Learning (DL) models have also achieved state-of-the-art performance in diagnosing multiple diseases using these reconstructed images as input. However, the image reconstruction process within the MRI pipeline, which requires the use of complex hardware and adjustment of a large number of scanner parameters, is highly susceptible to noise of various forms, resulting in arbitrary artifacts within the images. Furthermore, the noise distribution is not stationary and varies within a machine, across machines, and patients, leading to varying artifacts within the images. Unfortunately, DL models are quite sensitive to these varying artifacts as it leads to changes in the input data distribution between the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22836;&#37096;&#38598;&#21644;&#23614;&#37096;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#36830;&#32493;&#30340;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#23450;&#29702;&#35777;&#26126;&#25345;&#32493;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;</title><link>http://arxiv.org/abs/2306.13275</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#33021;&#25913;&#36827;&#38271;&#23614;&#35782;&#21035;&#21527;&#65311;&#36208;&#21521;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework. (arXiv:2306.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22836;&#37096;&#38598;&#21644;&#23614;&#37096;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#36830;&#32493;&#30340;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#23450;&#29702;&#35777;&#26126;&#25345;&#32493;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#26679;&#26412;&#25968;&#37327;&#26497;&#24230;&#22833;&#34913;&#20250;&#20986;&#29616;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#38382;&#39064;&#12290;LTR&#26041;&#27861;&#26088;&#22312;&#20934;&#30830;&#22320;&#23398;&#20064;&#21253;&#21547;&#19968;&#20010;&#36739;&#22823;&#8220;&#22836;&#8221;&#38598;&#21644;&#19968;&#20010;&#36739;&#23567;&#8220;&#23614;&#8221;&#38598;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#20551;&#35774;&#25439;&#22833;&#20989;&#25968;&#26159;&#24378;&#20984;&#30340;&#65292;&#37027;&#20040;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#22312;&#21516;&#19968;&#20010;&#23398;&#20064;&#32773;&#20005;&#26684;&#35757;&#32451;&#22836;&#38598;&#26102;&#30340;&#26435;&#37325;&#19978;&#38480;&#20043;&#20869;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22768;&#31216;&#23558;&#22836;&#38598;&#21644;&#23614;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#36830;&#32493;&#27493;&#39588;&#65292;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#29609;&#20855;MNIST-LT&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26631;&#20934;LTR&#22522;&#20934;&#65288;CIFAR100-LT&#21644;CIFAR10-L&#65289;&#30340;&#22810;&#20010;&#19981;&#24179;&#34913;&#21464;&#20307;&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;CL&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Long-Tailed Recognition (LTR) problem emerges in the context of learning from highly imbalanced datasets, in which the number of samples among different classes is heavily skewed. LTR methods aim to accurately learn a dataset comprising both a larger Head set and a smaller Tail set. We propose a theorem where under the assumption of strong convexity of the loss function, the weights of a learner trained on the full dataset are within an upper bound of the weights of the same learner trained strictly on the Head. Next, we assert that by treating the learning of the Head and Tail as two separate and sequential steps, Continual Learning (CL) methods can effectively update the weights of the learner to learn the Tail without forgetting the Head. First, we validate our theoretical findings with various experiments on the toy MNIST-LT dataset. We then evaluate the efficacy of several CL strategies on multiple imbalanced variations of two standard LTR benchmarks (CIFAR100-LT and CIFAR10-L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; V-CPDC &#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#22495;&#22833;&#30495;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#22240;&#26524;&#39044;&#27979;&#23376;&#32435;&#20837;&#22495;&#33258;&#36866;&#24212;&#30340;&#27010;&#24565;&#20013;&#65292;&#20197;&#20351;&#20854;&#33021;&#22815;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13271</link><description>&lt;p&gt;
&#36816;&#34892;&#26102;&#22495;&#22833;&#30495;&#19979;&#30340;&#21464;&#20998;&#21453;&#20107;&#23454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Variational Counterfactual Prediction under Runtime Domain Corruption. (arXiv:2306.13271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; V-CPDC &#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#22495;&#22833;&#30495;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#22240;&#26524;&#39044;&#27979;&#23376;&#32435;&#20837;&#22495;&#33258;&#36866;&#24212;&#30340;&#27010;&#24565;&#20013;&#65292;&#20197;&#20351;&#20854;&#33021;&#22815;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#35266;&#23519;&#25968;&#25454;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#31070;&#32463;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#40664;&#35748;&#20551;&#35774;&#26159;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#65288;&#21363;&#36816;&#34892;&#26102;&#65289;&#38454;&#27573;&#21464;&#37327;&#30340;&#30456;&#21516;&#20998;&#24067;&#21644;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36816;&#34892;&#26102;&#21487;&#33021;&#20250;&#21457;&#29983;&#20998;&#24067;&#20559;&#31227;&#65288;&#21363;&#22495;&#20559;&#31227;&#65289;&#65292;&#32780;&#21464;&#37327;&#38590;&#20197;&#35775;&#38382;&#24102;&#26469;&#30340;&#25361;&#25112;&#26356;&#22823;&#12290;&#36825;&#36890;&#24120;&#26159;&#30001;&#20110;&#19981;&#26029;&#22686;&#21152;&#30340;&#38544;&#31169;&#21644;&#20262;&#29702;&#38382;&#39064;&#36896;&#25104;&#30340;&#65292;&#36825;&#21487;&#33021;&#20351;&#24471;&#25972;&#20010;&#36816;&#34892;&#26102;&#25968;&#25454;&#20013;&#30340;&#20219;&#24847;&#21464;&#37327;&#19981;&#21487;&#29992;&#19988;&#26080;&#27861;&#22635;&#34917;&#12290;&#25105;&#20204;&#31216;&#22495;&#36716;&#31227;&#21644;&#19981;&#21487;&#35775;&#38382;&#21464;&#37327;&#30340;&#20849;&#21516;&#21457;&#29983;&#20026;&#36816;&#34892;&#26102;&#22495;&#22833;&#30495;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#35757;&#32451;&#21518;&#21453;&#20107;&#23454;&#39044;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#23545;&#25239;&#36816;&#34892;&#26102;&#22495;&#22833;&#30495;&#65292;&#25105;&#20204;&#23558;&#21453;&#20107;&#23454;&#39044;&#27979;&#23376;&#32435;&#20837;&#22495;&#33258;&#36866;&#24212;&#30340;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#28304;&#22495;&#35823;&#24046;&#21644;&#22495;&#38388;&#24046;&#24322;&#20043;&#21644;&#26469;&#19978;&#38480;&#30446;&#26631;&#22495;&#65288;&#21363;&#36816;&#34892;&#26102;&#21327;&#21464;&#37327;&#65289;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#21453;&#20107;&#23454;&#39044;&#27979;&#34920;&#36848;&#20026;&#19968;&#20010;&#20989;&#25968;&#31354;&#38388;&#25628;&#32034;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24102;&#26377;&#22495;&#22833;&#30495;&#30340;&#21464;&#20998;&#21453;&#20107;&#23454;&#39044;&#27979;&#23376;&#30340;&#26032;&#26041;&#27861;&#65288;V-CPDC&#65289;&#65292;&#35813;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#19978;&#30028;&#65292;&#24182;&#26088;&#22312;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#22235;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#36816;&#34892;&#26102;&#22495;&#22833;&#30495;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
To date, various neural methods have been proposed for causal effect estimation based on observational data, where a default assumption is the same distribution and availability of variables at both training and inference (i.e., runtime) stages. However, distribution shift (i.e., domain shift) could happen during runtime, and bigger challenges arise from the impaired accessibility of variables. This is commonly caused by increasing privacy and ethical concerns, which can make arbitrary variables unavailable in the entire runtime data and imputation impractical. We term the co-occurrence of domain shift and inaccessible variables runtime domain corruption, which seriously impairs the generalizability of a trained counterfactual predictor. To counter runtime domain corruption, we subsume counterfactual prediction under the notion of domain adaptation. Specifically, we upper-bound the error w.r.t. the target domain (i.e., runtime covariates) by the sum of source domain error and inter-dom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26412;&#22320;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#26469;&#21152;&#36895;&#24322;&#26500;&#25968;&#25454;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#29616;&#26377;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13263</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#37325;&#25490;&#21152;&#36895;&#24322;&#26500;&#25968;&#25454;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;(arXiv:2306.13263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity. (arXiv:2306.13263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26412;&#22320;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#26469;&#21152;&#36895;&#24322;&#26500;&#25968;&#25454;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#29616;&#26377;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23545;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#36827;&#34892;&#27927;&#29260;&#65292;&#20197;&#21516;&#36136;&#21270;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#36829;&#21453;&#25968;&#25454;&#35775;&#38382;&#26435;&#21033;&#65292;&#32780;&#23545;&#20110;&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#37325;&#25490;&#21487;&#20197;&#21152;&#36895;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#65292;&#30446;&#21069;&#23578;&#26410;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#19982;&#25910;&#25947;&#36895;&#29575;&#21442;&#25968;&#20043;&#38388;&#30340;&#31934;&#30830;&#21487;&#37327;&#21270;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#37325;&#25490;&#21487;&#20197;&#25353;&#30334;&#20998;&#27604;&#24179;&#26041;&#20943;&#23569;&#26799;&#24230;&#24046;&#24322;&#65292;&#20174;&#32780;&#21152;&#36895;&#25910;&#25947;&#12290;&#21463;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26412;&#22320;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#26469;&#35299;&#20915;&#25968;&#25454;&#35775;&#38382;&#26435;&#38382;&#39064;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#29616;&#26377;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, data heterogeneity is a critical challenge. A straightforward solution is to shuffle the clients' data to homogenize the distribution. However, this may violate data access rights, and how and when shuffling can accelerate the convergence of a federated optimization algorithm is not theoretically well understood. In this paper, we establish a precise and quantifiable correspondence between data heterogeneity and parameters in the convergence rate when a fraction of data is shuffled across clients. We prove that shuffling can quadratically reduce the gradient dissimilarity with respect to the shuffling percentage, accelerating convergence. Inspired by the theory, we propose a practical approach that addresses the data access rights issue by shuffling locally generated synthetic data. The experimental results show that shuffling synthetic data improves the performance of multiple existing federated learning algorithms by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.13255</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#19979;&#22810;&#31867;&#20998;&#31867;&#30340;&#28176;&#36827;&#27867;&#21270;&#31934;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#21327;&#21464;&#37327;&#21452;&#23618;&#27169;&#22411;&#19979;&#65292;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#28176;&#36827;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#25968;&#12289;&#29305;&#24449;&#21644;&#31867;&#21035;&#25968;&#37117;&#21516;&#26102;&#22686;&#38271;&#12290;&#25105;&#20204;&#23436;&#20840;&#35299;&#20915;&#20102;Subramanian&#31561;&#20154;&#22312;'22&#24180;&#25152;&#25552;&#20986;&#30340;&#29468;&#24819;&#65292;&#19982;&#39044;&#27979;&#30340;&#27867;&#21270;&#21306;&#38388;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#30340;&#19979;&#30028;&#31867;&#20284;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#65306;&#23427;&#20204;&#33021;&#22815;&#30830;&#31435;&#35823;&#20998;&#31867;&#29575;&#36880;&#28176;&#36235;&#36817;&#20110;0&#25110;1.&#25105;&#20204;&#32039;&#23494;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#26159;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22238;&#24402;&#22120;&#26368;&#20248;&#30340;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22312;&#28176;&#36827;&#19978;&#27425;&#20248;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31181;&#26032;&#30340;Hanson-Wright&#19981;&#31561;&#24335;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#20855;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#22810;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#31867;&#22411;&#20998;&#26512;&#22312;&#20960;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al.~'22, where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al.~'22, matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.  The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#21363;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#26469;&#21028;&#26029;&#21518;&#32493;&#26159;&#21542;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#20351;&#29992;&#27874;&#24418;&#25391;&#33633;&#21644;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;</title><link>http://arxiv.org/abs/2306.13253</link><description>&lt;p&gt;
&#25552;&#21069;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#65306;&#30740;&#31350;&#25484;&#25569;&#25216;&#33021;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;
&lt;/p&gt;
&lt;p&gt;
Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#21363;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#26469;&#21028;&#26029;&#21518;&#32493;&#26159;&#21542;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#20351;&#29992;&#27874;&#24418;&#25391;&#33633;&#21644;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#39044;&#27979;&#65292;&#35813;&#29616;&#35937;&#26159;&#23436;&#32654;&#27010;&#25324;&#22312;&#20986;&#29616;&#36807;&#25311;&#21512;&#25110;&#35760;&#24518;&#36857;&#35937;&#20043;&#21518;&#24456;&#38271;&#19968;&#27573;&#26102;&#38388;&#25165;&#20986;&#29616;&#12290;&#25253;&#21578;&#31216;&#65292;&#21482;&#26377;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#19979;&#25165;&#33021;&#35266;&#23519;&#21040;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#36825;&#20351;&#24471;&#30830;&#23450;&#23548;&#33268;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29702;&#35299;&#21069;&#28010;&#28526;&#38656;&#35201;&#22823;&#37327;&#30340;&#36845;&#20195;&#36718;&#25968;&#65292;&#22240;&#27492;&#23547;&#25214;&#23548;&#33268;&#23427;&#30340;&#36229;&#21442;&#25968;&#26159;&#24456;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22823;&#37327;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#22312;&#21069;&#20960;&#36718;&#20013;&#20986;&#29616;&#26576;&#20123;&#25391;&#33633;&#65292;&#37027;&#20040;&#21487;&#20197;&#26399;&#26395;&#22312;&#27169;&#22411;&#35757;&#32451;&#26356;&#38271;&#26102;&#38388;&#21518;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#26469;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#24369;&#28151;&#28102;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#39033;&#24335;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2306.13242</link><description>&lt;p&gt;
&#24369;&#28151;&#28102;&#19979;&#30340;&#36817;&#20284;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Approximate Causal Effect Identification under Weak Confounding. (arXiv:2306.13242v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#24369;&#28151;&#28102;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#39033;&#24335;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#12290;&#38024;&#23545;&#21487;&#35782;&#21035;&#22240;&#26524;&#26597;&#35810;&#30340;&#28857;&#20272;&#35745;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#27491;&#30830;&#23436;&#22791;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#26597;&#35810;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22810;&#39033;&#24335;&#31243;&#24207;&#65292;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#32039;&#23494;&#30028;&#38480;&#12290;&#20294;&#23545;&#20110;&#25903;&#25345;&#22823;&#23567;&#36739;&#22823;&#30340;&#21464;&#37327;&#65292;&#20248;&#21270;&#36825;&#20123;&#22810;&#39033;&#24335;&#31243;&#24207;&#22312;&#35745;&#31639;&#19978;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#8220;&#24369;&#28151;&#28102;&#8221;&#23545;&#22240;&#26524;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#29109;&#24456;&#23567;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32447;&#24615;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#20986;&#22240;&#26524;&#25928;&#24212;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#19968;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#29109;&#36235;&#36817;&#20110;&#38646;&#26102;&#65292;&#19978;&#38480;&#21644;&#19979;&#38480;&#20043;&#38388;&#30340;&#24046;&#24322;&#20250;&#28040;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#27169;&#25311;&#65292;&#20197;&#27604;&#36739;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#39033;&#24335;&#31243;&#24207;&#24471;&#21040;&#30340;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#24615;&#33021;&#20063;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation has been studied by many researchers when only observational data is available. Sound and complete algorithms have been developed for pointwise estimation of identifiable causal queries. For non-identifiable causal queries, researchers developed polynomial programs to estimate tight bounds on causal effect. However, these are computationally difficult to optimize for variables with large support sizes. In this paper, we analyze the effect of "weak confounding" on causal estimands. More specifically, under the assumption that the unobserved confounders that render a query non-identifiable have small entropy, we propose an efficient linear program to derive the upper and lower bounds of the causal effect. We show that our bounds are consistent in the sense that as the entropy of unobserved confounders goes to zero, the gap between the upper and lower bound vanishes. Finally, we conduct synthetic and real data simulations to compare our bounds with the bounds obta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#65292;&#38416;&#26126;&#20102;&#22823;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#22120;&#20013;&#30340;&#38543;&#26426;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#20013;&#28023;&#26862;&#30697;&#38453;&#36857;&#36817;&#20284;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#20854;&#23545;&#24212;&#31471;&#23545;&#31471;&#30697;&#38453;&#21442;&#25968;&#30340;Schatten 1-&#33539;&#25968;&#20056;&#31215;&#12290;</title><link>http://arxiv.org/abs/2306.13239</link><description>&lt;p&gt;
&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#20013;&#24179;&#22374;&#24615;&#27491;&#21017;&#21270;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Inductive Bias of Flatness Regularization for Deep Matrix Factorization. (arXiv:2306.13239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#65292;&#38416;&#26126;&#20102;&#22823;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#22120;&#20013;&#30340;&#38543;&#26426;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#20013;&#28023;&#26862;&#30697;&#38453;&#36857;&#36817;&#20284;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#20854;&#23545;&#24212;&#31471;&#23545;&#31471;&#30697;&#38453;&#21442;&#25968;&#30340;Schatten 1-&#33539;&#25968;&#20056;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#36807;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20248;&#21270;&#22120;&#20013;&#30340;&#38543;&#26426;&#24615;&#20855;&#26377;&#38544;&#24335;&#30340;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#21487;&#20197;&#36890;&#36807;&#20943;&#23567;&#25439;&#22833;&#20989;&#25968;&#30340;&#23574;&#38160;&#24230;&#65288;&#29305;&#21035;&#26159;&#20854;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#65289;&#26469;&#26368;&#23567;&#21270;&#38646;&#25439;&#22833;&#35299;&#26063;&#12290;&#26356;&#26126;&#30830;&#30340;&#24179;&#22374;&#24615;&#27491;&#21017;&#21270;&#24418;&#24335;&#20063;&#22312;&#32463;&#39564;&#19978;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24179;&#22374;&#24615;&#27491;&#21017;&#21270;&#20309;&#26102;&#21644;&#20026;&#20160;&#20040;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20197;&#37325;&#35201;&#30340;&#23398;&#20064;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#35774;&#32622;&#20026;&#20363;&#65292;&#21363;&#20174;&#32447;&#24615;&#27979;&#37327;&#20013;&#23398;&#20064;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#65292;&#20026;&#29702;&#35299;&#26368;&#23567;&#21270;&#28023;&#26862;&#30697;&#38453;&#36857;&#35299;&#30340;&#24402;&#32435;&#20559;&#24046;&#36808;&#20986;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#28385;&#36275;&#27979;&#37327;&#26631;&#20934;&#21463;&#38480;&#31561;&#36317;&#24615;&#36136;&#65288;RIP&#65289;&#30340;&#25152;&#26377;&#28145;&#24230;&#22823;&#20110;&#19968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#23567;&#21270;&#28023;&#26862;&#30697;&#38453;&#36857;&#36817;&#20284;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#30456;&#24212;&#31471;&#23545;&#31471;&#30697;&#38453;&#21442;&#25968;&#30340;Schatten 1-&#33539;&#25968;&#65288;&#21363;&#25152;&#26377;&#22855;&#24322;&#20540;&#20043;&#21644;&#65289;&#30340;&#20056;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step toward understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as \emph{deep matrix factorization}. We show that for all depth greater than one, with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;DSS&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21363;&#20415;&#21482;&#24341;&#20837;&#23569;&#37327;&#31232;&#30095;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13237</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pruning for Better Domain Generalizability. (arXiv:2306.13237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;DSS&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21363;&#20415;&#21482;&#24341;&#20837;&#23569;&#37327;&#31232;&#30095;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#22914;L2&#24050;&#32463;&#21487;&#20197;&#22312;&#30446;&#26631;&#22495;&#24615;&#33021;&#19978;&#25552;&#20379;&#23567;&#24133;&#24230;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;&#65292;&#31216;&#20026;DSS&#65292;&#35774;&#35745;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#29978;&#33267;&#21487;&#20197;&#19982;MIRO(Cha&#31561;&#20154;&#65292;2022&#24180;)&#31561;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;MNIST&#21040;MNIST-M&#19978;&#65292;&#36890;&#36807;&#23558;60%&#36890;&#36947;&#31232;&#30095;&#24341;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#32447;&#24615;&#33021;&#25552;&#39640;5&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#12290;&#22312;DomainBed&#22522;&#20934;&#21644;&#26368;&#20808;&#36827;&#30340;MIRO&#19978;&#65292;&#20165;&#36890;&#36807;&#23558;10%&#31232;&#30095;&#24341;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#36951;&#25022;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.13233</link><description>&lt;p&gt;
&#22522;&#20110;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#30340;&#23545;&#25968;&#36951;&#25022;&#23545;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback. (arXiv:2306.13233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#36951;&#25022;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#27599;&#27493;&#34892;&#36873;&#25163;&#36873;&#25321;&#19968;&#34892;$i$&#65292;&#21015;&#36873;&#25163;&#36873;&#25321;&#19968;&#21015;$j$&#65292;&#34892;&#36873;&#25163;&#25910;&#21040;&#24179;&#22343;&#20540;&#20026;$A_{i,j}$&#30340;&#22024;&#26434;&#22870;&#21169;&#12290;&#34892;&#36873;&#25163;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22320;&#32047;&#31215;&#22870;&#21169;&#65292;&#21363;&#20351;&#23545;&#25163;&#26159;&#19968;&#20010;&#23545;&#25163;&#24615;&#21015;&#36873;&#25163;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35777;&#26126;&#22312;$m \times n$&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{mnT})$&#23545;&#25968;&#36951;&#25022;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;UCB&#39118;&#26684;&#31639;&#27861;&#25152;&#33719;&#24471;&#30340;$O(m\sqrt{nT})$&#23545;&#25968;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a variant of zero-sum matrix games where at each timestep the row player chooses row $i$, the column player chooses column $j$, and the row player receives a noisy reward with mean $A_{i,j}$. The objective of the row player is to accumulate as much reward as possible, even against an adversarial column player. If the row player uses the EXP3 strategy, an algorithm known for obtaining $\sqrt{T}$ regret against an arbitrary sequence of rewards, it is immediate that the row player also achieves $\sqrt{T}$ regret relative to the Nash equilibrium in this game setting. However, partly motivated by the fact that the EXP3 strategy is myopic to the structure of the game, O'Donoghue et al. (2021) proposed a UCB-style algorithm that leverages the game structure and demonstrated that this algorithm greatly outperforms EXP3 empirically. While they showed that this UCB-style algorithm achieved $\sqrt{T}$ regret, in this paper we ask if there exists an algorithm that provably ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13229</link><description>&lt;p&gt;
TACO&#65306;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#21019;&#24314;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;&#20016;&#23500;&#20195;&#29702;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#25511;&#21046;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#34920;&#31034;&#26368;&#20248;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#23567;&#30340;&#25277;&#35937;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TACO&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;TACO&#36890;&#36807;&#20248;&#21270;&#37325;&#26032;&#33719;&#24471;&#35266;&#23519;&#19982;&#26368;&#36817;&#30340;&#22810;&#20010;&#20808;&#21069;&#35266;&#23519;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#19982;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
&lt;/p&gt;</description></item><item><title>&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21512;&#25104;&#25968;&#25454;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13216</link><description>&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#29992;&#20110;&#25968;&#25454;&#38544;&#31169;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Diverse Community Data for Benchmarking Data Privacy Algorithms. (arXiv:2306.13216v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13216
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21512;&#25104;&#25968;&#25454;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#26159;&#32654;&#22269;&#22269;&#23478;&#26631;&#20934;&#21644;&#25216;&#26415;&#30740;&#31350;&#25152;&#65288;NIST&#65289;&#35745;&#21010;&#30340;&#26680;&#24515;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#34920;&#26684;&#25968;&#25454;&#21435;&#35782;&#21035;&#25216;&#26415;&#65288;&#22914;&#21512;&#25104;&#25968;&#25454;&#65289;&#30340;&#29702;&#35299;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#27665;&#20027;&#21270;&#22823;&#25968;&#25454;&#21033;&#30410;&#30340;&#19968;&#39033;&#38596;&#24515;&#21187;&#21187;&#30340;&#23581;&#35797;&#65307;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#37325;&#26032;&#21019;&#24314;&#25935;&#24863;&#20010;&#20154;&#25968;&#25454;&#65292;&#20197;&#20415;&#20844;&#24320;&#21457;&#24067;&#12290;&#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#29978;&#33267;&#21487;&#33021;&#25918;&#22823;&#36825;&#20123;&#38382;&#39064;&#12290;&#24403;&#21435;&#35782;&#21035;&#25968;&#25454;&#20998;&#24067;&#24341;&#20837;&#20559;&#24046;&#25110;&#24037;&#20214;&#65292;&#25110;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20250;&#23558;&#36825;&#20123;&#38382;&#39064;&#20256;&#25773;&#21040;&#19979;&#28216;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#35843;&#26597;&#26465;&#20214;&#65288;&#22914;&#22810;&#26679;&#23376;&#32676;&#12289;&#24322;&#36136;&#38750;&#26377;&#24207;&#25968;&#25454;&#31354;&#38388;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65289;&#23545;&#21512;&#25104;&#25968;&#25454;&#31639;&#27861;&#25552;&#20986;&#20102;&#20855;&#20307;&#25361;&#25112;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#20419;&#20351;&#38656;&#35201;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#26469;&#25903;&#25345;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#32780;&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Diverse Communities Data Excerpts are the core of a National Institute of Standards and Technology (NIST) program to strengthen understanding of tabular data deidentification technologies such as synthetic data. Synthetic data is an ambitious attempt to democratize the benefits of big data; it uses generative models to recreate sensitive personal data with new records for public release. However, it is vulnerable to the same bias and privacy issues that impact other machine learning applications, and can even amplify those issues. When deidentified data distributions introduce bias or artifacts, or leak sensitive information, they propagate these problems to downstream applications. Furthermore, real-world survey conditions such as diverse subpopulations, heterogeneous non-ordinal data spaces, and complex dependencies between features pose specific challenges for synthetic data algorithms. These observations motivate the need for real, diverse, and complex benchmark data to support
&lt;/p&gt;</description></item><item><title>ovla&#26159;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#27700;&#21360;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#20351;&#27700;&#21360;&#20445;&#25345;&#20241;&#30496;&#29366;&#24577;&#65292;&#21482;&#26377;&#22312;&#25152;&#26377;&#32773;&#30340;&#31192;&#23494;&#23494;&#38053;&#34987;&#24212;&#29992;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#65292;&#35813;&#26041;&#27861;&#22312;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#29575;&#12289;&#35823;&#25253;&#29575;&#21644;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13215</link><description>&lt;p&gt;
ovla&#65306;&#20351;&#29992;&#38544;&#24335;&#27700;&#21360;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ovla: Neural Network Ownership Verification using Latent Watermarks. (arXiv:2306.13215v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13215
&lt;/p&gt;
&lt;p&gt;
ovla&#26159;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#27700;&#21360;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#20351;&#27700;&#21360;&#20445;&#25345;&#20241;&#30496;&#29366;&#24577;&#65292;&#21482;&#26377;&#22312;&#25152;&#26377;&#32773;&#30340;&#31192;&#23494;&#23494;&#38053;&#34987;&#24212;&#29992;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#65292;&#35813;&#26041;&#27861;&#22312;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#29575;&#12289;&#35823;&#25253;&#29575;&#21644;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#23545;&#20110;&#20445;&#25252;&#36825;&#20123;&#27169;&#22411;&#20813;&#21463;&#38750;&#27861;&#22797;&#21046;&#12289;&#20813;&#36153;&#39569;&#36710;&#12289;&#37325;&#26032;&#20998;&#37197;&#21644;&#20854;&#20182;&#30693;&#35782;&#20135;&#26435;&#30340;&#28389;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#27700;&#21360;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#32593;&#32476;&#30340;&#27491;&#24120;&#25805;&#20316;&#21644;&#23545;&#24102;&#27700;&#21360;&#36755;&#20837;&#30340;&#21709;&#24212;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#35757;&#32451;&#32593;&#32476;&#65292;&#20351;&#24471;&#27700;&#21360;&#20445;&#25345;&#20241;&#30496;&#29366;&#24577;&#65292;&#38500;&#38750;&#24212;&#29992;&#25152;&#26377;&#32773;&#30340;&#31192;&#23494;&#23494;&#38053;&#26469;&#28608;&#27963;&#23427;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#29575;&#12289;&#35823;&#25253;&#29575;&#21644;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ownership verification for neural networks is important for protecting these models from illegal copying, free-riding, re-distribution and other intellectual property misuse. We present a novel methodology for neural network ownership verification based on the notion of latent watermarks. Existing ownership verification methods either modify or introduce constraints to the neural network parameters, which are accessible to an attacker in a white-box attack and can be harmful to the network's normal operation, or train the network to respond to specific watermarks in the inputs similar to data poisoning-based backdoor attacks, which are susceptible to backdoor removal techniques. In this paper, we address these problems by decoupling a network's normal operation from its responses to watermarked inputs during ownership verification. The key idea is to train the network such that the watermarks remain dormant unless the owner's secret key is applied to activate it. The secret key is real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13213</link><description>&lt;p&gt;
&#35270;&#35273;&#23545;&#25239;&#26679;&#26412;&#36234;&#29425;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Adversarial Examples Jailbreak Large Language Models. (arXiv:2306.13213v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#39640;&#24230;&#20851;&#27880;&#12290;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#20363;&#22914;Flamingo&#12289;BLIP-2&#21644;GPT-4&#65292;&#26631;&#24535;&#30528;&#35270;&#35273;&#21644;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20808;&#36827;&#21457;&#23637;&#30456;&#20114;&#34701;&#21512;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#28041;&#21450;&#30340;&#39118;&#38505;&#20173;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#22312;&#26412;&#36136;&#19978;&#20351;&#20854;&#25104;&#20026;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#25193;&#22823;&#20102;LLMs&#30340;&#25915;&#20987;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;LLMs&#30340;&#24191;&#27867;&#21151;&#33021;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#65292;&#23558;&#23433;&#20840;&#22833;&#36133;&#30340;&#24433;&#21709;&#25193;&#23637;&#21040;&#20102;&#31616;&#21333;&#30340;&#38169;&#35823;&#20998;&#31867;&#20043;&#22806;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLM&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;KD-&#26641;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104; $\epsilon $-&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20854;&#26680;&#23494;&#24230;&#31867;&#20284;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#26680;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#65292;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13211</link><description>&lt;p&gt;
&#20351;&#29992;KD-&#26641;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data Using KD-Trees. (arXiv:2306.13211v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;KD-&#26641;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104; $\epsilon $-&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20854;&#26680;&#23494;&#24230;&#31867;&#20284;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#26680;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#65292;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#24544;&#23454;&#22320;&#20195;&#34920;&#25968;&#25454;&#20998;&#24067;&#24182;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#31354;&#38388;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#22238;&#31572;&#32479;&#35745;&#26597;&#35810;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#31354;&#38388;&#20998;&#21106;&#25216;&#26415;&#21644;&#22122;&#22768;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#30452;&#35266;&#36879;&#26126;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#29420;&#31435;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104; $\epsilon $-&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20854;&#26680;&#23494;&#24230;&#31867;&#20284;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#26680;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#26435;&#21033;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#30456;&#20851;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#24182;&#23548;&#33268;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#30340;&#23454;&#29992;&#25913;&#36827;&#65292;&#24182;&#35752;&#35770;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Creation of a synthetic dataset that faithfully represents the data distribution and simultaneously preserves privacy is a major research challenge. Many space partitioning based approaches have emerged in recent years for answering statistical queries in a differentially private manner. However, for synthetic data generation problem, recent research has been mainly focused on deep generative models. In contrast, we exploit space partitioning techniques together with noise perturbation and thus achieve intuitive and transparent algorithms. We propose both data independent and data dependent algorithms for $\epsilon$-differentially private synthetic data generation whose kernel density resembles that of the real dataset. Additionally, we provide theoretical results on the utility-privacy trade-offs and show how our data dependent approach overcomes the curse of dimensionality and leads to a scalable algorithm. We show empirical utility improvements over the prior work, and discuss perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23545;&#25968;&#32047;&#31215;&#27861;&#25913;&#36827;&#20102;$\mathcal{G}^0$&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24658;&#23450;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#31895;&#31961;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;SAR&#22270;&#20687;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.13200</link><description>&lt;p&gt;
&#25552;&#39640;&#22522;&#20110;&#23545;&#25968;&#32047;&#31215;&#30340;SAR&#22270;&#20687;&#31895;&#31961;&#24230;&#20449;&#24687;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Log-Cumulant Based Estimation of Roughness Information in SAR imagery. (arXiv:2306.13200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23545;&#25968;&#32047;&#31215;&#27861;&#25913;&#36827;&#20102;$\mathcal{G}^0$&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24658;&#23450;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#31895;&#31961;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;SAR&#22270;&#20687;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#30340;&#29702;&#35299;&#23545;&#20110;&#36965;&#24863;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21463;&#20854;&#20869;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#65288;&#31216;&#20026;&#26001;&#28857;&#65289;&#30340;&#24433;&#21709;&#12290;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#65288;&#20363;&#22914;$\mathcal{G}^0$&#20998;&#24067;&#26063;&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;SAR&#25968;&#25454;&#65292;&#35768;&#22810;&#24403;&#21069;&#22312;&#22788;&#29702;&#27492;&#31867;&#22270;&#20687;&#26041;&#38754;&#30340;&#36827;&#23637;&#37117;&#26159;&#36890;&#36807;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#25552;&#21462;&#20449;&#24687;&#26469;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;$\mathcal{G}^0$&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#25968;&#32047;&#31215;&#30340;&#26041;&#27861;&#36827;&#34892;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#24314;&#27169;&#65292;&#25105;&#20204;&#26500;&#24314;&#20986;&#21487;&#22312;$\mathcal{G}^0_A$&#21644;$\mathcal{G}^0_I$&#27169;&#22411;&#19979;&#37117;&#20135;&#29983;&#21487;&#38752;&#31895;&#31961;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#35282;&#20989;&#25968;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#24658;&#23450;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#31895;&#31961;&#24230;&#20272;&#35745;&#65292;&#20351;&#20854;&#27604;&#29616;&#26377;&#26041;&#27861;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20174;SAR&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#31895;&#31961;&#24230;&#20449;&#24687;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;SAR&#22270;&#20687;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Aperture Radar (SAR) image understanding is crucial in remote sensing applications, but it is hindered by its intrinsic noise contamination, called speckle. Sophisticated statistical models, such as the $\mathcal{G}^0$ family of distributions, have been employed to SAR data and many of the current advancements in processing this imagery have been accomplished through extracting information from these models. In this paper, we propose improvements to parameter estimation in $\mathcal{G}^0$ distributions using the Method of Log-Cumulants. First, using Bayesian modeling, we construct that regularly produce reliable roughness estimates under both $\mathcal{G}^0_A$ and $\mathcal{G}^0_I$ models. Second, we make use of an approximation of the Trigamma function to compute the estimated roughness in constant time, making it considerably faster than the existing method for this task. Finally, we show how we can use this method to achieve fast and reliable SAR image understanding based 
&lt;/p&gt;</description></item><item><title>&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.13197</link><description>&lt;p&gt;
Gradient-based Attribution Methods&#20013;Pre&#25110;Post-Softmax Scores&#65292;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13197
&lt;/p&gt;
&lt;p&gt;
&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24037;&#20316;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#20351;&#29992;&#32593;&#32476;&#20998;&#25968;&#30340;&#26799;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#21644;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#23454;&#38469;&#24046;&#24322;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient based attribution methods for neural networks working as classifiers use gradients of network scores. Here we discuss the practical differences between using gradients of pre-softmax scores versus post-softmax scores, and their respective advantages and disadvantages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24179;&#26041;&#26681;Lipschitz&#25439;&#22833;&#30340;&#19968;&#33268;&#25910;&#25947;&#24615;&#65292;&#23545;&#19968;&#33324;&#30340;&#39640;&#26031;&#25968;&#25454;&#24314;&#31435;&#20102;&#20445;&#35777;&#65292;&#20801;&#35768;&#22788;&#29702;&#24191;&#27867;&#30340;&#25439;&#22833;&#31867;&#21035;&#65292;&#24182;&#37325;&#26032;&#25512;&#23548;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#8220;&#20048;&#35266;&#29575;&#8221;&#30340;&#23398;&#20064;&#20445;&#35777;&#21644;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.13188</link><description>&lt;p&gt;
&#24179;&#26041;&#26681;Lipschitz&#25439;&#22833;&#30340;&#19968;&#33268;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uniform Convergence with Square-Root Lipschitz Loss. (arXiv:2306.13188v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24179;&#26041;&#26681;Lipschitz&#25439;&#22833;&#30340;&#19968;&#33268;&#25910;&#25947;&#24615;&#65292;&#23545;&#19968;&#33324;&#30340;&#39640;&#26031;&#25968;&#25454;&#24314;&#31435;&#20102;&#20445;&#35777;&#65292;&#20801;&#35768;&#22788;&#29702;&#24191;&#27867;&#30340;&#25439;&#22833;&#31867;&#21035;&#65292;&#24182;&#37325;&#26032;&#25512;&#23548;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#8220;&#20048;&#35266;&#29575;&#8221;&#30340;&#23398;&#20064;&#20445;&#35777;&#21644;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20551;&#35774;&#31867;&#30340;Rademacher&#22797;&#26434;&#24230;&#21644;&#26631;&#37327;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#26041;&#26681;&#30340;Lipschitz&#24120;&#25968;&#65292;&#22312;&#39640;&#26031;&#25968;&#25454;&#26041;&#38754;&#24314;&#31435;&#20102;&#19968;&#33324;&#30340;&#19968;&#33268;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20445;&#35777;&#22914;&#20309;&#22823;&#22823;&#27010;&#25324;&#20102;&#22522;&#20110;&#24179;&#28369;&#24615;(&#23548;&#25968;&#30340;Lipschitz&#24120;&#25968;)&#30340;&#20808;&#21069;&#32467;&#26524;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#22788;&#29702;&#26356;&#24191;&#27867;&#30340;&#24179;&#26041;&#26681;Lipschitz&#25439;&#22833;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#36866;&#29992;&#20110;&#30740;&#31350;&#30456;&#20301;&#24674;&#22797;&#21644;ReLU&#22238;&#24402;&#30340;&#38750;&#24179;&#28369;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#37325;&#26032;&#25512;&#23548;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#8220;&#20048;&#35266;&#29575;&#8221;&#30340;&#23398;&#20064;&#20445;&#35777;&#21644;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish generic uniform convergence guarantees for Gaussian data in terms of the Rademacher complexity of the hypothesis class and the Lipschitz constant of the square root of the scalar loss function. We show how these guarantees substantially generalize previous results based on smoothness (Lipschitz constant of the derivative), and allow us to handle the broader class of square-root-Lipschitz losses, which includes also non-smooth loss functions appropriate for studying phase retrieval and ReLU regression, as well as rederive and better understand "optimistic rate" and interpolation learning guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#20013;&#36807;&#25311;&#21512;&#25104;&#26412;&#65292;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#20197;&#20998;&#26512;&#26679;&#26412;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#23545;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.13185</link><description>&lt;p&gt;
(&#26680;) &#23725;&#22238;&#24402;&#20013;&#36807;&#24230;&#25311;&#21512;&#25104;&#26412;&#30340;&#19981;&#21487;&#30693;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression. (arXiv:2306.13185v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#20013;&#36807;&#25311;&#21512;&#25104;&#26412;&#65292;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#20197;&#20998;&#26512;&#26679;&#26412;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#23545;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#22122;&#22768;&#30340;&#26680;&#23725;&#22238;&#24402; (KRR) &#20013;&#36807;&#25311;&#21512;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#25554;&#20540;&#26080;&#23725;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#19982;&#26368;&#20248;&#35843;&#33410;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#20043;&#27604;&#12290;&#25105;&#20204;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#21363;&#23545;&#20110;&#20219;&#20309;&#30446;&#26631;&#20989;&#25968;&#65292;&#21363;&#20351;&#26679;&#26412;&#37327;&#19981;&#36275;&#20197;&#36798;&#21040;&#19968;&#33268;&#24615;&#25110;&#30446;&#26631;&#20989;&#25968;&#19981;&#22312; RKHS &#20013;&#65292;&#25105;&#20204;&#20063;&#23558;&#25104;&#26412;&#30475;&#20316;&#26679;&#26412;&#37327;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#36817;&#25512;&#23548;&#20986;&#30340;&#65288;&#38750;&#20005;&#26684;&#30340;&#65289;&#39118;&#38505;&#35780;&#20272;&#65292;&#20197;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#39640;&#26031;&#26222;&#36866;&#24615;&#20551;&#35774;&#20998;&#26512;&#36807;&#24230;&#25311;&#21512;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#33391;&#24615;&#12289;&#32531;&#21644;&#21644;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#65288;&#21442;&#35265; Mallinar &#31561;&#20154; 2022&#65289;&#30340;&#26356;&#31934;&#32454;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an "agnostic" view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24490;&#29615;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#31934;&#30830;&#39044;&#27979;&#26497;&#22320;&#21306;&#22495;&#24180;&#38477;&#38634;&#37327;&#65292;&#19988;&#22312;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13181</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#39044;&#27979;&#24180;&#38477;&#38634;&#37327;
&lt;/p&gt;
&lt;p&gt;
Prediction of Annual Snow Accumulation Using a Recurrent Graph Convolutional Approach. (arXiv:2306.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24490;&#29615;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#31934;&#30830;&#39044;&#27979;&#26497;&#22320;&#21306;&#22495;&#24180;&#38477;&#38634;&#37327;&#65292;&#19988;&#22312;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#36319;&#36394;&#21644;&#39044;&#27979;&#26497;&#22320;&#20912;&#23618;&#30340;&#31215;&#38634;&#24773;&#20917;&#21487;&#20197;&#25581;&#31034;&#20854;&#21382;&#21490;&#36235;&#21183;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807; radar &#20256;&#24863;&#22120;&#65292;&#22914; Snow Radar &#65292;&#33021;&#22815;&#20197;&#36739;&#39640;&#30340;&#22402;&#30452;&#20998;&#36776;&#29575;&#27979;&#37327;&#36825;&#20123;&#20869;&#37096;&#20912;&#23618;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#32473;&#20986;&#21253;&#21547;&#28145;&#20912;&#23618;&#21402;&#24230;&#30340;&#26102;&#24577;&#22270;&#26102;&#65292;&#26102;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#39044;&#27979;&#26410;&#26469;&#31215;&#38634;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#29992;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#27169;&#22411;&#39044;&#27979;&#26356;&#22810;&#30340;&#24180;&#31215;&#38634;&#25968;&#25454;&#28857;&#65292;&#24182;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#36755;&#20837;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#36739;&#22823;&#30340;&#21464;&#21270;&#21482;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#38750;&#24120;&#36731;&#24494;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise tracking and prediction of polar ice layers can unveil historic trends in snow accumulation. In recent years, airborne radar sensors, such as the Snow Radar, have been shown to be able to measure these internal ice layers over large areas with a fine vertical resolution. In our previous work, we found that temporal graph convolutional networks perform reasonably well in predicting future snow accumulation when given temporal graphs containing deep ice layer thickness. In this work, we experiment with a graph attention network-based model and used it to predict more annual snow accumulation data points with fewer input data points on a larger dataset. We found that these large changes only very slightly negatively impacted performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#27880;&#24847;&#21147;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#20851;&#38190;&#24103;&#65292;&#25552;&#21462;&#30340;&#20851;&#38190;&#24103;&#21487;&#20197;&#29992;&#20316;&#35201;&#20351;&#29992;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#25688;&#35201;&#21644;&#23433;&#38450;&#31561;&#34892;&#19994;&#20013;&#30340;&#33258;&#21160;&#21270;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.13176</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#24103;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Key Frame Extraction with Attention Based Deep Neural Networks. (arXiv:2306.13176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#27880;&#24847;&#21147;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#20851;&#38190;&#24103;&#65292;&#25552;&#21462;&#30340;&#20851;&#38190;&#24103;&#21487;&#20197;&#29992;&#20316;&#35201;&#20351;&#29992;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#25688;&#35201;&#21644;&#23433;&#38450;&#31561;&#34892;&#19994;&#20013;&#30340;&#33258;&#21160;&#21270;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#20851;&#38190;&#24103;&#26159;&#36873;&#25321;&#33021;&#22815;&#26368;&#22909;&#22320;&#27010;&#25324;&#38271;&#35270;&#39057;&#20869;&#23481;&#30340;&#22330;&#26223;&#30340;&#32451;&#20064;&#12290;&#25552;&#20379;&#35270;&#39057;&#25688;&#35201;&#26159;&#20419;&#36827;&#24555;&#36895;&#27983;&#35272;&#21644;&#20869;&#23481;&#24635;&#32467;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#33719;&#24471;&#30340;&#20851;&#38190;&#24103;&#21487;&#20197;&#29992;&#20316;&#35201;&#20351;&#29992;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#27880;&#24847;&#21147;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20851;&#38190;&#24103;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#20174;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;k-means&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20998;&#27573;&#65292;&#23558;&#31867;&#20284;&#30340;&#29305;&#24449;&#21644;&#24103;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#28982;&#21518;&#65292;&#20174;&#36825;&#20123;&#24103;&#20013;&#36873;&#25321;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic keyframe detection from videos is an exercise in selecting scenes that can best summarize the content for long videos. Providing a summary of the video is an important task to facilitate quick browsing and content summarization. The resulting photos are used for automated works (e.g. summarizing security footage, detecting different scenes used in music clips) in different industries. In addition, processing high-volume videos in advanced machine learning methods also creates resource costs. Keyframes obtained; It can be used as an input feature to the methods and models to be used. In this study; We propose a deep learning-based approach for keyframe detection using a deep auto-encoder model with an attention layer. The proposed method first extracts the features from the video frames using the encoder part of the autoencoder and applies segmentation using the k-means clustering algorithm to group these features and similar frames together. Then, keyframes are selected from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AmicroN&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#24494;&#21160;&#20316;&#27880;&#37322;&#65292;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#20047;&#32454;&#31890;&#24230;&#27880;&#37322;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2306.13149</link><description>&lt;p&gt;
AmicroN&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#32454;&#31890;&#24230;&#24494;&#21160;&#20316;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27880;&#37322;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities. (arXiv:2306.13149v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AmicroN&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#24494;&#21160;&#20316;&#27880;&#37322;&#65292;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#20047;&#32454;&#31890;&#24230;&#27880;&#37322;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#26410;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22686;&#38271;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20154;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#24120;&#23548;&#33268;&#25910;&#38598;&#26356;&#27973;&#30340;&#27880;&#37322;&#12290;&#36825;&#20123;&#27973;&#23618;&#27880;&#37322;&#24573;&#30053;&#20102;&#32452;&#25104;&#26085;&#24120;&#29983;&#27963;&#20013;&#20219;&#20309;&#22797;&#26434;&#27963;&#21160;&#30340;&#32454;&#31890;&#24230;&#24494;&#21160;&#20316;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21487;&#29992;&#30340;&#39044;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#32454;&#31890;&#24230;&#27880;&#37322;&#30340;&#21407;&#22240;&#21644;&#32570;&#38519;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#35843;&#26597;&#20197;&#20102;&#35299;&#19982;&#27880;&#37322;&#30456;&#20851;&#30340;&#20154;&#31867;&#24863;&#30693;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AmicroN&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#36816;&#21160;&#29305;&#24449;&#21644;&#21487;&#29992;&#30340;&#31895;&#31890;&#24230;&#23439;&#25805;&#20316;&#26631;&#31614;&#33258;&#21160;&#29983;&#25104;&#24494;&#21160;&#20316;&#27880;&#37322;&#12290;&#22312;&#21518;&#21488;&#65292;AmicroN&#24212;&#29992;&#21464;&#28857;&#26816;&#27979;&#65292;&#28982;&#21518;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient human activity recognition (HAR) using sensor data needs a significant volume of annotated data. The growing volume of unlabelled sensor data has challenged conventional practices for gathering HAR annotations with human-in-the-loop approaches, often leading to the collection of shallower annotations. These shallower annotations ignore the fine-grained micro-activities that constitute any complex activities of daily living (ADL). Understanding this, we, in this paper, first analyze this lack of granular annotations from available pre-annotated datasets to understand the practical inconsistencies and also perform a detailed survey to look into the human perception surrounding annotations. Drawing motivations from these, we next develop the framework AmicroN that can automatically generate micro-activity annotations using locomotive signatures and the available coarse-grain macro-activity labels. In the backend, AmicroN applies change-point detection followed by zero-shot learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39034;&#24207;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#19981;&#23545;&#23545;&#25239;&#24615;&#26679;&#20363;&#36827;&#34892;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31639;&#27861;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13119</link><description>&lt;p&gt;
&#36890;&#36807;&#24323;&#26435;&#23454;&#29616;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39034;&#24207;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#19981;&#23545;&#23545;&#25239;&#24615;&#26679;&#20363;&#36827;&#34892;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31639;&#27861;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24102;&#26377;&#20801;&#35768;&#27880;&#20837;&#24178;&#20928;&#26631;&#31614;&#23545;&#25239;&#24615;&#65288;&#25110;&#36229;&#20986;&#20998;&#24067;&#65289;&#31034;&#20363;&#30340;&#23545;&#25239;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#30340;&#39034;&#24207;&#39044;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#32431;&#38543;&#26426;&#25968;&#25454;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#27492;&#31867;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#21307;&#23398;&#24314;&#35758;&#65292;&#36825;&#37324;&#24323;&#26435;&#19981;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#39044;&#27979;&#20248;&#20110;&#35823;&#20998;&#31867;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20551;&#35774;&#23436;&#20840;&#23545;&#25239;&#24615;&#25968;&#25454;&#23548;&#33268;&#38750;&#24120;&#24754;&#35266;&#30340;&#30028;&#38480;&#65292;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26159;&#31354;&#27934;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#39044;&#27979;&#27169;&#22411;&#65292;&#23427;&#20301;&#20110;&#32431;&#38543;&#26426;&#21644;&#23436;&#20840;&#23545;&#25239;&#24615;&#35774;&#32622;&#20043;&#38388;&#65292;&#36890;&#36807;&#20801;&#35768;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#26679;&#20363;&#19978;&#26080;&#20195;&#20215;&#22320;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#26469;&#23454;&#29616;&#12290;&#20551;&#35774;&#35775;&#38382;&#38750;&#23545;&#25239;&#26679;&#20363;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#35823;&#24046;&#38543;&#30528;VC&#32500;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#27668;&#20307;&#21387;&#21147;&#65292;&#20026;&#31649;&#36947;&#31995;&#32479;&#30417;&#25511;&#25552;&#20379;&#20102;&#31532;&#19968;&#27493;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#20445;&#30495;&#24230;&#19988;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13116</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27682;&#33030;&#30340;&#26426;&#22120;&#23398;&#20064;&#21387;&#21147;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Pressure Emulator for Hydrogen Embrittlement. (arXiv:2306.13116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#27668;&#20307;&#21387;&#21147;&#65292;&#20026;&#31649;&#36947;&#31995;&#32479;&#30417;&#25511;&#25552;&#20379;&#20102;&#31532;&#19968;&#27493;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#20445;&#30495;&#24230;&#19988;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27682;&#19982;&#22825;&#28982;&#27668;&#28151;&#21512;&#21518;&#20316;&#20026;&#27682;&#30340;&#26367;&#20195;&#21697;&#34987;&#29992;&#20110;&#27682;&#36816;&#36755;&#12290;&#20294;&#26159;&#65292;&#29289;&#36136;&#30340;&#27682;&#33030;&#24615;&#26159;&#31185;&#23398;&#23478;&#21644;&#22825;&#28982;&#27668;&#23433;&#35013;&#35774;&#35745;&#24072;&#38656;&#35201;&#36991;&#20813;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#22788;&#29702;&#20013;&#30340;&#22833;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#27668;&#20307;&#21387;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#27169;&#25311;&#22120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#21387;&#21147;&#65292;&#36825;&#26159;&#31649;&#36947;&#31995;&#32479;&#30417;&#25511;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#28385;&#36275;&#27668;&#27969;&#31995;&#32479;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent alternative for hydrogen transportation as a mixture with natural gas is blending it into natural gas pipelines. However, hydrogen embrittlement of material is a major concern for scientists and gas installation designers to avoid process failures. In this paper, we propose a physics-informed machine learning model to predict the gas pressure on the pipes' inner wall. Despite its high-fidelity results, the current PDE-based simulators are time- and computationally-demanding. Using simulation data, we train an ML model to predict the pressure on the pipelines' inner walls, which is a first step for pipeline system surveillance. We found that the physics-based method outperformed the purely data-driven method and satisfy the physical constraints of the gas flow system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20811;&#26381;&#35760;&#24405;&#35774;&#22791;&#21644;&#30005;&#26497;&#24067;&#23616;&#21464;&#21270;&#24102;&#26469;&#30340;&#38480;&#21046;&#65292;&#23398;&#20064;&#19981;&#21516;&#23454;&#39564;&#23460;&#30340;&#22810;&#26679;&#21270;EEG&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25991;&#20013;&#30740;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#38750;&#20405;&#20837;&#24615;&#21160;&#20316;&#24819;&#35937;&#65288;MI&#65289;EEG&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2306.13109</link><description>&lt;p&gt;
&#20351;&#29992;Transfer Learning&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20855;&#26377;&#24322;&#26500;&#30005;&#26497;&#37197;&#32622;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;EEG Decoding
&lt;/p&gt;
&lt;p&gt;
EEG Decoding for Datasets with Heterogenous Electrode Configurations using Transfer Learning Graph Neural Networks. (arXiv:2306.13109v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20811;&#26381;&#35760;&#24405;&#35774;&#22791;&#21644;&#30005;&#26497;&#24067;&#23616;&#21464;&#21270;&#24102;&#26469;&#30340;&#38480;&#21046;&#65292;&#23398;&#20064;&#19981;&#21516;&#23454;&#39564;&#23460;&#30340;&#22810;&#26679;&#21270;EEG&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25991;&#20013;&#30740;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#38750;&#20405;&#20837;&#24615;&#21160;&#20316;&#24819;&#35937;&#65288;MI&#65289;EEG&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BMI&#65289;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#21463;&#30410;&#21290;&#27973;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26080;&#27861;&#20174;&#21333;&#20010;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#12290;&#22312;&#19968;&#20010;&#23454;&#39564;&#23460;&#20869;&#29978;&#33267;&#36328;&#23454;&#39564;&#23460;&#21512;&#24182;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#35760;&#24405;&#35774;&#22791;&#21644;&#30005;&#26497;&#24067;&#23616;&#30340;&#21464;&#21270;&#20250;&#23548;&#33268;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#65292;&#25968;&#25454;&#32500;&#24230;&#30340;&#21464;&#21270;&#21644;&#25968;&#25454;&#32500;&#24230;&#26631;&#35782;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#23454;&#39564;&#23460;&#30340;&#22810;&#31181;&#19981;&#21516;&#21644;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#38750;&#20405;&#20837;&#24615;&#21160;&#20316;&#24819;&#35937;&#65288;MI&#65289;EEG&#35299;&#30721;&#20026;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20174;&#20855;&#26377;&#19981;&#21516;&#30005;&#26497;&#24067;&#23616;&#21644;&#19981;&#21516;&#25968;&#37327;&#30005;&#26497;&#30340;EEG&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Machine Interfacing (BMI) has greatly benefited from adopting machine learning methods for feature learning that require extensive data for training, which are often unavailable from a single dataset. Yet, it is difficult to combine data across labs or even data within the same lab collected over the years due to the variation in recording equipment and electrode layouts resulting in shifts in data distribution, changes in data dimensionality, and altered identity of data dimensions. Our objective is to overcome this limitation and learn from many different and diverse datasets across labs with different experimental protocols. To tackle the domain adaptation problem, we developed a novel machine learning framework combining graph neural networks (GNNs) and transfer learning methodologies for non-invasive Motor Imagery (MI) EEG decoding, as an example of BMI. Empirically, we focus on the challenges of learning from EEG data with different electrode layouts and varying numbers of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#21644;&#29305;&#24449;&#21270;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;IQST&#31561;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#22810;&#37325;&#20219;&#21153;&#20248;&#21270;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#21512;&#25104;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#39318;&#20010;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;&#22522;&#20934;&#27979;&#35797;&#26679;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.13105</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-task Learning for Radar Signal Characterisation. (arXiv:2306.13105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#21644;&#29305;&#24449;&#21270;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;IQST&#31561;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#22810;&#37325;&#20219;&#21153;&#20248;&#21270;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#21512;&#25104;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#39318;&#20010;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;&#22522;&#20934;&#27979;&#35797;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#27665;&#29992;&#36824;&#26159;&#20891;&#20107;&#24212;&#29992;&#65292;&#26080;&#32447;&#30005;&#20449;&#21495;&#35782;&#21035;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26410;&#30693;&#20449;&#21495;&#30340;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#35782;&#21035;&#26159;&#39057;&#35889;&#31649;&#29702;&#21644;&#30005;&#23376;&#25112;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#37324;&#65292;&#22823;&#37096;&#20998;&#30340;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#36827;&#34892;&#35843;&#21046;&#20998;&#31867;&#65292;&#32780;&#27809;&#33021;&#20805;&#20998;&#30740;&#31350;&#20449;&#21495;&#29305;&#24449;&#21270;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#21644;&#29305;&#24449;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; IQ Signal Transformer (IQST) &#21644;&#20854;&#20182;&#21442;&#32771;&#26550;&#26500;&#65292;&#20351;&#24471;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#22810;&#37325;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; MTL &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#39318;&#27425;&#30340;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;&#22522;&#20934;&#27979;&#35797;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio signal recognition is a crucial task in both civilian and military applications, as accurate and timely identification of unknown signals is an essential part of spectrum management and electronic warfare. The majority of research in this field has focused on applying deep learning for modulation classification, leaving the task of signal characterisation as an understudied area. This paper addresses this gap by presenting an approach for tackling radar signal classification and characterisation as a multi-task learning (MTL) problem. We propose the IQ Signal Transformer (IQST) among several reference architectures that allow for simultaneous optimisation of multiple regression and classification tasks. We demonstrate the performance of our proposed MTL model on a synthetic radar dataset, while also providing a first-of-its-kind benchmark for radar signal characterisation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.13104</link><description>&lt;p&gt;
&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21069;&#21015;&#33146;&#22312;&#24674;&#22797;&#22833;&#21435;&#30340;&#24863;&#23448;&#21151;&#33021;&#21644;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#35774;&#22791;&#20135;&#29983;&#30340;&#24863;&#35273;&#36890;&#24120;&#20284;&#20046;&#19981;&#33258;&#28982;&#25110;&#25197;&#26354;&#12290;&#26893;&#20837;&#22120;&#30340;&#30830;&#20999;&#20301;&#32622;&#21644;&#20010;&#20307;&#24863;&#30693;&#30340;&#24046;&#24322;&#23548;&#33268;&#21050;&#28608;&#21709;&#24212;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#20351;&#20010;&#24615;&#21270;&#21050;&#28608;&#20248;&#21270;&#25104;&#20026;&#20851;&#38190;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#29992;&#20110;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#22122;&#22768;&#35266;&#23519;&#25968;&#25454;&#30340;&#24739;&#32773;&#19987;&#23646;&#21050;&#28608;&#21442;&#25968;&#65292;&#20294;&#23545;&#20110;&#39640;&#32500;&#21050;&#28608;&#19981;&#21487;&#34892;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#21050;&#28608;&#32534;&#30721;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#26377;&#20851;&#24739;&#32773;&#29305;&#23450;&#21464;&#21270;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21453;&#28436;&#23558;&#30005;&#21050;&#28608;&#26144;&#23556;&#21040;&#35270;&#35273;&#24863;&#30693;&#30340;&#21069;&#21521;&#27169;&#22411;&#65292;&#35757;&#32451;&#28145;&#24230;&#32534;&#30721;&#22120;&#32593;&#32476;&#20197;&#20026;&#20219;&#20309;&#20010;&#20307;&#24739;&#32773;&#20135;&#29983;&#26368;&#20339;&#21050;&#28608;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#36873;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#65292;&#25104;&#21151;&#20351;&#30693;&#35273;&#21050;&#28608;&#26356;&#21152;&#36924;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#26041;&#27861;&#22312;&#21160;&#29289;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I DMs&#65289;&#36827;&#34892;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#29616;&#23454;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#32771;&#34385;&#30001;&#20154;&#31867;&#21487;&#20197;&#20135;&#29983;&#30340;&#29616;&#23454;&#35823;&#24046;&#25152;&#26500;&#25104;&#30340;&#25915;&#20987;&#33539;&#22260;&#20197;&#30830;&#20445;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#30446;&#26631;&#20855;&#26377;&#36739;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13103</link><description>&lt;p&gt;
&#23545;&#25239;&#29616;&#23454;&#19990;&#30028;&#25915;&#20987;&#19979;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks. (arXiv:2306.13103v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I DMs&#65289;&#36827;&#34892;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#29616;&#23454;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#32771;&#34385;&#30001;&#20154;&#31867;&#21487;&#20197;&#20135;&#29983;&#30340;&#29616;&#23454;&#35823;&#24046;&#25152;&#26500;&#25104;&#30340;&#25915;&#20987;&#33539;&#22260;&#20197;&#30830;&#20445;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#30446;&#26631;&#20855;&#26377;&#36739;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#23637;&#31034;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#20294;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;T2I DM&#26159;&#21542;&#20855;&#26377;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#21464;&#21270;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#29616;&#23454;&#25915;&#20987;&#30340;T2I DM&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#19982;&#20197;&#24448;&#20851;&#27880;&#28041;&#21450;&#36755;&#20837;&#25991;&#26412;&#30340;&#34394;&#20551;&#20462;&#25913;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#30001;&#20154;&#31867;&#21487;&#20197;&#20135;&#29983;&#30340;&#29616;&#23454;&#35823;&#24046;&#65288;&#20363;&#22914;&#65292;&#25171;&#23383;&#38169;&#35823;&#12289;&#23383;&#31526;&#12289;&#35821;&#38899;&#65289;&#25152;&#26500;&#25104;&#30340;&#25915;&#20987;&#33539;&#22260;&#65292;&#20197;&#30830;&#20445;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#32771;&#34385;&#21040;&#29983;&#25104;&#36807;&#31243;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#25915;&#20987;&#30446;&#26631;&#20197;&#35823;&#23548;T2I DM&#12290;&#25105;&#20204;&#20197;&#40657;&#30418;&#26041;&#24335;&#36827;&#34892;&#25915;&#20987;&#65292;&#19981;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#30340;&#20219;&#20309;&#20449;&#24687;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25928;&#26524;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) diffusion models (DMs) have shown promise in generating high-quality images from textual descriptions. The real-world applications of these models require particular attention to their safety and fidelity, but this has not been sufficiently explored. One fundamental question is whether existing T2I DMs are robust against variations over input texts. To answer it, this work provides the first robustness evaluation of T2I DMs against real-world attacks. Unlike prior studies that focus on malicious attacks involving apocryphal alterations to the input texts, we consider an attack space spanned by realistic errors (e.g., typo, glyph, phonetic) that humans can make, to ensure semantic consistency. Given the inherent randomness of the generation process, we develop novel distribution-based attack objectives to mislead T2I DMs. We perform attacks in a black-box manner without any knowledge of the model. Extensive experiments demonstrate the effectiveness of our method for 
&lt;/p&gt;</description></item><item><title>MBrain&#26159;&#19968;&#31181;&#38024;&#23545;&#33041;&#20449;&#21495;&#30340;&#22810;&#36890;&#36947;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#39640;&#25104;&#26412;&#20020;&#24202;&#26631;&#31614;&#21644;&#19981;&#21516;&#27979;&#37327;&#26041;&#27861;&#20043;&#38388;&#20020;&#24202;&#27169;&#24335;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13102</link><description>&lt;p&gt;
MBrain&#65306;&#19968;&#31181;&#29992;&#20110;&#33041;&#20449;&#21495;&#30340;&#22810;&#36890;&#36947;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals. (arXiv:2306.13102v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13102
&lt;/p&gt;
&lt;p&gt;
MBrain&#26159;&#19968;&#31181;&#38024;&#23545;&#33041;&#20449;&#21495;&#30340;&#22810;&#36890;&#36947;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#39640;&#25104;&#26412;&#20020;&#24202;&#26631;&#31614;&#21644;&#19981;&#21516;&#27979;&#37327;&#26041;&#27861;&#20043;&#38388;&#20020;&#24202;&#27169;&#24335;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#26159;&#20102;&#35299;&#20154;&#31867;&#22823;&#33041;&#30340;&#29983;&#29702;&#27963;&#21160;&#21644;&#30142;&#30149;&#30340;&#37325;&#35201;&#23450;&#37327;&#25968;&#25454;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39640;&#25104;&#26412;&#30340;&#20020;&#24202;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#20405;&#20837;&#24335;&#65288;&#20363;&#22914;SEEG&#65289;&#21644;&#38750;&#20405;&#20837;&#24335;&#65288;&#20363;&#22914;EEG&#65289;&#26041;&#27861;&#27979;&#37327;&#30340;&#33041;&#20449;&#21495;&#30340;&#20020;&#24202;&#27169;&#24335;&#20043;&#38388;&#24040;&#22823;&#30340;&#24046;&#24322;&#23548;&#33268;&#32570;&#20047;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30740;&#31350;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;SEEG&#25110;EEG&#25968;&#25454;&#12290;&#30452;&#35266;&#22320;&#65292;&#30001;&#31070;&#32463;&#20803;&#30340;&#21457;&#23556;&#20135;&#29983;&#30340;&#33041;&#20449;&#21495;&#22312;&#20154;&#31867;&#22823;&#33041;&#20013;&#20256;&#36755;&#21040;&#19981;&#21516;&#30340;&#36830;&#25509;&#32467;&#26500;&#20043;&#38388;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBrain&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#36890;&#36947;&#65288;&#21363;&#30005;&#26497;&#30340;&#25509;&#35302;&#28857;&#65292;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#33041;&#21306;&#65289;&#20043;&#38388;&#30340;&#38544;&#24335;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20316;&#20026;&#32479;&#19968;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#33041;&#20449;&#21495;&#30340;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose MBrain to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;SEEG&#25968;&#25454;&#38598;&#36827;&#34892;&#30315;&#30187;&#27874;&#26816;&#27979;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#20998;&#26512;&#25193;&#25955;&#36335;&#24452;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.13101</link><description>&lt;p&gt;
BrainNet&#65306;&#22522;&#20110;&#20998;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;SEEG&#30315;&#30187;&#27874;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning. (arXiv:2306.13101v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;SEEG&#25968;&#25454;&#38598;&#36827;&#34892;&#30315;&#30187;&#27874;&#26816;&#27979;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#20998;&#26512;&#25193;&#25955;&#36335;&#24452;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#20840;&#29699;1-2&#65285;&#30340;&#20154;&#21475;&#12290;&#30315;&#30187;&#30340;&#35786;&#26029;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23545;&#30315;&#30187;&#27874;&#30340;&#35782;&#21035;&#65292;&#21363;&#24739;&#32773;&#22823;&#33041;&#20013;&#30340;&#26080;&#24207;&#30005;&#33041;&#27963;&#21160;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#30005;&#26497;&#38142;&#35760;&#24405;&#30382;&#23618;&#33041;&#30005;&#22270;(EEG)&#26816;&#27979;&#30315;&#30187;&#27874;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#31435;&#20307;&#30005;&#26497;&#33041;&#30005;&#22270;(SEEG)&#26041;&#27861;&#25552;&#20379;&#30340;&#31435;&#20307;&#20449;&#24687;&#27604;&#20256;&#32479;&#30340;EEG&#26356;&#31934;&#30830;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;SEEG&#25968;&#25454;&#38598;&#26469;&#26816;&#27979;&#30315;&#30187;&#27874;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;&#22312;&#25552;&#20379;&#26032;&#26426;&#20250;&#30340;&#21516;&#26102;&#65292;SEEG&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#30315;&#30187;&#27874;&#27963;&#21160;&#34987;&#35748;&#20026;&#22312;&#22823;&#33041;&#30340;&#19981;&#21516;&#21306;&#22495;&#38388;&#20256;&#25773;&#12290;&#36825;&#20123;&#20256;&#25773;&#36335;&#24452;&#65292;&#20063;&#31216;&#20026;&#30315;&#30187;&#21457;&#29983;&#32593;&#32476;&#65292;&#22312;&#30315;&#30187;&#25163;&#26415;&#30340;&#32972;&#26223;&#19979;&#34987;&#35748;&#20026;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is one of the most serious neurological diseases, affecting 1-2% of the world's population. The diagnosis of epilepsy depends heavily on the recognition of epileptic waves, i.e., disordered electrical brainwave activity in the patient's brain. Existing works have begun to employ machine learning models to detect epileptic waves via cortical electroencephalogram (EEG). However, the recently developed stereoelectrocorticography (SEEG) method provides information in stereo that is more precise than conventional EEG, and has been broadly applied in clinical practice. Therefore, we propose the first data-driven study to detect epileptic waves in a real-world SEEG dataset. While offering new opportunities, SEEG also poses several challenges. In clinical practice, epileptic wave activities are considered to propagate between different regions in the brain. These propagation paths, also known as the epileptogenic network, are deemed to be a key factor in the context of epilepsy surger
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13089</link><description>&lt;p&gt;
GIMLET&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25351;&#20196;&#20998;&#23376;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#23454;&#39564;&#36896;&#25104;&#30340;&#26631;&#31614;&#19981;&#36275;&#38382;&#39064;&#23558;&#26159;&#20854;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#20998;&#23376;-&#25991;&#26412;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22788;&#29702;&#25351;&#20196;&#19981;&#36275;&#20197;&#21450;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIMLET&#65292;&#23427;&#32479;&#19968;&#20102;&#22270;&#24418;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#24191;&#20041;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#25193;&#23637;&#20197;&#32534;&#30721;&#22270;&#24418;&#32467;&#26500;&#21644;&#25351;&#20196;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22270;&#24418;&#32534;&#30721;&#27169;&#22359;&#12290;GIMLET&#36824;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#35299;&#32806;&#20102;&#22270;&#24418;&#30340;&#32534;&#30721;&#21644;&#20219;&#21153;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;&#36328;&#26032;&#20219;&#21153;&#30340;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#29992;&#25143;&#32456;&#31471;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#25152;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12859</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering. (arXiv:2306.12859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#29992;&#25143;&#32456;&#31471;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#25152;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20849;&#20139;&#35745;&#31639;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#22312;&#21442;&#19982;&#35774;&#22791;&#19978;&#26412;&#22320;&#25191;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#24182;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#26469;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#12290;&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#22312;&#19981;&#21516;&#29992;&#25143;&#32456;&#31471;&#19978;&#30340;&#38750;&#29420;&#31435;&#21644;&#30456;&#21516;&#20998;&#24067;&#25152;&#23548;&#33268;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#22686;&#24378;&#22411;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#23558;&#32858;&#31867;&#29615;&#22659;&#35270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23545;&#21442;&#25968;&#25628;&#32034;&#26041;&#21521;&#30340;&#35843;&#25972;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#32858;&#31867;&#21442;&#25968;&#20197;&#36798;&#21040;&#26368;&#20339;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning technology, which realizes the balance between data privacy protection and data sharing computing. To protect data privacy, feder-ated learning learns shared models by locally executing distributed training on participating devices and aggregating local models into global models. There is a problem in federated learning, that is, the negative impact caused by the non-independent and identical distribu-tion of data across different user terminals. In order to alleviate this problem, this paper pro-poses a strengthened federation aggregation method based on adaptive OPTICS clustering. Specifically, this method perceives the clustering environment as a Markov decision process, and models the adjustment process of parameter search direction, so as to find the best clus-tering parameters to achieve the best federated aggregation method. The core contribution of this paper is to propose an adaptive OPTICS clustering algorithm for federated
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12802</link><description>&lt;p&gt;
Otter-Knowledge&#65306;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21033;&#29992;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#25110;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#22914;&#39044;&#27979;&#33647;&#29289;&#21644;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#27169;&#24577;&#30340;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#24207;&#21015;&#25110;SMILES&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;7&#20010;&#20844;&#20849;&#26469;&#28304;&#30340;&#39044;&#22788;&#29702;&#21644;&#25972;&#21512;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;30M&#20010;&#19977;&#20803;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;Therapeutic Data Commons (TDC)&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#25253;&#21578;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.12703</link><description>&lt;p&gt;
OptIForest: &#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#20248;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35832;&#22810;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#35832;&#22914;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#12289;&#37329;&#34701;&#39118;&#38505;&#30417;&#25511;&#12289;&#20154;&#31867;&#20581;&#24247;&#30417;&#27979;&#31561;&#12290;&#26681;&#25454;&#38548;&#31163;&#26862;&#26519;&#26426;&#21046;&#25552;&#20986;&#30340;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20854;&#31616;&#27905;&#12289;&#26377;&#25928;&#12289;&#39640;&#25928;&#32780;&#22791;&#21463;&#38738;&#30544;&#65292;&#20363;&#22914;&#38024;&#23545;&#23454;&#38469;&#37096;&#32626;&#65292;iForest&#26159;&#26368;&#24120;&#29992;&#30340;&#26816;&#27979;&#22120;&#20043;&#19968;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#38548;&#31163;&#26862;&#26519;&#37319;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;&#20294;&#26694;&#26550;LSHiForest&#24050;&#32463;&#35777;&#26126;&#20102;&#22810;&#21449;&#38548;&#31163;&#26641;&#32467;&#26500;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23578;&#26080;&#29702;&#35770;&#24037;&#20316;&#22238;&#31572;&#20851;&#20110;&#38548;&#31163;&#26862;&#26519;&#30340;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26681;&#26412;&#21644;&#23454;&#36341;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#20309;&#31181;&#20998;&#25903;&#22240;&#23376;&#30340;&#38548;&#31163;&#26641;&#32467;&#26500;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#38548;&#31163;&#25928;&#29575;&#29702;&#35770;&#26469;&#35299;&#31572;&#35813;&#38382;&#39064;&#65292;&#36827;&#32780;&#30830;&#23450;&#20102;&#19968;&#20010;&#38548;&#31163;&#26641;&#30340;&#26368;&#20248;&#20998;&#25903;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
&lt;/p&gt;</description></item><item><title>SIDDMs&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#24555;&#36895;&#25910;&#25947;&#19988;&#19968;&#23450;&#31243;&#24230;&#19978;&#20445;&#35777;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12511</link><description>&lt;p&gt;
&#21322;&#38544;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;SIDDMs&#65289;
&lt;/p&gt;
&lt;p&gt;
Semi-Implicit Denoising Diffusion Models (SIDDMs). (arXiv:2306.12511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12511
&lt;/p&gt;
&lt;p&gt;
SIDDMs&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#24555;&#36895;&#25910;&#25947;&#19988;&#19968;&#23450;&#31243;&#24230;&#19978;&#20445;&#35777;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#24555;&#36895;&#37319;&#26679;&#32780;&#19981;&#29306;&#29298;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914; DDPMS&#65289;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#65292;&#20016;&#23500;&#22810;&#26679;&#30340;&#26679;&#26412;&#65292;&#20294;&#21463;&#36845;&#20195;&#27493;&#39588;&#25968;&#37327;&#30340;&#22266;&#26377;&#38480;&#21046;&#32780;&#36895;&#24230;&#36739;&#24930;&#12290;Denoising Diffusion Generative Adversarial Networks (DDGAN) &#35797;&#22270;&#36890;&#36807;&#38598;&#25104; GAN &#27169;&#22411;&#29992;&#20110;&#25193;&#25955;&#36807;&#31243;&#30340;&#36739;&#22823;&#36339;&#36291;&#26469;&#35268;&#36991;&#27492;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;DDGAN &#36935;&#21040;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#38544;&#24335;&#27169;&#22411;&#26469;&#21305;&#37197;&#22024;&#26434;&#25968;&#25454;&#30340;&#36793;&#32536;&#20998;&#24067;&#21644;&#21069;&#21521;&#25193;&#25955;&#30340;&#26174;&#24335;&#26465;&#20214;&#20998;&#24067;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#21305;&#37197;&#32852;&#21512;&#21435;&#22122;&#20998;&#24067;&#12290;&#19982; DDPMS &#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21322;&#38544;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;SIDDMs&#65289;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#22810;&#39033;&#36164;&#20135;&#27874;&#21160;&#29575;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#21450;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#26368;&#20248;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.12446</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#20803;&#25968;&#25454;&#19979;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Comparing deep learning models for volatility prediction using multivariate data. (arXiv:2306.12446v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#22810;&#39033;&#36164;&#20135;&#27874;&#21160;&#29575;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#21450;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#26368;&#20248;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20351;&#29992;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#27874;&#21160;&#29575;&#30340;&#25928;&#26524;&#65292;&#20174;&#31616;&#21333;&#27973;&#23618;&#30340;&#27169;&#22411;&#21040;&#26356;&#28145;&#23618;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22825;&#30495;&#39044;&#27979;&#21644;&#32463;&#20856;GARCH&#27169;&#22411;&#30340;&#21464;&#21270;&#30456;&#27604;&#36739;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;GARCH&#27169;&#22411;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#39044;&#27979;&#20102;&#20116;&#31181;&#36164;&#20135;&#65288;&#21363;S\&amp;P500&#12289;&#32435;&#26031;&#36798;&#20811;100&#12289;&#40644;&#37329;&#12289;&#30333;&#38134;&#21644;&#30707;&#27833;&#65289;&#30340;&#27874;&#21160;&#29575;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#32988;&#36807;&#20102;&#32463;&#20856;&#30340;&#26041;&#27861;&#21644;&#27973;&#23618;&#32593;&#32476;&#12290;&#36825;&#20123;&#23454;&#39564;&#34987;&#37325;&#22797;&#36827;&#34892;&#65292;&#24182;&#19988;&#31454;&#20105;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#65292;&#22240;&#27492;&#40723;&#21169;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims at comparing several deep learning-based forecasters in the task of volatility prediction using multivariate data, proceeding from simpler or shallower to deeper and more complex models and compare them to the naive prediction and variations of classical GARCH models. Specifically, the volatility of five assets (i.e., S\&amp;P500, NASDAQ100, gold, silver, and oil) was predicted with the GARCH models, Multi-Layer Perceptrons, recurrent neural networks, Temporal Convolutional Networks, and the Temporal Fusion Transformer. In most cases the Temporal Fusion Transformer followed by variants of Temporal Convolutional Network outperformed classical approaches and shallow networks. These experiments were repeated, and the difference between competing models was shown to be statistically significant, therefore encouraging their use in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12212</link><description>&lt;p&gt;
MimiC&#65306;&#27169;&#20223;&#20013;&#24515;&#26356;&#26032;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#20219;&#21153;&#20998;&#21457;&#32473;&#23458;&#25143;&#31471;&#65292;&#21482;&#38656;&#35201;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#25910;&#38598;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#37096;&#32626;&#26102;&#65292;&#23458;&#25143;&#31471;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21487;&#33021;&#20250;&#26080;&#39044;&#35686;&#22320;&#36864;&#20986;&#20219;&#20309;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#36825;&#20250;&#38459;&#30861;&#32852;&#37030;&#23398;&#20064;&#36798;&#21040;&#25910;&#25947;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#21517;&#20026; MimiC &#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20013;&#24515;&#26381;&#21153;&#22120;&#20462;&#25913;&#20854;&#26356;&#26032;&#20197;&#27169;&#20223;&#32570;&#22833;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MimiC &#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;INT4&#31639;&#26415;&#35757;&#32451;Transformer&#30340;&#26041;&#27861;&#65292;&#24182;&#32454;&#33268;&#22320;&#20998;&#26512;&#20102;&#36716;&#25442;&#22120;&#20013;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#20026;&#23427;&#20204;&#25552;&#20986;&#20102;&#19987;&#29992;&#30340;&#37327;&#21270;&#22120;&#12290;&#31639;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11987</link><description>&lt;p&gt;
&#20351;&#29992;4&#20301;&#25972;&#25968;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;INT4&#31639;&#26415;&#35757;&#32451;Transformer&#30340;&#26041;&#27861;&#65292;&#24182;&#32454;&#33268;&#22320;&#20998;&#26512;&#20102;&#36716;&#25442;&#22120;&#20013;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#20026;&#23427;&#20204;&#25552;&#20986;&#20102;&#19987;&#29992;&#30340;&#37327;&#21270;&#22120;&#12290;&#31639;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26799;&#24230;&#37327;&#21270;&#20026;4&#20301;&#26377;&#26395;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;4&#20301;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#33258;&#23450;&#20041;&#25968;&#20540;&#26684;&#24335;&#65292;&#32780;&#36825;&#20123;&#26684;&#24335;&#19981;&#21463;&#24403;&#20195;&#30828;&#20214;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;INT4&#31639;&#26415;&#23454;&#29616;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;Transformer&#35757;&#32451;&#26041;&#27861;&#12290;&#20197;&#26497;&#20302;&#30340;INT4&#31934;&#24230;&#35757;&#32451;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;Transformer&#20013;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#20026;&#23427;&#20204;&#25552;&#20986;&#20102;&#19987;&#29992;&#37327;&#21270;&#22120;&#12290;&#23545;&#20110;&#21069;&#21521;&#20256;&#25773;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#31163;&#32676;&#20540;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21704;&#36798;&#29595;&#37327;&#21270;&#22120;&#26469;&#25233;&#21046;&#31163;&#32676;&#20540;&#12290;&#23545;&#20110;&#21453;&#21521;&#20256;&#25773;&#65292;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#25552;&#20986;&#20301;&#20998;&#35010;&#21644;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#25216;&#26415;&#26469;&#31934;&#30830;&#37327;&#21270;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;XGBoost&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21644;&#29615;&#22659;&#21361;&#23475;&#26333;&#38706;&#29305;&#24449;&#26159;&#24433;&#21709;&#22478;&#24066;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#20851;&#38190;&#65292;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#23454;&#39564;&#25552;&#20986;&#20102;&#22686;&#21152;&#32511;&#21270;&#31354;&#38388;&#12289;&#20943;&#23569;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#21487;&#20197;&#32531;&#35299;&#30284;&#30151;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.11847</link><description>&lt;p&gt;
&#22478;&#24066;&#20581;&#24247;&#32852;&#31995;&#30340;&#35299;&#30721;&#65306;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#22522;&#20110;&#38169;&#32508;&#22797;&#26434;&#22478;&#24066;&#29305;&#24449;&#30340;&#30284;&#30151;&#24739;&#30149;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features. (arXiv:2306.11847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;XGBoost&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21644;&#29615;&#22659;&#21361;&#23475;&#26333;&#38706;&#29305;&#24449;&#26159;&#24433;&#21709;&#22478;&#24066;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#20851;&#38190;&#65292;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#23454;&#39564;&#25552;&#20986;&#20102;&#22686;&#21152;&#32511;&#21270;&#31354;&#38388;&#12289;&#20943;&#23569;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#21487;&#20197;&#32531;&#35299;&#30284;&#30151;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21644;&#29615;&#22659;&#21361;&#23475;&#26333;&#38706;&#29305;&#24449;&#22312;&#20915;&#23450;&#31038;&#21306;&#32423;&#30284;&#30151;&#24739;&#30149;&#29575;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#21033;&#29992;&#32654;&#22269;&#20116;&#20010;&#22823;&#37117;&#24066;&#32479;&#35745;&#21306;&#30340;&#25968;&#25454;&#65306;&#33437;&#21152;&#21733;&#12289;&#36798;&#25289;&#26031;&#12289;&#20241;&#26031;&#25958;&#12289;&#27931;&#26441;&#30710;&#21644;&#32445;&#32422;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;XGBoost&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20102;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#24180;&#40836;&#12289;&#23569;&#25968;&#27665;&#26063;&#36523;&#20221;&#21644;&#20154;&#21475;&#23494;&#24230;&#26159;&#24433;&#21709;&#30284;&#30151;&#24739;&#30149;&#29575;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22478;&#24066;&#21457;&#23637;&#21644;&#35774;&#35745;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30284;&#30151;&#24739;&#30149;&#29575;&#65292;&#37325;&#28857;&#20851;&#27880;&#32511;&#22320;&#12289;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22686;&#21152;&#32511;&#21270;&#31354;&#38388;&#65292;&#20943;&#23569;&#24320;&#21457;&#21306;&#21644;&#24635;&#25490;&#25918;&#37327;&#21487;&#20197;&#32531;&#35299;&#30284;&#30151;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the interplay among social demographics, built environment characteristics, and environmental hazard exposure features in determining community level cancer prevalence. Utilizing data from five Metropolitan Statistical Areas in the United States: Chicago, Dallas, Houston, Los Angeles, and New York, the study implemented an XGBoost machine learning model to predict the extent of cancer prevalence and evaluate the importance of different features. Our model demonstrates reliable performance, with results indicating that age, minority status, and population density are among the most influential factors in cancer prevalence. We further explore urban development and design strategies that could mitigate cancer prevalence, focusing on green space, developed areas, and total emissions. Through a series of experimental evaluations based on causal inference, the results show that increasing green space and reducing developed areas and total emissions could alleviate can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;Transformer&#22312;&#28857;&#20113;&#39046;&#22495;&#20013;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#26041;&#26696;&#33021;&#22815;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#20960;&#20309;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20923;&#31574;&#30053;&#65292;&#22312;&#19981;&#24341;&#20837;&#20854;&#20182;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#22312;Transformer&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10798</link><description>&lt;p&gt;
ExpPoint-MAE&#65306;&#33258;&#30417;&#30563;&#28857;&#20113;Transformer&#30340;&#26356;&#22909;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers. (arXiv:2306.10798v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;Transformer&#22312;&#28857;&#20113;&#39046;&#22495;&#20013;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#26041;&#26696;&#33021;&#22815;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#20960;&#20309;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20923;&#31574;&#30053;&#65292;&#22312;&#19981;&#24341;&#20837;&#20854;&#20182;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#22312;Transformer&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;Transformer&#22312;&#28857;&#20113;&#39046;&#22495;&#20013;&#30340;&#29305;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Masked Autoencoding&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;Momentum Contrast&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;Transformer&#23398;&#20064;&#20851;&#27880;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#20960;&#20309;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24494;&#35843;&#36807;&#31243;&#21450;&#20854;&#23545;&#25152;&#23398;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20923;&#31574;&#30053;&#65292;&#23427;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#20854;&#20182;&#20462;&#25913;&#27169;&#22411;&#25110;&#35757;&#32451;&#27969;&#31243;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#25105;&#20204;&#30340;&#22522;&#32447;&#65292;&#24182;&#22312;Transformer&#27169;&#22411;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.03819</link><description>&lt;p&gt;
LEACE&#65306;&#38381;&#21512;&#24418;&#24335;&#20013;&#30340;&#23436;&#32654;&#32447;&#24615;&#27010;&#24565;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#20174;&#34920;&#24449;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#65292;&#38450;&#27490;&#20998;&#31867;&#22120;&#20351;&#29992;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#20363;&#22914;&#65292;&#21024;&#38500;&#27010;&#24565;&#20197;&#35266;&#23519;&#27169;&#22411;&#34892;&#20026;&#30340;&#21464;&#21270;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LEAst-squares&#27010;&#24565;&#25830;&#38500;&#65288;LEACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#22914;&#24191;&#27867;&#31867;&#21035;&#30340;&#33539;&#25968;&#25152;&#27979;&#37327;&#30340;&#37027;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#30340;&#26032;&#26041;&#27861;&#23558;LEACE&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25830;&#38500;&#27599;&#20010;&#23618;&#20013;&#30340;&#30446;&#26631;&#27010;&#24565;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#20449;&#24687;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/EleutherAI/concept-erasure&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#30340;DNN&#36793;&#32536;&#25512;&#29702;&#26694;&#26550;DVFO&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01811</link><description>&lt;p&gt;
DVFO&#65306;DNN&#36793;&#32536;&#25512;&#29702;&#30340;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference. (arXiv:2306.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30005;&#21387;&#12289;&#39057;&#29575;&#32553;&#25918;&#21644;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#30340;DNN&#36793;&#32536;&#25512;&#29702;&#26694;&#26550;DVFO&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#32536;&#35774;&#22791;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#19981;&#21516;&#29305;&#24615;&#65292;&#20248;&#21270;&#36793;&#32536;&#35774;&#22791;&#19978;DNN&#25512;&#29702;&#24615;&#33021;&#65288;&#22312;&#33021;&#28304;&#28040;&#32791;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#65289;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#38500;&#20102;&#21160;&#24577;&#30005;&#21387;&#39057;&#29575;&#32553;&#25918;&#65288;DVFS&#65289;&#25216;&#26415;&#65292;&#36793;&#32536;&#20113;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;DNN&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;&#23578;&#26410;&#23545;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21508;&#31181;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DVFO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;DVFS&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#25512;&#29702;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32852;&#21512;&#20248;&#21270;DVFS&#21644;&#21368;&#36733;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DVFO&#33258;&#21160;&#20849;&#21516;&#20248;&#21270;&#20102;1&#65289;&#36793;&#32536;&#35774;&#22791;&#30340;CPU&#12289;GPU&#21644;&#20869;&#23384;&#39057;&#29575;&#65292;&#20197;&#21450;2&#65289;&#35201;&#21368;&#36733;&#21040;&#20113;&#26381;&#21153;&#22120;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#24605;&#32771;&#21363;&#34892;&#21160;&#30340;&#24182;&#21457;&#26426;&#21046;&#21152;&#36895;DRL&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#36827;&#19968;&#27493;&#38477;&#20302;DNN&#27169;&#22411;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DVFO&#22312;&#25512;&#29702;&#20934;&#30830;&#24230;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to edge device resource constraints and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and inference latency on edge devices. In addition to the dynamic voltage frequency scaling (DVFS) technique, the edge-cloud architecture provides a collaborative approach to efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which jointly optimize DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) CPU, GPU and memory frequencies of edge devices, and 2) feature maps to be offloaded to cloud servers. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatialchannel attention mec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01110</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#20013;&#22122;&#22768;&#24433;&#21709;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31359;&#25140;&#24335;&#20581;&#24247;&#35774;&#22791;&#27491;&#22312;&#24341;&#39046;&#19968;&#31181;&#26032;&#26102;&#20195;&#30340;&#36830;&#32493;&#21644;&#38750;&#20405;&#20837;&#24615;&#36828;&#31243;&#30417;&#27979;&#12290;&#20854;&#20013;&#19968;&#39033;&#24212;&#29992;&#26159;&#29992;&#20110;&#28966;&#34385;&#26816;&#27979;&#12290;&#35768;&#22810;&#28966;&#34385;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#21457;&#29983;&#22312;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#20294;&#22122;&#22768;&#38459;&#27490;&#20102;&#36825;&#20123;&#36827;&#23637;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#24182;&#24320;&#21457;&#23545;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#26085;&#24120;&#29983;&#27963;&#20013;&#28151;&#20081;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#23581;&#35797;&#30740;&#31350;&#20808;&#21069;&#30340;&#26041;&#27861;&#20026;&#20309;&#22833;&#36133;&#65292;&#24182;&#20351;&#29992;&#21487;&#31359;&#25140;&#36127;&#33655;&#19982;&#24773;&#24863;&#26816;&#27979;&#65288;WESAD&#65289;&#25968;&#25454;&#38598;&#65292;&#22312;&#19977;&#31867;&#20998;&#31867;&#38382;&#39064;&#65288;&#22522;&#20934;&#20540; vs. &#21387;&#21147; vs. &#24841;&#24742;&#65289;&#20013;&#27604;&#36739;&#19981;&#21516;&#24378;&#24230;&#22122;&#22768;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#29983;&#29702;&#21796;&#37266;&#31561;&#32423;&#30340;&#24433;&#21709;&#12290;&#22312;&#24341;&#20837;&#22122;&#22768;&#20043;&#21069;&#65292;&#25105;&#20204;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;98.7&#65285;&#65292;&#32780;Schmidt 2018&#24180;&#30340;&#27169;&#22411;&#20165;&#36798;&#21040;&#20102;80.3&#65285;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#39044;&#27979;&#22312;&#38654;&#29615;&#22659;&#19979;&#39044;&#27979;&#22797;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#22810;&#20313;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#21482;&#26377;&#24494;&#23567;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2306.00575</link><description>&lt;p&gt;
&#22312;&#38654;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#22797;&#21046;&#30340;&#26102;&#38388;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments. (arXiv:2306.00575v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#39044;&#27979;&#22312;&#38654;&#29615;&#22659;&#19979;&#39044;&#27979;&#22797;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#22810;&#20313;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#21482;&#26377;&#24494;&#23567;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#38654;&#29615;&#22659;&#30340;&#22909;&#22788;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#20301;&#32622;&#38750;&#24120;&#37325;&#35201;&#12290;&#30450;&#30446;&#25110;&#21453;&#24212;&#24335;&#30340;&#25968;&#25454;&#22797;&#21046;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#38654;&#35745;&#31639;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#39044;&#27979;&#23458;&#25143;&#31471;&#20309;&#26102;&#20309;&#22320;&#36830;&#25509;&#12290;&#34429;&#28982;&#31354;&#38388;&#39044;&#27979;&#21463;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26102;&#38388;&#39044;&#27979;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23558;&#26102;&#38388;&#39044;&#27979;&#32435;&#20837;&#29616;&#26377;&#31354;&#38388;&#39044;&#27979;&#27169;&#22411;&#30340;&#20248;&#21183;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#22312;&#39044;&#27979;&#22797;&#21046;&#30340;&#32972;&#26223;&#19979;&#23545;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39034;&#24207;&#21644;&#21608;&#26399;&#24615;&#29992;&#25143;&#31227;&#21160;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#36712;&#36857;&#30340;&#38654;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20313;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;15&#65285;&#30340;&#38477;&#20302;&#65292;&#32780;&#25968;&#25454;&#21487;&#29992;&#24615;&#21482;&#26377;1&#65285;&#30340;&#24494;&#23567;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
To fully exploit the benefits of the fog environment, efficient management of data locality is crucial. Blind or reactive data replication falls short in harnessing the potential of fog computing, necessitating more advanced techniques for predicting where and when clients will connect. While spatial prediction has received considerable attention, temporal prediction remains understudied.  Our paper addresses this gap by examining the advantages of incorporating temporal prediction into existing spatial prediction models. We also provide a comprehensive analysis of spatio-temporal prediction models, such as Deep Neural Networks and Markov models, in the context of predictive replication. We propose a novel model using Holt-Winter's Exponential Smoothing for temporal prediction, leveraging sequential and periodical user movement patterns. In a fog network simulation with real user trajectories our model achieves a 15% reduction in excess data with a marginal 1% decrease in data availabi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19185</link><description>&lt;p&gt;
Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#19979;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24120;&#35265;&#31867;&#22411;&#30340;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#23558;&#22352;&#26631;&#26144;&#23556;&#21040;&#20449;&#21495;&#20540;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#20301;&#32622;&#21040;RGB&#20540;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21151;&#33021;&#34920;&#31034;&#36827;&#34892;&#36229;&#25311;&#21512;&#65292;&#28982;&#21518;&#32534;&#30721;&#32593;&#32476;&#26435;&#37325;&#26469;&#21387;&#32553;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23558;&#31934;&#24230;&#37327;&#21270;&#21040;&#20302;&#27604;&#29305;&#20250;&#22823;&#24133;&#38477;&#20302;&#37325;&#26500;&#36136;&#37327;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36807;&#24230;&#25311;&#21512;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#36817;&#20284;&#21518;&#39564;&#26435;&#37325;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#23427;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#20808;&#39564;&#26435;&#37325;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#37319;&#29992;&#20027;&#21160;&#23610;&#23544;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.03609</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#25552;&#31034;&#65306;&#25105;&#20204;&#30495;&#30340;&#20570;&#24471;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#20986;&#33394;&#30340;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#23398;&#20064;&#32773;&#65292;&#22312;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#38543;&#21518;&#36827;&#34892;&#20102;&#35797;&#22270;&#33258;&#21160;&#21270;&#20154;&#31867;&#25552;&#31034;&#30340;&#23581;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#38543;&#21518;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;K-shot&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#21487;&#20197;&#20248;&#20110;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#25552;&#31034;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#26356;&#22823;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33258;&#21160;&#25552;&#31034;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#31616;&#21333;&#30340;&#25163;&#21160;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#38500;&#20102;&#24494;&#35843;&#20043;&#22806;&#65292;&#25163;&#21160;&#25552;&#31034;&#24212;&#20316;&#20026;&#22522;&#32447;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.01430</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#20851;&#27880;&#21147;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#20998;&#31163;&#27133;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22522;&#30784;&#30495;&#20540;&#25110;&#30417;&#30563;&#12290;&#23427;&#30001;&#22522;&#20110;&#27133;&#20851;&#27880;&#30340;&#23545;&#25239;&#26465;&#20214;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#32452;&#25104;&#65292;&#20462;&#25913;&#20026;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#30721;&#20809;&#27969;&#65292;&#32780;&#19981;&#26159;&#23581;&#35797;&#37325;&#26500;&#22270;&#20687;&#26412;&#36523;&#12290;&#22312;&#32467;&#26524;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#20013;&#65292;&#19968;&#31181;&#27169;&#24335;&#65288;&#27969;&#65289;&#23558;&#39304;&#36865;&#32473;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#21333;&#29420;&#30340;&#28508;&#22312;&#20195;&#30721;&#65288;&#27133;&#65289;&#65292;&#32780;&#21478;&#19968;&#31181;&#27169;&#24335;&#65288;&#22270;&#20687;&#65289;&#23558;&#20915;&#23450;&#35299;&#30721;&#22120;&#20174;&#27133;&#29983;&#25104;&#31532;&#19968;&#20010;&#27169;&#24335;&#65288;&#27969;&#65289;&#12290;&#30001;&#20110;&#24815;&#24120;&#30340;&#33258;&#32534;&#30721;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#24182;&#19981;&#33021;&#38450;&#27490;&#25972;&#20010;&#27969;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#27133;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25439;&#22833;&#20462;&#25913;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20998;&#31163;&#30340;&#23545;&#25239;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
&lt;/p&gt;</description></item><item><title>InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.11435</link><description>&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65306;&#22270;&#20687;&#20462;&#22797;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11435
&lt;/p&gt;
&lt;p&gt;
InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65288;InDI&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#23427;&#36991;&#20813;&#20102;&#25152;&#35859;&#30340;&#8220;&#22343;&#20540;&#22238;&#24402;&#8221;&#25928;&#24212;&#65292;&#24182;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#23454;&#29616;&#65292;&#31867;&#20284;&#20110;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#20010;&#27424;&#23450;&#38382;&#39064;&#65292;&#22810;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#37117;&#21487;&#33021;&#26159;&#32473;&#23450;&#20302;&#36136;&#37327;&#36755;&#20837;&#30340;&#21487;&#34892;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#21333;&#27493;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#36890;&#24120;&#26159;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#32454;&#33410;&#21644;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SC-Block&#65292;&#19968;&#31181;&#38459;&#22622;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23450;&#20301;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35760;&#24405;&#24182;&#20351;&#29992;&#26368;&#36817;&#37051;&#25628;&#32034;&#24314;&#31435;&#20505;&#36873;&#38598;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.03132</link><description>&lt;p&gt;
SC-Block&#65306;&#23454;&#20307;&#35299;&#26512;&#27969;&#27700;&#32447;&#20013;&#30340;&#30417;&#30563;&#23545;&#27604;&#38459;&#22622;
&lt;/p&gt;
&lt;p&gt;
SC-Block: Supervised Contrastive Blocking within Entity Resolution Pipelines. (arXiv:2303.03132v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SC-Block&#65292;&#19968;&#31181;&#38459;&#22622;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23450;&#20301;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35760;&#24405;&#24182;&#20351;&#29992;&#26368;&#36817;&#37051;&#25628;&#32034;&#24314;&#31435;&#20505;&#36873;&#38598;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#20195;&#34920;&#21516;&#19968;&#23454;&#38469;&#23454;&#20307;&#30340;&#35760;&#24405;&#12290;&#20026;&#20102;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#65292;&#23454;&#20307;&#35299;&#26512;&#27969;&#27700;&#32447;&#30001;&#20004;&#37096;&#20998;&#26500;&#25104;&#65306;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#20302;&#30340;&#26041;&#27861;&#36873;&#25321;&#20505;&#36873;&#35760;&#24405;&#23545;&#30340;&#38459;&#22622;&#22120;&#21644;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#26041;&#27861;&#20174;&#35813;&#38598;&#21512;&#20013;&#35782;&#21035;&#21305;&#37197;&#23545;&#30340;&#21305;&#37197;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SC-Block&#65292;&#19968;&#31181;&#38459;&#22622;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23450;&#20301;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35760;&#24405;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#24314;&#31435;&#20505;&#36873;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;SC-Block&#19982;&#20843;&#31181;&#26368;&#20808;&#36827;&#30340;&#38459;&#22622;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#23558;SC-Block&#30340;&#35757;&#32451;&#26102;&#38388;&#19982;&#23454;&#20307;&#35299;&#26512;&#27969;&#27700;&#32447;&#30340;&#24635;&#36816;&#34892;&#26102;&#38388;&#30340;&#32553;&#30701;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#23558;SC-Block&#19982;&#22235;&#31181;&#21305;&#37197;&#26041;&#27861;&#32467;&#21512;&#25104;&#23436;&#25972;&#30340;&#27969;&#27700;&#32447;&#12290;&#20026;&#20102;&#27979;&#37327;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SC-Block&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of entity resolution is to identify records in multiple datasets that represent the same real-world entity. However, comparing all records across datasets can be computationally intensive, leading to long runtimes. To reduce these runtimes, entity resolution pipelines are constructed of two parts: a blocker that applies a computationally cheap method to select candidate record pairs, and a matcher that afterwards identifies matching pairs from this set using more expensive methods. This paper presents SC-Block, a blocking method that utilizes supervised contrastive learning for positioning records in the embedding space, and nearest neighbour search for candidate set building. We benchmark SC-Block against eight state-of-the-art blocking methods. In order to relate the training time of SC-Block to the reduction of the overall runtime of the entity resolution pipeline, we combine SC-Block with four matching methods into complete pipelines. For measuring the overall runtime, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;MC-SAG&#31639;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;token &#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.14428</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent under Markovian Sampling Schemes. (arXiv:2302.14428v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;MC-SAG&#31639;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;token &#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21464;&#24418;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#20013;&#20248;&#21270;&#22120;&#21482;&#33021;&#35775;&#38382;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#26696;&#28085;&#30422;&#20174;&#20855;&#26377;&#38543;&#26426;&#34892;&#36208;&#32773;&#65288;token&#31639;&#27861;&#65289;&#30340;&#20998;&#25955;&#20248;&#21270;&#21040;RL&#21644;&#22312;&#32447;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#23545;&#22522;&#30784;&#39532;&#23572;&#31185;&#22827;&#38142;&#21644;&#20248;&#21270;&#20989;&#25968;&#26045;&#21152;&#26368;&#19981;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#26679;&#26412;&#38543;&#26426;&#26799;&#24230;&#27839;&#30528;&#39532;&#23572;&#21487;&#22827;&#38142;&#36335;&#24452;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#19979;&#30028;&#65292;&#20351;&#20986;&#29616;&#20102;&#23545;&#22522;&#30784;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#21629;&#20013;&#26102;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#20043;&#21069;&#20316;&#21697;&#26356;&#28201;&#21644;&#30340;&#35268;&#24459;&#24615;&#20551;&#35774;&#19979;&#30340;Markov&#38142;SGD&#65288;MC-SGD&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MC-SAG&#65292;&#36825;&#26159;MC-SGD&#30340;&#19968;&#31181;&#24102;&#26377;&#26041;&#24046;&#32553;&#20943;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20165;&#21462;&#20915;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#30896;&#25758;&#26102;&#38388;&#65292;&#22240;&#27492;&#33719;&#24471;&#20102;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;token &#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variation of vanilla stochastic gradient descent where the optimizer only has access to a Markovian sampling scheme. These schemes encompass applications that range from decentralized optimization with a random walker (token algorithms), to RL and online system identification problems. We focus on obtaining rates of convergence under the least restrictive assumptions possible on the underlying Markov chain and on the functions optimized. We first unveil the theoretical lower bound for methods that sample stochastic gradients along the path of a Markov chain, making appear a dependency in the hitting time of the underlying Markov chain. We then study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior works. We finally introduce MC-SAG, an alternative to MC-SGD with variance reduction, that only depends on the hitting time of the Markov chain, therefore obtaining a communication-efficient token algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#23558;&#20004;&#20010;&#25968;&#25454;&#25209;&#27425;&#36716;&#25442;&#20026;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#24182;&#36827;&#34892;&#20998;&#24067;&#21305;&#37197;&#30340;&#26041;&#27861;&#26469;&#25554;&#34917;&#32570;&#22833;&#20540;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10363</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#25554;&#20540;&#30340;&#36716;&#25442;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transformed Distribution Matching for Missing Value Imputation. (arXiv:2302.10363v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#23558;&#20004;&#20010;&#25968;&#25454;&#25209;&#27425;&#36716;&#25442;&#20026;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#24182;&#36827;&#34892;&#20998;&#24067;&#21305;&#37197;&#30340;&#26041;&#27861;&#26469;&#25554;&#34917;&#32570;&#22833;&#20540;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#20540;&#25554;&#20540;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#32570;&#22833;&#20540;&#25554;&#20540;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#19981;&#23436;&#25972;&#26679;&#26412;&#25429;&#33719;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#30456;&#24212;&#22320;&#25554;&#34917;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#20219;&#24847;&#20004;&#20010;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#25209;&#27425;&#26469;&#33258;&#21516;&#19968;&#25968;&#25454;&#20998;&#24067;&#30340;&#20107;&#23454;&#65292;&#24314;&#35758;&#36890;&#36807;&#28145;&#24230;&#21487;&#36870;&#20989;&#25968;&#23558;&#23427;&#20204;&#21464;&#25442;&#20026;&#28508;&#22312;&#31354;&#38388;&#24182;&#36827;&#34892;&#20998;&#24067;&#21305;&#37197;&#26469;&#25554;&#34917;&#20004;&#20010;&#26679;&#26412;&#25209;&#27425;&#30340;&#32570;&#22833;&#20540;&#12290;&#20026;&#20102;&#21516;&#26102;&#23398;&#20064;&#21464;&#25442;&#21644;&#25554;&#20540;&#32570;&#22833;&#20540;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#21160;&#26426;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26377;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#38656;&#35843;&#25972;&#65292;&#26080;&#35770;&#32570;&#22833;&#20540;&#22914;&#20309;&#29983;&#25104;&#65292;&#37117;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25554;&#34917;&#32467;&#26524;&#12290;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#21644;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of imputing missing values in a dataset, which has important applications in many domains. The key to missing value imputation is to capture the data distribution with incomplete samples and impute the missing values accordingly. In this paper, by leveraging the fact that any two batches of data with missing values come from the same data distribution, we propose to impute the missing values of two batches of samples by transforming them into a latent space through deep invertible functions and matching them distributionally. To learn the transformations and impute the missing values simultaneously, a simple and well-motivated algorithm is proposed. Our algorithm has fewer hyperparameters to fine-tune and generates high-quality imputations regardless of how missing values are generated. Extensive experiments over a large number of datasets and competing benchmark algorithms show that our method achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08560</link><description>&lt;p&gt;
&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65306;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#32479;&#19968;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22238;&#25253;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#24615;&#32422;&#26463;&#19979;&#20248;&#21270;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#34920;&#31034;&#12290;&#36825;&#20010;&#34920;&#36848;&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65292;&#26159;&#26080;&#32422;&#26463;&#30340;&#24182;&#19988;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#27169;&#20223;&#23398;&#20064;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#34987;&#35270;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#32479;&#19968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#30740;&#31350;&#21644;&#35782;&#21035;&#36825;&#20123;&#26041;&#27861;&#25104;&#21151;&#30340;&#26500;&#25104;&#37096;&#20998;&#65292;&#24182;&#25581;&#31034;&#36825;&#20123;&#26041;&#27861;&#30340;&#20849;&#21516;&#32570;&#28857;&#21644;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20197;&#21069;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#19981;&#29616;&#23454;&#30340;&#35206;&#30422;&#29575;&#20551;&#35774;&#65292;&#24182;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#20195;&#29702;&#21644;&#19987;&#23478;&#35775;&#38382;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;f-&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22312;&#21516;&#26679;&#30340;&#21452;&#37325;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#23545;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the l
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#32447;&#24615;&#25193;&#23637;&#30340;&#21367;&#31215;&#26680;&#20197;&#29992;&#20110;&#22788;&#29702;&#24207;&#21015;&#21644;&#22270;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#24314;&#31435;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#27604;&#36215;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26356;&#24555;&#19988;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35299;&#37322;&#24615;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.03294</link><description>&lt;p&gt;
&#32447;&#24615;&#32553;&#25918;&#26680;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#23567;&#20998;&#23376;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35299;&#37322;&#24615;&#20248;&#21270;&#65292;&#32988;&#36807;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Linear-scaling kernels for protein sequences and small molecules outperform deep learning while providing uncertainty quantitation and improved interpretability. (arXiv:2302.03294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#32447;&#24615;&#25193;&#23637;&#30340;&#21367;&#31215;&#26680;&#20197;&#29992;&#20110;&#22788;&#29702;&#24207;&#21015;&#21644;&#22270;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#24314;&#31435;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#27604;&#36215;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26356;&#24555;&#19988;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35299;&#37322;&#24615;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#65292;&#20855;&#26377;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25913;&#36827;&#30340;&#35299;&#37322;&#24615;&#31561;&#20248;&#28857;&#65292;&#24182;&#24050;&#34987;&#24212;&#29992;&#20110;&#20998;&#26512;&#24207;&#21015;&#65288;&#22914;&#27688;&#22522;&#37240;&#21644;&#26680;&#33527;&#37240;&#24207;&#21015;&#65289;&#21644;&#22270;&#65288;&#22914;&#34920;&#31034;&#23567;&#20998;&#23376;&#30340;&#22270;&#65289;&#31561;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#36866;&#29992;&#20110;GP&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#21487;&#32447;&#24615;&#25193;&#23637;&#30340;&#21367;&#31215;&#26680;&#65292;&#26500;&#24314;&#20102;&#21517;&#20026;xGPR&#30340;&#24320;&#28304;Python&#24211;&#12290;&#25105;&#20204;&#23545;&#27604;&#20102;xGPR&#21644;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;20&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65288;&#21253;&#25324;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#65289;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;xGPR&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#24615;&#33021;&#20063;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#24207;&#21015;&#21644;&#22270;&#28155;&#21152;&#20102;&#26032;&#30340;&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process (GP) is a Bayesian model which provides several advantages for regression tasks in machine learning such as reliable quantitation of uncertainty and improved interpretability. Their adoption has been precluded by their excessive computational cost and by the difficulty in adapting them for analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g. ones representing small molecules). In this study, we develop efficient and scalable approaches for fitting GP models as well as fast convolution kernels which scale linearly with graph or sequence size. We implement these improvements by building an open-source Python library called xGPR. We compare the performance of xGPR with the reported performance of various deep learning models on 20 benchmarks, including small molecule, protein sequence and tabular data. We show that xGRP achieves highly competitive performance with much shorter training time. Furthermore, we also develop new kernels for sequence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#19981;&#21516;ially private optimization&#12290;</title><link>http://arxiv.org/abs/2302.01463</link><description>&lt;p&gt;
&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#29702;&#35770;&#21450;&#24212;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy. (arXiv:2302.01463v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#19981;&#21516;ially private optimization&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#30001;&#26368;&#36817;&#38024;&#23545;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#20248;&#21270;&#30340;&#23454;&#36341;&#26041;&#27861;&#25152;&#21551;&#21457;&#30340;&#65292;&#20363;&#22914;DP-FTRL&#65292;&#22312;&#26080;&#27861;&#20351;&#29992;&#38544;&#31169;&#25918;&#22823;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;&#27880;&#20837;&#38544;&#31169;&#22122;&#22768;&#65292;&#20351;&#22122;&#22768;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#21576;&#32447;&#24615;&#30456;&#20851;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#29615;&#22659;&#65292;&#31934;&#31616;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#38754;&#35980;&#65292;&#24182;&#20998;&#31163;&#20102;&#32447;&#24615;&#30456;&#20851;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#19979;&#38477;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#26080;&#35770;&#26159;&#20984;&#20989;&#25968;&#36824;&#26159;&#38750;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26126;&#26174;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#32039;&#65292;&#24182;&#31934;&#30830;&#22320;&#24674;&#22797;&#20102;&#22810;&#20010;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65288;&#21253;&#25324;&#21453;&#30456;&#20851;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#24320;&#21457;&#20102;&#26032;&#30340;&#65292;&#26377;&#25928;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#21516;ially private optimization&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20123;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#24490;&#29615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21033;&#29992;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#26368;&#36817;&#36830;&#32493;&#24180;&#20221;&#30340;&#31215;&#38634;&#31215;&#32047;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#20855;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00817</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26426;&#36733;&#38647;&#36798;&#26102;&#31354;&#38477;&#38634;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Recurrent Graph Convolutional Networks for Spatiotemporal Prediction of Snow Accumulation Using Airborne Radar. (arXiv:2302.00817v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#24490;&#29615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21033;&#29992;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#26368;&#36817;&#36830;&#32493;&#24180;&#20221;&#30340;&#31215;&#38634;&#31215;&#32047;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#20855;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#21644;&#20840;&#29699;&#22823;&#27668;&#28201;&#24230;&#30340;&#19978;&#21319;&#65292;&#20934;&#30830;&#39044;&#27979;&#21644;&#20272;&#35745;&#24180;&#24230;&#31215;&#38634;&#31215;&#32047;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#36733;&#38647;&#36798;&#20256;&#24863;&#22120;&#65292;&#22914;&#38634;&#22320;&#38647;&#36798;&#65292;&#33021;&#22815;&#27979;&#37327;&#22823;&#23610;&#24230;&#19978;&#30340;&#31215;&#38634;&#31215;&#32047;&#36895;&#29575;&#20998;&#24067;&#22270;&#65292;&#30417;&#27979;&#27668;&#20505;&#21464;&#21270;&#23545;&#26684;&#38517;&#20848;&#38477;&#27700;&#21644;&#24452;&#27969;&#30340;&#24433;&#21709;&#12290;&#35813;&#38647;&#36798;&#20256;&#24863;&#22120;&#20351;&#29992;&#36229;&#23485;&#24102;&#22686;&#24378;&#20102;&#22402;&#30452;&#20998;&#36776;&#29575;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#20869;&#37096;&#20869;&#37096;&#20912;&#23618;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#38024;&#23545;&#27492;&#38647;&#36798;&#25968;&#25454;&#20013;&#20808;&#21069;&#24180;&#20221;&#30340;&#31215;&#38634;&#31215;&#32047;&#37327;&#65292;&#39044;&#27979;&#36817;&#24180;&#26469;&#26576;&#20010;&#20301;&#32622;&#30340;&#31215;&#38634;&#31215;&#32047;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#31561;&#25928;&#30340;&#38750;&#20960;&#20309;&#21644;&#38750;&#26102;&#24577;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#20855;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate prediction and estimation of annual snow accumulation has grown in importance as we deal with the effects of climate change and the increase of global atmospheric temperatures. Airborne radar sensors, such as the Snow Radar, are able to measure accumulation rate patterns at a large-scale and monitor the effects of ongoing climate change on Greenland's precipitation and run-off. The Snow Radar's use of an ultra-wide bandwidth enables a fine vertical resolution that helps in capturing internal ice layers. Given the amount of snow accumulation in previous years using the radar data, in this paper, we propose a machine learning model based on recurrent graph convolutional networks to predict the snow accumulation in recent consecutive years at a certain location. We found that the model performs better and with more consistency than equivalent nongeometric and nontemporal models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;ISS&#65292;&#21487;&#25104;&#21151;&#24212;&#23545;12&#31181;&#26368;&#20808;&#36827;&#30340;PAP&#26041;&#27861;&#65292;&#24674;&#22797;&#20102;CIFAR-10&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#34920;&#29616;&#20248;&#20110;&#23545;&#25239;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.13838</link><description>&lt;p&gt;
&#22270;&#20687;&#21387;&#32553;&#32553;&#30701;&#36335;&#24452;&#65306;&#29992;&#21387;&#32553;&#25216;&#26415;&#25269;&#24481;PAP&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression. (arXiv:2301.13838v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;ISS&#65292;&#21487;&#25104;&#21151;&#24212;&#23545;12&#31181;&#26368;&#20808;&#36827;&#30340;PAP&#26041;&#27861;&#65292;&#24674;&#22797;&#20102;CIFAR-10&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#34920;&#29616;&#20248;&#20110;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#29289;&#25915;&#20987;(PAPs)&#22312;&#22270;&#20687;&#19978;&#22686;&#21152;&#24494;&#23567;&#30340;&#21464;&#21270;&#20197;&#38450;&#27490;&#20854;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#23454;&#29992;&#32780;&#26377;&#25928;&#30340;&#23545;&#25239;PAP&#26041;&#27861;&#24182;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#25216;&#26415;&#8212;&#8212;&#22270;&#20687;&#21387;&#32553;&#32553;&#30701;&#36335;&#24452;(ISS)&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;12&#31181;&#26368;&#20808;&#36827;&#30340;PAP&#26041;&#27861;&#65292;&#24674;&#22797;&#20102;CIFAR-10&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;ISS&#30340;&#34920;&#29616;&#20248;&#20110;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to $81.73\%$, surpassing the previous best preprocessing-based countermeasures by $37.97\%$ absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturba
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36816;&#29992;&#28151;&#21512;&#31934;&#24230;&#28014;&#28857;&#25968;&#36816;&#31639;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#31934;&#24230;&#20998;&#37197;&#65292;&#30456;&#27604;&#20302;&#31934;&#24230;&#28014;&#28857;&#25968;&#35757;&#32451;&#20013;&#32771;&#34385;&#30340;&#31934;&#24230;&#20998;&#37197;&#65292;&#20854;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#65292;&#21516;&#26102;&#20063;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#21367;&#31215;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2301.13464</link><description>&lt;p&gt;
&#28151;&#21512;&#31934;&#24230;&#28014;&#28857;&#25968;&#36816;&#31639;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training with Mixed-Precision Floating-Point Assignments. (arXiv:2301.13464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13464
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36816;&#29992;&#28151;&#21512;&#31934;&#24230;&#28014;&#28857;&#25968;&#36816;&#31639;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#31934;&#24230;&#20998;&#37197;&#65292;&#30456;&#27604;&#20302;&#31934;&#24230;&#28014;&#28857;&#25968;&#35757;&#32451;&#20013;&#32771;&#34385;&#30340;&#31934;&#24230;&#20998;&#37197;&#65292;&#20854;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#65292;&#21516;&#26102;&#20063;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;&#23558;&#25152;&#26377;&#24352;&#37327;&#20445;&#25345;&#22312;&#39640;&#31934;&#24230;&#65288;&#20363;&#22914;32&#20301;&#29978;&#33267;16&#20301;&#28014;&#28857;&#25968;&#65289;&#36890;&#24120;&#26159;&#28010;&#36153;&#30340;&#12290;&#20294;&#26159;&#65292;&#23558;&#25152;&#26377;&#24352;&#37327;&#37117;&#20445;&#25345;&#22312;&#20302;&#31934;&#24230;&#65288;&#20363;&#22914;8&#20301;&#28014;&#28857;&#25968;&#65289;&#20250;&#23548;&#33268;&#19981;&#21487;&#25509;&#21463;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#19968;&#20010;&#31934;&#24230;&#20998;&#37197; - &#21363;&#23558;&#25152;&#26377;&#24352;&#37327;&#65288;&#22312;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#65289;&#26144;&#23556;&#21040;&#31934;&#24230;&#32423;&#21035;&#65288;&#39640;&#25110;&#20302;&#65289; - &#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#23558;&#22823;&#22810;&#25968;&#24352;&#37327;&#20445;&#25345;&#22312;&#20302;&#31934;&#24230;&#24182;&#23548;&#33268;&#36275;&#22815;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment -- a mapping from all tensors (arising in training) to precision levels (high or low) -- that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classification tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides &gt; 2x memory reduction over a bas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13152</link><description>&lt;p&gt;
STEEL: &#22855;&#24322;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#24635;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#30446;&#26631;&#31574;&#30053;&#35825;&#23548;&#30340;&#20998;&#24067;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#20197;&#20415;&#36890;&#36807;&#21464;&#25442;&#27979;&#24230;&#20351;&#29992;&#25209;&#37327;&#25968;&#25454;&#26469;&#26657;&#20934;&#30446;&#26631;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32477;&#23545;&#36830;&#32493;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#31639;&#27861;&#20026;STEEL&#65306;SingulariTy-awarE rEinforcement Learning&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;&#20851;&#20110;&#31163;&#32447;&#35780;&#20272;&#30340;&#26032;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#65292;&#20197;&#21450;&#24102;&#26377;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31574;&#30053;&#23450;&#21521;&#35823;&#24046;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22855;&#24322;&#24773;&#20917;&#30340;&#23450;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#24418;&#26354;&#29575;&#25551;&#36848;&#31526;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#12289;&#34920;&#36798;&#24615;&#30340;&#25551;&#36848;&#31526;&#12290;</title><link>http://arxiv.org/abs/2301.12906</link><description>&lt;p&gt;
&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#30340;&#26354;&#29575;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Curvature Filtrations for Graph Generative Model Evaluation. (arXiv:2301.12906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#24418;&#26354;&#29575;&#25551;&#36848;&#31526;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#12289;&#34920;&#36798;&#24615;&#30340;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#20102;&#35299;&#20998;&#24067;&#32423;&#21035;&#19978;&#30340;&#22270;&#24418;&#24046;&#24322;&#65292;&#36825;&#38656;&#35201;&#33021;&#22815;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#22270;&#24418;&#30340;&#26174;&#33879;&#23646;&#24615;&#12290;&#26354;&#29575;&#26159;&#22270;&#24418;&#30340;&#19968;&#31181;&#23646;&#24615;&#65292;&#26368;&#36817;&#24320;&#22987;&#35777;&#26126;&#20854;&#22312;&#25551;&#36848;&#22270;&#24418;&#26041;&#38754;&#24456;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20854;&#34920;&#36798;&#24615;&#36136;&#12289;&#31283;&#23450;&#24615;&#21644;&#22312;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#23454;&#38469;&#25928;&#29992;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#22270;&#24418;&#26354;&#29575;&#25551;&#36848;&#31526;&#19982;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#12289;&#34920;&#36798;&#24615;&#30340;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property of graphs, and has recently started to prove useful in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.08918</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26377;&#31526;&#21495;&#20256;&#25773;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20256;&#36882;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21516;&#36136;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#36136;&#22270;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#21364;&#24456;&#24046;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#26696;&#12290;&#29305;&#21035;&#22320;&#65292;&#32763;&#36716;&#36793;&#30340;&#31526;&#21495;&#26159;&#22522;&#20110;&#22362;&#23454;&#29702;&#35770;&#22522;&#30784;&#30340;&#24182;&#19988;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20998;&#26512;&#20551;&#23450;&#20102;&#20108;&#20803;&#20998;&#31867;&#22330;&#26223;&#65292;&#22240;&#27492;&#21463;&#21040;&#24212;&#29992;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#23558;&#20197;&#21069;&#30340;&#29702;&#35299;&#25193;&#23637;&#21040;&#22810;&#31867;&#21035;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20004;&#20010;&#32570;&#28857;&#65306;&#65288;1&#65289;&#22810;&#36339;&#37051;&#23621;&#30340;&#31526;&#21495;&#21462;&#20915;&#20110;&#28040;&#24687;&#20256;&#36882;&#36335;&#24452;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36825;&#20063;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#65292;&#20914;&#31361;&#35777;&#25454;&#65289;&#65292;&#21487;&#33021;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#30340;&#22270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#32467;&#21512;&#20102;&#21407;&#26377;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20854;&#32570;&#28857;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#23427;&#26082;&#20855;&#26377;&#26368;&#20248;&#24615;&#33021;&#21448;&#35745;&#31639;&#19978;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2301.03785</link><description>&lt;p&gt;
&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;: &#36229;&#36234;$\beta-$&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification in Stochastic Bandits: Beyond $\beta-$optimality. (arXiv:2301.03785v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#23427;&#26082;&#20855;&#26377;&#26368;&#20248;&#24615;&#33021;&#21448;&#35745;&#31639;&#19978;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22266;&#23450;&#32622;&#20449;&#27700;&#24179;&#19979;&#65292;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#30340;&#19968;&#20010;&#26410;&#26366;&#35299;&#20915;&#30340;&#26041;&#38754;&#12290;&#35780;&#20272;&#36172;&#21338;&#31639;&#27861;&#30340;&#20004;&#20010;&#20851;&#38190;&#25351;&#26631;&#26159;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26368;&#20248;&#24615;&#65288;&#20363;&#22914;&#37319;&#26679;&#22797;&#26434;&#24230;&#65289;&#12290;&#22312;&#38543;&#26426;BAI&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#26377;&#20102;&#35774;&#35745;&#31639;&#27861;&#20197;&#23454;&#29616;&#26368;&#20248;&#24615;&#33021;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#35745;&#31639;&#19978;&#26114;&#36149;&#65288;&#20363;&#22914;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65289;&#12290;&#20063;&#23384;&#22312;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#19982;&#26368;&#20248;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#21487;&#35777;&#26126;&#30340;&#24046;&#36317;&#65288;&#20363;&#22914;&#65292;&#21069;&#20004;&#31181;&#26041;&#27861;&#20013;&#30340;$\beta$-&#26368;&#20248;&#26041;&#27861;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;BAI&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#19968;&#32452;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#20915;&#31574;&#35268;&#21017;&#23454;&#29616;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;&#23454;&#29616;&#36825;&#19968;&#28857;&#30340;&#20013;&#24515;&#27969;&#31243;&#26159;&#19968;&#20010;&#25353;&#39034;&#24207;&#20272;&#35745;&#26368;&#20339;&#20998;&#37197;&#30340;&#20363;&#31243;&#65292;&#30452;&#21040;&#36275;&#22815;&#20934;&#30830;&#22320;&#20272;&#35745;&#20026;&#27490;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#20272;&#35745;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates a hitherto unaddressed aspect of best arm identification (BAI) in stochastic multi-armed bandits in the fixed-confidence setting. Two key metrics for assessing bandit algorithms are computational efficiency and performance optimality (e.g., in sample complexity). In stochastic BAI literature, there have been advances in designing algorithms to achieve optimal performance, but they are generally computationally expensive to implement (e.g., optimization-based methods). There also exist approaches with high computational efficiency, but they have provable gaps to the optimal performance (e.g., the $\beta$-optimal approaches in top-two methods). This paper introduces a framework and an algorithm for BAI that achieves optimal performance with a computationally efficient set of decision rules. The central process that facilitates this is a routine for sequentially estimating the optimal allocations up to sufficient fidelity. Specifically, these estimates are accurate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#35299;&#20915;&#36328;&#21333;&#20803;&#26684;&#36328;&#20999;&#29255;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#21152;&#36895;&#31574;&#30053;&#37096;&#32626;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#20223;&#30495;&#35780;&#20272;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.03262</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#32593;&#32476;&#20999;&#29255;
&lt;/p&gt;
&lt;p&gt;
Network Slicing via Transfer Learning aided Distributed Deep Reinforcement Learning. (arXiv:2301.03262v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#35299;&#20915;&#36328;&#21333;&#20803;&#26684;&#36328;&#20999;&#29255;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#21152;&#36895;&#31574;&#30053;&#37096;&#32626;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#20223;&#30495;&#35780;&#20272;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#22788;&#29702;&#32593;&#32476;&#20999;&#29255;&#30340;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#36164;&#28304;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#37096;&#32626;DRL&#31574;&#30053;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#32454;&#32990;&#26465;&#20214;&#26159;&#24322;&#26500;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#22320;&#23558;&#36164;&#28304;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#32593;&#32476;&#20999;&#29255;&#65292;&#26469;&#21327;&#35843;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20998;&#26512;&#26469;&#35299;&#20915;&#36328;&#21333;&#20803;&#26684;&#36328;&#20999;&#29255;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#21327;&#35843;&#30340;MADRL&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#26469;&#26234;&#33021;&#22320;&#23558;&#36164;&#28304;&#20998;&#37197;&#32473;&#20999;&#29255;&#65292;&#24182;&#31649;&#29702;&#21333;&#20803;&#38388;&#24178;&#25200;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#30340;TL&#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#21516;&#30340;&#26412;&#22320;&#26234;&#33021;&#20307;&#20043;&#38388;&#20256;&#36755;&#23398;&#20064;&#21040;&#30340;DRL&#31574;&#30053;&#65292;&#21152;&#36895;&#31574;&#30053;&#37096;&#32626;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36716;&#31227;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20309;&#26102;&#20256;&#36755;&#21644;&#22914;&#20309;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20223;&#30495;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has been increasingly employed to handle the dynamic and complex resource management in network slicing. The deployment of DRL policies in real networks, however, is complicated by heterogeneous cell conditions. In this paper, we propose a novel transfer learning (TL) aided multi-agent deep reinforcement learning (MADRL) approach with inter-agent similarity analysis for inter-cell inter-slice resource partitioning. First, we design a coordinated MADRL method with information sharing to intelligently partition resource to slices and manage inter-cell interference. Second, we propose an integrated TL method to transfer the learned DRL policies among different local agents for accelerating the policy deployment. The method is composed of a new domain and task similarity measurement approach and a new knowledge transfer approach, which resolves the problem of from whom to transfer and how to transfer. We evaluated the proposed solution with extensive simul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Transkribus&#24179;&#21488;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21253;&#25324;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#30340;&#21512;&#20316;&#24615;&#36136;&#21644;&#25968;&#25454;&#20998;&#20139;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2212.11146</link><description>&lt;p&gt;
HTR&#27169;&#22411;&#35757;&#32451;&#30340;&#25361;&#25112;&#65306;&#12298;&#25968;&#23383;&#26102;&#20195;&#30340;&#26723;&#26696;&#20445;&#25252;&#35745;&#21010;&#12299;&#39033;&#30446;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique. (arXiv:2212.11146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Transkribus&#24179;&#21488;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21253;&#25324;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#30340;&#21512;&#20316;&#24615;&#36136;&#21644;&#25968;&#25454;&#20998;&#20139;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#20889;&#35782;&#21035;&#25216;&#26415;&#30340;&#20986;&#29616;&#20026;&#25991;&#21270;&#36951;&#20135;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#29616;&#22312;&#38656;&#35201;&#21453;&#24605;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#30340;&#32463;&#39564;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#33258;2018&#24180;&#20197;&#26469;&#20351;&#29992;Transkribus&#24179;&#21488;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#36716;&#24405;17&#19990;&#32426;&#30340;&#27861;&#35821;&#25163;&#20889;&#25991;&#26412;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;HTR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23558;&#25152;&#26377;&#36825;&#20123;&#20803;&#32032;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#65288;&#22914;Transkribus&#65289;&#30340;&#21512;&#20316;&#24615;&#36136;&#20197;&#21450;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20998;&#20139;&#20854;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwritten text recognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Deepjoin&#65292;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#31934;&#30830;&#39640;&#25928;&#22320;&#21457;&#29616;&#21487;&#36830;&#25509;&#34920;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#24335;&#26816;&#32034;&#65292;&#33021;&#22815;&#21516;&#26102;&#26381;&#21153;&#20110;&#31561;&#20540;&#21644;&#35821;&#20041;&#36830;&#25509;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#27169;&#26495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; Deepjoin &#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07588</link><description>&lt;p&gt;
DeepJoin: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36830;&#25509;&#34920;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepJoin: Joinable Table Discovery with Pre-trained Language Models. (arXiv:2212.07588v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Deepjoin&#65292;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#31934;&#30830;&#39640;&#25928;&#22320;&#21457;&#29616;&#21487;&#36830;&#25509;&#34920;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#24335;&#26816;&#32034;&#65292;&#33021;&#22815;&#21516;&#26102;&#26381;&#21153;&#20110;&#31561;&#20540;&#21644;&#35821;&#20041;&#36830;&#25509;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#27169;&#26495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; Deepjoin &#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36830;&#25509;&#34920;&#21457;&#29616;&#22312;&#25968;&#25454;&#28246;&#31649;&#29702;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#20016;&#23500;&#25968;&#25454;&#24182;&#24110;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#31561;&#20540;&#36830;&#25509;&#25110;&#35821;&#20041;&#36830;&#25509;&#65292;&#20294;&#35201;&#20040;&#26159;&#31934;&#30830;&#20294;&#36816;&#34892;&#26102;&#38388;&#38271;&#65292;&#35201;&#20040;&#32570;&#20047;&#20934;&#30830;&#24615;&#30340;&#36817;&#20284;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Deepjoin&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#31934;&#30830;&#39640;&#25928;&#22320;&#21457;&#29616;&#21487;&#36830;&#25509;&#34920;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#32034;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#26381;&#21153;&#20110;&#31561;&#20540;&#21644;&#35821;&#20041;&#36830;&#25509;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#19978;&#19979;&#25991;&#21270;&#36873;&#39033;&#65292;&#23558;&#21015;&#20869;&#23481;&#36716;&#25442;&#20026;&#25991;&#26412;&#24207;&#21015;&#12290;PLM&#35835;&#21462;&#24207;&#21015;&#65292;&#24182;&#36827;&#34892;&#24494;&#35843;&#65292;&#23558;&#21015;&#23884;&#20837;&#21521;&#37327;&#20013;&#65292;&#20351;&#24471;&#22914;&#26524;&#23427;&#20204;&#21487;&#20197;&#24418;&#25104;&#36830;&#25509;&#65292;&#21017;&#36825;&#20123;&#21521;&#37327;&#20250;&#26356;&#25509;&#36817;&#12290;Deepjoin&#36824;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#27169;&#26495;&#65292;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36830;&#25509;&#26465;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Deepjoin&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the usefulness in data enrichment for data analysis tasks, joinable table discovery has become an important operation in data lake management. Existing approaches target equi-joins, the most common way of combining tables for creating a unified view, or semantic joins, which tolerate misspellings and different formats to deliver more join results. They are either exact solutions whose running time is linear in the sizes of query column and target table repository or approximate solutions lacking precision. In this paper, we propose Deepjoin, a deep learning model for accurate and efficient joinable table discovery. Our solution is an embedding-based retrieval, which employs a pre-trained language model (PLM) and is designed as one framework serving both equi- and semantic joins. We propose a set of contextualization options to transform column contents to a text sequence. The PLM reads the sequence and is fine-tuned to embed columns to vectors such that columns are expected to b
&lt;/p&gt;</description></item><item><title>DLKoopman&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#23558;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#32534;&#30721;&#20026;&#32447;&#24615;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20248;&#21270;&#65292;&#36719;&#20214;&#21253;&#21253;&#25324;&#24179;&#22343;&#24402;&#19968;&#21270;&#32477;&#23545;&#35823;&#24046;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2211.08992</link><description>&lt;p&gt;
DLKoopman&#65306;&#19968;&#31181;&#24212;&#29992;&#20110;Koopman&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
DLKoopman: A deep learning software package for Koopman theory. (arXiv:2211.08992v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08992
&lt;/p&gt;
&lt;p&gt;
DLKoopman&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#23558;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#32534;&#30721;&#20026;&#32447;&#24615;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20248;&#21270;&#65292;&#36719;&#20214;&#21253;&#21253;&#25324;&#24179;&#22343;&#24402;&#19968;&#21270;&#32477;&#23545;&#35823;&#24046;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DLKoopman&#8212;&#8212;&#19968;&#31181;&#24212;&#29992;&#20110;Koopman&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#21253;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#32534;&#30721;&#65292;&#36827;&#32780;&#23558;&#20854;&#36716;&#21270;&#25104;&#32447;&#24615;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;DLKoopman&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20248;&#21270;&#12290;&#23427;&#21487;&#20197;&#22312;&#31995;&#32479;&#30340;&#21333;&#20010;&#29366;&#24577;&#65288;&#24555;&#29031;&#65289;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#39044;&#27979;&#20854;&#26410;&#30693;&#29366;&#24577;&#65292;&#20063;&#21487;&#20197;&#22312;&#31995;&#32479;&#36712;&#36857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#39044;&#27979;&#26032;&#30340;&#21021;&#22987;&#29366;&#24577;&#30340;&#26410;&#30693;&#36712;&#36857;&#12290;DLKoopman&#21487;&#36890;&#36807;Python&#36719;&#20214;&#21253;&#32034;&#24341;&#65288;PyPI&#65289;&#33719;&#21462;&#65292;&#24182;&#21253;&#25324;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#25945;&#31243;&#12290;&#35813;&#36719;&#20214;&#21253;&#30340;&#20854;&#20182;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#24615;&#33021;&#30340;&#26032;&#22411;&#25351;&#26631;&#8212;&#8212;&#24179;&#22343;&#24402;&#19968;&#21270;&#32477;&#23545;&#35823;&#24046;&#65292;&#20197;&#21450;&#19968;&#20010;&#21363;&#29992;&#22411;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DLKoopman -- a software package for Koopman theory that uses deep learning to learn an encoding of a nonlinear dynamical system into a linear space, while simultaneously learning the linear dynamics. While several previous efforts have either restricted the ability to learn encodings, or been bespoke efforts designed for specific systems, DLKoopman is a generalized tool that can be applied to data-driven learning and optimization of any dynamical system. It can either be trained on data from individual states (snapshots) of a system and used to predict its unknown states, or trained on data from trajectories of a system and used to predict unknown trajectories for new initial states. DLKoopman is available on the Python Package Index (PyPI) as 'dlkoopman', and includes extensive documentation and tutorials. Additional contributions of the package include a novel metric called Average Normalized Absolute Error for evaluating performance, and a ready-to-use hyperparameter sear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#26469;&#22788;&#29702;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.08604</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#19981;&#24179;&#34913;PU&#26631;&#31614;&#30340;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels. (arXiv:2211.08604v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#26469;&#22788;&#29702;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#22810;&#20154;&#22312;&#32447;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#65292;&#28216;&#25103;&#28857;&#21345;&#33021;&#22815;&#30452;&#25509;&#36716;&#25442;&#20026;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#25110;Klaytn&#31561;&#21152;&#23494;&#36135;&#24065;&#65292;&#22240;&#27492;play-to-earn&#65288;P2E&#65289;&#31995;&#32479;&#30340;&#20986;&#29616;&#20351;&#24471;&#28216;&#25103;&#29289;&#21697;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20215;&#20540;&#20132;&#25442;&#27604;&#20197;&#24448;&#26356;&#21152;&#39057;&#32321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#29609;&#23478;&#30340;&#28216;&#25103;&#34892;&#20026;&#12289;P2E&#20195;&#24065;&#20132;&#26131;&#27169;&#24335;&#65292;&#21516;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#22788;&#29702;&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#38469;&#30340;P2E MMORPGs&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of play-to-earn (P2E) systems in massively multiplayer online role-playing games (MMORPGs) has made in-game goods interchangeable with real-world values more than ever before. The goods in the P2E MMORPGs can be directly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytn via blockchain networks. Unlike traditional in-game goods, once they had been written to the blockchains, P2E goods cannot be restored by the game operation teams even with chargeback fraud such as payment fraud, cancellation, or refund. To tackle the problem, we propose a novel chargeback fraud prediction method, PU GNN, which leverages graph attention networks with PU loss to capture both the players' in-game behavior with P2E token transaction patterns. With the adoption of modified GraphSMOTE, the proposed model handles the imbalanced distribution of labels in chargeback fraud datasets. The conducted experiments on three real-world P2E MMORPG datasets demonstrate that PU GNN achi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21450;&#35821;&#38899;&#19982;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36890;&#36807;&#25429;&#33719;&#20854;&#26465;&#20214;&#20381;&#36182;&#24615;&#24471;&#20986;&#39044;&#27979;&#32467;&#26524;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#25233;&#37057;&#21644;&#20854;&#30456;&#20851;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2211.04924</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#22312;&#20351;&#29992;&#35821;&#38899;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#27979;&#25233;&#37057;&#21450;&#20854;&#30151;&#29366;&#30340;&#40065;&#26834;&#19988;&#26080;&#20559;&#39044;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data. (arXiv:2211.04924v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04924
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21450;&#35821;&#38899;&#19982;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36890;&#36807;&#25429;&#33719;&#20854;&#26465;&#20214;&#20381;&#36182;&#24615;&#24471;&#20986;&#39044;&#27979;&#32467;&#26524;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#25233;&#37057;&#21644;&#20854;&#30456;&#20851;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21644;&#35748;&#30693;&#20449;&#21495;&#39044;&#27979;&#37325;&#24230;&#25233;&#37057;&#38556;&#30861;&#65288;MDD&#65289;&#30340;&#23384;&#22312;&#26159;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;MDD&#30340;&#24322;&#36136;&#24615;&#20020;&#24202;&#29305;&#24449;&#24847;&#21619;&#30528;&#20219;&#20309;&#32473;&#23450;&#30340;&#35821;&#38899;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;/&#25110;&#35266;&#23519;&#21040;&#30340;&#35748;&#30693;&#27169;&#24335;&#37117;&#21487;&#33021;&#19982;&#25233;&#37057;&#30151;&#29366;&#30340;&#29420;&#29305;&#32452;&#21512;&#30456;&#20851;&#32852;&#12290;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#32570;&#20047;&#31283;&#20581;&#22320;&#24314;&#27169;&#36825;&#31181;&#24322;&#36136;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#21487;&#33021;&#26356;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#20123;&#32593;&#32476;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#25429;&#33719;&#38543;&#26426;&#21464;&#37327;&#30340;&#26465;&#20214;&#20381;&#36182;&#24615;&#65292;&#39640;&#25928;&#22320;&#25551;&#36848;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20010;&#26694;&#26550;&#30456;&#27604;&#20110;&#26631;&#20934;&#30340;&#21028;&#21035;&#24615;&#24314;&#27169;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#32467;&#26500;&#20013;&#34701;&#21512;&#19987;&#23478;&#24847;&#35265;&#65292;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#65292;&#20102;&#35299;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#33258;&#28982;&#22320;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#38899;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#27979;&#25233;&#37057;&#30151;&#21450;&#20854;&#30456;&#20851;&#30151;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#30001;&#26377;&#21644;&#27809;&#26377;MDD&#30340;&#20010;&#20307;&#37319;&#38598;&#30340;&#35821;&#38899;&#21644;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#19982;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30456;&#20114;&#20316;&#29992;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;MDD&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the presence of major depressive disorder (MDD) using behavioural and cognitive signals is a highly non-trivial task. The heterogeneous clinical profile of MDD means that any given speech, facial expression and/or observed cognitive pattern may be associated with a unique combination of depressive symptoms. Conventional discriminative machine learning models potentially lack the complexity to robustly model this heterogeneity. Bayesian networks, however, may instead be well-suited to such a scenario. These networks are probabilistic graphical models that efficiently describe the joint probability distribution over a set of random variables by explicitly capturing their conditional dependencies. This framework provides further advantages over standard discriminative modelling by offering the possibility to incorporate expert opinion in the graphical structure of the models, generating explainable model predictions, informing about the uncertainty of predictions, and naturally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#39640;&#24615;&#33021;&#20294;&#22797;&#26434;&#30340;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#39640;&#25928;&#30340;CNN&#27169;&#22411;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#20248;&#20110;&#20197;&#21069;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21442;&#25968;&#20197;&#21450;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.04772</link><description>&lt;p&gt;
&#22522;&#20110;Transformer-to-CNN&#30693;&#35782;&#33976;&#39311;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#38899;&#39057;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Efficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge Distillation. (arXiv:2211.04772v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#39640;&#24615;&#33021;&#20294;&#22797;&#26434;&#30340;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#39640;&#25928;&#30340;CNN&#27169;&#22411;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#20248;&#20110;&#20197;&#21069;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21442;&#25968;&#20197;&#21450;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35889;&#21464;&#25442;&#22120;&#27169;&#22411;&#32479;&#27835;&#30528;&#38899;&#39057;&#26631;&#35760;&#39046;&#22495;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#23427;&#20204;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#25193;&#23637;&#21644;&#21033;&#29992;&#20687;AudioSet&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#20294;&#26159;&#65292;&#19982;CNN&#30456;&#27604;&#65292;&#21464;&#25442;&#22120;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#35201;&#27714;&#26041;&#38754;&#35201;&#27714;&#26356;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;&#20294;&#22797;&#26434;&#30340;&#21464;&#25442;&#22120;&#30340;&#31163;&#32447;&#30693;&#35782;&#33976;&#39311;(KD)&#30340;&#39640;&#25928;CNN&#35757;&#32451;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26550;&#26500;&#21644;&#22522;&#20110;MobileNetV3&#30340;&#39640;&#25928;CNN&#35774;&#35745;&#23548;&#33268;&#20102;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#20197;&#21450;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#21516;&#22797;&#26434;&#24230;&#32423;&#21035;&#30340;&#27169;&#22411;&#65292;&#20174;&#20302;&#22797;&#26434;&#24230;&#27169;&#22411;&#21040;&#26032;&#30340;AudioSet .483 mAP&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/fschmid56/EfficientAT&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio Spectrogram Transformer models rule the field of Audio Tagging, outrunning previously dominating Convolutional Neural Networks (CNNs). Their superiority is based on the ability to scale up and exploit large-scale datasets such as AudioSet. However, Transformers are demanding in terms of model size and computational requirements compared to CNNs. We propose a training procedure for efficient CNNs based on offline Knowledge Distillation (KD) from high-performing yet complex transformers. The proposed training schema and the efficient CNN design based on MobileNetV3 results in models outperforming previous solutions in terms of parameter and computational efficiency and prediction performance. We provide models of different complexity levels, scaling from low-complexity models up to a new state-of-the-art performance of .483 mAP on AudioSet. Source Code available at: https://github.com/fschmid56/EfficientAT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.01595</link><description>&lt;p&gt;
&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Van Roy&#21450;&#20854;&#21512;&#20316;&#32773;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#24182;&#26126;&#30830;&#20102;&#24403;&#22312;&#35813;&#33539;&#24335;&#19978;&#24212;&#29992;Q&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#30001;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#20195;&#29702;&#35774;&#35745;&#30340;&#26631;&#20934;&#24212;&#26159;&#23547;&#25214;&#26576;&#20123;&#26465;&#20214;&#35268;&#24459;&#30340;&#33391;&#22909;&#36817;&#20284;&#12290;&#21463;&#32463;&#20856;&#38543;&#26426;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#24402;&#32467;&#20026;&#36882;&#24402;&#35745;&#31639;&#36817;&#20284;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20195;&#29702;&#35774;&#35745;&#26041;&#26696;&#65292;&#25105;&#20204;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the novel paradigm developed by Van Roy and coauthors for reinforcement learning in arbitrary non-Markovian environments, we propose a related formulation and explicitly pin down the error caused by non-Markovianity of observations when the Q-learning algorithm is applied on this formulation. Based on this observation, we propose that the criterion for agent design should be to seek good approximations for certain conditional laws. Inspired by classical stochastic control, we show that our problem reduces to that of recursive computation of approximate sufficient statistics. This leads to an autoencoder-based scheme for agent design which is then numerically tested on partially observed reinforcement learning environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2211.01407</link><description>&lt;p&gt;
&#20851;&#20110;&#30417;&#30563;&#20449;&#21495;&#30340;&#20449;&#24687;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#20391;&#37325;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#20016;&#23500;&#30340;&#27880;&#37322;&#65288;&#22914;&#36719;&#26631;&#31614;&#65289;&#27604;&#31232;&#30095;&#30340;&#27880;&#37322;&#65288;&#22914;&#30828;&#26631;&#31614;&#65289;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#38598;&#25104;&#26412;&#20063;&#26356;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#20110;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#33021;&#21147;&#22914;&#20309;&#21463;&#21040;&#26631;&#31614;&#25968;&#12289;&#31867;&#21035;&#12289;&#32500;&#24230;&#21644;&#22122;&#22768;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati
&lt;/p&gt;</description></item><item><title>PINO-CDE&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32806;&#21512;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29289;&#29702;-&#30693;&#24773;&#30340;&#31070;&#32463;&#31639;&#31526;&#29702;&#35770;&#65292;&#20351;&#29992;&#21333;&#20010;&#32593;&#32476;&#22788;&#29702;&#25152;&#26377;&#37327;&#30340;CDEs&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#24555;&#36895;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.00222</link><description>&lt;p&gt;
&#20351;&#29992;PINO-CDE&#27714;&#35299;&#32806;&#21512;&#24494;&#20998;&#26041;&#31243;&#32452;
&lt;/p&gt;
&lt;p&gt;
Solving Coupled Differential Equation Groups Using PINO-CDE. (arXiv:2210.00222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00222
&lt;/p&gt;
&lt;p&gt;
PINO-CDE&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32806;&#21512;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29289;&#29702;-&#30693;&#24773;&#30340;&#31070;&#32463;&#31639;&#31526;&#29702;&#35770;&#65292;&#20351;&#29992;&#21333;&#20010;&#32593;&#32476;&#22788;&#29702;&#25152;&#26377;&#37327;&#30340;CDEs&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#24555;&#36895;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#24494;&#20998;&#26041;&#31243;&#32452;&#26159;&#35768;&#22810;&#24037;&#31243;&#23398;&#31185;&#20013;&#30340;&#22522;&#26412;&#25968;&#23398;&#24037;&#20855;&#65292;&#29992;&#20110;&#24314;&#27169;&#21253;&#21547;&#22810;&#20010;&#29289;&#29702;&#37327;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#22312;&#35774;&#35745;&#38454;&#27573;&#65292;&#24037;&#31243;&#24072;&#19981;&#26029;&#35843;&#25972;&#32467;&#26500;&#21442;&#25968;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20852;&#36215;&#20026;&#36825;&#39033;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#40657;&#30418;&#27169;&#22411;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#36739;&#24046;&#65292;&#32780;&#21333;&#36755;&#20986;&#31639;&#31526;&#22238;&#24402;&#30340;&#20808;&#36827;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#37327;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PINO-CDE&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32806;&#21512;&#24494;&#20998;&#26041;&#31243;&#32452;(CDEs)&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#20010;&#26041;&#31243;&#24402;&#19968;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#22522;&#20110;&#29289;&#29702;-&#30693;&#24773;&#30340;&#31070;&#32463;&#31639;&#31526;(PINO)&#29702;&#35770;&#65292;PINO-CDE&#20351;&#29992;&#21333;&#20010;&#32593;&#32476;&#22788;&#29702;CDEs&#20013;&#30340;&#25152;&#26377;&#37327;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#25968;&#21313;&#20010;&#29978;&#33267;&#25968;&#30334;&#20010;&#32593;&#32476;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PINO-CDE&#22312;&#20445;&#25345;&#24555;&#36895;&#35745;&#31639;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;CDEs&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a fundamental mathmatical tool in many engineering disciplines, coupled differential equation groups are being widely used to model complex structures containing multiple physical quantities. Engineers constantly adjust structural parameters at the design stage, which requires a highly efficient solver. The rise of deep learning technologies has offered new perspectives on this task. Unfortunately, existing black-box models suffer from poor accuracy and robustness, while the advanced methodologies of single-output operator regression cannot deal with multiple quantities simultaneously. To address these challenges, we propose PINO-CDE, a deep learning framework for solving coupled differential equation groups (CDEs) along with an equation normalization algorithm for performance enhancing. Based on the theory of physics-informed neural operator (PINO), PINO-CDE uses a single network for all quantities in a CDEs, instead of training dozens, or even hundreds of networks as in the existi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpeedLimit&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#37327;&#21270;&#30340;Transformer&#27169;&#22411;&#20013;&#28155;&#21152;&#19978;&#38480;&#24310;&#36831;&#32422;&#26463;&#65292;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;Transformer&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12127</link><description>&lt;p&gt;
SpeedLimit&#65306;&#37327;&#21270;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeedLimit: Neural Architecture Search for Quantized Transformer Models. (arXiv:2209.12127v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpeedLimit&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#37327;&#21270;&#30340;Transformer&#27169;&#22411;&#20013;&#28155;&#21152;&#19978;&#38480;&#24310;&#36831;&#32422;&#26463;&#65292;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;Transformer&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#35832;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24230;&#36825;&#26679;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#20005;&#26684;&#32771;&#34385;&#25512;&#29702;&#24310;&#36831;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SpeedLimit&#65292;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#23427;&#22312;&#20445;&#25345;&#19978;&#38480;&#24310;&#36831;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;8&#20301;&#25972;&#25968;&#37327;&#21270;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#21644;&#24310;&#36831;&#20043;&#38388;&#23547;&#27714;&#26368;&#20339;&#24179;&#34913;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20026;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#21306;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#65292;&#23398;&#20064;&#36763;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#28436;&#21270;&#65292;&#26377;&#25928;&#25233;&#21046;&#32047;&#31215;&#35823;&#24046;&#65292;&#24182;&#25104;&#21151;&#20445;&#25345;Jacobi&#31215;&#20998;&#30340;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2208.14148</link><description>&lt;p&gt;
&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26469;&#33258;&#20998;&#21306;&#25968;&#25454;&#30340;&#36763;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-step neural network for learning the symplectic evolution from partitioned data. (arXiv:2208.14148v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#21306;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#65292;&#23398;&#20064;&#36763;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#28436;&#21270;&#65292;&#26377;&#25928;&#25233;&#21046;&#32047;&#31215;&#35823;&#24046;&#65292;&#24182;&#25104;&#21151;&#20445;&#25345;Jacobi&#31215;&#20998;&#30340;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#38656;&#35201;&#39044;&#27979;&#36763;&#26144;&#23556;&#29983;&#25104;&#30340;&#22352;&#26631;&#65288;q&#65289;&#21644;&#21160;&#37327;&#65288;p&#65289;&#21464;&#37327;&#12290;&#22522;&#20110;Chen&#65286;Tao&#65288;2021&#65289;&#30340;&#30740;&#31350;&#65292;&#36763;&#26144;&#23556;&#30001;&#29983;&#25104;&#20989;&#25968;&#34920;&#31034;&#12290;&#20026;&#20102;&#24310;&#38271;&#39044;&#27979;&#26102;&#38388;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#24207;&#21015;&#65288;q_i&#12289;p_i&#65289;&#20998;&#25104;&#20960;&#20010;&#21306;&#38388;&#65292;&#24182;&#29992;&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#65288;LSNN&#65289;&#26469;&#36924;&#36817;&#31532;&#19968;&#21306;&#38388;&#65288;&#21363;&#21021;&#22987;&#26465;&#20214;&#65289;&#21644;&#20854;&#20313;&#21508;&#20010;&#21306;&#38388;&#20043;&#38388;&#30340;&#29983;&#25104;&#20989;&#25968;&#12290;&#36825;&#31181;&#20998;&#21306;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;LSNN&#22312;&#39044;&#27979;&#31995;&#32479;&#28436;&#21270;&#26102;&#33021;&#26377;&#25928;&#25233;&#21046;&#32047;&#31215;&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;LSNN&#23398;&#20064;25000&#24180;&#30340;2&#65306;3&#20849;&#25391;&#26607;&#20234;&#20271;&#24102;&#23545;&#35937;&#30340;&#36816;&#21160;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25105;&#20204;&#20808;&#21069;&#24037;&#20316;&#20013;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Li&#31561;&#65292;2022&#65289;&#22522;&#30784;&#19978;&#65292;&#26377;&#20004;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#65306;&#65288;1&#65289;Jacobi&#31215;&#20998;&#30340;&#23432;&#24658;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on learning Hamiltonian systems, which involves predicting the coordinate (q) and momentum (p) variables generated by a symplectic mapping. Based on Chen &amp; Tao (2021), the symplectic mapping is represented by a generating function. To extend the prediction time period, we develop a new learning scheme by splitting the time series (q_i, p_i) into several partitions. We then train a large-step neural network (LSNN) to approximate the generating function between the first partition (i.e. the initial condition) and each one of the remaining partitions. This partition approach makes our LSNN effectively suppress the accumulative error when predicting the system evolution. Then we train the LSNN to learn the motions of the 2:3 resonant Kuiper belt objects for a long time period of 25000 yr. The results show that there are two significant improvements over the neural network constructed in our previous work (Li et al. 2022): (1) the conservation of the Jacobi integral,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SPRT&#26694;&#26550;&#35774;&#35745;&#30340;BAI&#31639;&#27861;&#24182;&#36816;&#29992;&#20110;&#25351;&#25968;&#26063;&#36172;&#21338;&#26426;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#28176;&#36817;&#26368;&#20248;&#21644;$\delta-$PAC&#20445;&#35777;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2207.11158</link><description>&lt;p&gt;
&#22522;&#20110;SPRT&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;&#26368;&#20339;&#33218;&#36776;&#35782;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SPRT-based Efficient Best Arm Identification in Stochastic Bandits. (arXiv:2207.11158v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SPRT&#26694;&#26550;&#35774;&#35745;&#30340;BAI&#31639;&#27861;&#24182;&#36816;&#29992;&#20110;&#25351;&#25968;&#26063;&#36172;&#21338;&#26426;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#28176;&#36817;&#26368;&#20248;&#21644;$\delta-$PAC&#20445;&#35777;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#22312;&#22266;&#23450;&#32622;&#20449;&#24230;&#22330;&#26223;&#19979;&#30340;&#26368;&#20339;&#33218;&#36776;&#35782;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#24191;&#20041;&#25351;&#25968;&#26063;&#36172;&#21338;&#26426;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#25351;&#25968;&#26063;&#36172;&#21338;&#26426;&#31639;&#27861;&#38754;&#20020;&#35745;&#31639;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#20339;&#33218;&#36776;&#35782;&#38382;&#39064;&#34987;&#35270;&#20026;&#24207;&#36143;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#20219;&#21153;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#27979;&#35797;&#65292;&#36825;&#31181;&#27979;&#35797;&#24050;&#32463;&#35777;&#26126;&#23545;&#20110;&#24207;&#21015;&#27979;&#35797;&#26159;&#26377;&#25928;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;BAI&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#32463;&#20856;&#30340;&#24207;&#36143;&#27010;&#29575;&#27604;&#27979;&#35797;&#36827;&#34892;&#33218;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#26131;&#20110;&#20998;&#26512;&#25351;&#25968;&#26063;&#36172;&#21338;&#26426;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#29305;&#28857;&#65306;&#65288;1&#65289;&#23427;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#65292;&#65288;2&#65289;&#23427;&#20445;&#35777;&#26159;$\delta-$PAC&#30340;&#12290;&#29616;&#26377;&#30340;&#26377;&#25928;&#26041;&#27861;&#38598;&#20013;&#22312;&#39640;&#26031;&#26465;&#20214;&#19979;&#65292;&#24182;&#35201;&#27714;&#23545;&#34987;&#35748;&#20026;&#26159;&#26368;&#20339;&#33218;&#30340;&#33218;&#20351;&#29992;Thompson&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the best arm identification (BAI) problem in stochastic multi-armed bandits in the fixed confidence setting. The general class of the exponential family of bandits is considered. The existing algorithms for the exponential family of bandits face computational challenges. To mitigate these challenges, the BAI problem is viewed and analyzed as a sequential composite hypothesis testing task, and a framework is proposed that adopts the likelihood ratio-based tests known to be effective for sequential testing. Based on this test statistic, a BAI algorithm is designed that leverages the canonical sequential probability ratio tests for arm selection and is amenable to tractable analysis for the exponential family of bandits. This algorithm has two key features: (1) its sample complexity is asymptotically optimal, and (2) it is guaranteed to be $\delta-$PAC. Existing efficient approaches focus on the Gaussian setting and require Thompson sampling for the arm deemed the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#24615;&#22320;&#22686;&#21152;GAN&#29983;&#25104;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21521;&#25439;&#22833;&#20989;&#25968;&#28155;&#21152;&#27491;&#21017;&#21270;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#20445;&#25345;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22686;&#21152;&#20854;&#22810;&#26679;&#24615;&#65292;&#39564;&#35777;&#32467;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.01561</link><description>&lt;p&gt;
&#26377;&#36873;&#25321;&#22320;&#22686;&#21152;GAN&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Selectively increasing the diversity of GAN-generated samples. (arXiv:2207.01561v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#24615;&#22320;&#22686;&#21152;GAN&#29983;&#25104;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21521;&#25439;&#22833;&#20989;&#25968;&#28155;&#21152;&#27491;&#21017;&#21270;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#20445;&#25345;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22686;&#21152;&#20854;&#22810;&#26679;&#24615;&#65292;&#39564;&#35777;&#32467;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26159;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#21512;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#23494;&#20999;&#30456;&#20284;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#20294;&#26159;GAN&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#21463;&#21040;&#25152;&#35859;&#30340;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#30340;&#38480;&#21046;&#12290;&#29305;&#21035;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#30340;&#26159;&#26465;&#20214;GAN&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#24573;&#30053;&#36755;&#20837;&#22122;&#22768;&#21521;&#37327;&#65292;&#19987;&#27880;&#20110;&#26465;&#20214;&#20449;&#24687;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;&#36825;&#31181;&#38480;&#21046;&#65292;&#22686;&#21152;&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#24403;&#38656;&#35201;&#26679;&#26412;&#30456;&#20284;&#24615;&#26102;&#65292;&#23427;&#20204;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26377;&#36873;&#25321;&#22320;&#22686;&#21152;GAN&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21521;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#28155;&#21152;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#40723;&#21169;&#29983;&#25104;&#22120;&#20026;&#19982;&#22810;&#26679;&#21270;&#36755;&#20986;&#30456;&#20851;&#30340;&#36755;&#20837;&#21457;&#29616;&#26032;&#30340;&#25968;&#25454;&#27169;&#24335;&#65292;&#21516;&#26102;&#20026;&#20854;&#20313;&#20854;&#20313;&#30340;&#36755;&#20837;&#29983;&#25104;&#19968;&#33268;&#30340;&#26679;&#26412;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#19981;&#21516;&#26465;&#20214;&#36755;&#20837;&#29983;&#25104;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#27604;&#19982;&#30456;&#21516;&#26465;&#20214;&#36755;&#20837;&#29983;&#25104;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#21152;GAN&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#36136;&#37327;&#65292;&#22914;Frechet Inception&#36317;&#31163;(FID)&#21644;Inception&#20998;&#25968;(IS)&#25152;&#34913;&#37327;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) are powerful models able to synthesize data samples closely resembling the distribution of real data, yet the diversity of those generated samples is limited due to the so-called mode collapse phenomenon observed in GANs. Especially prone to mode collapse are conditional GANs, which tend to ignore the input noise vector and focus on the conditional information. Recent methods proposed to mitigate this limitation increase the diversity of generated samples, yet they reduce the performance of the models when similarity of samples is required. To address this shortcoming, we propose a novel method to selectively increase the diversity of GAN-generated samples. By adding a simple, yet effective regularization to the training loss function we encourage the generator to discover new data modes for inputs related to diverse outputs while generating consistent samples for the remaining ones. More precisely, we maximise the ratio of distances between gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#30340;&#26159;&#38543;&#26426;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#19988;&#38024;&#23545;&#26102;&#38388;&#38271;&#24230;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13606</link><description>&lt;p&gt;
&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation under Horizon Uncertainty. (arXiv:2206.13606v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#30340;&#26159;&#38543;&#26426;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#19988;&#38024;&#23545;&#26102;&#38388;&#38271;&#24230;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65306;&#20915;&#31574;&#32773;&#38656;&#35201;&#23558;&#26377;&#38480;&#36164;&#28304;&#20998;&#37197;&#32473;&#20197;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#30340;&#39034;&#24207;&#21040;&#36798;&#30340;&#35831;&#27714;&#65292;&#20197;&#26368;&#22823;&#21270;&#25910;&#30410;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#35831;&#27714;&#26159;&#20174;&#19968;&#20010;&#26410;&#30693;&#20110;&#20915;&#31574;&#32773;&#30340;&#20998;&#24067;&#20013;&#29420;&#31435;&#22320;&#25277;&#21462;&#30340;&#12290;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#21450;&#20854;&#29305;&#20363;&#22312;&#36807;&#21435;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20197;&#21069;&#30340;&#32467;&#26524;&#37117;&#33267;&#20851;&#37325;&#35201;&#19988;&#26222;&#36941;&#22320;&#20381;&#36182;&#20110;&#19968;&#20010;&#24378;&#20551;&#35774;&#65306;&#21363;&#20915;&#31574;&#32773;&#39044;&#20808;&#30693;&#36947;&#35831;&#27714;&#30340;&#24635;&#25968;&#65288;&#21363;&#26102;&#38388;&#38271;&#24230;&#65289;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#25910;&#20837;&#31649;&#29702;&#21644;&#22312;&#32447;&#24191;&#21578;&#65292;&#30001;&#20110;&#38656;&#27714;&#25110;&#29992;&#25143;&#27969;&#37327;&#24378;&#24230;&#30340;&#27874;&#21160;&#65292;&#35831;&#27714;&#30340;&#25968;&#37327;&#21487;&#33021;&#20250;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23545;&#26102;&#38388;&#38271;&#24230;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;&#19982;&#24050;&#30693;&#26102;&#38388;&#38271;&#24230;&#29615;&#22659;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#27809;&#26377;&#20219;&#20309;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#29420;&#31435;&#20110;&#26102;&#38388;&#38271;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#24658;&#23450;&#28176;&#36817;&#31454;&#20105;&#27604;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
We study stochastic online resource allocation: a decision maker needs to allocate limited resources to stochastically-generated sequentially-arriving requests in order to maximize reward. At each time step, requests are drawn independently from a distribution that is unknown to the decision maker. Online resource allocation and its special cases have been studied extensively in the past, but prior results crucially and universally rely on the strong assumption that the total number of requests (the horizon) is known to the decision maker in advance. In many applications, such as revenue management and online advertising, the number of requests can vary widely because of fluctuations in demand or user traffic intensity. In this work, we develop online algorithms that are robust to horizon uncertainty. In sharp contrast to the known-horizon setting, no algorithm can achieve even a constant asymptotic competitive ratio that is independent of the horizon uncertainty. We introduce a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#27969;&#30340;&#26080;&#20284;&#28982;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#33021;&#37327;&#30446;&#26631;&#65292;&#25903;&#25345;&#21322;&#33258;&#22238;&#24402;&#33021;&#37327;&#27969;&#31561;&#28789;&#27963;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#30456;&#23545;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#27969;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#36136;&#30097;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20197;&#26368;&#22823;&#20284;&#28982;&#20316;&#20026;&#30446;&#26631;&#25110;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2206.06672</link><description>&lt;p&gt;
&#21322;&#33258;&#22238;&#24402;&#33021;&#37327;&#27969;&#65306;&#25506;&#32034;&#27491;&#21017;&#21270;&#27969;&#30340;&#26080;&#20284;&#28982;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Autoregressive Energy Flows: Exploring Likelihood-Free Training of Normalizing Flows. (arXiv:2206.06672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#27969;&#30340;&#26080;&#20284;&#28982;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#33021;&#37327;&#30446;&#26631;&#65292;&#25903;&#25345;&#21322;&#33258;&#22238;&#24402;&#33021;&#37327;&#27969;&#31561;&#28789;&#27963;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#30456;&#23545;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#27969;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#36136;&#30097;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20197;&#26368;&#22823;&#20284;&#28982;&#20316;&#20026;&#30446;&#26631;&#25110;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#27491;&#21017;&#21270;&#27969;&#29983;&#25104;&#27169;&#22411;&#26102;&#65292;&#30001;&#20110;&#38656;&#35201;&#35745;&#31639;Jacobian&#34892;&#21015;&#24335;&#65292;&#22240;&#27492;&#20250;&#38754;&#20020;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27969;&#30340;&#26080;&#20284;&#28982;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#33021;&#37327;&#30446;&#26631;&#65292;&#19968;&#31181;&#22522;&#20110;&#36866;&#24403;&#24471;&#20998;&#35268;&#21017;&#30340;&#26367;&#20195;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#33021;&#37327;&#30446;&#26631;&#26159;&#19981;&#38656;&#35201;&#34892;&#21015;&#24335;&#30340;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#19981;&#23481;&#26131;&#19982;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#20860;&#23481;&#65292;&#21253;&#25324;&#21322;&#33258;&#22238;&#24402;&#33021;&#37327;&#27969;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26063;&#65292;&#21487;&#20197;&#25554;&#20540;&#20026;&#20840;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20043;&#38388;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#27969;&#65292;&#33021;&#37327;&#27969;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#26679;&#26412;&#36136;&#37327;&#12289;&#21518;&#39564;&#25512;&#26029;&#21644;&#29983;&#25104;&#36895;&#24230;&#31561;&#24615;&#33021;&#65307;&#35813;&#24615;&#33021;&#19982;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#30340;&#36136;&#37327;&#36890;&#24120;&#38750;&#24120;&#24046;&#30340;&#36136;&#37327;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#26368;&#22823;&#20284;&#28982;&#20316;&#20026;&#30446;&#26631;&#25110;&#25351;&#26631;&#30340;&#20351;&#29992;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#26377;&#21161;&#20110;&#23545;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#36827;&#34892;&#31185;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training normalizing flow generative models can be challenging due to the need to calculate computationally expensive determinants of Jacobians. This paper studies the likelihood-free training of flows and proposes the energy objective, an alternative sample-based loss based on proper scoring rules. The energy objective is determinant-free and supports flexible model architectures that are not easily compatible with maximum likelihood training, including semi-autoregressive energy flows, a novel model family that interpolates between fully autoregressive and non-autoregressive models. Energy flows feature competitive sample quality, posterior inference, and generation speed relative to likelihood-based flows; this performance is decorrelated from the quality of log-likelihood estimates, which are generally very poor. Our findings question the use of maximum likelihood as an objective or a metric, and contribute to a scientific study of its role in generative modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedTHE+&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#20351;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25269;&#24481;&#21508;&#31181;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.10920</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#38388;&#40065;&#26834;&#20010;&#24615;&#21270;&#26041;&#27861;(&#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Test-Time Robust Personalization for Federated Learning. (arXiv:2205.10920v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedTHE+&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#20351;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25269;&#24481;&#21508;&#31181;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#24335;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#35768;&#22810;&#23458;&#25143;&#31471;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#36866;&#24212;&#20110;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#33268;&#30340;&#26412;&#22320;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#19979;&#21462;&#24471;&#26377;&#30410;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#36824;&#38656;&#35201;&#26356;&#36827;&#19968;&#27493;&#65306;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#22312;&#28436;&#21270;&#30340;&#26412;&#22320;&#27979;&#35797;&#38598;&#19979;&#65292;&#24314;&#31435;&#33021;&#22815;&#25269;&#24481;&#21508;&#31181;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#35770;&#25991;&#22522;&#20110;&#27492;&#65292;&#25552;&#20986;&#20102;Federated Test-time Head Ensemble plus tuning(FedTHE+)&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35753;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25269;&#24481;&#21508;&#31181;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;CIFAR10&#21644;ImageNet&#19978;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;CNN&#65292;ResNet&#21644;Transformer&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FedTHE+&#65288;&#21450;&#20854;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#21464;&#31181;FedTHE&#65289;&#30456;&#23545;&#20110;&#20854;&#20182;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm where many clients collaboratively learn a shared global model with decentralized training data. Personalized FL additionally adapts the global model to different clients, achieving promising results on consistent local training and test distributions. However, for real-world personalized FL applications, it is crucial to go one step further: robustifying FL models under the evolving local test set during deployment, where various distribution shifts can arise. In this work, we identify the pitfalls of existing works under test-time distribution shifts and propose Federated Test-time Head Ensemble plus tuning(FedTHE+), which personalizes FL models with robustness to various test-time distribution shifts. We illustrate the advancement of FedTHE+ (and its computationally efficient variant FedTHE) over strong competitors, by training various neural architectures (CNN, ResNet, and Transformer) on CIFAR10 andImageNet with various test d
&lt;/p&gt;</description></item><item><title>DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.00701</link><description>&lt;p&gt;
DeepGraviLens&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00701
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#36879;&#38236;&#26159;&#30001;&#22823;&#36136;&#37327;&#29289;&#20307;&#20135;&#29983;&#30340;&#30456;&#23545;&#35770;&#25928;&#24212;&#65292;&#20250;&#24367;&#26354;&#20854;&#21608;&#22260;&#30340;&#26102;&#31354;&#12290;&#36825;&#26159;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#28145;&#20837;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#20801;&#35768;&#39564;&#35777;&#29702;&#35770;&#30456;&#23545;&#35770;&#32467;&#26524;&#24182;&#30740;&#31350;&#19968;&#20123;&#21542;&#21017;&#19981;&#21487;&#35265;&#30340;&#24494;&#24369;&#22825;&#20307;&#29289;&#20307;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24341;&#21147;&#36879;&#38236;&#29616;&#35937;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#20142;&#24230;&#21464;&#21270;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#36879;&#38236;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#32771;&#34385;&#22270;&#20687;&#32780;&#24573;&#30053;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35201;&#20040;&#22312;&#26368;&#22256;&#38590;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30456;&#23545;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; DeepGraviLens&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19968;&#20010;&#38750;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#21644;&#19977;&#20010;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#30340;&#26102;&#31354;&#25968;&#25454;&#12290;&#23427;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#24403;&#21069;&#30340; state-of-art &#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#32422; 19% &#21040; 43%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21463;&#38480;&#38750;&#20984;&#20248;&#21270;&#20013;&#30456;&#20851;&#25968;&#25454;&#37319;&#26679;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#26356;&#21152;&#28201;&#21644;&#30340;&#28151;&#21512;&#26465;&#20214;&#65292;&#23558;&#22797;&#26434;&#24230;&#20174;$\tilde{O}(\varepsilon^{-8})$&#25552;&#21319;&#33267;$\tilde{O}(\varepsilon^{-4})$&#12290;</title><link>http://arxiv.org/abs/2203.15797</link><description>&lt;p&gt;
&#21463;&#38480;&#38750;&#20984;&#20248;&#21270;&#20013;&#20855;&#26377;&#30456;&#20851;&#25968;&#25454;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data. (arXiv:2203.15797v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21463;&#38480;&#38750;&#20984;&#20248;&#21270;&#20013;&#30456;&#20851;&#25968;&#25454;&#37319;&#26679;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#26356;&#21152;&#28201;&#21644;&#30340;&#28151;&#21512;&#26465;&#20214;&#65292;&#23558;&#22797;&#26434;&#24230;&#20174;$\tilde{O}(\varepsilon^{-8})$&#25552;&#21319;&#33267;$\tilde{O}(\varepsilon^{-4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21463;&#38480;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#22312;&#19968;&#33324;&#30340;&#30456;&#20851;&#25968;&#25454;&#37319;&#26679;&#26041;&#26696;&#19979;&#30340;&#32463;&#20856;&#38543;&#26426;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21033;&#29992;Moreau&#21253;&#32476;&#21644;&#26799;&#24230;&#26144;&#23556;&#33539;&#25968;&#23454;&#29616;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#26368;&#22351;&#24773;&#20917;&#25910;&#25947;&#36895;&#29575;&#20026;$\tilde{O}(t^{-1/4})$&#65292;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(\varepsilon^{-4})$&#12290;&#20256;&#32479;&#30340;&#25910;&#25947;&#20445;&#35777;&#38656;&#35201;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#36827;&#34892;i.i.d.&#25968;&#25454;&#37319;&#26679;&#65292;&#32780;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#19968;&#31181;&#36739;&#28201;&#21644;&#30340;&#28151;&#21512;&#26465;&#20214;&#21363;&#21487;&#65292;&#35813;&#26465;&#20214;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#37319;&#26679;&#31639;&#27861;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#21463;&#38480;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#21644;&#30456;&#20851;&#25968;&#25454;&#30340;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(\varepsilon^{-8})$&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32463;&#36807;&#31616;&#21270;&#20998;&#26512;&#21518;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(\varepsilon^{-4})$&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#30456;&#20851;&#25968;&#25454;&#24773;&#20917;&#19979;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on analyzing the classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate of convergence $\tilde{O}(t^{-1/4})$ and complexity $\tilde{O}(\varepsilon^{-4})$ for achieving an $\varepsilon$-near stationary point in terms of the norm of the gradient of Moreau envelope and gradient mapping. While classical convergence guarantee requires i.i.d. data sampling from the target distribution, we only require a mild mixing condition of the conditional distribution, which holds for a wide class of Markov chain sampling algorithms. This improves the existing complexity for the constrained smooth nonconvex optimization with dependent data from $\tilde{O}(\varepsilon^{-8})$ to $\tilde{O}(\varepsilon^{-4})$ with a significantly simpler analysis. We illustrate the generality of our approach by deriving convergence results with dependent data for stochastic proximal gradient methods, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#24314;&#35758;&#26694;&#26550;&#35299;&#20915;&#20102;&#21333;&#31243;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#35797;&#22270;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2202.10939</link><description>&lt;p&gt;
&#21333;&#31243;&#25910;&#30410;&#31649;&#29702;&#20013;&#32771;&#34385;&#24314;&#35758;&#30340;&#20248;&#21270;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Single-Leg Revenue Management with Advice. (arXiv:2202.10939v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#24314;&#35758;&#26694;&#26550;&#35299;&#20915;&#20102;&#21333;&#31243;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#35797;&#22270;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#31243;&#25910;&#30410;&#31649;&#29702;&#26159;&#25910;&#30410;&#31649;&#29702;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#33322;&#31354;&#21644;&#37202;&#24215;&#34892;&#19994;&#26377;&#30528;&#24456;&#22823;&#30340;&#24433;&#21709;&#65306;&#32473;&#23450;$n$&#20010;&#36164;&#28304;&#21333;&#20301;&#65292;&#20363;&#22914;&#33322;&#29677;&#24231;&#20301;&#65292;&#24182;&#19988;&#39034;&#24207;&#21040;&#36798;&#30340;&#23458;&#25143;&#26681;&#25454;&#19981;&#21516;&#20215;&#26684;&#20998;&#27573;&#65292;&#25105;&#20204;&#38656;&#35201;&#30830;&#23450;&#36825;&#20010;&#36164;&#28304;&#30340;&#26368;&#20248;&#22312;&#32447;&#20998;&#37197;&#31574;&#30053;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#35774;&#35745;&#24403;&#39044;&#27979;&#21487;&#29992;&#26102;&#30340;&#31639;&#27861;&#65292;&#20294;&#26159;&#36825;&#31181;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#24182;&#19981;&#31283;&#20581;&#65292;&#25110;&#32773;&#26159;&#22312;&#32447;&#31639;&#27861;&#20197;&#20445;&#35777;&#26368;&#24046;&#24615;&#33021;&#65292;&#28982;&#32780;&#36825;&#26679;&#23454;&#36341;&#20013;&#21487;&#33021;&#36807;&#20110;&#20445;&#23432;&#65292;&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#24314;&#35758;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#20339;&#22320;&#25972;&#21512;&#26377;&#20851;&#26410;&#26469;&#30340;&#24314;&#35758;&#36827;&#20837;&#22312;&#32447;&#31639;&#27861;&#20013;&#65292;&#35797;&#22270;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#22686;&#21152;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#35299;&#20915;&#21333;&#31243;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#25429;&#33719;&#19968;&#33268;&#24615;&#65288;&#24314;&#35758;&#20934;&#30830;&#26102;&#30340;&#24615;&#33021;&#65289;&#21644;&#31454;&#20105;&#27604;&#29575;&#65288;&#24314;&#35758;&#19981;&#20934;&#30830;&#26102;&#30340;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#31181;&#33258;&#28982;&#31639;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#22312;&#27492;&#38382;&#39064;&#30340;&#23454;&#20363;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#24179;&#34913;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-leg revenue management is a foundational problem of revenue management that has been particularly impactful in the airline and hotel industry: Given $n$ units of a resource, e.g. flight seats, and a stream of sequentially-arriving customers segmented by fares, what is the optimal online policy for allocating the resource. Previous work focused on designing algorithms when forecasts are available, which are not robust to inaccuracies in the forecast, or online algorithms with worst-case performance guarantees, which can be too conservative in practice. In this work, we look at the single-leg revenue management problem through the lens of the algorithms-with-advice framework, which attempts to harness the increasing prediction accuracy of machine learning methods by optimally incorporating advice about the future into online algorithms. In particular, we characterize the Pareto frontier that captures the tradeoff between consistency (performance when advice is accurate) and compet
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#35270;&#39057;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#27979;&#35797;&#38598;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#26368;&#20248;&#21442;&#25968;&#19982;&#28508;&#22312;&#32534;&#30721;&#19968;&#36215;&#20256;&#36755;&#21040;&#25509;&#25910;&#31471;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#20219;&#20309;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.10302</link><description>&lt;p&gt;
&#23454;&#20363;&#33258;&#36866;&#24212;&#35270;&#39057;&#21387;&#32553;&#65306;&#36890;&#36807;&#23545;&#27979;&#35797;&#38598;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set. (arXiv:2111.10302v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#35270;&#39057;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#27979;&#35797;&#38598;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#26368;&#20248;&#21442;&#25968;&#19982;&#28508;&#22312;&#32534;&#30721;&#19968;&#36215;&#20256;&#36755;&#21040;&#25509;&#25910;&#31471;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#20219;&#20309;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#35270;&#39057;&#21387;&#32553;&#31639;&#27861;&#12290;&#22312;&#35201;&#20256;&#36755;&#30340;&#27599;&#20010;&#35270;&#39057;&#24207;&#21015;&#19978;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21387;&#32553;&#27169;&#22411;&#12290;&#26368;&#20248;&#21442;&#25968;&#19982;&#28508;&#22312;&#32534;&#30721;&#19968;&#36215;&#20256;&#36755;&#21040;&#25509;&#25910;&#31471;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#28151;&#21512;&#27169;&#22411;&#20808;&#39564;&#19979;&#29109;&#32534;&#30721;&#21442;&#25968;&#26356;&#26032;&#65292;&#25105;&#20204;&#30830;&#20445;&#32593;&#32476;&#21442;&#25968;&#21487;&#20197;&#39640;&#25928;&#22320;&#32534;&#30721;&#12290;&#36825;&#31181;&#23454;&#20363;&#33258;&#36866;&#24212;&#21387;&#32553;&#31639;&#27861;&#23545;&#20110;&#22522;&#27169;&#22411;&#30340;&#36873;&#25321;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#20219;&#20309;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;UVG&#12289;HEVC&#21644;Xiph&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#32534;&#35299;&#30721;&#22120;&#23558;&#27969;&#31354;&#38388;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;21%&#33267;27%&#30340;BD-rate&#33410;&#30465;&#65292;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;B&#24103;&#27169;&#22411;&#30340;17%&#33267;20%&#30340;BD-rate&#33410;&#30465;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#24494;&#35843;&#25913;&#21892;&#20102;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#23481;&#37327;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#20943;&#23569;&#32593;&#32476;&#22823;&#23567;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a video compression algorithm based on instance-adaptive learning. On each video sequence to be transmitted, we finetune a pretrained compression model. The optimal parameters are transmitted to the receiver along with the latent code. By entropy-coding the parameter updates under a suitable mixture model prior, we ensure that the network parameters can be encoded efficiently. This instance-adaptive compression algorithm is agnostic about the choice of base model and has the potential to improve any neural video codec. On UVG, HEVC, and Xiph datasets, our codec improves the performance of a scale-space flow model by between 21% and 27% BD-rate savings, and that of a state-of-the-art B-frame model by 17 to 20% BD-rate savings. We also demonstrate that instance-adaptive finetuning improves the robustness to domain shift. Finally, our approach reduces the capacity requirements of compression models. We show that it enables a competitive performance even after reducing the net
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2110.05367</link><description>&lt;p&gt;
&#22312;&#19981;&#20135;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEEP&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#27809;&#26377;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36879;&#36807;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#65292;GEEP&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#24182;&#22312;GLUE&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#36890;&#24120;&#24314;&#31435;&#19968;&#20010;&#23567;&#22411;&#30340;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#31532;&#20108;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#19988;&#38598;&#20013;&#20851;&#27880;&#65292;&#31532;&#20108;&#38454;&#27573;&#39044;&#35757;&#32451;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#24536;&#35760;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;GLUE&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#35777;&#22320;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20013;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;GEnder Equality Prompt (GEEP)&#65292;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#19988;&#36951;&#24536;&#36739;&#23569;&#12290; GEEP&#20250;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24615;&#21035;&#20013;&#24615;&#25968;&#25454;&#23398;&#20064;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;GEEP&#19981;&#20165;&#22312;&#24615;&#21035;&#20844;&#24179;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;SOTA&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;GLUE&#19978;&#36951;&#24536;&#36739;&#23569;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model's downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2106.16046</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65306;&#22522;&#20934;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.16046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#29305;&#24449;&#26159;&#26500;&#24314;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#65288;STCFP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#22312;&#20110;&#19981;&#21516;&#22330;&#26223;&#20013;&#19978;&#19979;&#25991;&#29305;&#24449;&#65288;&#20363;&#22914;&#22825;&#27668;&#12289;&#20551;&#26085;&#21644;&#20852;&#36259;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#26410;&#30693;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#30001;&#22823;&#35268;&#27169;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#25968;&#25454;&#12289;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22478;&#24066;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#23450;&#37327;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#24314;&#27169;&#25216;&#26415;&#30340;&#27867;&#21270;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#27969;&#34892;&#30740;&#31350;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24320;&#21457;&#20102;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#21644;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#25968;&#30334;&#31181;&#27169;&#22411;&#20197;&#25429;&#25417;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;STCFP&#20013;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22312;&#24102;&#26377;&#38544;&#34255;&#21464;&#37327;&#30340;&#22240;&#26524;&#22270;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#21453;&#21521;&#38376;&#25511;&#35843;&#25972;&#38598;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#38382;&#39064;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#26465;&#20214;&#21450;&#20854;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27492;&#31867;&#22270;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#27809;&#26377;&#26368;&#20248;&#38598;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2102.10324</link><description>&lt;p&gt;
&#24102;&#26377;&#38544;&#34255;&#21464;&#37327;&#30340;&#22240;&#26524;&#22270;&#27169;&#22411;&#20013;&#26368;&#20248;&#35843;&#25972;&#38598;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables. (arXiv:2102.10324v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.10324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22312;&#24102;&#26377;&#38544;&#34255;&#21464;&#37327;&#30340;&#22240;&#26524;&#22270;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#21453;&#21521;&#38376;&#25511;&#35843;&#25972;&#38598;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#38382;&#39064;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#26465;&#20214;&#21450;&#20854;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27492;&#31867;&#22270;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#27809;&#26377;&#26368;&#20248;&#38598;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#38544;&#34255;&#21644;&#26465;&#20214;&#21464;&#37327;&#30340;&#22270;&#24418;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#21453;&#21521;&#38376;&#25511;&#35843;&#25972;&#38598;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#26368;&#20248;&#23450;&#20041;&#20026;&#23454;&#29616;&#26368;&#23567;&#28176;&#36817;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26080;&#38544;&#34255;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#38598;&#12290;&#23545;&#20110;&#23384;&#22312;&#38544;&#34255;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#21487;&#33021;&#23384;&#22312;&#27809;&#26377;&#26368;&#20248;&#38598;&#30340;&#35774;&#32622;&#65292;&#24182;&#19988;&#30446;&#21069;&#20165;&#25512;&#23548;&#20986;&#20102;&#21463;&#38480;&#24212;&#29992;&#30340;&#20805;&#20998;&#22270;&#20248;&#21270;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#26368;&#20248;&#24615;&#34987;&#34920;&#24449;&#20026;&#26368;&#22823;&#21270;&#26576;&#31181;&#35843;&#25972;&#20449;&#24687;&#65292;&#36825;&#20801;&#35768;&#23548;&#20986;&#20851;&#20110;&#23384;&#22312;&#26368;&#20248;&#35843;&#25972;&#38598;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#20934;&#21017;&#20197;&#21450;&#19968;&#20010;&#23450;&#20041;&#21644;&#26500;&#36896;&#23427;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#24403;&#19988;&#20165;&#24403;&#23384;&#22312;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#19988;&#20854;&#35843;&#25972;&#20449;&#24687;&#39640;&#20110;&#31561;&#20110;Perkovi{\'c}&#31561;&#25552;&#20986;&#30340;Adjust-set&#26102;&#65292;&#26368;&#20248;&#38598;&#25165;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of selecting optimal backdoor adjustment sets to estimate causal effects in graphical models with hidden and conditioned variables is addressed. Previous work has defined optimality as achieving the smallest asymptotic estimation variance and derived an optimal set for the case without hidden variables. For the case with hidden variables there can be settings where no optimal set exists and currently only a sufficient graphical optimality criterion of limited applicability has been derived. In the present work optimality is characterized as maximizing a certain adjustment information which allows to derive a necessary and sufficient graphical criterion for the existence of an optimal adjustment set and a definition and algorithm to construct it. Further, the optimal set is valid if and only if a valid adjustment set exists and has higher (or equal) adjustment information than the Adjust-set proposed in Perkovi{\'c} et al. [Journal of Machine Learning Research, 18: 1--62, 20
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#35748;&#20026;&#36825;&#31867;&#39034;&#24207;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#24182;&#33021;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2101.09855</link><description>&lt;p&gt;
&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#30340;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Weak Signal Asymptotics for Sequentially Randomized Experiments. (arXiv:2101.09855v5 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.09855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#35748;&#20026;&#36825;&#31867;&#39034;&#24207;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#24182;&#33021;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#24369;&#20449;&#21495;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#65292;&#21253;&#25324;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#23454;&#39564;&#12290;&#22312;&#19968;&#20010;$n$&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35753;&#19981;&#21516;&#21160;&#20316;&#30340;&#24179;&#22343;&#22870;&#21169;&#38388;&#38553;&#25353;&#29031;$1/\sqrt{n}$&#30340;&#27604;&#20363;&#32553;&#25918;&#65292;&#20197;&#20445;&#25345;&#23398;&#20064;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;$n$&#30340;&#22686;&#38271;&#32780;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31867;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#20250;&#24369;&#25910;&#25947;&#21040;&#25193;&#25955;&#26497;&#38480;&#65292;&#20854;&#20013;&#65292;&#33218;&#36873;&#25321;&#30340;&#27010;&#29575;&#20250;&#38543;&#30528;&#29366;&#24577;&#30340;&#21464;&#21270;&#32780;&#25345;&#32493;&#21464;&#21270;&#65292;&#24182;&#22312;&#28385;&#36275;&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;&#25193;&#25955;&#26497;&#38480;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#31934;&#32454;&#30340;&#12289;&#29305;&#23450;&#20110;&#23454;&#20363;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#33719;&#24471;&#20851;&#20110;&#20960;&#31181;&#39034;&#24207;&#23454;&#39564;&#30340;&#21518;&#24724;&#21644;&#20449;&#24565;&#28436;&#21464;&#30340;&#22810;&#20010;&#35265;&#35299;&#65292;&#21253;&#25324;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;&#20294;&#19981;&#21253;&#25324;&#19981;&#28385;&#36275;&#25105;&#20204;&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;UCB&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#39034;&#24207;&#23454;&#39564;&#30340;&#34920;&#29616;&#37117;&#33021;&#22312;&#38271;&#26102;&#38388;&#20869;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multi-armed bandit problems. In an experiment with $n$ time steps, we let the mean reward gaps between actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the learning task as $n$ grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments -- adapted to this scaling regime and with arm selection probabilities that vary continuously with state -- converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics, and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not UCB, which does not satisfy our continuity assumption). We show that all sequential experiments wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#32593;&#32476;&#26469;&#31283;&#23450;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#31163;&#32447;&#23398;&#20064;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#30340;&#31639;&#27861;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#36798;&#21040;&#20102;&#25910;&#25947;&#21040;&#27491;&#21017;&#21270;TD&#22266;&#23450;&#28857;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2101.08862</link><description>&lt;p&gt;
&#36890;&#36807;&#30446;&#26631;&#32593;&#32476;&#25171;&#30772;&#33268;&#21629;&#19977;&#35282;&#65288;Reinforcement Learning&#65289;
&lt;/p&gt;
&lt;p&gt;
Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.08862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#32593;&#32476;&#26469;&#31283;&#23450;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#31163;&#32447;&#23398;&#20064;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#30340;&#31639;&#27861;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#36798;&#21040;&#20102;&#25910;&#25947;&#21040;&#27491;&#21017;&#21270;TD&#22266;&#23450;&#28857;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#33268;&#21629;&#19977;&#35282;&#8221;&#26159;&#25351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#12289;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#26102;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30446;&#26631;&#32593;&#32476;&#20316;&#20026;&#25171;&#30772;&#8220;&#33268;&#21629;&#19977;&#35282;&#8221;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#30446;&#26631;&#32593;&#32476;&#31283;&#23450;&#35757;&#32451;&#30340;&#24120;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#65292;&#23558;&#24120;&#29992;&#30340; Polyak &#24179;&#22343;&#39118;&#26684;&#26356;&#26032;&#19982;&#20004;&#20010;&#25237;&#24433;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#31639;&#27861;&#20013;&#24212;&#29992;&#30446;&#26631;&#32593;&#32476;&#21644;&#23725;&#27491;&#21017;&#21270;&#65292;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#21017;&#21270; TD &#22266;&#23450;&#28857;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#26159;&#31163;&#32447;&#23398;&#20064;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#30340;&#65292;&#28085;&#30422;&#25919;&#31574;&#35780;&#20272;&#21644;&#25511;&#21046;&#65292;&#20197;&#21450;&#25240;&#25187;&#21644;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#25910;&#25947;&#30340;&#32447;&#24615;$Q$&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#38750;&#38480;&#21046;&#24615;&#21644;&#21464;&#21270;&#34892;&#20026;&#31574;&#30053;&#19979;&#22343;&#25104;&#31435;&#65292;&#19981;&#38656;&#35201;&#21452;&#23618;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#26102;&#24207;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#20174;&#25506;&#32034;&#24615;&#28436;&#31034;&#20013;&#25512;&#26029;&#21307;&#29992;&#36229;&#22768;&#25195;&#25551;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2002.01240</link><description>&lt;p&gt;
&#21033;&#29992;&#27010;&#29575;&#26102;&#24207;&#25490;&#21517;&#23398;&#20064;&#21307;&#29992;&#36229;&#22768;&#25195;&#25551;&#26426;&#22120;&#20154;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Learning rewards for robotic ultrasound scanning using probabilistic temporal ranking. (arXiv:2002.01240v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#26102;&#24207;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#20174;&#25506;&#32034;&#24615;&#28436;&#31034;&#20013;&#25512;&#26029;&#21307;&#29992;&#36229;&#22768;&#25195;&#25551;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#26159;&#26426;&#22120;&#20154;&#35270;&#35273;&#20282;&#26381;&#21644;&#20027;&#21160;&#35270;&#28857;&#36873;&#25321;&#30340;&#19968;&#31181;&#25104;&#29087;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#20551;&#23450;&#24050;&#30693;&#36866;&#24403;&#30340;&#25104;&#26412;&#20989;&#25968;&#25110;&#30446;&#26631;&#29366;&#24577;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36870;&#38382;&#39064;&#65292;&#21363;&#20219;&#21153;&#30340;&#30446;&#26631;&#26410;&#30693;&#65292;&#38656;&#35201;&#20174;&#31034;&#25945;&#32773;&#25552;&#20379;&#30340;&#25506;&#32034;&#24615;&#31034;&#20363;&#28436;&#31034;&#20013;&#25512;&#26029;&#20986;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#22312;&#19979;&#28216;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#31574;&#30053;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25506;&#32034;&#24615;&#31034;&#33539;&#30340;&#24615;&#36136;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#22870;&#21169;&#25512;&#26029;&#31574;&#30053;&#19981;&#36866;&#29992;&#20110;&#36825;&#31867;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#36825;&#31867;&#38382;&#39064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22312;&#38656;&#35201;&#21457;&#29616;&#30340;&#20219;&#21153;&#20013;&#65292;&#20219;&#20309;&#28436;&#31034;&#30340;&#36830;&#32493;&#29366;&#24577;&#36234;&#26469;&#36234;&#21487;&#33021;&#19982;&#26356;&#39640;&#30340;&#22870;&#21169;&#20851;&#32852;&#65292;&#24182;&#20351;&#29992;&#35813;&#20551;&#35774;&#29983;&#25104;&#22522;&#20110;&#26102;&#38388;&#30340;&#20108;&#36827;&#21046;&#27604;&#36739;&#32467;&#26524;&#65292;&#25512;&#26029;&#20986;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informative path-planning is a well established approach to visual-servoing and active viewpoint selection in robotics, but typically assumes that a suitable cost function or goal state is known. This work considers the inverse problem, where the goal of the task is unknown, and a reward function needs to be inferred from exploratory example demonstrations provided by a demonstrator, for use in a downstream informative path-planning policy. Unfortunately, many existing reward inference strategies are unsuited to this class of problems, due to the exploratory nature of the demonstrations. In this paper, we propose an alternative approach to cope with the class of problems where these sub-optimal, exploratory demonstrations occur. We hypothesise that, in tasks which require discovery, successive states of any demonstration are progressively more likely to be associated with a higher reward, and use this hypothesis to generate time-based binary comparison outcomes and infer reward functio
&lt;/p&gt;</description></item></channel></rss>