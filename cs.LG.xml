<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00290</link><description>&lt;p&gt;
CO2&#27969;&#21160;&#27169;&#24335;&#30340;&#25512;&#26029;--&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00290
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#30899;&#25429;&#33719;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#25216;&#26415;&#22312;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#26007;&#20105;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#24314;&#31435;&#31283;&#20581;&#30340;&#30417;&#27979;&#21644;&#26816;&#27979;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#28508;&#22312;&#30340;&#22320;&#19979;CO2&#27844;&#28431;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#23384;&#20648;&#24211;&#23553;&#22581;&#30340;&#39044;&#20808;&#23384;&#22312;&#25110;&#35825;&#23548;&#30340;&#26029;&#23618;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#34429;&#28982;&#35832;&#22914;&#21382;&#21490;&#21305;&#37197;&#21644;CO2&#20648;&#23384;&#30340;&#26102;&#38388;&#24207;&#21015;&#22320;&#38663;&#30417;&#27979;&#31561;&#25216;&#26415;&#24050;&#25104;&#21151;&#29992;&#20110;&#36319;&#36394;&#22320;&#19979;CO2&#28183;&#28431;&#30340;&#28436;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#23545;CO2&#27874;&#21160;&#34892;&#20026;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#31995;&#32479;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#32435;&#20837;&#23545;&#20110;&#39118;&#38505;&#32531;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;CO2&#27874;&#21160;&#35825;&#21457;&#30340;&#21464;&#21270;&#24456;&#23567;&#19988;&#22320;&#38663;&#25968;&#25454;&#22122;&#22768;&#24456;&#22823;&#65307;&#65288;ii&#65289;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#65288;&#20363;&#22914;&#27844;&#28431;&#24341;&#36215;&#30340;&#65289;&#27969;&#21160;&#27169;&#24335;&#20043;&#38388;&#30340;&#21464;&#21270;&#24456;&#23567;&#65307;&#65288;iii&#65289;&#25511;&#21046;&#27969;&#21160;&#30340;&#20648;&#23618;&#29305;&#24615;&#24378;&#28872;&#24322;&#36136;&#19988;&#36890;&#24120;
&lt;/p&gt;
&lt;p&gt;
As the global deployment of carbon capture and sequestration (CCS) technology intensifies in the fight against climate change, it becomes increasingly imperative to establish robust monitoring and detection mechanisms for potential underground CO2 leakage, particularly through pre-existing or induced faults in the storage reservoir's seals. While techniques such as history matching and time-lapse seismic monitoring of CO2 storage have been used successfully in tracking the evolution of CO2 plumes in the subsurface, these methods lack principled approaches to characterize uncertainties related to the CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is essential for risk mitigation for the following reasons: (i) CO2 plume-induced changes are small and seismic data is noisy; (ii) changes between regular and irregular (e.g., caused by leakage) flow patterns are small; and (iii) the reservoir properties that control the flow are strongly heterogeneous and typically 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18348</link><description>&lt;p&gt;
&#24847;&#20041;&#34920;&#24449;&#26469;&#33258;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#25193;&#23637;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#26469;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#12290;&#36825;&#31181;&#31574;&#30053;&#26159;&#26080;&#25552;&#31034;&#30340;&#65292;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#24449;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#24067;&#30340;&#34920;&#24449;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20284;&#28982;&#20989;&#25968;&#20043;&#38388;&#30340;&#20195;&#25968;&#36816;&#31639;&#26469;&#24314;&#27169;&#38750;&#23545;&#31216;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#36923;&#36753;&#34164;&#28085;&#30340;&#26041;&#21521;&#65292;&#19978;&#20301;&#35789;/&#19979;&#20301;&#35789;&#20851;&#31995;&#65289;&#12290;&#36825;&#20123;&#24819;&#27861;&#22522;&#20110;&#35821;&#20041;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#24182;&#19982;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#26500;&#36896;&#30456;&#36830;&#25509;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#22823;&#22411;&#27169;&#22411;&#33719;&#24471;&#30340;&#34920;&#24449;&#19982;&#20154;&#31867;&#27880;&#37322;&#24456;&#22909;&#22320;&#19968;&#33268;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#21644;&#26080;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#34164;&#28085;&#21644;&#21253;&#21547;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17462</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#36816;&#21160;&#23450;&#24459;&#20174;2D&#26631;&#27880;&#20013;&#23398;&#20064;&#21333;&#30446;3D&#29289;&#20307;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#30340;&#36816;&#21160;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#20010;&#26657;&#20934;&#30456;&#26426;&#30340;&#21333;&#20010;&#22270;&#20687;&#20013;&#31934;&#30830;&#23450;&#20301;3D&#29289;&#20307;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#38544;&#21547;&#30340;&#31532;&#19977;&#20010;&#32500;&#24230;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#27492;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20316;&#20026;&#23398;&#20064;3D&#29289;&#20307;&#23450;&#20301;&#20272;&#35745;&#30340;&#19968;&#27493;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12819</link><description>&lt;p&gt;
&#24102;&#26377;&#23436;&#25972;&#24615;&#20445;&#35777;&#30340;&#39640;&#25928;&#35268;&#21010;&#30340;&#28151;&#21512;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#32570;&#20047;&#23436;&#25972;&#24615;&#20445;&#35777;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#23384;&#22312;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#23436;&#25972;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#26469;&#25191;&#34892;&#22810;&#23618;&#27425;&#65288;&#28151;&#21512;&#65289;&#25628;&#32034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#25972;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#12290;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#21644;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#30340;&#26368;&#20339;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#23436;&#25972;&#23376;&#30446;&#26631;&#25628;&#32034;&#19981;&#20165;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#65292;&#36824;&#21487;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11518</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23545;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#21103;&#26412;&#20132;&#20114;&#26469;&#23398;&#20064;&#12290;&#33258;&#25105;&#23545;&#25239;&#23545;&#20110;&#29983;&#25104;&#22823;&#37327;&#30340;&#23398;&#20064;&#25968;&#25454;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#30340;&#32570;&#28857;&#26159;&#35757;&#32451;&#21518;&#23398;&#20064;&#32773;&#23558;&#38754;&#23545;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#19982;&#36890;&#36807;&#19982;&#33258;&#36523;&#20132;&#20114;&#26102;&#25152;&#26399;&#26395;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#25130;&#28982;&#19981;&#21516;&#12290;&#23545;&#20110;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#33258;&#25105;&#23545;&#25239;&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#23545;&#20219;&#20309;&#35757;&#32451;&#21518;&#23545;&#25163;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20154;&#28216;&#25103;&#26469;&#35828;&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36817;&#20284;&#20998;&#35299;&#20026;&#19968;&#32452;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#65288;&#31216;&#20026;&#22810;&#30697;&#38453;&#28216;&#25103;&#65289;&#30340;&#28216;&#25103;&#20013;&#65292;&#20854;&#20013;&#20840;&#23616; $\epsilon$-&#32435;&#20160;&#22343;&#34913;&#22312;&#27599;&#20010;&#23376;&#28216;&#25103;&#20013;&#37117;&#19982;&#32435;&#20160;&#22343;&#34913;&#26377;&#26377;&#30028;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#23558;&#20135;&#29983;&#19968;&#20010;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#30830;&#23450;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08812</link><description>&lt;p&gt;
&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27169;&#24577;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#24182;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20869;&#22312;&#27169;&#24577;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#26263;&#21547;&#27874;&#21160;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#24403;&#21069;&#12289;&#24555;&#36895;&#28436;&#21464;&#21644;&#27874;&#21160;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;-&#38598;&#25104;&#33539;&#24335;&#65292;&#21363;VMD-LSTM-GARCH&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#31639;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#20026;K&#20010;&#23376;&#27169;&#24577;&#12290;&#38543;&#21518;&#65292;GARCH&#27169;&#22411;&#20174;&#36825;&#20123;&#23376;&#27169;&#24577;&#20013;&#25552;&#21462;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;LSTM&#30340;&#36755;&#20837;&#12290;&#27599;&#20010;&#23376;&#27169;&#24577;&#30340;&#25968;&#20540;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#39044;&#27979;&#23376;&#27169;&#24577;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#25152;&#26377;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes 
&lt;/p&gt;</description></item><item><title>GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.08235</link><description>&lt;p&gt;
GROOT: &#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08235
&lt;/p&gt;
&lt;p&gt;
GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#36981;&#24490;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35266;&#30475;&#35270;&#39057;&#20316;&#20026;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#25552;&#20379;&#20102;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#30446;&#26631;&#35268;&#33539;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28216;&#25103;&#35270;&#39057;&#20013;&#23398;&#20064;&#36825;&#31181;&#25351;&#20196;&#36981;&#24490;&#25511;&#21046;&#22120;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#33021;&#20135;&#29983;&#32467;&#26500;&#21270;&#30446;&#26631;&#31354;&#38388;&#30340;&#35270;&#39057;&#25351;&#20196;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#22240;&#26524;&#21464;&#21387;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;GROOT&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25552;&#20986;&#30340;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;GROOT&#36827;&#34892;&#20102;&#19982;&#24320;&#25918;&#19990;&#30028;&#23545;&#25163;&#21644;&#20154;&#31867;&#29609;&#23478;&#30340;&#35780;&#20272;&#12290;Elo&#35780;&#32423;&#28165;&#26970;&#22320;&#26174;&#31034;GROOT&#27491;&#22312;&#32553;&#23567;&#20154;&#26426;&#24046;&#36317;&#65292;&#24182;&#19988;&#23545;&#26368;&#22909;&#30340;&#36890;&#29992;&#20195;&#29702;&#31243;&#24207;&#22522;&#32447;&#20855;&#26377;70%&#30340;&#32988;&#29575;&#12290;&#23545;&#25152;&#20135;&#29983;&#30340;&#30446;&#26631;&#31354;&#38388;&#30340;&#23450;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#26032;&#39062;&#24615;&#36136;&#65292;&#21253;&#25324;&#30446;&#26631;&#30340;&#32452;&#25104;&#21644;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.05703</link><description>&lt;p&gt;
Siamese&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21477;&#23376;&#36716;&#25442;&#22120;&#31561;Siamese&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#20851;&#27880;&#30340;&#36755;&#20837;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#38556;&#30861;&#26159;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#33021;&#24402;&#22240;&#20110;&#20010;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#30340;&#26159;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#19968;&#20010;&#36755;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#29305;&#24449;&#23545;&#24402;&#22240;&#30340;&#24418;&#24335;&#65292;&#24182;&#21487;&#23558;&#20854;&#31616;&#21270;&#20026;&#21477;&#23376;&#36716;&#25442;&#22120;&#30340;&#20196;&#29260;-&#20196;&#29260;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#38598;&#25104;&#38597;&#21487;&#27604;&#30697;&#38453;&#65292;&#24182;&#32487;&#25215;&#20102;&#38598;&#25104;&#26799;&#24230;&#30340;&#20248;&#21183;&#24418;&#24335;&#29305;&#24615;&#65306;&#23427;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23436;&#25972;&#35745;&#31639;&#22270;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#23454;&#38469;&#39044;&#27979;&#32467;&#26524;&#12290;&#19968;&#39033;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21477;&#23376;&#36716;&#25442;&#22120;&#20013;&#65292;&#24456;&#23569;&#30340;&#20196;&#29260;&#23545;&#24448;&#24448;&#21487;&#20197;&#35299;&#37322;&#22823;&#37096;&#20998;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#38656;&#35201;&#20851;&#27880;&#22823;&#22810;&#25968;&#30340;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03605</link><description>&lt;p&gt;
FASER: &#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#36328;&#26550;&#26500;&#36719;&#20214;&#20013;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#23545;&#20110;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12289;&#20445;&#25252;&#36719;&#20214;&#20379;&#24212;&#38142;&#25110;&#36827;&#34892;&#28431;&#27934;&#30740;&#31350;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36328;&#26550;&#26500;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#24050;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#25506;&#32034;&#65292;&#24182;&#20351;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#20174;&#20108;&#36827;&#21046;&#25991;&#20214;&#20013;&#25552;&#21462;&#30340;&#24120;&#35265;&#32467;&#26500;&#65292;&#22914;&#20989;&#25968;&#25511;&#21046;&#27969;&#22270;&#25110;&#20108;&#36827;&#21046;&#32423;&#35843;&#29992;&#22270;&#65292;&#21453;&#27719;&#32534;&#36807;&#31243;&#30340;&#36755;&#20986;&#25110;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#30340;&#36755;&#20986;&#12290;&#20854;&#20013;&#19968;&#31181;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#12290;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#20004;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#23427;&#20204;&#30340;&#36328;&#26550;&#26500;&#24615;&#36136;&#20197;&#21450;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#20989;&#25968;&#23383;&#31526;&#20018;&#32534;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#25991;&#26723;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>DeepDecipher&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;API&#21644;&#25509;&#21475;&#65292;&#23427;&#36890;&#36807;&#25552;&#20379;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#20351;&#24471;&#23545;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#30340;&#20998;&#26512;&#26356;&#21152;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.01870</link><description>&lt;p&gt;
DeepDecipher: &#35775;&#38382;&#21644;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models. (arXiv:2310.01870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01870
&lt;/p&gt;
&lt;p&gt;
DeepDecipher&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;API&#21644;&#25509;&#21475;&#65292;&#23427;&#36890;&#36807;&#25552;&#20379;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#20351;&#24471;&#23545;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#30340;&#20998;&#26512;&#26356;&#21152;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#23545;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#24037;&#20855;&#30340;&#32039;&#36843;&#38656;&#27714;&#36880;&#28176;&#22686;&#21152;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#65292;&#24182;&#19988;&#32570;&#20047;&#20998;&#26512;&#27169;&#22411;&#20869;&#37096;&#30340;&#21487;&#35775;&#38382;&#24037;&#20855;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepDecipher - &#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#36716;&#25442;&#22120;&#27169;&#22411;MLP&#23618;&#20013;&#31070;&#32463;&#20803;&#30340;API&#21644;&#25509;&#21475;&#12290;DeepDecipher&#23558;LLMs&#30340;&#20808;&#36827;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#36755;&#20986;&#21464;&#24471;&#23481;&#26131;&#33719;&#21462;&#12290;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#36824;&#20351;&#24471;&#23545;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#30340;&#26816;&#26597;&#26356;&#21152;&#30452;&#35266;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;DeepDecipher&#30340;&#35774;&#35745;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20998;&#26512;&#31070;&#32463;&#20803;&#65292;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#26377;&#20851;&#27169;&#22411;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23558;DeepDecipher&#30340;&#21151;&#33021;&#19982;&#31867;&#20284;&#30340;&#24037;&#20855;&#22914;Neuroscope&#21644;OpenAI&#30340;Neuron Explainer&#36827;&#34892;&#23545;&#27604;&#12290;DeepDecipher&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#20998;&#26512;&#12290;&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;DeepDecipher&#20351;LLMs&#21464;&#24471;&#26356;&#21152;&#36879;&#26126;&#12289;&#21487;&#20449;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Research
&lt;/p&gt;</description></item><item><title>BackDiff&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#21644;&#20960;&#20309;&#34920;&#31034;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21508;&#31181;&#31895;&#31890;&#21270;&#27169;&#22411;&#21644;&#34507;&#30333;&#36136;&#20013;&#30340;&#24191;&#20041;&#21270;&#21644;&#21487;&#38752;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.01768</link><description>&lt;p&gt;
BackDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#21487;&#20256;&#36882;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Backdiff: a diffusion model for generalized transferable protein backmapping. (arXiv:2310.01768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01768
&lt;/p&gt;
&lt;p&gt;
BackDiff&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#21644;&#20960;&#20309;&#34920;&#31034;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21508;&#31181;&#31895;&#31890;&#21270;&#27169;&#22411;&#21644;&#34507;&#30333;&#36136;&#20013;&#30340;&#24191;&#20041;&#21270;&#21644;&#21487;&#38752;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31895;&#31890;&#21270;&#65288;CG&#65289;&#27169;&#22411;&#22312;&#30740;&#31350;&#34507;&#30333;&#36136;&#32467;&#26500;&#12289;&#28909;&#21147;&#23398;&#24615;&#36136;&#21644;&#26500;&#35937;&#21160;&#21147;&#23398;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#31895;&#31890;&#21270;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#22312;&#35768;&#22810;&#34507;&#30333;&#36136;&#35774;&#35745;&#21644;&#33647;&#29289;&#21457;&#29616;&#24212;&#29992;&#20013;&#38656;&#35201;&#35814;&#32454;&#30340;&#21407;&#23376;&#34920;&#31034;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#26102;&#65292;&#20174;CG&#26144;&#23556;&#22238;&#21407;&#23376;&#32423;&#26500;&#22411;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#32972;&#26144;&#23556;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#26222;&#36941;&#24212;&#29992;&#20110;&#19981;&#21516;CG&#27169;&#22411;&#21644;&#34507;&#30333;&#36136;&#30340;&#32972;&#26144;&#23556;&#26041;&#27861;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackDiff&#65292;&#19968;&#31181;&#26088;&#22312;&#23454;&#29616;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#38382;&#39064;&#30340;&#27867;&#21270;&#21644;&#21487;&#38752;&#24615;&#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;BackDiff&#21033;&#29992;&#20855;&#26377;&#20960;&#20309;&#34920;&#31034;&#30340;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;CG&#27169;&#22411;&#21487;&#20197;&#21253;&#21547;&#19981;&#21516;&#30340;&#31895;&#31890;&#21270;&#20301;&#28857;&#65292;&#20854;&#20013;&#21253;&#25324;&#36873;&#23450;&#30340;&#21407;&#23376;&#65288;CG&#21407;&#23376;&#65289;&#21644;&#21407;&#23376;&#22352;&#26631;&#30340;&#31616;&#21333;CG&#36741;&#21161;&#20989;&#25968;&#65288;CG&#36741;&#21161;&#65289;
&lt;/p&gt;
&lt;p&gt;
Coarse-grained (CG) models play a crucial role in the study of protein structures, protein thermodynamic properties, and protein conformation dynamics. Due to the information loss in the coarse-graining process, backmapping from CG to all-atom configurations is essential in many protein design and drug discovery applications when detailed atomic representations are needed for in-depth studies. Despite recent progress in data-driven backmapping approaches, devising a backmapping method that can be universally applied across various CG models and proteins remains unresolved. In this work, we propose BackDiff, a new generative model designed to achieve generalization and reliability in the protein backmapping problem. BackDiff leverages the conditional score-based diffusion model with geometric representations. Since different CG models can contain different coarse-grained sites which include selected atoms (CG atoms) and simple CG auxiliary functions of atomistic coordinates (CG auxiliar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27169;&#20223;&#23398;&#20064;&#20013;&#31070;&#32463;&#20195;&#29702;&#30340;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#23637;&#29616;&#20854;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00489</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#27169;&#20223;&#23398;&#20064;&#30340;&#21160;&#24577;DAG&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Dynamic DAG Discovery for Interpretable Imitation Learning. (arXiv:2310.00489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27169;&#20223;&#23398;&#20064;&#20013;&#31070;&#32463;&#20195;&#29702;&#30340;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#23637;&#29616;&#20854;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#20195;&#29702;&#31574;&#30053;&#65292;&#22312;&#21307;&#30103;&#27835;&#30103;&#26041;&#26696;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#20195;&#29702;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22256;&#38590;&#20027;&#35201;&#26469;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36890;&#24120;&#23454;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;2&#65289;&#20195;&#29702;&#20915;&#31574;&#32972;&#21518;&#30340;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#21487;&#33021;&#38543;&#30528;&#36712;&#36857;&#32780;&#21464;&#21270;&#65292;&#32780;&#19981;&#26159;&#22312;&#25972;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20445;&#25345;&#38745;&#24577;&#19981;&#21464;&#12290;&#20026;&#20102;&#22686;&#21152;&#31070;&#32463;&#20195;&#29702;&#30340;&#36879;&#26126;&#24230;&#21644;&#25552;&#20379;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#23637;&#31034;&#20854;&#25152;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#21160;&#20316;&#21644;&#29366;&#24577;&#21464;&#37327;&#65292;&#36793;&#34920;&#31034;&#39044;&#27979;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#36825;&#20010;&#22240;&#26524;&#21457;&#29616;&#36807;&#31243;&#26159;&#20381;&#36182;&#29366;&#24577;&#30340;&#65292;&#20351;&#20854;&#33021;&#22815;&#23545;&#28508;&#22312;&#22240;&#26524;&#22270;&#20013;&#30340;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, which learns agent policy by mimicking expert demonstration, has shown promising results in many applications such as medical treatment regimes and self-driving vehicles. However, it remains a difficult task to interpret control policies learned by the agent. Difficulties mainly come from two aspects: 1) agents in imitation learning are usually implemented as deep neural networks, which are black-box models and lack interpretability; 2) the latent causal mechanism behind agents' decisions may vary along the trajectory, rather than staying static throughout time steps. To increase transparency and offer better interpretability of the neural agent, we propose to expose its captured knowledge in the form of a directed acyclic causal graph, with nodes being action and state variables and edges denoting the causal relations behind predictions. Furthermore, we design this causal discovery process to be state-dependent, enabling it to model the dynamics in latent causal gr
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12931</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#30340;&#20998;&#21035;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12931
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65289;&#36890;&#24120;&#20250;&#20026;[CLS]&#31526;&#21495;&#21644;&#26631;&#35760;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#23558;&#30456;&#21516;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#20004;&#31181;&#26631;&#35760;&#31867;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#19982;&#23427;&#20204;&#21508;&#33258;&#30340;&#35282;&#33394;&#26368;&#20339;&#21305;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;[CLS]&#23884;&#20837;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#22659;&#20449;&#24687;&#65292;&#24182;&#22312;&#20854;&#38750;&#21508;&#21521;&#21516;&#24615;&#31354;&#38388;&#20013;&#20998;&#24067;&#26356;&#22343;&#21248;&#12290;&#24403;&#29992;&#36825;&#20004;&#20010;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#26367;&#25442;&#24120;&#35268;&#30340;&#24402;&#19968;&#21270;&#23618;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#20102;2.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03579</link><description>&lt;p&gt;
DTW+S: &#20351;&#29992;&#26377;&#24207;&#23616;&#37096;&#36235;&#21183;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;&#26102;&#38388;&#24207;&#21015;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#30340;&#27979;&#37327;&#26159;&#35768;&#22810;&#24212;&#29992;&#21253;&#25324;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#23616;&#37096;&#36235;&#21183;&#65288;&#24418;&#29366;&#65289;&#32780;&#26080;&#27861;&#25429;&#25417;&#21040;&#30456;&#20284;&#20043;&#22788;&#65292;&#29978;&#33267;&#21487;&#33021;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#23547;&#25214;&#22312;&#30456;&#20284;&#26102;&#38388;&#21608;&#22260;&#21457;&#29983;&#30340;&#30456;&#20284;&#36235;&#21183;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#26377;&#24207;&#30340;&#26377;&#24847;&#20041;&#30340;&#23616;&#37096;&#36235;&#21183;&#24207;&#21015;&#30340;&#24212;&#29992;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#20013;&#65288;&#20174;&#22686;&#38271;&#21040;&#23792;&#20540;&#20877;&#21040;&#20943;&#23569;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;DTW+S&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#8220;&#20445;&#25345;&#25509;&#36817;&#24615;&#8221;&#30340;&#30697;&#38453;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#19968;&#21015;&#20195;&#34920;&#23616;&#37096;&#36235;&#21183;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#31181;&#34920;&#31034;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DTW+S&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
&lt;/p&gt;</description></item><item><title>CoLA&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#35268;&#27169;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#30340;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;&#35843;&#24230;&#35268;&#21017;&#21644;&#32447;&#24615;&#25805;&#20316;&#31526;&#25277;&#35937;&#65292;&#33258;&#21160;&#26500;&#24314;&#20102;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#39640;&#25928;&#30340;&#25968;&#20540;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#20302;&#31934;&#24230;&#35745;&#31639;&#21644;GPU&#21152;&#36895;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#24212;&#19979;&#28216;&#36719;&#20214;&#21253;&#20013;&#30340;&#26032;&#23545;&#35937;&#12289;&#25805;&#20316;&#21644;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2309.03060</link><description>&lt;p&gt;
CoLA: &#28145;&#20837;&#21033;&#29992;&#32452;&#21512;&#32467;&#26500;&#23454;&#29616;&#33258;&#21160;&#21644;&#39640;&#25928;&#30340;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra. (arXiv:2309.03060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03060
&lt;/p&gt;
&lt;p&gt;
CoLA&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#35268;&#27169;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#30340;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;&#35843;&#24230;&#35268;&#21017;&#21644;&#32447;&#24615;&#25805;&#20316;&#31526;&#25277;&#35937;&#65292;&#33258;&#21160;&#26500;&#24314;&#20102;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#39640;&#25928;&#30340;&#25968;&#20540;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#20302;&#31934;&#24230;&#35745;&#31639;&#21644;GPU&#21152;&#36895;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#24212;&#19979;&#28216;&#36719;&#20214;&#21253;&#20013;&#30340;&#26032;&#23545;&#35937;&#12289;&#25805;&#20316;&#21644;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#39046;&#22495;&#28041;&#21450;&#21040;&#22823;&#35268;&#27169;&#30340;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#65292;&#22914;&#29305;&#24449;&#20998;&#35299;&#12289;&#35299;&#32447;&#24615;&#31995;&#32479;&#12289;&#35745;&#31639;&#30697;&#38453;&#25351;&#25968;&#21644;&#36857;&#20272;&#35745;&#31561;&#12290;&#28041;&#21450;&#30340;&#30697;&#38453;&#36890;&#24120;&#20855;&#26377;Krondor&#12289;&#21367;&#31215;&#12289;&#22359;&#23545;&#35282;&#12289;&#27714;&#21644;&#25110;&#20056;&#31215;&#31561;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#22823;&#35268;&#27169;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;CoLA&#65288;&#32452;&#21512;&#32447;&#24615;&#20195;&#25968;&#65289;&#12290;&#36890;&#36807;&#23558;&#32447;&#24615;&#25805;&#20316;&#31526;&#25277;&#35937;&#19982;&#32452;&#21512;&#35843;&#24230;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;CoLA&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#39640;&#25928;&#30340;&#25968;&#20540;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;CoLA&#36824;&#25552;&#20379;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#20302;&#31934;&#24230;&#35745;&#31639;&#21644;JAX&#21644;PyTorch&#20013;&#30340;GPU&#21152;&#36895;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#36890;&#36807;&#22810;&#37325;&#35843;&#24230;&#36866;&#24212;&#19979;&#28216;&#36719;&#20214;&#21253;&#20013;&#30340;&#26032;&#23545;&#35937;&#12289;&#25805;&#20316;&#21644;&#35268;&#21017;&#12290;CoLA&#21487;&#20197;&#21152;&#36895;&#35768;&#22810;&#20195;&#25968;&#25805;&#20316;&#65292;&#21516;&#26102;&#20063;&#20415;&#20110;&#21407;&#22411;&#21270;&#30697;&#38453;&#32467;&#26500;&#21644;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#30340;&#38477;&#20302;-
&lt;/p&gt;
&lt;p&gt;
Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named CoLA (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02685</link><description>&lt;p&gt;
Diffusion-EDFs: &#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#31561;&#21464;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#23558;&#31354;&#38388;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#21363;SE(3)&#31561;&#21464;&#24615;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;SE(3)&#31561;&#21464;&#24615;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#21482;&#38656;5&#21040;10&#20010;&#20219;&#21153;&#28436;&#31034;&#21363;&#21487;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#25805;&#20316;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01029</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#65292;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#20026;&#19979;&#28216;&#24212;&#29992;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38416;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#27010;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;LLMs&#30340;&#35757;&#32451;&#33539;&#24335;&#23558;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65306;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#21644;&#25552;&#31034;&#33539;&#24335;&#12290;&#23545;&#20110;&#27599;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#25104;&#20010;&#20307;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#25972;&#20307;&#27169;&#22411;&#30693;&#35782;&#30340;&#20840;&#23616;&#35299;&#37322;&#30340;&#30446;&#26631;&#21644;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#39640;&#32500;&#22522;&#22240;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#21644;&#36716;&#25442;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36873;&#25321;&#30456;&#20851;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#22312;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.09791</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#39640;&#32500;&#22522;&#22240;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#29983;&#29289;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification. (arXiv:2308.09791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#39640;&#32500;&#22522;&#22240;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#21644;&#36716;&#25442;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36873;&#25321;&#30456;&#20851;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#22312;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Horse Herd&#20248;&#21270;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;&#24180;&#40836;&#30340;&#39532;&#30340;&#34892;&#20026;&#30340;&#26032;&#22411;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;HOA&#34987;&#24341;&#20837;&#26469;&#35299;&#20915;&#22797;&#26434;&#21644;&#39640;&#32500;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#20108;&#36827;&#21046;&#29256;&#26412;&#65288;BHOA&#65289;&#65292;&#20197;&#35299;&#20915;&#31163;&#25955;&#38382;&#39064;&#21644;&#36873;&#25321;&#26174;&#33879;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;BHOA&#21644;&#26368;&#23567;&#20887;&#20313;&#26368;&#22823;&#30456;&#20851;&#65288;MRMR&#65289;&#36807;&#28388;&#26041;&#27861;&#30340;&#26032;&#22411;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#12290;&#36825;&#31181;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#33021;&#22815;&#20135;&#29983;&#19968;&#32452;&#30456;&#20851;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#26377;&#30410;&#29305;&#24449;&#23376;&#38598;&#12290;&#30001;&#20110;&#29305;&#24449;&#36873;&#25321;&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#20989;&#25968;&#65288;TF&#65289;&#65292;&#31216;&#20026;X&#24418;&#29366;TF&#65292;&#23558;&#36830;&#32493;&#38382;&#39064;&#36716;&#25442;&#20026;&#20108;&#36827;&#21046;&#25628;&#32034;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#34987;&#29992;&#20110;&#26816;&#39564;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#21313;&#20010;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#65288;&#21363;&#28107;&#24052;&#30244;&#12289;&#21069;&#21015;&#33146;&#31561;&#65289;&#19978;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36755;&#20837;&#25200;&#21160;&#19979;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#33258;&#36866;&#24212;&#26041;&#27861;&#20351;&#29992;&#20102;&#19981;&#30456;&#20851;&#30340;&#39057;&#29575;&#65292;&#23548;&#33268;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06703</link><description>&lt;p&gt;
&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#40065;&#26834;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods. (arXiv:2308.06703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06703
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36755;&#20837;&#25200;&#21160;&#19979;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#33258;&#36866;&#24212;&#26041;&#27861;&#20351;&#29992;&#20102;&#19981;&#30456;&#20851;&#30340;&#39057;&#29575;&#65292;&#23548;&#33268;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#22914;Adam&#21644;RMSProp&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26631;&#20934;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#23567;&#65292;&#20294;&#20351;&#29992;SGD&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36755;&#20837;&#25200;&#21160;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#19981;&#30456;&#20851;&#30340;&#39057;&#29575;&#65292;&#23545;&#36825;&#20123;&#39057;&#29575;&#36827;&#34892;&#21464;&#21160;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#36825;&#20123;&#21464;&#21270;&#26174;&#31034;&#20986;&#25935;&#24863;&#24615;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#20351;&#29992;&#30340;&#19981;&#30456;&#20851;&#39057;&#29575;&#20250;&#23548;&#33268;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#24046;&#24322;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#65288;signGD&#65289;&#22312;&#27169;&#25311;&#33258;&#28982;&#20449;&#21495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#22312;&#19977;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;GD&#21644;signGD&#20248;&#21270;&#30340;&#27169;&#22411;&#20855;&#26377;&#26631;&#20934;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risk
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#31232;&#30095;&#32534;&#30721;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#65292;&#20026;&#29616;&#26377;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03818</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32534;&#30721;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#21450;&#22312;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A sparse coding approach to inverse problems with application to microwave tomography imaging. (arXiv:2308.03818v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#31232;&#30095;&#32534;&#30721;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#65292;&#20026;&#29616;&#26377;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#20250;&#36935;&#21040;&#19968;&#20123;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#22270;&#20687;&#65292;&#36825;&#20123;&#39046;&#22495;&#21253;&#25324;&#21307;&#23398;&#35786;&#26029;&#21644;&#22825;&#25991;&#30740;&#31350;&#31561;&#12290;&#20026;&#20102;&#20174;&#19981;&#23436;&#25972;&#21644;&#22833;&#30495;&#30340;&#25968;&#25454;&#20013;&#37325;&#24314;&#22270;&#20687;&#65292;&#38656;&#35201;&#21019;&#24314;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#29983;&#25104;&#36825;&#20123;&#27979;&#37327;&#30340;&#29289;&#29702;&#26426;&#21046;&#21644;&#25152;&#20998;&#26512;&#22270;&#20687;&#30340;&#26412;&#36136;&#29305;&#24615;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22270;&#20687;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#23427;&#26159;&#21463;&#21754;&#20083;&#21160;&#29289;&#35270;&#35273;&#31995;&#32479;&#21551;&#21457;&#30340;&#19968;&#31181;&#23454;&#38469;&#12289;&#32039;&#20945;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#22270;&#20687;&#38598;&#19978;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#20915;&#19981;&#36866;&#23450;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#32534;&#30721;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#35299;&#20915;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#20013;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36866;&#23450;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#25913;&#36827;&#29616;&#26377;&#30340;&#31639;&#27861;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2307.13903</link><description>&lt;p&gt;
&#33104;&#36133;&#40065;&#26834;&#30340;Lipschitz&#19978;&#19979;&#25991;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#32773;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#30001;&#23545;&#25163;&#36873;&#25321;&#30340;Lipschitz&#20989;&#25968;$f$&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#23545;&#25163;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#19978;&#19979;&#25991;&#21521;&#37327;$x_t$&#65292;&#23398;&#20064;&#32773;&#23545;&#30495;&#23454;&#20989;&#25968;&#20540;$f(x_t)$&#36827;&#34892;&#29468;&#27979;&#65292;&#24182;&#25509;&#25910;&#19968;&#20010;&#25351;&#31034;&#29468;&#27979;&#26159;&#39640;&#36824;&#26159;&#20302;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;&#22312;&#24635;&#20849;$C$&#36718;&#20013;&#65292;&#20449;&#21495;&#21487;&#33021;&#34987;&#31713;&#25913;&#65292;&#20294;&#23398;&#20064;&#32773;&#19981;&#30693;&#36947;$C$&#30340;&#20540;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#36896;&#25104;&#23567;&#30340;&#32047;&#31215;&#25439;&#22833;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#32780;&#24378;&#22823;&#30340;&#25216;&#26415;&#39564;&#35777;&#65292;&#23545;&#35774;&#35745;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#35774;&#35745;&#20102;&#19968;&#20123;&#31639;&#27861;&#65288;&#23558;Lipschitz&#21442;&#25968;$L$&#35270;&#20026;&#24120;&#25968;&#65289;&#65306;&#23545;&#20110;&#23545;&#31216;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d=1$&#26102;&#36798;&#21040;&#21518;&#24724;$O(C\log T)$&#65292;&#22312;$d&gt;1$&#26102;&#36798;&#21040;&#21518;&#24724;$O_d(C\log T + T^{(d-1)/d})$&#65307;&#23545;&#20110;&#35745;&#20215;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d/(d+1)$&#26102;&#36798;&#21040;&#21518;&#24724;$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$&#12290;
&lt;/p&gt;
&lt;p&gt;
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#35266;&#27979;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#20272;&#35745;&#27867;&#21270;&#21040;&#20219;&#24847;&#32500;&#24230;&#25110;&#21487;&#27979;&#31354;&#38388;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21517;&#20041;&#21464;&#37327;&#30340;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#31574;&#30053;&#22312;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#21151;&#29575;&#26041;&#38754;&#26377;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.13868</link><description>&lt;p&gt;
&#20174;&#39640;&#32500;&#35266;&#27979;&#30740;&#31350;&#20013;&#23398;&#20064;&#21464;&#24322;&#28304;
&lt;/p&gt;
&lt;p&gt;
Learning sources of variability from high-dimensional observational studies. (arXiv:2307.13868v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#35266;&#27979;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#20272;&#35745;&#27867;&#21270;&#21040;&#20219;&#24847;&#32500;&#24230;&#25110;&#21487;&#27979;&#31354;&#38388;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21517;&#20041;&#21464;&#37327;&#30340;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#31574;&#30053;&#22312;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#21151;&#29575;&#26041;&#38754;&#26377;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21464;&#37327;&#24433;&#21709;&#35266;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#35832;&#22914;&#8220;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#8221;&#31561;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#19968;&#33539;&#24335;&#22312;&#35768;&#22810;&#29983;&#29289;&#39046;&#22495;&#20013;&#34987;&#37319;&#29992;&#65292;&#20174;&#30123;&#33495;&#21644;&#33647;&#29289;&#24320;&#21457;&#21040;&#25919;&#31574;&#24178;&#39044;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#24120;&#20165;&#38480;&#20110;&#21333;&#21464;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#22240;&#26524;&#20272;&#35745;&#27867;&#21270;&#21040;&#20219;&#24847;&#32500;&#24230;&#25110;&#21487;&#27979;&#31354;&#38388;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#22240;&#26524;&#20272;&#35745;&#24418;&#24335;&#21270;&#20026;&#21517;&#20041;&#21464;&#37327;&#30340;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#35843;&#25972;&#19968;&#33268;&#24615;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#27979;&#35797;&#26159;&#19968;&#33268;&#24615;&#22240;&#26524;&#20559;&#24046;&#27979;&#35797;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Causal CDcorr&#22312;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#21151;&#29575;&#26041;&#38754;&#22343;&#26377;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#22312;github.com/ebridge2/cdcorr&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15832</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24471;&#20998;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#33021;&#20250;&#22240;&#31354;&#38388;&#22343;&#20540;&#30340;&#38169;&#35823;&#32780;&#20986;&#29616;&#39068;&#33394;&#20559;&#31227;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#36739;&#22823;&#30340;&#22270;&#20687;&#20013;&#20250;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#39068;&#33394;&#20559;&#31227;&#12290;&#25105;&#20204;&#22312;&#24471;&#20998;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#65292;&#29992;&#20110;&#22788;&#29702;&#36755;&#20837;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#24182;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#22343;&#20540;&#12290;&#36825;&#31181;&#32593;&#32476;&#26550;&#26500;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25913;&#36827;&#19982;&#29983;&#25104;&#22270;&#20687;&#22823;&#23567;&#30340;&#20851;&#31995;&#36817;&#20284;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#36328;&#22270;&#20687;&#23610;&#23544;&#30340;&#39068;&#33394;&#20559;&#31227;&#38382;&#39064;&#25552;&#20379;&#20102;&#30456;&#23545;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#29702;&#24819;&#21270;&#24773;&#20917;&#19979;&#39068;&#33394;&#20559;&#31227;&#30340;&#36215;&#28304;&#65292;&#20197;&#25512;&#21160;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generated images of score-based models can suffer from errors in their spatial means, an effect, referred to as a color shift, which grows for larger images. This paper introduces a computationally inexpensive solution to mitigate color shifts in score-based diffusion models. We propose a simple nonlinear bypass connection in the score network, designed to process the spatial mean of the input and to predict the mean of the score function. This network architecture substantially improves the resulting spatial means of the generated images, and we show that the improvement is approximately independent of the size of the generated images. As a result, our solution offers a comparatively inexpensive solution for the color shift problem across image sizes. Lastly, we discuss the origin of color shifts in an idealized setting in order to motivate our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#26469;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#35813;&#36807;&#31243;&#36890;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#65292;&#19981;&#21516;&#20110;&#20219;&#21153;&#25968;&#25454;&#30340;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;&#23884;&#20837;&#30340;&#27700;&#21360;&#12290;</title><link>http://arxiv.org/abs/2306.03436</link><description>&lt;p&gt;
&#36890;&#36807;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;
&lt;/p&gt;
&lt;p&gt;
Protecting the Intellectual Property of Diffusion Models by the Watermark Diffusion Process. (arXiv:2306.03436v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#26469;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#35813;&#36807;&#31243;&#36890;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#65292;&#19981;&#21516;&#20110;&#20219;&#21153;&#25968;&#25454;&#30340;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#65292;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;&#23884;&#20837;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20219;&#21153;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#26550;&#26500;&#12290;&#35757;&#32451;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#38656;&#35201;&#39640;&#36164;&#28304;&#25104;&#26412;&#65292;&#22240;&#27492;&#38656;&#35201;&#20445;&#25252;&#23427;&#20204;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#27700;&#21360;&#26041;&#27861;WDM&#65292;&#21253;&#25324;&#27700;&#21360;&#23884;&#20837;&#12289;&#25552;&#21462;&#21644;&#39564;&#35777;&#12290;WDM&#36890;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#20010;&#27700;&#21360;&#25193;&#25955;&#36807;&#31243;&#65288;WDP&#65289;&#26469;&#23884;&#20837;&#27700;&#21360;&#25968;&#25454;&#65292;&#35813;&#36807;&#31243;&#19982;&#20219;&#21153;&#25968;&#25454;&#30340;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#19981;&#21516;&#12290;&#23884;&#20837;&#30340;&#27700;&#21360;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;WDP&#30340;&#20849;&#20139;&#21453;&#21521;&#22122;&#22768;&#36827;&#34892;&#37319;&#26679;&#32780;&#19981;&#20250;&#38477;&#20302;&#21407;&#22987;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;WDP&#19982;&#20462;&#25913;&#25193;&#25955;&#36807;&#31243;&#36830;&#25509;&#26469;&#25552;&#20379;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as state-of-the-art deep generative architectures with the increasing demands for generation tasks. Training large diffusion models for good performance requires high resource costs, making them valuable intellectual properties to protect. While most of the existing ownership solutions, including watermarking, mainly focus on discriminative models. This paper proposes WDM, a novel watermarking method for diffusion models, including watermark embedding, extraction, and verification. WDM embeds the watermark data through training or fine-tuning the diffusion model to learn a Watermark Diffusion Process (WDP), different from the standard diffusion process for the task data. The embedded watermark can be extracted by sampling using the shared reverse noise from the learned WDP without degrading performance on the original task. We also provide theoretical foundations and analysis of the proposed method by connecting the WDP to the diffusion process with a modi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19891</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;LDAS&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22788;&#29702;&#22810;&#36798;&#20960;&#30334;&#19975;&#20010;&#21160;&#20316;&#30340;&#38750;&#32467;&#26500;&#21270;LDAS&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#27969;&#12289;&#29983;&#20135;&#21644;&#36816;&#36755;&#31995;&#32479;&#31561;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#65292;&#20854;&#35268;&#27169;&#29978;&#33267;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#20063;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#20010;&#21160;&#20316;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#21160;&#20316;&#31354;&#38388;&#21576;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#31561;&#38388;&#36317;&#30340;&#31163;&#25955;&#36164;&#28304;&#21333;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22788;&#29702;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;LDAS&#65288;SLDAS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#29992;&#20110;SLDAS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#22312;&#20855;&#26377;&#39640;&#36798;$10^{73}$&#20010;&#21160;&#20316;&#30340;&#32467;&#26500;&#21270;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#26631;&#26438;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18409</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#65306;&#31616;&#21333;&#19988;&#21487;&#35777;&#26126;&#30340;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19982;&#22810;&#20010;&#30446;&#26631;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65288;&#22914;&#22810;&#26631;&#20934;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#65289;&#20013;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#26041;&#21521;&#30340;&#37051;&#22495;&#20869;&#38480;&#21046;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#26469;&#35268;&#33539;&#32447;&#24615;&#32452;&#21512;&#30446;&#26631;&#30340;&#26368;&#20248;&#26041;&#21521;&#65292;&#20363;&#22914;MTL&#20013;&#30340;&#24179;&#22343;&#25439;&#22833;&#12290; &#36825;&#20010;&#20844;&#24335;&#21253;&#25324;GD&#21644;MGDA&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#20139;&#21463;&#20687;CAGrad&#20013;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#26377;&#21033;&#20110;&#38543;&#26426;&#31639;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26041;&#21521;&#23548;&#21521;&#22810;&#30446;&#26631;&#26799;&#24230;&#19979;&#38477;&#65288;SDMGrad&#65289;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;SGD&#31867;&#22411;&#30340;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#30446;&#26631;&#25968;&#37327;&#36739;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#30446;&#26631;&#37319;&#26679;&#30340;SDMGrad-OS&#31639;&#27861;&#12290; &#23545;&#20110;&#24658;&#23450;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#955;&#65292;&#25105;&#20204;&#35777;&#26126;SDMGrad&#21644;SDMGrad-OS&#30830;&#23454;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>SVDinsTN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#20013;&#25554;&#20837;&#23545;&#35282;&#22240;&#23376;&#65292;&#21516;&#26102;&#35745;&#31639;&#24352;&#37327;&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;10&#21040;10^3&#20493;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14912</link><description>&lt;p&gt;
SVDinsTN: &#19968;&#31181;&#38598;&#25104;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#21450;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search. (arXiv:2305.14912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14912
&lt;/p&gt;
&lt;p&gt;
SVDinsTN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#20013;&#25554;&#20837;&#23545;&#35282;&#22240;&#23376;&#65292;&#21516;&#26102;&#35745;&#31639;&#24352;&#37327;&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;10&#21040;10^3&#20493;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#34920;&#31034;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;TN-SS&#65289;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#20197;&#23454;&#29616;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#30001;&#20110;&#37325;&#22797;&#30340;&#32467;&#26500;&#35780;&#20272;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38598;&#25104;&#65288;&#21333;&#23618;&#65289;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SVDinsTN&#65292;&#28040;&#38500;&#20102;&#37325;&#22797;&#32321;&#29712;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#36890;&#36807;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;TN&#30340;&#27599;&#20010;&#36793;&#25554;&#20837;&#19968;&#20010;&#23545;&#35282;&#22240;&#23376;&#65292;&#25105;&#20204;&#21516;&#26102;&#35745;&#31639;TN&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#22240;&#23376;&#31232;&#30095;&#24615;&#25581;&#31034;&#20102;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#23454;&#29616;&#20102;&#32422;10&#21040;10^3&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network (TN) representation is a powerful technique for data analysis and machine learning. It practically involves a challenging TN structure search (TN-SS) problem, which aims to search for the optimal structure to achieve a compact representation. Existing TN-SS methods mainly adopt a bi-level optimization method that leads to excessive computational costs due to repeated structure evaluations. To address this issue, we propose an efficient integrated (single-level) method named SVD-inspired TN decomposition (SVDinsTN), eliminating the need for repeated tedious structure evaluation. By inserting a diagonal factor for each edge of the fully-connected TN, we calculate TN cores and diagonal factors simultaneously, with factor sparsity revealing the most compact TN structure. Experimental results on real-world data demonstrate that SVDinsTN achieves approximately $10\sim{}10^3$ times acceleration in runtime compared to the existing TN-SS methods while maintaining a comparable lev
&lt;/p&gt;</description></item><item><title>BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.12534</link><description>&lt;p&gt;
BertRLFuzzer: &#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12534
&lt;/p&gt;
&lt;p&gt;
BertRLFuzzer&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#24037;&#20855;&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#65292;BertRLFuzzer&#30456;&#23545;&#20110;&#20854;&#20182;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#12289;&#26032;&#28431;&#27934;&#21457;&#29616;&#21644;&#25915;&#20987;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;BertRLFuzzer&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Fuzzer&#65292;&#26088;&#22312;&#21457;&#29616;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;BertRLFuzzer&#30340;&#24037;&#20316;&#21407;&#29702;&#22914;&#19979;&#65306;&#32473;&#23450;&#19968;&#32452;&#31181;&#23376;&#36755;&#20837;&#65292;Fuzzer&#23545;&#23427;&#20204;&#25191;&#34892;&#36981;&#24490;&#35821;&#27861;&#24182;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#65292;&#20197;&#29983;&#25104;&#20505;&#36873;&#25915;&#20987;&#21521;&#37327;&#12290;BertRLFuzzer&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#20351;&#29992;BERT&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#25351;&#23548;Fuzzer&#39640;&#25928;&#23398;&#20064;&#36981;&#24490;&#35821;&#27861;&#21644;&#24341;&#21457;&#25915;&#20987;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#12290;&#20026;&#20102;&#39564;&#35777;BertRLFuzzer&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20849;&#35745;13&#20010;&#40657;&#30418;&#21644;&#30333;&#30418;Fuzzer&#22312;9&#20010;&#21463;&#23475;&#32773;&#32593;&#31449;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#28041;&#21450;&#36229;&#36807;16K&#34892;&#30340;&#28304;&#20195;&#30721;&#12290;&#30456;&#23545;&#20110;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#24037;&#20855;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26102;&#38388;&#21040;&#39318;&#27425;&#25915;&#20987;&#30340;&#26174;&#33879;&#25913;&#36827;&#65288;&#20943;&#23569;54&#65285;&#65289;&#65292;&#21457;&#29616;&#30340;&#26032;&#28431;&#27934;&#65288;17&#20010;&#26032;&#28431;&#27934;&#65289;&#21644;&#25915;&#20987;&#29575;&#65288;&#29983;&#25104;&#30340;&#25915;&#20987;&#21521;&#37327;&#22686;&#21152;&#20102;4.4&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07598</link><description>&lt;p&gt;
RHINO&#65306;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#30340;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#30340;&#26059;&#36716;DETR
&lt;/p&gt;
&lt;p&gt;
RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DINO&#30340;&#21457;&#24067;&#65292;&#19968;&#31181;DETR&#30340;&#21464;&#20307;&#65292;&#26816;&#27979;&#21464;&#21387;&#22120;&#27491;&#22312;&#36890;&#36807;&#20854;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#20013;&#21047;&#26032;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#39044;&#35745;&#20174;&#20854;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#22909;&#22788;&#65292;&#22914;&#28040;&#38500;NMS&#21644;&#19982;&#38170;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#20294;&#23578;&#26410;&#24443;&#24213;&#30740;&#31350;DETR&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;DETR&#36827;&#34892;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#24182;&#19981;&#33021;&#20445;&#35777;&#19981;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#26412;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#26469;&#36807;&#28388;&#20887;&#20313;&#30340;&#24102;&#22122;&#22768;&#30340;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#26597;&#35810;&#23545;&#40784;&#26469;&#20445;&#25345;Transformer&#35299;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26059;&#36716;DETR&#21644;&#20854;&#20182;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#65292;&#21487;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#21560;&#38468;&#28909;&#12290;</title><link>http://arxiv.org/abs/2304.01628</link><description>&lt;p&gt;
&#22810;&#23380;&#26230;&#24577;&#26448;&#26009;&#30340;&#31561;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Networks for Porous Crystalline Materials. (arXiv:2304.01628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#65292;&#21487;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#21560;&#38468;&#28909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#20855;&#26377;&#21152;&#36895;&#24320;&#21457;&#26032;&#26448;&#26009;&#30340;&#39640;&#36890;&#37327;&#31579;&#36873;&#36807;&#31243;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#20351;&#29992;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#36827;&#34892;&#30340;&#27169;&#25311;&#24448;&#24448;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#27169;&#36825;&#20123;&#26448;&#26009;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#26230;&#20307;&#20013;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#65292;&#36825;&#20123;&#23545;&#31216;&#24615;&#30001;&#23427;&#20204;&#30340;&#31354;&#38388;&#32676;&#23450;&#20041;&#12290;&#29616;&#26377;&#30340;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#20855;&#26377;&#36807;&#20110;&#20005;&#26684;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#65292;&#35201;&#20040;&#20165;&#21253;&#25324;&#21333;&#20803;&#26684;&#20043;&#38388;&#30340;&#23545;&#31216;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#27809;&#26377;&#26126;&#30830;&#22320;&#24314;&#27169;&#26230;&#20307;&#30340;&#22810;&#23380;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#20854;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#26500;&#22411;&#30340;&#33707;&#23572;&#23450;&#30707;&#27832;&#30707;&#30340;CO$_2$&#21560;&#38468;&#28909;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently predicting properties of porous crystalline materials has great potential to accelerate the high throughput screening process for developing new materials, as simulations carried out using first principles model are often computationally expensive. To effectively make use of Deep Learning methods to model these materials, we need to utilize the symmetries present in the crystals, which are defined by their space group. Existing methods for crystal property prediction either have symmetry constraints that are too restrictive or only incorporate symmetries between unit cells. In addition, these models do not explicitly model the porous structure of the crystal. In this paper, we develop a model which incorporates the symmetries of the unit cell of a crystal in its architecture and explicitly models the porous structure. We evaluate our model by predicting the heat of adsorption of CO$_2$ for different configurations of the mordenite zeolite. Our results confirm that our metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15799</link><description>&lt;p&gt;
&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#21442;&#19982;&#32773;&#20043;&#38388;&#39057;&#32321;&#30340;&#36890;&#20449;&#23558;&#20943;&#32531;&#25910;&#25947;&#36895;&#29575;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24120;&#35268;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#32858;&#21512;&#26799;&#24230;&#26469;&#25913;&#21892;&#26412;&#22320;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36827;&#19968;&#27493;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#21644;&#20840;&#23616;&#21442;&#25968;&#30340;&#20559;&#24046;&#12290;&#20197;&#19978;&#31574;&#30053;&#35201;&#27714;&#22312;&#27599;&#20010;&#26412;&#22320;&#36845;&#20195;&#20013;&#25910;&#38598;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21442;&#25968;&#21644;&#26799;&#24230;&#65292;&#30001;&#20110;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#27809;&#26377;&#36890;&#20449;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#24341;&#20837;&#31216;&#20026;&#20840;&#23616;&#22343;&#20540;&#22330;&#21644;&#26412;&#22320;&#22343;&#20540;&#22330;&#30340;&#20004;&#20010;&#22343;&#20540;&#22330;&#26415;&#35821;&#26469;&#23436;&#25104;&#32858;&#21512;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#26029;&#21487;&#35266;&#23519;&#26102;&#38388;&#24207;&#21015;&#25152;&#26263;&#31034;&#30340;&#22266;&#26377;&#28508;&#22312;&#22240;&#32032;&#26469;&#36827;&#34892;&#20449;&#21495;&#25104;&#20998;&#20998;&#35299;&#24182;&#37325;&#26500;&#26410;&#26469;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#25928;&#29575;&#30340;&#31232;&#30095;&#20851;&#31995;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#22312;&#32570;&#20047;&#36275;&#22815;&#21464;&#37327;&#19979;&#30340;&#24314;&#27169;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10426</link><description>&lt;p&gt;
&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21487;&#39044;&#27979;&#30340;&#28508;&#22312;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Discovering Predictable Latent Factors for Time Series Forecasting. (arXiv:2303.10426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#26029;&#21487;&#35266;&#23519;&#26102;&#38388;&#24207;&#21015;&#25152;&#26263;&#31034;&#30340;&#22266;&#26377;&#28508;&#22312;&#22240;&#32032;&#26469;&#36827;&#34892;&#20449;&#21495;&#25104;&#20998;&#20998;&#35299;&#24182;&#37325;&#26500;&#26410;&#26469;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#25928;&#29575;&#30340;&#31232;&#30095;&#20851;&#31995;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#22312;&#32570;&#20047;&#36275;&#22815;&#21464;&#37327;&#19979;&#30340;&#24314;&#27169;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#22914;Transformer&#21450;&#20854;&#21464;&#31181;&#65292;&#24050;&#32463;&#22312;&#39034;&#24207;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#24615;&#33021;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#38752;&#20887;&#20313;&#30340;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#26469;&#24314;&#27169;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#35843;&#25972;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#32570;&#20047;&#36275;&#22815;&#30340;&#21464;&#37327;&#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#29702;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#36825;&#31181;&#39044;&#27979;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#65292;&#26102;&#38388;&#24207;&#21015;&#20284;&#20046;&#21463;&#21040;&#35768;&#22810;&#22806;&#29983;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24314;&#27169;&#21464;&#24471;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#25512;&#26029;&#21487;&#35266;&#23519;&#26102;&#38388;&#24207;&#21015;&#25152;&#26263;&#31034;&#30340;&#22266;&#26377;&#28508;&#22312;&#22240;&#32032;&#12290;&#25512;&#26029;&#20986;&#30340;&#22240;&#32032;&#34987;&#29992;&#26469;&#24418;&#25104;&#22810;&#20010;&#29420;&#31435;&#21487;&#39044;&#27979;&#30340;&#20449;&#21495;&#25104;&#20998;&#65292;&#36825;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#25928;&#29575;&#30340;&#31232;&#30095;&#20851;&#31995;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#33021;&#37325;&#26500;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern time series forecasting methods, such as Transformer and its variants, have shown strong ability in sequential data modeling. To achieve high performance, they usually rely on redundant or unexplainable structures to model complex relations between variables and tune the parameters with large-scale data. Many real-world data mining tasks, however, lack sufficient variables for relation reasoning, and therefore these methods may not properly handle such forecasting problems. With insufficient data, time series appear to be affected by many exogenous variables, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this paper, we develop a novel algorithmic framework for inferring the intrinsic latent factors implied by the observable time series. The inferred factors are used to form multiple independent and predictable signal components that enable not only sparse relation reasoning for long-term efficiency but also reconstructing the future
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30896;&#25758;&#20132;&#21449;&#29109;&#30340;&#33258;&#26631;&#27880;&#20998;&#31867;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#35813;&#25439;&#22833;&#26041;&#27861;&#30340;EM-like&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#26631;&#27880;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07321</link><description>&lt;p&gt;
&#33258;&#26631;&#27880;&#20998;&#31867;&#20013;&#30340;&#30896;&#25758;&#20132;&#21449;&#29109;&#25439;&#22833;&#21450;EM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Collision Cross-entropy and EM Algorithm for Self-labeled Classification. (arXiv:2303.07321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30896;&#25758;&#20132;&#21449;&#29109;&#30340;&#33258;&#26631;&#27880;&#20998;&#31867;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#35813;&#25439;&#22833;&#26041;&#27861;&#30340;EM-like&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#26631;&#27880;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21518;&#39564;&#27169;&#22411;&#30340;&#33258;&#26631;&#27880;&#20998;&#31867;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#30896;&#25758;&#20132;&#21449;&#29109;&#8221;&#20316;&#20026;&#39321;&#20892;&#20132;&#21449;&#29109;&#30340;&#19968;&#20010;&#20581;&#22766;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EM-like&#31639;&#27861;&#26469;&#36890;&#36807;&#20132;&#26367;&#25311;&#21512;&#21518;&#39564;&#27010;&#29575;y&#21644;&#26356;&#26032;&#27169;&#22411;&#39044;&#27979;&#26469;&#20248;&#21270;&#25105;&#20204;&#30340;&#25439;&#22833;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#30896;&#25758;&#25439;&#22833;&#22312;&#21508;&#31181;&#33258;&#26631;&#27880;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#25110;&#33267;&#23569;&#19982;&#29616;&#26377;&#25439;&#22833;&#30456;&#21305;&#37197;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose "collision cross-entropy" as a robust alternative to the Shannon's cross-entropy in the context of self-labeled classification with posterior models. Assuming unlabeled data, self-labeling works by estimating latent pseudo-labels, categorical distributions y, that optimize some discriminative clustering criteria, e.g. "decisiveness" and "fairness". All existing self-labeled losses incorporate Shannon's cross-entropy term targeting the model prediction, softmax, at the estimated distribution y. In fact, softmax is trained to mimic the uncertainty in y exactly. Instead, we propose the negative log-likelihood of "collision" to maximize the probability of equality between two random variables represented by distributions softmax and y. We show that our loss satisfies some properties of a generalized cross-entropy. Interestingly, it agrees with the Shannon's cross-entropy for one-hot pseudo-labels y, but the training from softer labels weakens. For example, if y is a uniform dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;{\sc LegendreTron}&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#22810;&#31867;&#21035;&#38382;&#39064;&#30340;&#27491;&#30830;&#26631;&#20934;&#25439;&#22833;&#21644;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#32463;&#24120;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11695</link><description>&lt;p&gt;
LegendreTron&#65306;&#21319;&#32423;&#29256;&#22810;&#31867;&#21035;&#27491;&#30830;&#22810;&#39033;&#25439;&#22833;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LegendreTron: Uprising Proper Multiclass Loss Learning. (arXiv:2301.11695v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;{\sc LegendreTron}&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#22810;&#31867;&#21035;&#38382;&#39064;&#30340;&#27491;&#30830;&#26631;&#20934;&#25439;&#22833;&#21644;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#32463;&#24120;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#26159;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#22312;&#27169;&#22411;&#24320;&#21457;&#20043;&#21069;&#36873;&#25321;&#12290;&#20026;&#36991;&#20813;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#21487;&#33021;&#20986;&#29616;&#30340;&#29305;&#23450;&#36873;&#25321;&#65292;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#25551;&#36848;&#20102;&#25439;&#22833;&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#31216;&#20026;&#8220;&#27491;&#30830;&#24615;&#8221;&#65292;&#23427;&#26029;&#35328;&#36125;&#21494;&#26031;&#35268;&#21017;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#32852;&#21512;&#23398;&#20064;&#25439;&#22833;&#21644;&#27169;&#22411;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25311;&#21512;&#19968;&#20010;&#23558;$\mathbb{R}$&#21333;&#35843;&#26144;&#23556;&#21040;$[0,1]$&#30340;&#21453;&#35299;&#26631;&#20934;&#38142;&#25509;&#20989;&#25968;&#26469;&#20272;&#35745;&#20108;&#20803;&#38382;&#39064;&#30340;&#27010;&#29575;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20984;&#20989;&#25968;&#26799;&#24230;&#30340;&#21333;&#35843;&#24615;&#23558;&#21333;&#35843;&#24615;&#25193;&#23637;&#21040;$\mathbb{R}^{C-1}$&#21040;&#27010;&#29575;&#30340;&#27491;&#25237;&#24433;$\tilde{\Delta}^{C-1}$&#30340;&#26144;&#23556;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;{\sc LegendreTron}&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#22810;&#31867;&#21035;&#38382;&#39064;&#30340;&#27491;&#30830;&#26631;&#20934;&#25439;&#22833;&#21644;&#27010;&#29575;&#12290;&#22312;&#26368;&#22810;1,000&#31181;&#31867;&#21035;&#30340;&#39046;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as \emph{properness}, which asserts that Bayes' rule is optimal. Recent works have sought to \emph{learn losses} and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps $\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between $\mathbb{R}^{C-1}$ and the projected probability simplex $\tilde{\Delta}^{C-1}$ by using monotonicity of gradients of convex functions. We present {\sc LegendreTron} as a novel and practical method that jointly learns \emph{proper canonical losses} and probabilities for multiclass problems. Tested on a benchmark of domains with up to 1,000 classes, our experimental results show that our method consistently ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#22312;&#23398;&#20064;&#20013;&#36873;&#25321;&#24178;&#20928;&#26679;&#26412;&#30340;&#26694;&#26550;Knockoffs-SPR&#65292;&#36890;&#36807;Scalable Penalized Regression&#65288;SPR&#65289;&#26041;&#27861;&#27169;&#22411;&#21270;&#32593;&#32476;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;Knockoff&#36807;&#28388;&#22120;&#25511;&#21046;&#20102;&#24178;&#20928;&#25968;&#25454;&#30340;&#35823;&#36873;&#25321;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00545</link><description>&lt;p&gt;
Knockoffs-SPR: &#26080;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26679;&#26412;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels. (arXiv:2301.00545v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#22312;&#23398;&#20064;&#20013;&#36873;&#25321;&#24178;&#20928;&#26679;&#26412;&#30340;&#26694;&#26550;Knockoffs-SPR&#65292;&#36890;&#36807;Scalable Penalized Regression&#65288;SPR&#65289;&#26041;&#27861;&#27169;&#22411;&#21270;&#32593;&#32476;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;Knockoff&#36807;&#28388;&#22120;&#25511;&#21046;&#20102;&#24178;&#20928;&#25968;&#25454;&#30340;&#35823;&#36873;&#25321;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#35757;&#32451;&#38598;&#36890;&#24120;&#20250;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#20013;&#65292;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#24178;&#20928;&#26679;&#26412;&#36873;&#25321;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24809;&#32602;&#22238;&#24402;&#65288;SPR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#32593;&#32476;&#29305;&#24449;&#21644;&#29420;&#28909;&#26631;&#31614;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;SPR&#20013;&#65292;&#36890;&#36807;&#22312;&#22238;&#24402;&#27169;&#22411;&#20013;&#27714;&#35299;&#30340;&#38646;&#22343;&#20540;&#28418;&#31227;&#21442;&#25968;&#26469;&#30830;&#23450;&#24178;&#20928;&#25968;&#25454;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;SPR&#33021;&#22815;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#33021;&#19981;&#20877;&#28385;&#36275;&#65307;&#24182;&#19988;&#19968;&#20123;&#22122;&#22768;&#25968;&#25454;&#34987;&#38169;&#35823;&#22320;&#36873;&#25321;&#20026;&#24178;&#20928;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Knockoff&#36807;&#28388;&#22120;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#25511;&#21046;&#36873;&#21462;&#30340;&#24178;&#20928;&#25968;&#25454;&#30340;&#35823;&#36873;&#25321;&#29575;&#65288;FSR&#65289;&#30340;Knockoffs-SPR&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#31639;&#27861;&#65292;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#20998;&#21106;&#20026;&#22810;&#20010;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
A noisy training set usually leads to the degradation of the generalization and robustness of neural networks. In this paper, we propose a novel theoretically guaranteed clean sample selection framework for learning with noisy labels. Specifically, we first present a Scalable Penalized Regression (SPR) method, to model the linear relation between network features and one-hot labels. In SPR, the clean data are identified by the zero mean-shift parameters solved in the regression model. We theoretically show that SPR can recover clean data under some conditions. Under general scenarios, the conditions may be no longer satisfied; and some noisy data are falsely selected as clean data. To solve this problem, we propose a data-adaptive method for Scalable Penalized Regression with Knockoff filters (Knockoffs-SPR), which is provable to control the False-Selection-Rate (FSR) in the selected clean data. To improve the efficiency, we further present a split algorithm that divides the whole trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#31867;&#21035;&#26631;&#31614;&#21644;&#22122;&#22768;&#22240;&#32032;&#30340;&#20381;&#36182;&#20851;&#31995;&#38543;&#22495;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15646</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21464;&#24615;&#65306;&#38024;&#23545;&#20855;&#26377;&#8220;&#34394;&#20551;&#8221;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#30340;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Invariance: Test-Time Label-Shift Adaptation for Distributions with "Spurious" Correlations. (arXiv:2211.15646v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#31867;&#21035;&#26631;&#31614;&#21644;&#22122;&#22768;&#22240;&#32032;&#30340;&#20381;&#36182;&#20851;&#31995;&#38543;&#22495;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#21487;&#33021;&#23545;&#39044;&#27979;&#27169;&#22411;p(y|x)&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#23384;&#22312;&#38468;&#21152;&#20803;&#25968;&#25454;&#26631;&#31614;&#65288;&#20363;&#22914;&#32452;&#26631;&#31614;&#65289;z&#30340;&#24773;&#20917;&#65292;&#35813;&#26631;&#31614;&#21487;&#20197;&#35828;&#26126;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20551;&#35774;&#25551;&#36848;&#31867;&#21035;&#26631;&#31614;y&#21644;&#8220;&#22122;&#22768;&#8221;&#22240;&#32032;z&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#20808;&#39564;&#20998;&#24067;p(y, z)&#21487;&#33021;&#20250;&#38543;&#30528;&#22495;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#36825;&#20123;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#20854;&#20013;&#19968;&#20010;&#21464;&#37327;&#30340;&#36793;&#38469;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20551;&#35774;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;p(x|y, z)&#22312;&#22495;&#38388;&#26159;&#19981;&#21464;&#30340;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#30456;&#24403;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#26631;&#31614;&#36716;&#31227;&#8221;&#20551;&#35774;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#20854;&#20013;&#26631;&#31614;&#29616;&#22312;&#20063;&#21253;&#25324;&#22122;&#22768;&#22240;&#32032;z&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#24212;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#26469;&#36866;&#24212;p(y, z)&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changes in the data distribution at test time can have deleterious effects on the performance of predictive models $p(y|x)$. We consider situations where there are additional meta-data labels (such as group labels), denoted by $z$, that can account for such changes in the distribution. In particular, we assume that the prior distribution $p(y, z)$, which models the dependence between the class label $y$ and the "nuisance" factors $z$, may change across domains, either due to a change in the correlation between these terms, or a change in one of their marginals. However, we assume that the generative model for features $p(x|y, z)$ is invariant across domains. We note that this corresponds to an expanded version of the widely used "label shift" assumption, where the labels now also include the nuisance factors $z$. Based on this observation, we propose a test-time label shift correction that adapts to changes in the joint distribution $p(y, z)$ using EM applied to unlabeled samples from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#21644;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#20854;&#21464;&#20998;&#28909;&#29366;&#24577;&#20272;&#35745;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.03803</link><description>&lt;p&gt;
&#37327;&#23376;&#27010;&#29575;&#21704;&#23494;&#39039;&#23398;&#20064;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum-probabilistic Hamiltonian learning for generative modelling &amp; anomaly detection. (arXiv:2211.03803v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#21644;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#20854;&#21464;&#20998;&#28909;&#29366;&#24577;&#20272;&#35745;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23396;&#31435;&#30340;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#20915;&#23450;&#20102;&#20854;&#21160;&#21147;&#23398;&#21644;&#29289;&#29702;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#20064;&#21644;&#21033;&#29992;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#20854;&#21464;&#20998;&#28909;&#29366;&#24577;&#20272;&#35745;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#37327;&#23376;&#21704;&#23494;&#39039;&#30340;&#27169;&#22411;&#26041;&#27861;&#23545;&#27169;&#25311;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#28151;&#21512;&#24577;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#21040;&#30340;&#21704;&#23494;&#39039;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#26679;&#26412;&#31867;&#22411;&#22312;&#34987;&#35270;&#20026;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#26102;&#21487;&#20197;&#24418;&#25104;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#37327;&#21270;&#26679;&#26412;&#31867;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35774;&#35745;&#29992;&#20110;&#22330;&#35770;&#35745;&#31639;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#21033;&#29992;&#65292;&#20174;&#32780;&#23558;&#29702;&#35770;&#26041;&#27861;&#24212;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hamiltonian of an isolated quantum mechanical system determines its dynamics and physical behaviour. This study investigates the possibility of learning and utilising a system's Hamiltonian and its variational thermal state estimation for data analysis techniques. For this purpose, we employ the method of Quantum Hamiltonian-Based Models for the generative modelling of simulated Large Hadron Collider data and demonstrate the representability of such data as a mixed state. In a further step, we use the learned Hamiltonian for anomaly detection, showing that different sample types can form distinct dynamical behaviours once treated as a quantum many-body system. We exploit these characteristics to quantify the difference between sample types. Our findings show that the methodologies designed for field theory computations can be utilised in machine learning applications to employ theoretical approaches in data analysis techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35777;&#26126;&#30340;&#38381;&#24335;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#19968;&#31867;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#28176;&#36827;&#24615;&#33021;&#65292;&#20026;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06591</link><description>&lt;p&gt;
&#20005;&#26684;&#30340;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rigorous dynamical mean field theory for stochastic gradient descent methods. (arXiv:2210.06591v2 [math-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35777;&#26126;&#30340;&#38381;&#24335;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#19968;&#31867;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#28176;&#36827;&#24615;&#33021;&#65292;&#20026;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#28176;&#36827;&#24615;&#33021;&#38381;&#24335;&#26041;&#31243;&#65292;&#35813;&#26041;&#27861;&#20174;&#39640;&#26031;&#25968;&#25454;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#23398;&#20064;&#20272;&#35745;&#22120;&#65288;&#20363;&#22914;M-&#20272;&#35745;&#22120;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;...&#65289;&#12290;&#36825;&#21253;&#25324;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25110;Nesterov&#21152;&#36895;&#12290;&#24471;&#21040;&#30340;&#26041;&#31243;&#19982;&#23558;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#26041;&#31243;&#31163;&#25955;&#21270;&#21518;&#24212;&#29992;&#20110;&#26799;&#24230;&#27969;&#26102;&#20135;&#29983;&#30340;&#26041;&#31243;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26126;&#30830;&#25551;&#36848;&#35760;&#24518;&#26680;&#22312;&#26377;&#25928;&#21160;&#21147;&#23398;&#20013;&#22914;&#20309;&#26500;&#24314;&#65292;&#24182;&#19988;&#21253;&#25324;&#38750;&#21487;&#20998;&#31163;&#30340;&#26356;&#26032;&#20989;&#25968;&#65292;&#20801;&#35768;&#20855;&#26377;&#38750;&#21333;&#20301;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#36890;&#29992;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;SGD&#26041;&#31243;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove closed-form equations for the exact high-dimensional asymptotics of a family of first order gradient-based methods, learning an estimator (e.g. M-estimator, shallow neural network, ...) from observations on Gaussian data with empirical risk minimization. This includes widely used algorithms such as stochastic gradient descent (SGD) or Nesterov acceleration. The obtained equations match those resulting from the discretization of dynamical mean-field theory (DMFT) equations from statistical physics when applied to gradient flow. Our proof method allows us to give an explicit description of how memory kernels build up in the effective dynamics, and to include non-separable update functions, allowing datasets with non-identity covariance matrices. Finally, we provide numerical implementations of the equations for SGD with generic extensive batch-size and with constant learning rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#24635;&#32467;&#19982;&#20998;&#31867;&#20026;&#19977;&#32452;&#12290;&#32435;&#20837;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26410;&#26469;&#38656;&#35201;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;MRL&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.04869</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-based Molecular Representation Learning. (arXiv:2207.04869v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#24635;&#32467;&#19982;&#20998;&#31867;&#20026;&#19977;&#32452;&#12290;&#32435;&#20837;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26410;&#26469;&#38656;&#35201;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;MRL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;(MRL)&#26159;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#21644;&#21270;&#23398;&#31185;&#23398;&#20043;&#38388;&#32852;&#31995;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#23427;&#23558;&#20998;&#23376;&#32534;&#30721;&#20026;&#25968;&#23383;&#21521;&#37327;&#65292;&#20445;&#30041;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#24615;&#36136;&#39044;&#27979;&#65289;&#12290;&#26368;&#36817;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#20998;&#23376;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#36825;&#20123;&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32435;&#20837;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;2D&#21644;3D&#20998;&#23376;&#22270;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#36755;&#20837;&#23558;MRL&#26041;&#27861;&#24635;&#32467;&#21644;&#20998;&#31867;&#20026;&#19977;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;MRL&#25903;&#25345;&#30340;&#19968;&#20123;&#20856;&#22411;&#21270;&#23398;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21015;&#20986;&#20102;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#22522;&#20934;&#21644;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#20851;&#20110;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;MRL&#27169;&#22411;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular representation learning (MRL) is a key step to build the connection between machine learning and chemical science. In particular, it encodes molecules as numerical vectors preserving the molecular structures and features, on top of which the downstream tasks (e.g., property prediction) can be performed. Recently, MRL has achieved considerable progress, especially in methods based on deep molecular graph learning. In this survey, we systematically review these graph-based molecular representation techniques, especially the methods incorporating chemical domain knowledge. Specifically, we first introduce the features of 2D and 3D molecular graphs. Then we summarize and categorize MRL methods into three groups based on their input. Furthermore, we discuss some typical chemical applications supported by MRL. To facilitate studies in this fast-developing area, we also list the benchmarks and commonly used datasets in the paper. Finally, we share our thoughts on future research dir
&lt;/p&gt;</description></item><item><title>CD-GAN&#26159;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#36965;&#24863;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#34701;&#21512;&#25216;&#26415;&#36827;&#23637;&#21644;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#20256;&#24863;&#22120;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#19981;&#21516;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2203.00948</link><description>&lt;p&gt;
CD-GAN:&#19968;&#31181;&#22522;&#20110;&#34701;&#21512;&#30340;&#24378;&#22823;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#26080;&#30417;&#30563;&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors. (arXiv:2203.00948v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00948
&lt;/p&gt;
&lt;p&gt;
CD-GAN&#26159;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#36965;&#24863;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#34701;&#21512;&#25216;&#26415;&#36827;&#23637;&#21644;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#20256;&#24863;&#22120;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#19981;&#21516;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29699;&#35266;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#21464;&#21270;&#26816;&#27979;&#26159;&#36890;&#36807;&#22810;&#26102;&#30456;&#22270;&#20687;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#22270;&#20687;&#30001;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#29978;&#33267;&#19981;&#21516;&#27169;&#24577;&#65288;&#20363;&#22914;&#20809;&#23398;&#65292;&#38647;&#36798;&#65289;&#30340;&#20256;&#24863;&#22120;&#33719;&#21462;&#12290; &#21363;&#20351;&#38480;&#21046;&#20026;&#20809;&#23398;&#27169;&#24577;&#65292;&#21482;&#35201;&#20256;&#24863;&#22120;&#20855;&#26377;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#65292;&#36825;&#39033;&#20219;&#21153;&#23601;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#36825;&#31181;&#25152;&#35859;&#30340;&#24322;&#26500;&#20809;&#23398;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#23558;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#24378;&#22823;&#30340;&#34701;&#21512;&#26694;&#26550;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28145;&#24230;&#23545;&#25239;&#32593;&#32476;&#65292;&#26088;&#22312;&#20107;&#20808;&#35774;&#35745;&#21644;&#35757;&#32451;&#20197;&#34701;&#21512;&#19968;&#23545;&#22810;&#27874;&#27573;&#20809;&#23398;&#22270;&#20687;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#26469;&#34917;&#20805;&#25191;&#34892;&#21464;&#21270;&#26816;&#27979;&#12290;&#29983;&#25104;&#30340;&#25972;&#20307;&#26550;&#26500;&#26412;&#36523;&#36981;&#24490;&#23545;&#25239;&#24615;&#31574;&#30053;&#65292;&#20854;&#20013;&#34701;&#21512;&#32593;&#32476;&#21644;&#20854;&#20182;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of Earth observation, the detection of changes is performed from multitemporal images acquired by sensors with possibly different spatial and/or spectral resolutions or even different modalities (e.g. optical, radar). Even limiting to the optical modality, this task has proved to be challenging as soon as the sensors have different spatial and/or spectral resolutions. This paper proposes a novel unsupervised change detection method dedicated to images acquired with such so-called heterogeneous optical sensors. This method capitalizes on recent advances which frame the change detection problem into a robust fusion framework. More precisely, we show that a deep adversarial network designed and trained beforehand to fuse a pair of multiband optical images can be easily complemented by a network with the same architecture to perform change detection. The resulting overall architecture itself follows an adversarial strategy where the fusion network and the additional network 
&lt;/p&gt;</description></item><item><title>NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2110.14053</link><description>&lt;p&gt;
NeuroBack: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14053
&lt;/p&gt;
&lt;p&gt;
NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#21040;&#35268;&#21010;&#12289;&#39564;&#35777;&#21644;&#23433;&#20840;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22686;&#24378;CDCL SAT&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20351;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#36827;&#34892;&#39057;&#32321;&#30340;&#22312;&#32447;&#27169;&#22411;&#25512;&#26029;&#12290;&#20026;&#20102;&#20351;GNN&#30340;&#25913;&#36827;&#21464;&#24471;&#23454;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroBack&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#20004;&#20010;&#27934;&#23519;&#19978;&#65306;&#65288;1&#65289;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#65288;&#29978;&#33267;&#20840;&#37096;&#65289;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65288;&#21363;&#20540;&#65289;&#23545;&#20110;CDCL SAT&#27714;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#65288;2&#65289;&#22312;SAT&#27714;&#35299;&#24320;&#22987;&#20043;&#21069;&#65292;&#21482;&#38656;&#26597;&#35810;&#19968;&#27425;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21363;&#21487;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31163;&#32447;&#27169;&#22411;&#25512;&#26029;&#20351;NeuroBack&#33021;&#22815;&#20165;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
&lt;/p&gt;</description></item></channel></rss>