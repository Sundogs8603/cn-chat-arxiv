<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65306;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#35760;&#20303;&#20102;&#38169;&#35823;&#21644;&#34394;&#20551;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.05371</link><description>&lt;p&gt;
&#37027;&#19981;&#26159;&#20320;&#30340;&#35760;&#24518;&#65292;&#23427;&#26159;&#21035;&#20154;&#30340;&#65306;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#35760;&#24518;&#20013;&#25773;&#25746;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65306;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#35760;&#20303;&#20102;&#38169;&#35823;&#21644;&#34394;&#20551;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#26159;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#20197;&#35760;&#20303;&#36807;&#21435;&#23545;&#35805;&#20013;&#30340;&#20449;&#24687;&#65292;&#20197;&#22686;&#21152;&#21709;&#24212;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#26426;&#22120;&#20154;&#34987;&#35774;&#35745;&#20026;&#20174;&#20854;&#23545;&#35805;&#20249;&#20276;&#20013;&#25552;&#21462;&#20010;&#20154;&#24615;&#36136;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#34920;&#26126;&#23545;&#29305;&#23450;&#39068;&#33394;&#30340;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35760;&#24518;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#20154;&#21487;&#20197;&#23558;&#20010;&#20154;&#38472;&#36848;&#19982;&#20449;&#24687;&#38472;&#36848;&#32467;&#21512;&#36215;&#26469;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#23558;&#20449;&#24687;&#38472;&#36848;&#19982;&#20010;&#20154;&#30693;&#35782;&#19968;&#36215;&#35760;&#24405;&#22312;&#20854;&#38271;&#26399;&#35760;&#24518;&#20013;&#12290;&#36825;&#24847;&#21619;&#30528;&#26426;&#22120;&#20154;&#21487;&#33021;&#34987;&#27450;&#39575;&#35760;&#20303;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#22312;&#22238;&#24518;&#19982;&#23545;&#35805;&#20027;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#26102;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;ParlAI&#24179;&#21488;&#23454;&#29616;&#30340;BlenderBot 2&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#28431;&#27934;&#65292;&#24182;&#22312;&#26356;&#36817;&#26399;&#12289;&#35268;&#27169;&#26356;&#22823;&#30340;BlenderBot 3&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#39044;&#35757;&#32451;&#20559;&#24046;&#65292;&#21363;&#36890;&#36807;&#25193;&#23637;&#25110;&#32553;&#23567;&#39592;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05369</link><description>&lt;p&gt;
&#19968;&#31181;&#24847;&#22806;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#25511;&#21046;&#39044;&#35757;&#32451;&#20559;&#24046;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36801;&#31227;&#65306;&#25193;&#23637;&#25110;&#32553;&#23567;&#20320;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation. (arXiv:2304.05369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#39044;&#35757;&#32451;&#20559;&#24046;&#65292;&#21363;&#36890;&#36807;&#25193;&#23637;&#25110;&#32553;&#23567;&#39592;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#20381;&#36182;&#20110;&#39044;&#25991;&#26412;&#20219;&#21153;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#30001;&#20110;&#36825;&#20010;&#20219;&#21153;&#19982;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#65292;&#22240;&#27492;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#23545;&#40784;&#25110;&#39044;&#35757;&#32451;&#20559;&#24046;&#12290;&#22312;SSL&#20013;&#24120;&#29992;&#30340;&#19968;&#20010;&#25216;&#24039;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#22312;&#39592;&#24178;&#32593;&#32476;&#20043;&#19978;&#28155;&#21152;&#19968;&#20010;&#23567;&#30340;&#25237;&#24433;&#26426;&#65288;&#36890;&#24120;&#26159;&#19968;&#20010;2&#25110;3&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#65292;&#20197;&#20351;&#28145;&#24230;&#32593;&#32476;&#26356;&#20855;&#25239;&#20559;&#24046;&#24615;&#12290;&#19982;&#20808;&#21069;&#30740;&#31350;&#25237;&#24433;&#26426;&#26550;&#26500;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#19968;&#20010;&#26356;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#39592;&#24178;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#25913;&#21464;&#39592;&#24178;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#20010;&#22359;&#30340;&#22823;&#23567;&#26469;&#25913;&#21464;&#20854;&#32500;&#25968;&#26159;&#19968;&#31181;&#26497;&#20854;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#36731;&#39044;&#35757;&#32451;&#20559;&#24046;&#12290;&#23427;&#26174;&#30528;&#25913;&#21892;&#20102;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19979;&#28216;&#20256;&#36755;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning (SSL) models rely on a pretext task to learn representations. Because this pretext task differs from the downstream tasks used to evaluate the performance of these models, there is an inherent misalignment or pretraining bias. A commonly used trick in SSL, shown to make deep networks more robust to such bias, is the addition of a small projector (usually a 2 or 3 layer multi-layer perceptron) on top of a backbone network during training. In contrast to previous work that studied the impact of the projector architecture, we here focus on a simpler, yet overlooked lever to control the information in the backbone representation. We show that merely changing its dimensionality -- by changing only the size of the backbone's very last block -- is a remarkably effective technique to mitigate the pretraining bias. It significantly improves downstream transfer performance for both Self-Supervised and Supervised pretrained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20559;&#22909;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.05366</link><description>&lt;p&gt;
&#12298;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12289;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24615;&#21450;&#24402;&#32435;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning. (arXiv:2304.05366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20559;&#22909;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#25351;&#20986;&#65292;&#27809;&#26377;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#65292;&#25110;&#32773;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#22312;&#22343;&#21248;&#20998;&#24067;&#30340;&#23398;&#20064;&#38382;&#39064;&#19978;&#24179;&#22343;&#31934;&#24230;&#36798;&#21040;&#23436;&#20840;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23450;&#29702;&#32463;&#24120;&#34987;&#24341;&#29992;&#26469;&#25903;&#25345;&#20010;&#21035;&#38382;&#39064;&#38656;&#35201;&#29305;&#21035;&#23450;&#21046;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#20960;&#20046;&#25152;&#26377;&#22343;&#21248;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#22797;&#26434;&#24615;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#19981;&#25104;&#27604;&#20363;&#22320;&#20135;&#29983;&#20302;&#22797;&#26434;&#24230;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20063;&#20855;&#26377;&#21516;&#26679;&#30340;&#20559;&#22909;&#65292;&#36825;&#31181;&#20559;&#22909;&#20351;&#29992;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#29305;&#23450;&#39046;&#22495;&#35774;&#35745;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#21487;&#20197;&#21387;&#32553;&#21508;&#31181;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#20808;&#35757;&#32451;&#21644;&#21363;&#20351;&#26159;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26356;&#21916;&#27426;&#29983;&#25104;&#20302;&#22797;&#26434;&#24230;&#30340;&#24207;&#21015;&#12290;&#23613;&#31649;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#20284;&#20046;&#34920;&#26126;&#21508;&#20010;&#38382;&#39064;&#38656;&#35201;&#19987;&#38376;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35299;&#37322;&#35828;&#65292;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we exp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2304.05365</link><description>&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#21527;&#65311;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20010;&#24615;&#21270;&#27835;&#30103;&#24207;&#21015;&#20197;&#25903;&#25345;&#29992;&#25143;&#37319;&#21462;&#26356;&#20581;&#24247;&#30340;&#34892;&#20026;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;&#28041;&#21450;&#21040;&#22522;&#20110;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#27963;&#21160;&#27700;&#24179;&#12289;&#20301;&#32622;&#31561;&#65289;&#22312;&#20309;&#26102;&#27835;&#30103;&#20197;&#21450;&#22914;&#20309;&#27835;&#30103;&#30340;&#20915;&#23450;&#12290;&#22312;&#32447;RL&#31639;&#27861;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#21382;&#21490;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20010;&#24615;&#21270;&#36825;&#20123;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35201;&#20915;&#23450;&#26159;&#21542;&#24212;&#22312;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#20248;&#21270;&#8221;&#24178;&#39044;&#20013;&#21253;&#21547;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25968;&#25454;&#35777;&#25454;&#65292;&#34920;&#26126;RL&#31639;&#27861;&#23454;&#38469;&#19978;&#27491;&#22312;&#23558;&#27835;&#30103;&#20010;&#24615;&#21270;&#36866;&#24212;&#20854;&#29992;&#25143;&#12290;&#30001;&#20110;RL&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#22312;&#26576;&#20123;&#29366;&#24577;&#19979;&#30340;&#23398;&#20064;&#24182;&#20351;&#29992;&#27492;&#23398;&#20064;&#26469;&#25552;&#20379;&#29305;&#23450;&#27835;&#30103;&#30340;&#33021;&#21147;&#20135;&#29983;&#35823;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#23450;&#20041;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#37325;&#22797;&#37319;&#26679;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#22312;&#32447;RL&#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#32422;&#26463;&#22495;&#30340;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#35825;&#23548;&#30340;&#23545;&#25968;&#38556;&#30861;&#24230;&#37327;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#20102;&#26426;&#22120;&#20154;&#21644;&#34507;&#30333;&#35774;&#35745;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.05364</link><description>&lt;p&gt;
&#32422;&#26463;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Constrained Domains. (arXiv:2304.05364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#32422;&#26463;&#22495;&#30340;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#35825;&#23548;&#30340;&#23545;&#25968;&#38556;&#30861;&#24230;&#37327;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#20102;&#26426;&#22120;&#20154;&#21644;&#34507;&#30333;&#35774;&#35745;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#26032;&#36817;&#28044;&#29616;&#30340;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#35821;&#38899;&#29983;&#25104;&#31561;&#20247;&#22810;&#39046;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23427;&#20204;&#30001;&#30772;&#22351;&#25968;&#25454;&#30340;&#21152;&#22122;&#36807;&#31243;&#21644;&#23450;&#20041;&#20026;&#21152;&#22122;&#25193;&#25955;&#30340;&#26102;&#38388;&#21453;&#28436;&#30340;&#21518;&#21521;&#38454;&#27573;&#32452;&#25104;&#12290;&#20197;&#36825;&#20123;&#25104;&#21151;&#20026;&#22522;&#30784;&#65292;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#40654;&#26364;&#27969;&#24418;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#35201;&#27714;&#22312;&#25152;&#26377;&#26102;&#38388;&#19978;&#23450;&#20041;&#27979;&#22320;&#32447;&#12290;&#34429;&#28982;&#35813;&#35774;&#32622;&#21253;&#25324;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#65292;&#20294;&#19981;&#21253;&#25324;&#30001;&#19981;&#31561;&#24335;&#32422;&#26463;&#38598;&#23450;&#20041;&#30340;&#27969;&#24418;&#65292;&#36825;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#21644;&#34507;&#30333;&#35774;&#35745;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#35825;&#23548;&#30340;&#23545;&#25968;&#38556;&#30861;&#24230;&#37327;&#30340;&#21152;&#22122;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#21152;&#22122;&#36807;&#31243;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#32422;&#26463;&#22495;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#32422;&#26463;&#22495;&#30340;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a recent class of generative models which achieve state-of-the-art results in many domains such as unconditional image generation and text-to-speech tasks. They consist of a noising process destroying the data and a backward stage defined as the time-reversal of the noising diffusion. Building on their success, diffusion models have recently been extended to the Riemannian manifold setting. Yet, these Riemannian diffusion models require geodesics to be defined for all times. While this setting encompasses many important applications, it does not include manifolds defined via a set of inequality constraints, which are ubiquitous in many scientific domains such as robotics and protein design. In this work, we introduce two methods to bridge this gap. First, we design a noising process based on the logarithmic barrier metric induced by the inequality constraints. Second, we introduce a noising process based on the reflected Brownian motion. As existing diffu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;&#65288;APL&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05361</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Polynomial Loss For Multi-Label Classification. (arXiv:2304.05361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;&#65288;APL&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20219;&#21153;&#34987;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;BCE&#65289;&#25439;&#22833;&#32463;&#24120;&#29992;&#20110;&#20248;&#21270;&#35774;&#35745;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32431;BCE&#25439;&#22833;&#26080;&#27861;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#20122;&#20248;&#12290;&#27492;&#22806;&#65292;&#20887;&#20313;&#36127;&#26679;&#21697;&#21644;&#32597;&#35265;&#27491;&#26679;&#21697;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;&#65288;APL&#65289;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;BCE&#25439;&#22833;&#36827;&#34892;&#27888;&#21202;&#23637;&#24320;&#65292;&#28982;&#21518;&#25913;&#21892;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#31995;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#38750;&#23545;&#31216;&#32858;&#28966;&#26426;&#21046;&#26469;&#35299;&#32806;&#26469;&#33258;&#36127;&#26679;&#26412;&#21644;&#27491;&#26679;&#26412;&#30340;&#26799;&#24230;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#65292;&#22810;&#39033;&#24335;&#31995;&#25968;&#21487;&#20197;&#37325;&#26032;&#26657;&#20934;&#38750;&#23545;&#31216;&#32858;&#28966;&#36229;&#21442;&#25968;&#12290;&#22312;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;APL&#25439;&#22833;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various tasks are reformulated as multi-label classification problems, in which the binary cross-entropy (BCE) loss is frequently utilized for optimizing well-designed models. However, the vanilla BCE loss cannot be tailored for diverse tasks, resulting in a suboptimal performance for different models. Besides, the imbalance between redundant negative samples and rare positive samples could degrade the model performance. In this paper, we propose an effective Asymmetric Polynomial Loss (APL) to mitigate the above issues. Specifically, we first perform Taylor expansion on BCE loss. Then we ameliorate the coefficients of polynomial functions. We further employ the asymmetric focusing mechanism to decouple the gradient contribution from the negative and positive samples. Moreover, we validate that the polynomial coefficients can recalibrate the asymmetric focusing hyperparameters. Experiments on relation extraction, text classification, and image classification show that our APL loss can 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21306;&#22359;&#38142;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#35753;&#29992;&#25143;&#36164;&#28304;&#36129;&#29486;&#21464;&#24471;&#21487;&#34892;&#65292;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05354</link><description>&lt;p&gt;
iDML: &#28608;&#21169;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
iDML: Incentivized Decentralized Machine Learning. (arXiv:2304.05354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05354
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#35753;&#29992;&#25143;&#36164;&#28304;&#36129;&#29486;&#21464;&#24471;&#21487;&#34892;&#65292;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21435;&#20013;&#24515;&#21270;&#21644;&#26426;&#20250;&#20027;&#20041;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20852;&#36215;&#65292;&#32456;&#31471;&#35774;&#22791;&#36234;&#26469;&#36234;&#38656;&#35201;&#20351;&#29992;&#20854;&#33258;&#24049;&#25910;&#38598;&#30340;&#20247;&#21253;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#36164;&#28304;&#28040;&#32791;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35282;&#24230;&#37117;&#26159;&#21487;&#21462;&#30340;&#12290;&#24403;&#35774;&#22791;&#30452;&#25509;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21463;&#30410;&#26102;&#65292;&#36164;&#28304;&#36129;&#29486;&#26159;&#34987;&#28608;&#21169;&#30340;&#65292;&#22240;&#20026;&#21327;&#20316;&#20250;&#20135;&#29983;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#35201;&#27714;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#20026;&#20165;&#20026;&#20182;&#20154;&#21463;&#30410;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#20026;&#19968;&#20010;&#26080;&#36275;&#36731;&#37325;&#30340;&#37051;&#23621;&#35757;&#32451;&#27169;&#22411;&#65289;&#36129;&#29486;&#33258;&#24049;&#30340;&#36164;&#28304;&#65288;&#20363;&#22914;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#25968;&#25454;&#65289;&#26102;&#65292;&#38656;&#35201;&#25552;&#20379;&#26126;&#30830;&#30340;&#28608;&#21169;&#26426;&#21046;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#29992;&#20110;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#21644;&#40723;&#21169;&#29992;&#25143;&#36164;&#28304;&#36129;&#29486;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising emergence of decentralized and opportunistic approaches to machine learning, end devices are increasingly tasked with training deep learning models on-devices using crowd-sourced data that they collect themselves. These approaches are desirable from a resource consumption perspective and also from a privacy preservation perspective. When the devices benefit directly from the trained models, the incentives are implicit contributing devices' resources are incentivized by the availability of the higher-accuracy model that results from collaboration. However, explicit incentive mechanisms must be provided when end-user devices are asked to contribute their resources (e.g., computation, communication, and data) to a task performed primarily for the benefit of others, e.g., training a model for a task that a neighbor device needs but the device owner is uninterested in. In this project, we propose a novel blockchain-based incentive mechanism for completely decentralized and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.05351</link><description>&lt;p&gt;
ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#38646;&#26679;&#26412;&#20998;&#26512;&#65306;&#21326;&#23572;&#34903;&#26032;&#25163;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#32929;&#24066;&#36208;&#21183;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#26377;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#25512;&#25991;&#21644;&#21382;&#21490;&#32929;&#31080;&#20215;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#31227;&#21160;&#39044;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#26159;&#19968;&#20010;&#8220;&#21326;&#23572;&#34903;&#26032;&#25163;&#8221;&#65292;&#22312;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#26041;&#38754;&#30340;&#25104;&#21151;&#26377;&#38480;&#65292;&#19981;&#20165;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#19981;&#22914;&#20351;&#29992;&#20215;&#26684;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#36825;&#26679;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#23613;&#31649;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#21644;&#25512;&#25991;&#30340;&#21253;&#21547;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#26356;&#19987;&#19994;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;RetinaNet&#27169;&#22411;&#38024;&#23545;&#37319;&#29992;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#31995;&#32479;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#36827;&#34892;&#26816;&#27979;&#21644;&#35745;&#25968;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20339;&#25928;&#26524;&#65292;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05339</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#30340;&#26816;&#27979;&#19982;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep-learning assisted detection and quantification of (oo)cysts of Giardia and Cryptosporidium on smartphone microscopy images. (arXiv:2304.05339v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;RetinaNet&#27169;&#22411;&#38024;&#23545;&#37319;&#29992;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#31995;&#32479;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#36827;&#34892;&#26816;&#27979;&#21644;&#35745;&#25968;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20339;&#25928;&#26524;&#65292;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29992;&#21463;&#24494;&#29983;&#29289;&#27745;&#26579;&#30340;&#39135;&#29289;&#21644;&#27700;&#27599;&#24180;&#36896;&#25104;&#25968;&#30334;&#19975;&#20154;&#27515;&#20129;&#12290;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#26174;&#24494;&#31995;&#32479;&#26159;&#19968;&#31181;&#20415;&#25658;&#12289;&#20302;&#25104;&#26412;&#21644;&#27604;&#20256;&#32479;&#30340;&#20142;&#22330;&#26174;&#24494;&#38236;&#26356;&#26131;&#25509;&#36817;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#30340;&#22270;&#20687;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#38656;&#35201;&#22521;&#35757;&#26377;&#32032;&#30340;&#25216;&#26415;&#20154;&#21592;&#36827;&#34892;&#25163;&#21160;&#22218;&#27873;&#35782;&#21035;&#65292;&#32780;&#36825;&#36890;&#24120;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#35937;&#26816;&#27979;&#33258;&#21160;&#26816;&#27979;&#21365;/&#26797;&#29366;&#20307;&#21487;&#33021;&#20026;&#27492;&#38480;&#21046;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#30340;&#25928;&#26524;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;&#34092;&#33756;&#26679;&#21697;&#20013;&#33719;&#21462;&#30340;&#26234;&#33021;&#25163;&#26426;&#21644;&#20142;&#22330;&#26174;&#24494;&#38236;&#22270;&#20687;&#12290;Faster RCNN&#12289;RetinaNet&#21644;You Only Look Once&#65288;YOLOv8s&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#26469;&#25506;&#32034;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#21365;/&#26797;&#29366;&#20307;&#65292;&#20294;RetinaNet&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20004;&#31181;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#26174;&#24494;&#31995;&#32479;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#33258;&#21160;&#26816;&#27979;&#21644;&#35745;&#25968;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The consumption of microbial-contaminated food and water is responsible for the deaths of millions of people annually. Smartphone-based microscopy systems are portable, low-cost, and more accessible alternatives for the detection of Giardia and Cryptosporidium than traditional brightfield microscopes. However, the images from smartphone microscopes are noisier and require manual cyst identification by trained technicians, usually unavailable in resource-limited settings. Automatic detection of (oo)cysts using deep-learning-based object detection could offer a solution for this limitation. We evaluate the performance of three state-of-the-art object detectors to detect (oo)cysts of Giardia and Cryptosporidium on a custom dataset that includes both smartphone and brightfield microscopic images from vegetable samples. Faster RCNN, RetinaNet, and you only look once (YOLOv8s) deep-learning models were employed to explore their efficacy and limitations. Our results show that while the deep-l
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36890;&#36807;&#25351;&#23450;&#20154;&#29289;&#35282;&#33394;&#65292;&#36755;&#20986;&#20250;&#28041;&#21450;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05335</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#20013;&#30340;&#27602;&#24615;&#65306;&#20998;&#26512;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05335
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36890;&#36807;&#25351;&#23450;&#20154;&#29289;&#35282;&#33394;&#65292;&#36755;&#20986;&#20250;&#28041;&#21450;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#27835;&#30103;&#12289;&#25945;&#32946;&#21644;&#23458;&#25143;&#26381;&#21153;&#31561;&#22810;&#31181;&#26381;&#21153;&#20013;&#12290;&#30001;&#20110;&#29992;&#25143;&#21253;&#25324;&#26377;&#37325;&#35201;&#20449;&#24687;&#38656;&#27714;&#30340;&#20154;&#65292;&#22914;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#23398;&#29983;&#25110;&#24739;&#32773;&#65292;&#22240;&#27492;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#26126;&#30830;&#20102;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#65292;&#36229;&#36807;&#21322;&#30334;&#19975;&#27425;Generation&#34987;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20026;ChatGPT&#25351;&#23450;&#19968;&#20010;&#20154;&#29289;&#35282;&#33394;&#65292;&#27604;&#22914;&#25331;&#20987;&#25163;&#31302;&#32597;&#40664;&#24503;&#183;&#38463;&#37324;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#20135;&#29983;&#30340;&#27602;&#24615;&#12290;&#26681;&#25454;&#25351;&#23450;&#32473;ChatGPT&#30340;&#35282;&#33394;&#65292;&#20854;&#27602;&#24615;&#21487;&#33021;&#20250;&#22686;&#21152;&#21040;6&#20493;&#65292;&#20854;&#36755;&#20986;&#20250;&#28041;&#21450;&#19981;&#27491;&#30830;&#30340;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#30340;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#36825;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#25439;&#23475;&#20154;&#29289;&#35282;&#33394;&#30340;&#21517;&#35465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#24352;&#37327;&#33609;&#22270;&#26469;&#36817;&#20284;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#25216;&#26415;&#35299;&#20915;&#32447;&#24615;&#26041;&#31243;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#20854;&#31639;&#27861;&#22797;&#26434;&#24230;&#22312;&#39640;&#32500;&#23494;&#24230;&#32500;&#24230;&#19978;&#21576;&#32447;&#24615;&#35268;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.05305</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#24352;&#37327;&#33609;&#22270;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling via Hierarchical Tensor Sketching. (arXiv:2304.05305v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#24352;&#37327;&#33609;&#22270;&#26469;&#36817;&#20284;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#25216;&#26415;&#35299;&#20915;&#32447;&#24615;&#26041;&#31243;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#20854;&#31639;&#27861;&#22797;&#26434;&#24230;&#22312;&#39640;&#32500;&#23494;&#24230;&#32500;&#24230;&#19978;&#21576;&#32447;&#24615;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#39564;&#20998;&#24067;&#26469;&#36817;&#20284;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#20998;&#23618;&#24352;&#37327;&#32593;&#32476;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#25216;&#26415;&#65292;&#24182;&#28041;&#21450;&#22312;&#35813;&#24352;&#37327;&#32593;&#32476;&#20013;&#35299;&#32447;&#24615;&#26041;&#31243;&#20197;&#33719;&#24471;&#24352;&#37327;&#26680;&#24515;&#12290;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#22312;&#39640;&#32500;&#23494;&#24230;&#30340;&#32500;&#24230;&#19978;&#21576;&#32447;&#24615;&#35268;&#27169;&#12290;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#23545;&#20272;&#35745;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#27492;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical tensor-network approach for approximating high-dimensional probability density via empirical distribution. This leverages randomized singular value decomposition (SVD) techniques and involves solving linear equations for tensor cores in this tensor network. The complexity of the resulting algorithm scales linearly in the dimension of the high-dimensional density. An analysis of estimation error demonstrates the effectiveness of this method through several numerical experiments.
&lt;/p&gt;</description></item><item><title>TACOS &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#36895;&#24230;&#25552;&#39640;&#20102; 3.73 &#20493;&#65292;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#21482;&#38656; 6.1 &#20998;&#38047;&#12290;</title><link>http://arxiv.org/abs/2304.05301</link><description>&lt;p&gt;
TACOS: &#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#38598;&#21512;&#31639;&#27861;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training. (arXiv:2304.05301v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05301
&lt;/p&gt;
&lt;p&gt;
TACOS &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#36895;&#24230;&#25552;&#39640;&#20102; 3.73 &#20493;&#65292;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#21482;&#38656; 6.1 &#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#35759;&#26159;&#20998;&#24067;&#24335;&#35757;&#32451;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#36816;&#34892;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#31639;&#27861;&#23545;&#20110;&#20248;&#21270;&#36890;&#35759;&#24615;&#33021;&#20197;&#26368;&#23567;&#21270;&#25317;&#22622;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#65292;&#27492;&#31867;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#19968;&#23567;&#37096;&#20998;&#31616;&#21333;&#25299;&#25169;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#35757;&#32451;&#38598;&#32676;&#20013;&#37319;&#29992;&#30340;&#25299;&#25169;&#32467;&#26500;&#24182;&#22788;&#29702;&#30001;&#20110;&#32593;&#32476;&#25925;&#38556;&#32780;&#20135;&#29983;&#30340;&#19981;&#35268;&#21017;&#25299;&#25169;&#32467;&#26500;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102; TACOS&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#27604;&#22522;&#32447;&#31639;&#27861;&#24555;&#20102; 3.73 &#20493;&#65292;&#24182;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#20165;&#38656; 6.1 &#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective communications are an indispensable part of distributed training. Running a topology-aware collective algorithm is crucial for optimizing communication performance by minimizing congestion. Today such algorithms only exist for a small set of simple topologies, limiting the topologies employed in training clusters and handling irregular topologies due to network failures. In this paper, we propose TACOS, an automated topology-aware collective synthesizer for arbitrary input network topologies. TACOS synthesized 3.73x faster All-Reduce algorithm over baselines, and synthesized collective algorithms for 512-NPU system in just 6.1 minutes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20405;&#20837;&#24335;&#31435;&#20307;&#30456;&#26426;&#30340;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;SiamMask&#36827;&#34892;&#39046;&#20808;&#36710;&#36742;&#36319;&#36394;&#65292;&#37319;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#21644;LightGBM&#36827;&#34892;&#36317;&#31163;&#24179;&#28369;&#21644;&#36895;&#24230;&#20272;&#35745;&#65292;&#20248;&#20110;SUBARU&#22270;&#20687;&#35782;&#21035;&#25361;&#25112;&#36187;&#30340;&#22522;&#20934;RMSE&#12290;</title><link>http://arxiv.org/abs/2304.05298</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#20405;&#20837;&#24335;&#31435;&#20307;&#30456;&#26426;&#30340;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of Vehicular Velocity based on Non-Intrusive stereo camera. (arXiv:2304.05298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20405;&#20837;&#24335;&#31435;&#20307;&#30456;&#26426;&#30340;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;SiamMask&#36827;&#34892;&#39046;&#20808;&#36710;&#36742;&#36319;&#36394;&#65292;&#37319;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#21644;LightGBM&#36827;&#34892;&#36317;&#31163;&#24179;&#28369;&#21644;&#36895;&#24230;&#20272;&#35745;&#65292;&#20248;&#20110;SUBARU&#22270;&#20687;&#35782;&#21035;&#25361;&#25112;&#36187;&#30340;&#22522;&#20934;RMSE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20405;&#20837;&#24335;&#31435;&#20307;&#30456;&#26426;&#30340;&#39046;&#20808;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;SiamMask&#36827;&#34892;&#39046;&#20808;&#36710;&#36742;&#36319;&#36394;&#65292;&#37319;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#24179;&#28369;&#20174;&#35270;&#24046;&#22270;&#20013;&#39044;&#27979;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;LightGBM&#36827;&#34892;&#39046;&#20808;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;0.416&#30340;RMSE&#65292;&#20248;&#20110;&#30001;SUBARU&#22270;&#20687;&#35782;&#21035;&#25361;&#25112;&#36187;&#25552;&#20379;&#30340;0.582&#30340;&#22522;&#20934;RMSE&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a modular approach for the estimation of a leading vehicle's velocity based on a non-intrusive stereo camera where SiamMask is used for leading vehicle tracking, Kernel Density estimate (KDE) is used to smooth the distance prediction from a disparity map, and LightGBM is used for leading vehicle velocity estimation.  Our approach yields an RMSE of 0.416 which outperforms the baseline RMSE of 0.582 for the SUBARU Image Recognition Challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#38754;&#21521;&#38750;&#32422;&#26463;&#29615;&#22659;&#30340;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#65292;&#21253;&#25324;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.05295</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#32422;&#26463;&#29615;&#22659;&#30340;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Object Detection Techniques in Unconstrained Environments. (arXiv:2304.05295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#38754;&#21521;&#38750;&#32422;&#26463;&#29615;&#22659;&#30340;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#65292;&#21253;&#25324;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#21644;&#23450;&#20301;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#12290;&#28145;&#24230;&#23398;&#20064;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#35937;&#26816;&#27979;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23545;&#38750;&#32422;&#26463;&#29615;&#22659;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21508;&#31181;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#38750;&#32422;&#26463;&#29615;&#22659;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection is a crucial task in computer vision that aims to identify and localize objects in images or videos. The recent advancements in deep learning and Convolutional Neural Networks (CNNs) have significantly improved the performance of object detection techniques. This paper presents a comprehensive study of object detection techniques in unconstrained environments, including various challenges, datasets, and state-of-the-art approaches. Additionally, we present a comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we provide some future research directions to further improve object detection in unconstrained environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.05294</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#36873;&#25321;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24378;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20581;&#30340;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#21019;&#24314;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#12289;&#28508;&#22312;&#20132;&#20114;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#38598;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#65288;M&#65289;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Tigramite Python&#21253;&#20013;&#23454;&#29616;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;PC1&#25110;PCMCI&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#25512;&#26029;&#22240;&#26524;&#22270;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#23558;&#21097;&#20313;&#22240;&#26524;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;ML&#27169;&#22411;&#65288;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65289;&#39044;&#27979;&#30446;&#26631;&#20043;&#21069;&#65292;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#35199;&#22826;&#24179;&#27915;&#28909;&#24102;&#22320;&#21306;&#30340;&#22320;&#38663;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#31216;&#31561;&#21464;GNN&#27169;&#22411;EuclidNet&#65292;&#36866;&#29992;&#20110;&#24102;&#30005;&#31890;&#23376;&#36319;&#36394;&#65292;&#36890;&#36807;&#26059;&#36716;&#23545;&#31216;&#24615;&#22788;&#29702;&#30896;&#25758;&#20107;&#20214;&#30340;&#22270;&#34920;&#31034;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;EuclidNet&#22312;&#23567;&#27169;&#22411;&#35268;&#27169;&#19979;&#27604;&#38750;&#31561;&#21464;&#22522;&#20934;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.05293</link><description>&lt;p&gt;
&#24102;&#30005;&#31890;&#23376;&#36319;&#36394;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Graph Neural Networks for Charged Particle Tracking. (arXiv:2304.05293v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#31216;&#31561;&#21464;GNN&#27169;&#22411;EuclidNet&#65292;&#36866;&#29992;&#20110;&#24102;&#30005;&#31890;&#23376;&#36319;&#36394;&#65292;&#36890;&#36807;&#26059;&#36716;&#23545;&#31216;&#24615;&#22788;&#29702;&#30896;&#25758;&#20107;&#20214;&#30340;&#22270;&#34920;&#31034;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;EuclidNet&#22312;&#23567;&#27169;&#22411;&#35268;&#27169;&#19979;&#27604;&#38750;&#31561;&#21464;&#22522;&#20934;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20854;&#25552;&#39640;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#28508;&#21147;&#32780;&#22312;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#29305;&#24615;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#20419;&#20351;&#23545;&#23545;&#31216;&#31561;&#21464;&#30340;&#32467;&#26500;&#36827;&#34892;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EuclidNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#31216;&#31561;&#21464;GNN&#65292;&#36866;&#29992;&#20110;&#24102;&#30005;&#31890;&#23376;&#36319;&#36394;&#12290;EuclidNet&#21033;&#29992;&#30896;&#25758;&#20107;&#20214;&#30340;&#22270;&#34920;&#31034;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#20197;&#25506;&#27979;&#22120;&#30340;&#26463;&#32447;&#36724;&#20026;&#20013;&#24515;&#30340;&#26059;&#36716;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;TrackML&#25968;&#25454;&#38598;&#19978;&#23558;EuclidNet&#19982;&#26368;&#20808;&#36827;&#30340;Interaction Network&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#27169;&#25311;&#39044;&#26399;&#22312;&#39640;&#20142;&#24230;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#65288;HL-LHC&#65289;&#20013;&#20986;&#29616;&#30340;&#39640;&#22534;&#21472;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#27169;&#22411;&#35268;&#27169;&#65288;&lt;1000&#20010;&#21442;&#25968;&#65289;&#19979;&#65292;EuclidNet&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#38750;&#31561;&#21464;&#22522;&#20934;&#12290;&#35813;&#30740;&#31350;&#20026;&#23558;&#26469;&#25506;&#32034;&#26356;&#20855;&#36164;&#28304;&#25928;&#29575;&#30340;&#24102;&#30005;&#31890;&#23376;&#36319;&#36394;GNN&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained traction in high-energy physics (HEP) for their potential to improve accuracy and scalability. However, their resource-intensive nature and complex operations have motivated the development of symmetry-equivariant architectures. In this work, we introduce EuclidNet, a novel symmetry-equivariant GNN for charged particle tracking. EuclidNet leverages the graph representation of collision events and enforces rotational symmetry with respect to the detector's beamline axis, leading to a more efficient model. We benchmark EuclidNet against the state-of-the-art Interaction Network on the TrackML dataset, which simulates high-pileup conditions expected at the High-Luminosity Large Hadron Collider (HL-LHC). Our results show that EuclidNet achieves near-state-of-the-art performance at small model scales (&lt;1000 parameters), outperforming the non-equivariant benchmarks. This study paves the way for future investigations into more resource-efficient GNN mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#38590;&#24230;&#24863;&#30693;&#30340;&#22686;&#37327;&#23398;&#20064;&#21442;&#25968;&#20998;&#37197;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#23398;&#20064;&#38590;&#24230;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#20998;&#37197;&#25110;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#22312;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.05288</link><description>&lt;p&gt;
&#20219;&#21153;&#38590;&#24230;&#24863;&#30693;&#30340;&#22686;&#37327;&#23398;&#20064;&#21442;&#25968;&#20998;&#37197;&#19982;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Task Difficulty Aware Parameter Allocation &amp; Regularization for Lifelong Learning. (arXiv:2304.05288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#38590;&#24230;&#24863;&#30693;&#30340;&#22686;&#37327;&#23398;&#20064;&#21442;&#25968;&#20998;&#37197;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#23398;&#20064;&#38590;&#24230;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#20998;&#37197;&#25110;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#22312;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#27491;&#21017;&#21270;&#25110;&#20998;&#37197;&#26041;&#27861;&#23545;&#20110;&#20811;&#26381;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24207;&#21015;&#20013;&#22343;&#21248;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#38590;&#24230;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#24403;&#23398;&#20064;&#19982;&#24050;&#23398;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#26102;&#65292;&#21442;&#25968;&#27491;&#21017;&#21270;&#26041;&#27861;&#20250;&#38754;&#20020;&#26174;&#30528;&#30340;&#36951;&#24536;&#65292;&#32780;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#22312;&#23398;&#20064;&#31616;&#21333;&#20219;&#21153;&#26102;&#21017;&#38754;&#20020;&#19981;&#24517;&#35201;&#30340;&#21442;&#25968;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21442;&#25968;&#20998;&#37197;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;PAR&#65289;&#65292;&#20854;&#21487;&#20197;&#26681;&#25454;&#23398;&#20064;&#38590;&#24230;&#20174;&#21442;&#25968;&#20998;&#37197;&#21644;&#27491;&#21017;&#21270;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;&#19968;&#20010;&#24050;&#32463;&#23398;&#20064;&#30456;&#20851;&#20219;&#21153;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#19968;&#20010;&#20219;&#21153;&#20250;&#21464;&#24471;&#23481;&#26131;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#21407;&#22411;&#36317;&#31163;&#30340;&#31163;&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#26032;&#20219;&#21153;&#30340;&#29305;&#24449;&#26469;&#24230;&#37327;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#25928;&#24615;&#30456;&#20851;&#24615;&#24863;&#30693;&#37319;&#26679;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation &amp; Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;softmax&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#23458;&#25143;&#31471;&#36951;&#24536;&#24182;&#23545;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.05260</link><description>&lt;p&gt;
&#25511;&#21046;&#32852;&#37030;&#23398;&#20064;&#20013;&#36951;&#24536;&#38382;&#39064;&#30340;&#37325;&#26032;&#21152;&#26435;Softmax&#20132;&#21449;&#29109;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning. (arXiv:2304.05260v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;softmax&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#23458;&#25143;&#31471;&#36951;&#24536;&#24182;&#23545;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#32858;&#21512;&#22312;&#19968;&#32452;&#29420;&#31435;&#23458;&#25143;&#33410;&#28857;&#35745;&#31639;&#30340;&#27169;&#22411;&#26356;&#26032;&#26469;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#20043;&#21069;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#25191;&#34892;&#22810;&#20010;&#26799;&#24230;&#27493;&#39588;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#30340;&#26412;&#22320;&#30446;&#26631;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#36807;&#24230;&#20943;&#23569;&#20854;&#33258;&#24049;&#30340;&#26412;&#22320;&#30446;&#26631;&#65292;&#20351;&#20854;&#19982;&#20840;&#23616;&#35299;&#20998;&#27495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#36827;&#34892;&#20462;&#25913;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;softmax&#30340;logits&#20197;&#35745;&#31639;&#25439;&#22833;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#23545;&#26469;&#33258;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#19981;&#22312;&#23458;&#25143;&#31471;&#26631;&#31614;&#38598;&#20013;&#30340;&#31867;&#21035;&#20813;&#21463;&#31361;&#28982;&#30340;&#34920;&#31034;&#21464;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#32531;&#35299;&#23458;&#25143;&#31471;&#36951;&#24536;&#24182;&#23545;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning, a global model is learned by aggregating model updates computed at a set of independent client nodes, to reduce communication costs multiple gradient steps are performed at each node prior to aggregation. A key challenge in this setting is data heterogeneity across clients resulting in differing local objectives which can lead clients to overly minimize their own local objective, diverging from the global solution. We demonstrate that individual client models experience a catastrophic forgetting with respect to data from other clients and propose an efficient approach that modifies the cross-entropy objective on a per-client basis by re-weighting the softmax logits prior to computing the loss. This approach shields classes outside a client's label set from abrupt representation change and we empirically demonstrate it can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms. Our method is particularly beneficia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.05257</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#23398;&#29983;&#21019;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#22312;&#32473;&#23450;&#27979;&#35797;&#20013;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;RIIID&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20316;&#20026;&#35299;&#30721;&#22120;&#36755;&#20837;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;LightGBM&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
&lt;/p&gt;</description></item><item><title>OpenAL&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#29616;&#23454;&#20219;&#21153;&#19978;&#36731;&#26494;&#36816;&#34892;&#21644;&#27604;&#36739;&#37319;&#26679;AL&#31574;&#30053;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#30340;&#19968;&#27425;&#24615;&#29305;&#24615;&#65292;&#20174;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.05246</link><description>&lt;p&gt;
OpenAL: &#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#35780;&#20272;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OpenAL: Evaluation and Interpretation of Active Learning Strategies. (arXiv:2304.05246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05246
&lt;/p&gt;
&lt;p&gt;
OpenAL&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#29616;&#23454;&#20219;&#21153;&#19978;&#36731;&#26494;&#36816;&#34892;&#21644;&#27604;&#36739;&#37319;&#26679;AL&#31574;&#30053;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#30340;&#19968;&#27425;&#24615;&#29305;&#24615;&#65292;&#20174;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#38754;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#25991;&#29486;&#65292;&#20294;&#26159;&#36824;&#27809;&#26377;&#19968;&#31181;&#20840;&#38754;&#19988;&#24320;&#25918;&#30340;&#22522;&#20934;&#21487;&#20197;&#26377;&#25928;&#22320;&#27604;&#36739;&#25552;&#20986;&#30340;&#37319;&#26679;&#22120;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23454;&#39564;&#35774;&#32622;&#30340;&#21464;&#24322;&#24615;&#20351;&#24471;&#36873;&#25321;&#37319;&#26679;&#31574;&#30053;&#21464;&#24471;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20027;&#21160;&#23398;&#20064;&#23454;&#39564;&#30340;&#19968;&#27425;&#24615;&#29305;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OpenAL&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#29616;&#23454;&#20219;&#21153;&#19978;&#36731;&#26494;&#36816;&#34892;&#21644;&#27604;&#36739;&#37319;&#26679;AL&#31574;&#30053;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36824;&#21152;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#20102;&#35299;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#19968;&#20123;&#37319;&#26679;&#22120;&#20248;&#20110;&#20854;&#20182;&#37319;&#26679;&#22120;&#12290;&#26368;&#21518;&#65292;&#20174;&#19994;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25552;&#20132;&#33258;&#24049;&#30340;AL&#37319;&#26679;&#22120;&#36731;&#26494;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the vast body of literature on Active Learning (AL), there is no comprehensive and open benchmark allowing for efficient and simple comparison of proposed samplers. Additionally, the variability in experimental settings across the literature makes it difficult to choose a sampling strategy, which is critical due to the one-off nature of AL experiments. To address those limitations, we introduce OpenAL, a flexible and open-source framework to easily run and compare sampling AL strategies on a collection of realistic tasks. The proposed benchmark is augmented with interpretability metrics and statistical analysis methods to understand when and why some samplers outperform others. Last but not least, practitioners can easily extend the benchmark by submitting their own AL samplers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;Softmax&#20989;&#25968;r-softmax&#65292;&#21487;&#20197;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#26696;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.05243</link><description>&lt;p&gt;
r-softmax: &#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#29575;&#30340;&#24191;&#20041;Softmax
&lt;/p&gt;
&lt;p&gt;
r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;Softmax&#20989;&#25968;r-softmax&#65292;&#21487;&#20197;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#26696;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#23558;&#27169;&#22411;&#25552;&#20379;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#21487;&#20998;&#21106;&#30340;&#26041;&#38754;&#12290;&#34429;&#28982;softmax&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#36890;&#24120;&#25509;&#21463;&#30340;&#27010;&#29575;&#26144;&#23556;&#20989;&#25968;&#65292;&#20294;&#23427;&#19981;&#33021;&#36820;&#22238;&#31232;&#30095;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#24635;&#26159;&#23558;&#27491;&#27010;&#29575;&#20998;&#25955;&#21040;&#25152;&#26377;&#20301;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;r-softmax&#65292;&#36825;&#26159;softmax&#30340;&#19968;&#31181;&#20462;&#25913;&#65292;&#23427;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#27010;&#29575;&#26144;&#23556;&#20989;&#25968;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#26426;&#21046;&#26469;&#25511;&#21046;&#36755;&#20986;&#31232;&#30095;&#24230;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;r-softmax&#20248;&#20110;&#20854;&#20182;&#31232;&#30095;&#30340;softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#19988;&#19982;&#21407;&#22987;&#30340;softmax&#30456;&#27604;&#20855;&#26377;&#39640;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36824;&#23558;r-softmax&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#34109;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#32473;&#23450;&#32959;&#30244;&#25513;&#27169;&#26465;&#20214;&#19979;&#30340;&#21512;&#25104; GI &#24687;&#32905;&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26080;&#38480;&#37327;&#39640;&#20445;&#30495;&#21512;&#25104;&#24687;&#32905;&#22270;&#20687;&#65292;&#20026;&#28040;&#38500;&#20869;&#38236;&#35786;&#26029;&#20013;&#30340;&#26377;&#38480;&#26631;&#27880;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05233</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#34109;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#30340;&#29983;&#25104;&#32963;&#32928;&#24687;&#32905;&#22270;&#20687;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mask-conditioned latent diffusion for generating gastrointestinal polyp images. (arXiv:2304.05233v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#34109;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#32473;&#23450;&#32959;&#30244;&#25513;&#27169;&#26465;&#20214;&#19979;&#30340;&#21512;&#25104; GI &#24687;&#32905;&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26080;&#38480;&#37327;&#39640;&#20445;&#30495;&#21512;&#25104;&#24687;&#32905;&#22270;&#20687;&#65292;&#20026;&#28040;&#38500;&#20869;&#38236;&#35786;&#26029;&#20013;&#30340;&#26377;&#38480;&#26631;&#27880;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20869;&#31397;&#38236;&#35786;&#26029;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#26377;&#38480;&#27880;&#37322;&#38382;&#39064;&#12290;&#36825;&#20123;&#38480;&#21046;&#26159;&#30001;&#21307;&#23398;&#39046;&#22495;&#30340;&#39640;&#20445;&#23494;&#24615;&#21644;&#38656;&#27714;&#23548;&#33268;&#30340;&#65292;&#38656;&#35201;&#20174;&#19987;&#23478;&#33719;&#21462;&#23494;&#38598;&#30340;&#65292;&#26114;&#36149;&#30340;&#21307;&#23398;&#25968;&#25454;&#26631;&#27880;&#36807;&#31243;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#30001;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#21644;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPM)&#30340;&#36827;&#23637;&#65292;&#22270;&#20687;&#21512;&#25104;&#36817;&#24180;&#26469;&#26377;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;&#26032;&#22411; DPM &#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110; GAN&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214; DPM &#26694;&#26550;&#65292;&#20197;&#29983;&#25104;&#32473;&#23450;&#30340;&#29983;&#25104;&#20998;&#21106;&#25513;&#27169;&#25152;&#38480;&#21046;&#30340;&#21512;&#25104; GI &#24687;&#32905;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#26080;&#38480;&#25968;&#37327;&#30340;&#20855;&#26377;&#30456;&#24212;&#32905;&#30244;&#30495;&#23454;&#26631;&#27880;&#25513;&#27169;&#30340;&#39640;&#20445;&#30495;&#21512;&#25104;&#32905;&#30244;&#22270;&#20687;&#12290;&#20026;&#20102;&#27979;&#35797;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20108;&#20540;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to take advantage of AI solutions in endoscopy diagnostics, we must overcome the issue of limited annotations. These limitations are caused by the high privacy concerns in the medical field and the requirement of getting aid from experts for the time-consuming and costly medical data annotation process. In computer vision, image synthesis has made a significant contribution in recent years as a result of the progress of generative adversarial networks (GANs) and diffusion probabilistic models (DPM). Novel DPMs have outperformed GANs in text, image, and video generation tasks. Therefore, this study proposes a conditional DPM framework to generate synthetic GI polyp images conditioned on given generated segmentation masks. Our experimental results show that our system can generate an unlimited number of high-fidelity synthetic polyp images with the corresponding ground truth masks of polyps. To test the usefulness of the generated data, we trained binary image segmentation model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#20102;Nextdoor&#31038;&#20132;&#32593;&#32476;&#65292;&#21457;&#29616;&#19981;&#21516;&#25910;&#20837;&#27700;&#24179;&#30340;&#31038;&#21306;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#34892;&#20026;&#19981;&#21516;&#65292;&#26356;&#23500;&#35029;&#30340;&#31038;&#21306;&#24773;&#24863;&#26356;&#31215;&#26497;&#65292;&#26356;&#22810;&#22320;&#35752;&#35770;&#29359;&#32618;&#65292;&#23613;&#31649;&#23454;&#38469;&#29359;&#32618;&#29575;&#35201;&#20302;&#24471;&#22810;&#65292;&#21516;&#26102;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#33021;&#22815;&#39044;&#27979;&#25910;&#20837;&#21644;&#19981;&#24179;&#31561;&#12290;</title><link>http://arxiv.org/abs/2304.05232</link><description>&lt;p&gt;
&#37051;&#37324;&#29356;&#19982;&#34903;&#22836;&#27969;&#28010;&#27721;&#65306;Nextdoor&#31038;&#20132;&#32593;&#32476;&#20013;&#30495;&#23454;&#19990;&#30028;&#19981;&#24179;&#31561;&#30340;&#22312;&#32447;&#20307;&#29616;
&lt;/p&gt;
&lt;p&gt;
Lady and the Tramp Nextdoor: Online Manifestations of Real-World Inequalities in the Nextdoor Social Network. (arXiv:2304.05232v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#20102;Nextdoor&#31038;&#20132;&#32593;&#32476;&#65292;&#21457;&#29616;&#19981;&#21516;&#25910;&#20837;&#27700;&#24179;&#30340;&#31038;&#21306;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#34892;&#20026;&#19981;&#21516;&#65292;&#26356;&#23500;&#35029;&#30340;&#31038;&#21306;&#24773;&#24863;&#26356;&#31215;&#26497;&#65292;&#26356;&#22810;&#22320;&#35752;&#35770;&#29359;&#32618;&#65292;&#23613;&#31649;&#23454;&#38469;&#29359;&#32618;&#29575;&#35201;&#20302;&#24471;&#22810;&#65292;&#21516;&#26102;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#33021;&#22815;&#39044;&#27979;&#25910;&#20837;&#21644;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#20837;&#24433;&#21709;&#30528;&#29983;&#27963;&#20013;&#24456;&#22810;&#36873;&#25321;&#65292;&#20174;&#20581;&#24247;&#21040;&#25945;&#32946;&#12290;&#35768;&#22810;&#30740;&#31350;&#37117;&#21033;&#29992;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#25968;&#25454;&#30740;&#31350;&#27492;&#38382;&#39064;&#12290;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#19981;&#21516;&#25910;&#20837;&#27700;&#24179;&#26159;&#21542;&#23548;&#33268;&#19981;&#21516;&#30340;&#22312;&#32447;&#34892;&#20026;&#65311;&#25105;&#20204;&#35777;&#26126;&#20102;&#30830;&#23454;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;Nextdoor&#31038;&#20132;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#22312;&#32654;&#22269;&#30340;64,283&#20010;&#31038;&#21306;&#21644;&#33521;&#22269;&#30340;3,325&#20010;&#31038;&#21306;&#25910;&#38598;&#20102;2.6&#19975;&#31687;&#24086;&#23376;&#65292;&#20197;&#30740;&#31350;&#22312;&#32447;&#35805;&#35821;&#26159;&#21542;&#21453;&#26144;&#20102;&#31038;&#21306;&#30340;&#25910;&#20837;&#21644;&#25910;&#20837;&#19981;&#24179;&#31561;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#25910;&#20837;&#30340;&#31038;&#21306;&#30340;&#24086;&#23376;&#30830;&#23454;&#19981;&#21516;&#65292;&#27604;&#22914;&#26356;&#23500;&#35029;&#30340;&#31038;&#21306;&#24773;&#24863;&#26356;&#31215;&#26497;&#65292;&#26356;&#22810;&#22320;&#35752;&#35770;&#29359;&#32618;&#65292;&#23613;&#31649;&#23454;&#38469;&#29359;&#32618;&#29575;&#35201;&#20302;&#24471;&#22810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#21487;&#39044;&#27979;&#25910;&#20837;&#21644;&#19981;&#24179;&#31561;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#25910;&#20837;&#65288;R-Square=0.841&#65289;&#21644;&#19981;&#24179;&#31561;&#65288;R-Sq&#8230;
&lt;/p&gt;
&lt;p&gt;
From health to education, income impacts a huge range of life choices. Many papers have leveraged data from online social networks to study precisely this. In this paper, we ask the opposite question: do different levels of income result in different online behaviors? We demonstrate it does. We present the first large-scale study of Nextdoor, a popular location-based social network. We collect 2.6 Million posts from 64,283 neighborhoods in the United States and 3,325 neighborhoods in the United Kingdom, to examine whether online discourse reflects the income and income inequality of a neighborhood. We show that posts from neighborhoods with different income indeed differ, e.g. richer neighborhoods have a more positive sentiment and discuss crimes more, even though their actual crime rates are much lower. We then show that user-generated content can predict both income and inequality. We train multiple machine learning models and predict both income (R-Square=0.841) and inequality (R-Sq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.05223</link><description>&lt;p&gt;
&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#19981;&#22343;&#21248;&#22270;&#36235;&#21183;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#20272;&#35745;&#20998;&#27573;&#24179;&#28369;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;$\ell_{2,0}$-&#33539;&#25968;&#24809;&#32602;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#21516;&#26102;&#26159;&#22522;&#20110;&#33410;&#28857;&#19978;&#30340;&#20449;&#21495;&#30340;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20854;&#20013;&#32858;&#31867;&#21644;&#21106;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#65306;&#19968;&#31181;&#26159;&#22522;&#20110;&#35889;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#22312;&#25932;&#23545;&#35774;&#32622;&#20013;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36798;&#21040;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2304.05219</link><description>&lt;p&gt;
BanditQ - &#22312;&#25932;&#23545;&#29615;&#22659;&#20013;&#20445;&#35777;&#29992;&#25143;&#27599;&#27425;&#22870;&#21169;&#30340;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#22312;&#25932;&#23545;&#35774;&#32622;&#20013;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36798;&#21040;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#22914;Hedge&#22312;&#35774;&#35745;&#19978;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23581;&#35797;&#23613;&#21487;&#33021;&#22810;&#22320;&#29609;&#26368;&#20855;&#22238;&#25253;&#30340;&#33218;&#32780;&#24573;&#30053;&#27425;&#20248;&#33218;&#65292;&#20197;&#23454;&#29616;&#20122;&#32447;&#24615;&#36951;&#25022;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20855;&#26377;&#23545;&#25152;&#26377;&#33218;&#32047;&#31215;&#22870;&#21169;&#36895;&#29575;&#19979;&#30028;&#30340;&#25932;&#23545;&#35774;&#32622;&#20013;&#65292;&#20197;&#20844;&#24179;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;&#25490;&#38431;&#35770;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#23427;&#22312;&#20840;&#20449;&#24687;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;BanditQ&#30340;&#35774;&#35745;&#21644;&#20998;&#26512;&#28041;&#21450;&#28508;&#22312;&#20989;&#25968;&#26041;&#27861;&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#24182;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; CGX &#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#25552;&#21462;&#35268;&#21017;&#65292;&#20248;&#21270;&#23545;&#40784;&#65292;&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05207</link><description>&lt;p&gt;
CGXplain: &#22522;&#20110;&#35268;&#21017;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#65292;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
CGXplain: Rule-Based Deep Neural Network Explanations Using Dual Linear Programs. (arXiv:2304.05207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; CGX &#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#25552;&#21462;&#35268;&#21017;&#65292;&#20248;&#21270;&#23545;&#40784;&#65292;&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#36817;&#20284;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36793;&#30028;&#30340;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#20351;&#20154;&#31867;&#26131;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#21363;&#37027;&#20123;&#32771;&#34385;&#21040;DNN&#30340;&#28508;&#22312;&#31354;&#38388;&#20197;&#25552;&#21462;&#26356;&#31934;&#30830;&#35268;&#21017;&#38598;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#20102;&#39640;&#31934;&#24230;&#30340;&#35268;&#21017;&#38598;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204; a) &#19981;&#33021;&#20445;&#35777;&#20195;&#29702;&#27169;&#22411;&#24050;&#20174;&#19982; DNN &#30456;&#21516;&#30340;&#21464;&#37327;&#20013;&#23398;&#20064;&#65288;&#23545;&#40784;&#65289;&#65292;b) &#21482;&#20801;&#35768;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#20934;&#30830;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#22823;&#30340;&#35268;&#21017;&#38598;&#65288;&#22797;&#26434;&#24615;&#65289;&#65292;&#24182;&#19988; c) &#20351;&#29992;&#20915;&#31574;&#26641;&#31639;&#27861;&#20316;&#20026;&#20013;&#38388;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30456;&#21516;DNN&#30340;&#19981;&#21516;&#35299;&#37322;&#65288;&#31283;&#23450;&#24615;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#20174;DNN&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#25552;&#21462;&#35268;&#21017;&#30340;&#20998;&#35299;&#26041;&#27861; CGX&#65288;&#21015;&#29983;&#25104;&#35299;&#37322;&#22120;&#65289; &#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#23545;&#40784;&#65292;&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based surrogate models are an effective and interpretable way to approximate a Deep Neural Network's (DNN) decision boundaries, allowing humans to easily understand deep learning models. Current state-of-the-art decompositional methods, which are those that consider the DNN's latent space to extract more exact rule sets, manage to derive rule sets at high accuracy. However, they a) do not guarantee that the surrogate model has learned from the same variables as the DNN (alignment), b) only allow to optimise for a single objective, such as accuracy, which can result in excessively large rule sets (complexity), and c) use decision tree algorithms as intermediate models, which can result in different explanations for the same DNN (stability). This paper introduces the CGX (Column Generation eXplainer) to address these limitations a decompositional method using dual linear programming to extract rules from the hidden representations of the DNN. This approach allows to optimise for a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36890;&#36947;&#30456;&#20851;&#65288;CD&#65289;&#19982;&#36890;&#36947;&#29420;&#31435;&#65288;CI&#65289;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CD&#31574;&#30053;&#23481;&#37327;&#26356;&#39640;&#20294;&#40065;&#26834;&#24615;&#36739;&#24046;&#65292;CI&#31574;&#30053;&#40065;&#26834;&#24615;&#26356;&#24378;&#19988;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#21644;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;</title><link>http://arxiv.org/abs/2304.05206</link><description>&lt;p&gt;
&#23481;&#37327;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65306;&#37325;&#35775;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting. (arXiv:2304.05206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36890;&#36947;&#30456;&#20851;&#65288;CD&#65289;&#19982;&#36890;&#36947;&#29420;&#31435;&#65288;CI&#65289;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CD&#31574;&#30053;&#23481;&#37327;&#26356;&#39640;&#20294;&#40065;&#26834;&#24615;&#36739;&#24046;&#65292;CI&#31574;&#30053;&#40065;&#26834;&#24615;&#26356;&#24378;&#19988;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#21644;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21253;&#21547;&#21508;&#31181;&#21464;&#37327;&#36890;&#36947;&#12290;&#22810;&#20803;&#39044;&#27979;&#27169;&#22411;&#38656;&#35201;&#25429;&#25417;&#36890;&#36947;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#20540;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#37319;&#29992;&#36890;&#36947;&#29420;&#31435;&#65288;CI&#65289;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35270;&#20026;&#20998;&#31163;&#30340;&#21333;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#24573;&#30053;&#36890;&#36947;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;CI&#31574;&#30053;&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#37319;&#29992;&#36890;&#36947;&#30456;&#20851;&#65288;CD&#65289;&#31574;&#30053;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24046;&#36317;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29616;&#35937;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#28145;&#20837;&#25506;&#35752;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;CI / CD&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CD&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23481;&#37327;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#40065;&#26834;&#24615;&#20197;&#20934;&#30830;&#39044;&#27979;&#20013;&#26029;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;CI&#26041;&#27861;&#29306;&#29298;&#20102;&#19968;&#20123;&#23481;&#37327;&#65292;&#20294;&#20855;&#26377;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#21644;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict dis
&lt;/p&gt;</description></item><item><title>TinyReptile&#26159;&#19968;&#20010;&#32852;&#37030;&#20803;&#23398;&#20064;&#23454;&#29616;&#30340;&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36855;&#20320;&#35774;&#22791;&#19978;&#21327;&#20316;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;&#25968;&#25454;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.05201</link><description>&lt;p&gt;
TinyReptile&#65306;&#32852;&#37030;&#20803;&#23398;&#20064;&#23454;&#29616;&#30340;&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyReptile: TinyML with Federated Meta-Learning. (arXiv:2304.05201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05201
&lt;/p&gt;
&lt;p&gt;
TinyReptile&#26159;&#19968;&#20010;&#32852;&#37030;&#20803;&#23398;&#20064;&#23454;&#29616;&#30340;&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36855;&#20320;&#35774;&#22791;&#19978;&#21327;&#20316;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;&#25968;&#25454;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#26159;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#24494;&#25511;&#21046;&#22120;&#65288;MCU&#65289;&#27665;&#20027;&#21270;&#26426;&#22120;&#23398;&#20064;&#12290;&#37492;&#20110;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24615;&#65292;&#26377;&#24517;&#35201;&#38382;&#26159;&#21542;TinyML&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#32858;&#21512;&#20182;&#20204;&#30340;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#20998;&#25955;&#30340;&#20195;&#29702;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#25935;&#24863;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#27599;&#20010;&#35774;&#22791;&#21487;&#29992;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#19981;&#33021;&#36866;&#29992;&#20110;&#25152;&#26377;&#35774;&#22791;&#12290;&#27492;&#22806;&#65292; TinyML &#30828;&#20214;&#30340;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#32422;&#26463;&#65292;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; TinyReptile&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#20803;&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#36855;&#20320;&#35774;&#22791;&#19978;&#21327;&#20316;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#22362;&#23454;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#36825;&#20123;&#35774;&#22791;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992; FL &#26469;&#35780;&#20272; TinyReptile&#65292;&#32467;&#26524;&#34920;&#26126; TinyReptile &#21487;&#20197;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#20840;&#23616;&#25968;&#25454;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#31934;&#30830;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny machine learning (TinyML) is a rapidly growing field aiming to democratize machine learning (ML) for resource-constrained microcontrollers (MCUs). Given the pervasiveness of these tiny devices, it is inherent to ask whether TinyML applications can benefit from aggregating their knowledge. Federated learning (FL) enables decentralized agents to jointly learn a global model without sharing sensitive local data. However, a common global model may not work for all devices due to the complexity of the actual deployment environment and the heterogeneity of the data available on each device. In addition, the deployment of TinyML hardware has significant computational and communication constraints, which traditional ML fails to address. Considering these challenges, we propose TinyReptile, a simple but efficient algorithm inspired by meta-learning and online learning, to collaboratively learn a solid initialization for a neural network (NN) across tiny devices that can be quickly adapted 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#36229;&#21442;&#25968;&#32593;&#32476;&#26041;&#26696;(HPN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#20351;&#29992;&#23458;&#25143;&#31471;&#32534;&#30721;&#26469;&#20915;&#23450;&#20010;&#24615;&#21270;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38500;&#20989;&#25968;&#35780;&#20272;&#20559;&#24046;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.05195</link><description>&lt;p&gt;
HPN: &#20010;&#24615;&#21270;&#32852;&#37030;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
HPN: Personalized Federated Hyperparameter Optimization. (arXiv:2304.05195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#36229;&#21442;&#25968;&#32593;&#32476;&#26041;&#26696;(HPN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#20351;&#29992;&#23458;&#25143;&#31471;&#32534;&#30721;&#26469;&#20915;&#23450;&#20010;&#24615;&#21270;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38500;&#20989;&#25968;&#35780;&#20272;&#20559;&#24046;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#20247;&#22810;&#30740;&#31350;&#20013;&#65292;&#20010;&#24615;&#21270;&#26159;&#35299;&#20915;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20294;&#24050;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35843;&#25972;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#24322;&#36136;&#24615;&#65292;&#20182;&#20204;&#27599;&#20010;&#20154;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#36825;&#26041;&#38754;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#36229;&#21442;&#25968;&#20248;&#21270; (pFedHPO) &#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#22914;&#20309;&#22788;&#29702;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#22914;&#20309;&#34920;&#24449;&#27599;&#20010;&#23458;&#25143;&#31471;&#32780;&#19981;&#25439;&#23475;&#20854;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36229;&#21442;&#25968;&#32593;&#32476; (HPN) &#30340;&#26041;&#26696;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#23458;&#25143;&#31471;&#32534;&#30721;&#26469;&#20915;&#23450;&#20010;&#24615;&#21270;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;&#23458;&#25143;&#31471;&#32534;&#30721;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#36807;&#31243;&#36827;&#34892;&#35745;&#31639;&#65292;&#20197;&#20445;&#25252;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#26469;&#21435;&#38500;&#20302;&#20445;&#30495;&#24230;&#20989;&#25968;&#35780;&#20272;&#26679;&#26412;&#30340;&#20559;&#24046;&#65292;&#20197;&#20415;&#23398;&#20064; HPN&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous research studies in the field of federated learning (FL) have attempted to use personalization to address the heterogeneity among clients, one of FL's most crucial and challenging problems. However, existing works predominantly focus on tailoring models. Yet, due to the heterogeneity of clients, they may each require different choices of hyperparameters, which have not been studied so far. We pinpoint two challenges of personalized federated hyperparameter optimization (pFedHPO): handling the exponentially increased search space and characterizing each client without compromising its data privacy. To overcome them, we propose learning a \textsc{H}yper\textsc{P}arameter \textsc{N}etwork (HPN) fed with client encoding to decide personalized hyperparameters. The client encoding is calculated with a random projection-based procedure to protect each client's privacy. Besides, we design a novel mechanism to debias the low-fidelity function evaluation samples for learning HPN. We con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20449;&#24687;&#23450;&#20041;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120; - &#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#12290;&#35813;&#31639;&#27861;&#22312;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#21367;&#31215;&#32593;&#32476;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#25163;&#21160;&#35843;&#25972;&#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05187</link><description>&lt;p&gt;
&#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#65306;&#26080;&#36229;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20449;&#24687;&#23450;&#20041;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120; - &#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#12290;&#35813;&#31639;&#27861;&#22312;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#21367;&#31215;&#32593;&#32476;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#25163;&#21160;&#35843;&#25972;&#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27966;&#29983;&#29305;&#23450;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26080;&#36229;&#21442;&#25968;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;&#8220;&#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#8221;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#26174;&#24335;&#22320;&#23450;&#20041;&#32593;&#32476;&#32467;&#26500;&#21442;&#25968;&#26469;&#20248;&#21270;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#21367;&#31215;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#19982;&#25163;&#21160;&#35843;&#25972;&#20248;&#21270;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;&#35813;&#31639;&#27861;&#25193;&#23637;&#20102;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#20197;&#22788;&#29702;&#38750;&#20984;&#24615;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;DSLAD&#65292;&#36890;&#36807;&#35299;&#32806;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#36827;&#34892;&#23646;&#24615;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29305;&#24449;&#31354;&#38388;&#35299;&#20915;&#20102;&#35821;&#20041;&#28151;&#21512;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05176</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#23646;&#24615;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#21028;&#21035;&#19982;&#34920;&#31034;&#23398;&#20064;&#35299;&#32806;&#65306;DSLAD&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decoupling anomaly discrimination and representation learning: self-supervised learning for anomaly detection on attributed graph. (arXiv:2304.05176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;DSLAD&#65292;&#36890;&#36807;&#35299;&#32806;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#36827;&#34892;&#23646;&#24615;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29305;&#24449;&#31354;&#38388;&#35299;&#20915;&#20102;&#35821;&#20041;&#28151;&#21512;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#22240;&#20027;&#35201;&#20851;&#27880;&#24322;&#24120;&#21028;&#21035;&#32780;&#24573;&#30053;&#34920;&#31034;&#23398;&#20064;&#65292;&#23384;&#22312;&#35821;&#20041;&#28151;&#21512;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#19982;&#24322;&#36136;&#24615;&#20551;&#35774;&#30456;&#20914;&#31361;&#65292;&#21363;&#24322;&#24120;&#33410;&#28857;&#36890;&#24120;&#30452;&#25509;&#19982;&#27491;&#24120;&#33410;&#28857;&#30456;&#36830;&#12290;&#27492;&#22806;&#65292;&#24322;&#24120;&#33410;&#28857;&#27604;&#27491;&#24120;&#33410;&#28857;&#23569;&#24471;&#22810;&#65292;&#34920;&#26126;&#25968;&#25454;&#20998;&#24067;&#21576;&#38271;&#23614;&#24418;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#31639;&#27861;DSLAD&#65292;&#23427;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#35299;&#32806;&#26469;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;DSLAD&#37319;&#29992;&#21452;&#32447;&#24615;&#27744;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#20316;&#20026;&#24322;&#24120;&#21028;&#21035;&#22120;&#12290;&#36890;&#36807;&#35299;&#32806;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20854;&#20013;&#33410;&#28857;&#26356;&#20855;&#35821;&#20041;&#37492;&#21035;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;DSLAD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on attributed graphs is a crucial topic for its practical application. Existing methods suffer from semantic mixture and imbalance issue because they mainly focus on anomaly discrimination, ignoring representation learning. It conflicts with the assortativity assumption that anomalous nodes commonly connect with normal nodes directly. Additionally, there are far fewer anomalous nodes than normal nodes, indicating a long-tailed data distribution. To address these challenges, a unique algorithm,Decoupled Self-supervised Learning forAnomalyDetection (DSLAD), is proposed in this paper. DSLAD is a self-supervised method with anomaly discrimination and representation learning decoupled for anomaly detection. DSLAD employs bilinear pooling and masked autoencoder as the anomaly discriminators. By decoupling anomaly discrimination and representation learning, a balanced feature space is constructed, in which nodes are more semantically discriminative, as well as imbalance issu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#32467;&#21512;&#39044;&#27979;&#30005;&#21147;&#38656;&#27714;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20044;&#20811;&#20848;&#30340;&#30005;&#21147;&#28040;&#32791;&#25968;&#25454;&#12290;&#26041;&#27861;&#20013;&#21253;&#25324;&#20102;&#23439;&#35266;&#32463;&#27982;&#22238;&#24402;&#20998;&#26512;&#12289;&#28201;&#24230;&#21644;&#26085;&#21382;&#22238;&#24402;&#21464;&#37327;&#30456;&#32467;&#21512;&#12289;ARIMA&#21644;LSTM&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#31561;&#65292;&#20351;&#24471;&#39044;&#27979;&#33021;&#22815;&#35206;&#30422;&#38271;&#12289;&#20013;&#12289;&#30701;&#26399;&#19981;&#21516;&#38454;&#27573;&#20197;&#21450;&#23567;&#26102;&#23395;&#33410;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05174</link><description>&lt;p&gt;
&#22522;&#20110;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#65306;&#20197;&#20044;&#20811;&#20848;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Electricity Demand Forecasting with Hybrid Statistical and Machine Learning Algorithms: Case Study of Ukraine. (arXiv:2304.05174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#32467;&#21512;&#39044;&#27979;&#30005;&#21147;&#38656;&#27714;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20044;&#20811;&#20848;&#30340;&#30005;&#21147;&#28040;&#32791;&#25968;&#25454;&#12290;&#26041;&#27861;&#20013;&#21253;&#25324;&#20102;&#23439;&#35266;&#32463;&#27982;&#22238;&#24402;&#20998;&#26512;&#12289;&#28201;&#24230;&#21644;&#26085;&#21382;&#22238;&#24402;&#21464;&#37327;&#30456;&#32467;&#21512;&#12289;ARIMA&#21644;LSTM&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#31561;&#65292;&#20351;&#24471;&#39044;&#27979;&#33021;&#22815;&#35206;&#30422;&#38271;&#12289;&#20013;&#12289;&#30701;&#26399;&#19981;&#21516;&#38454;&#27573;&#20197;&#21450;&#23567;&#26102;&#23395;&#33410;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#22269;&#23478;&#30005;&#21147;&#38656;&#27714;&#12290;&#30001;&#20110;&#26410;&#26469;&#33021;&#28304;&#31995;&#32479;&#30340;&#25237;&#36164;&#21644;&#36816;&#33829;&#38656;&#35201;&#20855;&#26377;&#23567;&#26102;&#20998;&#36776;&#29575;&#30340;&#38271;&#26399;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#25968;&#23398;&#27169;&#22411;&#22635;&#34917;&#20102;&#33021;&#28304;&#39044;&#27979;&#20013;&#30340;&#31354;&#30333;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20044;&#20811;&#20848;&#20174;2013&#24180;&#33267;2020&#24180;&#30340;&#30005;&#21147;&#28040;&#32791;&#23567;&#26102;&#25968;&#25454;&#36827;&#34892;&#26500;&#24314;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30005;&#21147;&#28040;&#36153;&#30340;&#23567;&#26102;&#12289;&#26085;&#21644;&#24180;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;&#36890;&#36807;&#23439;&#35266;&#32463;&#27982;&#22238;&#24402;&#20998;&#26512;&#35780;&#20272;&#20102;&#38271;&#26399;&#24180;&#36235;&#21183;&#12290;&#20013;&#26399;&#27169;&#22411;&#32467;&#21512;&#28201;&#24230;&#21644;&#26085;&#21382;&#22238;&#24402;&#21464;&#37327;&#26469;&#25551;&#36848;&#22522;&#26412;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;ARIMA&#21644;LSTM&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#26469;&#25551;&#36848;&#35823;&#24046;&#39033;&#12290;&#30701;&#26399;&#27169;&#22411;&#36890;&#36807;&#26085;&#21382;&#22238;&#24402;&#21464;&#37327;&#21644;&#22810;&#20010;ARMA&#27169;&#22411;&#26469;&#25429;&#25417;&#23567;&#26102;&#23395;&#33410;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#39044;&#27979;&#21487;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
This article presents a novel hybrid approach using statistics and machine learning to forecast the national demand of electricity. As investment and operation of future energy systems require long-term electricity demand forecasts with hourly resolution, our mathematical model fills a gap in energy forecasting. The proposed methodology was constructed using hourly data from Ukraine's electricity consumption ranging from 2013 to 2020. To this end, we analysed the underlying structure of the hourly, daily and yearly time series of electricity consumption. The long-term yearly trend is evaluated using macroeconomic regression analysis. The mid-term model integrates temperature and calendar regressors to describe the underlying structure, and combines ARIMA and LSTM ``black-box'' pattern-based approaches to describe the error term. The short-term model captures the hourly seasonality through calendar regressors and multiple ARMA models for the residual. Results show that the best forecast
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35760;&#24518;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#38598;&#21512;&#20013;&#26816;&#32034;&#30456;&#20284;&#23454;&#20363;&#65292;&#28040;&#38500;&#20102;&#19981;&#30456;&#20851;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#20445;&#30041;&#20102;&#26377;&#30410;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#23454;&#20363;&#12290;&#20351;&#29992;&#22823;&#35268;&#27169;&#35760;&#24518;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19968;&#27969;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05173</link><description>&lt;p&gt;
&#20174;&#32593;&#32476;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#26816;&#32034;&#25552;&#39640;&#22270;&#20687;&#35782;&#21035;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improving Image Recognition by Retrieving from Web-Scale Image-Text Data. (arXiv:2304.05173v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35760;&#24518;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#38598;&#21512;&#20013;&#26816;&#32034;&#30456;&#20284;&#23454;&#20363;&#65292;&#28040;&#38500;&#20102;&#19981;&#30456;&#20851;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#20445;&#30041;&#20102;&#26377;&#30410;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#23454;&#20363;&#12290;&#20351;&#29992;&#22823;&#35268;&#27169;&#35760;&#24518;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19968;&#27969;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#22686;&#24378;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#20013;&#30340;&#25104;&#21151;&#21518;&#12290;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#22120;&#38598;&#21512;&#20013;&#26816;&#32034;&#30456;&#20284;&#23454;&#20363;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35760;&#24518;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#23398;&#20064;&#20174;&#23384;&#20648;&#22120;&#26816;&#32034;&#21040;&#30340;&#27599;&#20010;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#19981;&#30456;&#20851;&#26816;&#32034;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#20102;&#26377;&#30410;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#24443;&#24213;&#30740;&#31350;&#20102;&#26500;&#24314;&#35760;&#24518;&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;10&#20159;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#22823;&#35268;&#27169;&#35760;&#24518;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#22823;&#30340;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#35760;&#24518;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20215;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#38271;&#23614;&#35782;&#21035;&#12289;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;&#21644;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#19968;&#27969;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented models are becoming increasingly popular for computer vision tasks after their recent success in NLP problems. The goal is to enhance the recognition capabilities of the model by retrieving similar examples for the visual input from an external memory set. In this work, we introduce an attention-based memory module, which learns the importance of each retrieved example from the memory. Compared to existing approaches, our method removes the influence of the irrelevant retrieved examples, and retains those that are beneficial to the input query. We also thoroughly study various ways of constructing the memory dataset. Our experiments show the benefit of using a massive-scale memory dataset of 1B image-text pairs, and demonstrate the performance of different memory representations. We evaluate our method in three different classification tasks, namely long-tailed recognition, learning with noisy labels, and fine-grained classification, and show that it achieves state-
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#22810;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#19987;&#19994;&#21270;&#22788;&#29702;&#23616;&#37096;&#20869;&#23481;&#65292;&#21516;&#26102;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#28608;&#21169;&#23613;&#21487;&#33021;&#22810;&#22320;&#28085;&#30422;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.05171</link><description>&lt;p&gt;
&#22522;&#20110;&#35838;&#31243;&#30340;&#22810;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum-Based Imitation of Versatile Skills. (arXiv:2304.05171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05171
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#22810;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#19987;&#19994;&#21270;&#22788;&#29702;&#23616;&#37096;&#20869;&#23481;&#65292;&#21516;&#26102;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#28608;&#21169;&#23613;&#21487;&#33021;&#22810;&#22320;&#28085;&#30422;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#25216;&#33021;&#26159;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#30452;&#35266;&#25945;&#23398;&#30340;&#26377;&#21069;&#36884;&#30340;&#27010;&#24565;&#12290;&#23398;&#20064;&#27492;&#31867;&#25216;&#33021;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#32473;&#23450;&#28436;&#31034;&#19979;&#30340;&#20284;&#28982;&#26469;&#23398;&#20064;&#21442;&#25968;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#28436;&#31034;&#24448;&#24448;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21363;&#30456;&#21516;&#30340;&#20219;&#21153;&#20197;&#22810;&#31181;&#26041;&#24335;&#35299;&#20915;&#65292;&#36825;&#23545;&#22823;&#22810;&#25968;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#30446;&#26631;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;ML&#30446;&#26631;&#24378;&#21046;&#27169;&#22411;&#28085;&#30422;&#25152;&#26377;&#25968;&#25454;&#65292;&#38450;&#27490;&#23427;&#22312;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#19987;&#19994;&#21270;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#34892;&#20026;&#31354;&#38388;&#20013;&#30340;&#27169;&#24335;&#24179;&#22343;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#25110;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#35838;&#31243;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#24341;&#20837;&#19968;&#20010;&#26435;&#37325;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20801;&#35768;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#23427;&#21487;&#20197;&#20195;&#34920;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;&#29109;&#22870;&#21169;&#26469;&#28608;&#21169;&#27169;&#22411;&#23613;&#21487;&#33021;&#28085;&#30422;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#32447;&#24615;&#19987;&#23478;&#30340;&#28151;&#21512;&#29289;&#65292;&#20351;&#21333;&#20010;&#32452;&#20214;&#21487;&#20197;&#19987;&#38376;&#22788;&#29702;&#23616;&#37096;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning skills by imitation is a promising concept for the intuitive teaching of robots. A common way to learn such skills is to learn a parametric model by maximizing the likelihood given the demonstrations. Yet, human demonstrations are often multi-modal, i.e., the same task is solved in multiple ways which is a major challenge for most imitation learning methods that are based on such a maximum likelihood (ML) objective. The ML objective forces the model to cover all data, it prevents specialization in the context space and can cause mode-averaging in the behavior space, leading to suboptimal or potentially catastrophic behavior. Here, we alleviate those issues by introducing a curriculum using a weight for each data point, allowing the model to specialize on data it can represent while incentivizing it to cover as much data as possible by an entropy bonus. We extend our algorithm to a Mixture of (linear) Experts (MoE) such that the single components can specialize on local context
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#20998;&#24067;&#24182;&#22810;&#27425;&#25277;&#26679;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#24863;&#30693;&#30340;&#22635;&#20805;&#21644;&#21487;&#25511;&#30340;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.05165</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification. (arXiv:2304.05165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05165
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#20998;&#24067;&#24182;&#22810;&#27425;&#25277;&#26679;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#24863;&#30693;&#30340;&#22635;&#20805;&#21644;&#21487;&#25511;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20219;&#24847;&#35270;&#22270;&#32570;&#22833;&#24191;&#27867;&#23384;&#22312;&#65292;&#22240;&#27492;&#20998;&#31867;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#22833;&#35270;&#22270;&#30340;&#30456;&#23545;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#26041;&#27861;&#20173;&#28982;&#24456;&#38590;&#33719;&#24471;&#21487;&#20449;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31283;&#23450;&#21487;&#38752;&#30340;&#26694;&#26550;&#19979;&#23545;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#24067;&#24182;&#36827;&#34892;&#22810;&#27425;&#25277;&#26679;&#20197;&#34920;&#24449;&#32570;&#22833;&#35270;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#26681;&#25454;&#25277;&#26679;&#36136;&#37327;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#21152;&#21487;&#24863;&#30693;&#30340;&#22635;&#20805;&#21644;&#21487;&#25511;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying incomplete multi-view data is inevitable since arbitrary view missing widely exists in real-world applications. Although great progress has been achieved, existing incomplete multi-view methods are still difficult to obtain a trustworthy prediction due to the relatively high uncertainty nature of missing views. First, the missing view is of high uncertainty, and thus it is not reasonable to provide a single deterministic imputation. Second, the quality of the imputed data itself is of high uncertainty. To explore and exploit the uncertainty, we propose an Uncertainty-induced Incomplete Multi-View Data Classification (UIMC) model to classify the incomplete multi-view data under a stable and reliable framework. We construct a distribution and sample multiple times to characterize the uncertainty of missing views, and adaptively utilize them according to the sampling quality. Accordingly, the proposed method realizes more perceivable imputation and controllable fusion. Specifi
&lt;/p&gt;</description></item><item><title>NeAT&#26159;&#19968;&#31181;&#31070;&#32463;&#33402;&#26415;&#36861;&#36394;&#25216;&#26415;&#65292;&#23427;&#37325;&#26032;&#23558;&#21069;&#39304;&#24335;&#39118;&#26684;&#36716;&#31227;&#34920;&#36848;&#20026;&#22270;&#20687;&#32534;&#36753;&#65292;&#33021;&#22312;&#20445;&#25345;&#28304;&#20869;&#23481;&#21644;&#21305;&#37197;&#30446;&#26631;&#39118;&#26684;&#26041;&#38754;&#37117;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#32780;&#19988;&#22312;&#35782;&#21035;&#21644;&#20462;&#22797;"&#26679;&#24335;&#20809;&#29615;"&#26041;&#38754;&#20063;&#26356;&#21152;&#20986;&#33394;&#65292;&#20351;&#29992;BBST-4M&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#20854;&#22312;&#21508;&#31181;&#39118;&#26684;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05139</link><description>&lt;p&gt;
NeAT&#65306;&#31070;&#32463;&#33402;&#26415;&#36861;&#36394;&#25216;&#26415;&#23454;&#29616;&#32654;&#23398;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
NeAT: Neural Artistic Tracing for Beautiful Style Transfer. (arXiv:2304.05139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05139
&lt;/p&gt;
&lt;p&gt;
NeAT&#26159;&#19968;&#31181;&#31070;&#32463;&#33402;&#26415;&#36861;&#36394;&#25216;&#26415;&#65292;&#23427;&#37325;&#26032;&#23558;&#21069;&#39304;&#24335;&#39118;&#26684;&#36716;&#31227;&#34920;&#36848;&#20026;&#22270;&#20687;&#32534;&#36753;&#65292;&#33021;&#22312;&#20445;&#25345;&#28304;&#20869;&#23481;&#21644;&#21305;&#37197;&#30446;&#26631;&#39118;&#26684;&#26041;&#38754;&#37117;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#32780;&#19988;&#22312;&#35782;&#21035;&#21644;&#20462;&#22797;"&#26679;&#24335;&#20809;&#29615;"&#26041;&#38754;&#20063;&#26356;&#21152;&#20986;&#33394;&#65292;&#20351;&#29992;BBST-4M&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#20854;&#22312;&#21508;&#31181;&#39118;&#26684;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#26159;&#22312;&#31532;&#20108;&#20010;&#30446;&#26631;&#22270;&#20687;&#30340;&#33402;&#26415;&#39118;&#26684;&#19979;&#37325;&#26032;&#29983;&#25104;&#28304;&#22270;&#20687;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NeAT&#65292;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#21069;&#39304;&#24335;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#12290;&#25105;&#20204;&#37325;&#26032;&#23558;&#21069;&#39304;&#24335;&#39118;&#26684;&#36716;&#31227;&#34920;&#36848;&#20026;&#22270;&#20687;&#32534;&#36753;&#65292;&#32780;&#19981;&#26159;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#28304;&#20869;&#23481;&#21644;&#21305;&#37197;&#30446;&#26631;&#39118;&#26684;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#25105;&#20204;&#27169;&#22411;&#25104;&#21151;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#35782;&#21035;&#21644;&#20462;&#22797;"&#26679;&#24335;&#20809;&#29615;"&#65292;&#36825;&#26159;&#35768;&#22810;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#20135;&#29289;&#12290;&#38500;&#20102;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BBST-4M&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;4M&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#31579;&#36873;&#36825;&#20123;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#19968;&#24352;&#22270;&#20687;&#36827;&#34892;&#39118;&#26684;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;BBST-4M&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#24182;&#27979;&#37327;NeAT&#22312;&#21508;&#31181;&#39118;&#26684;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;NeAT&#19981;&#20165;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#36136;&#37327;&#65292;&#36824;&#20855;&#26377;&#26356;&#22909;&#30340;&#20869;&#23481;&#20445;&#23384;&#21644;&#39118;&#26684;&#21305;&#37197;&#24615;&#33021;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#20248;&#31168;&#30340;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style transfer is the task of reproducing the semantic contents of a source image in the artistic style of a second target image. In this paper, we present NeAT, a new state-of-the art feed-forward style transfer method. We re-formulate feed-forward style transfer as image editing, rather than image generation, resulting in a model which improves over the state-of-the-art in both preserving the source content and matching the target style. An important component of our model's success is identifying and fixing "style halos", a commonly occurring artefact across many style transfer techniques. In addition to training and testing on standard datasets, we introduce the BBST-4M dataset, a new, large scale, high resolution dataset of 4M images. As a component of curating this data, we present a novel model able to classify if an image is stylistic. We use BBST-4M to improve and measure the generalization of NeAT across a huge variety of styles. Not only does NeAT offer state-of-the-art qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#21644;&#22686;&#26448;&#21046;&#36896;&#26469;&#24314;&#27169;&#21644;&#35774;&#35745;&#22522;&#20110;&#34584;&#34523;&#32593;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#19977;&#32500;&#32593;&#29366;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#20854;&#22797;&#26434;&#30340;&#35774;&#35745;&#29305;&#24449;&#24102;&#26469;&#30340;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05137</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#21644;&#22686;&#26448;&#21046;&#36896;&#30340;&#24322;&#36136;&#24615;&#20998;&#23618;&#29983;&#29289;&#21551;&#21457;&#24335;&#34584;&#34523;&#32593;&#32467;&#26500;&#30340;&#24314;&#27169;&#21450;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Modeling and design of heterogeneous hierarchical bioinspired spider web structures using generative deep learning and additive manufacturing. (arXiv:2304.05137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#21644;&#22686;&#26448;&#21046;&#36896;&#26469;&#24314;&#27169;&#21644;&#35774;&#35745;&#22522;&#20110;&#34584;&#34523;&#32593;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#19977;&#32500;&#32593;&#29366;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#20854;&#22797;&#26434;&#30340;&#35774;&#35745;&#29305;&#24449;&#24102;&#26469;&#30340;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34584;&#34523;&#32593;&#26159;&#38750;&#24120;&#19981;&#21487;&#24605;&#35758;&#30340;&#29983;&#29289;&#32467;&#26500;&#65292;&#30001;&#34180;&#20294;&#24378;&#38887;&#30340;&#34453;&#19997;&#32452;&#25104;&#65292;&#24182;&#25490;&#21015;&#25104;&#22797;&#26434;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#20855;&#26377;&#24778;&#20154;&#30340;&#21147;&#23398;&#24615;&#33021;&#65288;&#20363;&#22914;&#36731;&#37327;&#20294;&#39640;&#24378;&#24230;&#65292;&#23454;&#29616;&#21508;&#31181;&#21147;&#23398;&#21709;&#24212;&#65289;&#12290;&#31616;&#21333;&#30340;2D&#32593;&#24418;&#29366;&#24456;&#23481;&#26131;&#34987;&#27169;&#20223;&#65292;&#20294;&#24314;&#27169;&#21644;&#21512;&#25104;&#22522;&#20110;&#19977;&#32500;&#32593;&#29366;&#32467;&#26500;&#30340;&#32593;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#35774;&#35745;&#29305;&#24449;&#30340;&#20016;&#23500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34584;&#34523;&#32593;&#24322;&#36136;&#24615;&#22270;&#32467;&#26500;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#20154;&#24037;&#21512;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#19977;&#32500;&#32593;&#29366;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26159;&#22522;&#20110;&#20027;&#35201;&#20960;&#20309;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#65288;&#21253;&#25324;&#24179;&#22343;&#36793;&#38271;&#24230;&#65292;&#33410;&#28857;&#25968;&#65292;&#24179;&#22343;&#33410;&#28857;&#24230;&#31561;&#31561;&#65289;&#12290;&#20026;&#20102;&#35782;&#21035;&#22270;&#26500;&#24314;&#21407;&#21017;&#65292;&#25105;&#20204;&#20351;&#29992;&#24402;&#32435;&#34920;&#31034;&#37319;&#26679;&#30340;&#22823;&#22411;&#23454;&#39564;&#30830;&#23450;&#30340;&#34584;&#34523;&#32593;&#22270;&#24418;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#19977;&#20010;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;1) A
&lt;/p&gt;
&lt;p&gt;
Spider webs are incredible biological structures, comprising thin but strong silk filament and arranged into complex hierarchical architectures with striking mechanical properties (e.g., lightweight but high strength, achieving diverse mechanical responses). While simple 2D orb webs can easily be mimicked, the modeling and synthesis of 3D-based web structures remain challenging, partly due to the rich set of design features. Here we provide a detailed analysis of the heterogenous graph structures of spider webs, and use deep learning as a way to model and then synthesize artificial, bio-inspired 3D web structures. The generative AI models are conditioned based on key geometric parameters (including average edge length, number of nodes, average node degree, and others). To identify graph construction principles, we use inductive representation sampling of large experimentally determined spider web graphs, to yield a dataset that is used to train three conditional generative models: 1) A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RecUP-FL&#30340;&#29992;&#25143;&#21487;&#37197;&#32622;&#38544;&#31169;&#38450;&#25252;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#38544;&#31169;&#21644;&#25928;&#29992;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05135</link><description>&lt;p&gt;
RecUP-FL: &#36890;&#36807;&#29992;&#25143;&#21487;&#37197;&#32622;&#30340;&#38544;&#31169;&#38450;&#25252;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
RecUP-FL: Reconciling Utility and Privacy in Federated Learning via User-configurable Privacy Defense. (arXiv:2304.05135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RecUP-FL&#30340;&#29992;&#25143;&#21487;&#37197;&#32622;&#38544;&#31169;&#38450;&#25252;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#38544;&#31169;&#21644;&#25928;&#29992;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20801;&#35768;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#38544;&#31169;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#26799;&#24230;&#20173;&#28982;&#21487;&#33021;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#36890;&#24120;&#35201;&#27714;&#23458;&#25143;&#31471;&#22312;&#20849;&#20139;&#32473;&#26381;&#21153;&#22120;&#20043;&#21069;&#26412;&#22320;&#20462;&#25913;&#20854;&#26799;&#24230;&#65288;&#20363;&#22914;&#65292;&#24046;&#20998;&#38544;&#31169;&#65289;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#23558;&#25972;&#20010;&#25968;&#25454;&#35270;&#20026;&#21333;&#20010;&#23454;&#20307;&#26469;&#20445;&#25252;&#65292;&#36825;&#36890;&#24120;&#20250;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#27169;&#22411;&#25928;&#29992;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#29992;&#25143;&#21487;&#37197;&#32622;&#30340;&#38544;&#31169;&#38450;&#25252;&#26426;&#21046;RecUP-FL&#26469;&#21327;&#35843;FL&#20013;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20851;&#27880;&#29992;&#25143;&#25351;&#23450;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#24182;&#22312;&#20256;&#32479;&#38450;&#24481;&#26041;&#27861;&#20043;&#19978;&#33719;&#24471;&#26174;&#30528;&#30340;&#27169;&#22411;&#25928;&#29992;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#25512;&#26029;&#25915;&#20987;&#36890;&#24120;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#25552;&#21462;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#26377;&#25928;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RecUP-FL&#23454;&#29616;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) provides a variety of privacy advantages by allowing clients to collaboratively train a model without sharing their private data. However, recent studies have shown that private information can still be leaked through shared gradients. To further minimize the risk of privacy leakage, existing defenses usually require clients to locally modify their gradients (e.g., differential privacy) prior to sharing with the server. While these approaches are effective in certain cases, they regard the entire data as a single entity to protect, which usually comes at a large cost in model utility. In this paper, we seek to reconcile utility and privacy in FL by proposing a user-configurable privacy defense, RecUP-FL, that can better focus on the user-specified sensitive attributes while obtaining significant improvements in utility over traditional defenses. Moreover, we observe that existing inference attacks often rely on a machine learning model to extract the private inf
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35762;&#20041;&#27010;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#35270;&#35282;&#65292;&#24182;&#20171;&#32461;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#32473;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.05133</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Network Architectures. (arXiv:2304.05133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35762;&#20041;&#27010;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#35270;&#35282;&#65292;&#24182;&#20171;&#32461;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#32473;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35762;&#20041;&#20174;&#25968;&#23398;&#35282;&#24230;&#27010;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#34987;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#20171;&#32461;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#30693;&#35782;&#20197;&#21450;&#19979;&#21015;&#20960;&#31181;&#26550;&#26500;&#65306;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#27531;&#24046;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26412;&#22320;&#27493;&#39588;&#21644;&#35843;&#25972;&#21151;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05127</link><description>&lt;p&gt;
&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#31169;&#26377;&#32852;&#37030;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Performance of Private Federated Models in Medical Image Analysis. (arXiv:2304.05127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26412;&#22320;&#27493;&#39588;&#21644;&#35843;&#25972;&#21151;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#21307;&#23398;&#24212;&#29992;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#19982;&#21307;&#30103;&#25968;&#25454;&#30456;&#20851;&#30340;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25968;&#25454;&#25152;&#26377;&#26435;&#12290;&#27492;&#22806;&#65292;FL&#21487;&#20197;&#25552;&#39640;&#29992;&#20110;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;ML&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#21307;&#30103;&#25968;&#25454;&#36890;&#24120;&#26159;&#22810;&#26679;&#21270;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#30149;&#20154;&#32676;&#20307;&#32780;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#20934;&#30830;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;ML&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;FL&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#30340;&#21307;&#30103;&#25968;&#25454;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;ML&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#20445;&#25252;&#35813;&#36807;&#31243;&#23433;&#20840;&#21644;&#31169;&#23494;&#24615;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;&#26412;&#22320;&#27493;&#39588;&#65288;&#19968;&#31181;&#25552;&#39640;FL&#36890;&#20449;&#25928;&#29575;&#30340;&#24120;&#29992;&#26041;&#27861;&#65289;&#21644;&#35843;&#25972;&#36890;&#20449;&#27425;&#25968;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning (ML) approach that allows data to be trained without being centralized. This approach is particularly beneficial for medical applications because it addresses some key challenges associated with medical data, such as privacy, security, and data ownership. On top of that, FL can improve the quality of ML models used in medical applications. Medical data is often diverse and can vary significantly depending on the patient population, making it challenging to develop ML models that are accurate and generalizable. FL allows medical data to be used from multiple sources, which can help to improve the quality and generalizability of ML models. Differential privacy (DP) is a go-to algorithmic tool to make this process secure and private. In this work, we show that the model performance can be further improved by employing local steps, a popular approach to improving the communication efficiency of FL, and tuning the number of communica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OSTTP&#30340;&#26032;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#25152;&#24341;&#20837;&#30340;&#38480;&#21046;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#21644;&#23398;&#20064;&#26032;&#30340;&#20256;&#20837;&#25968;&#25454;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05124</link><description>&lt;p&gt;
&#24102;&#30446;&#26631;&#25237;&#24433;&#30340;&#22312;&#32447;&#26102;&#31354;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Spatio-Temporal Learning with Target Projection. (arXiv:2304.05124v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OSTTP&#30340;&#26032;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#25152;&#24341;&#20837;&#30340;&#38480;&#21046;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#21644;&#23398;&#20064;&#26032;&#30340;&#20256;&#20837;&#25968;&#25454;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#26102;&#38388;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;BPTT&#24341;&#20837;&#20102;&#20005;&#37325;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#38656;&#35201;&#21521;&#21518;&#36890;&#36807;&#26102;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#26435;&#37325;&#23545;&#31216;&#35201;&#27714;&#20197;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#26356;&#26032;&#38145;&#23450;&#12290;&#36825;&#20123;&#38382;&#39064;&#25104;&#20026;&#22312;&#32447;&#35757;&#32451;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#30340;AI&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#30446;&#26631;&#25237;&#24433;&#30340;&#22312;&#32447;&#26102;&#31354;&#23398;&#20064;&#65288;OSTTP&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;BPTT&#30340;&#25152;&#26377;&#21069;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;OSTTP&#20351;&#32593;&#32476;&#21516;&#26102;&#22788;&#29702;&#21644;&#23398;&#20064;&#26032;&#30340;&#20256;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#32531;&#35299;&#20102;&#26435;&#37325;&#23545;&#31216;&#21644;&#26356;&#26032;&#38145;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26102;&#38388;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;OSTTP&#65292;&#23637;&#31034;&#20102;&#19982;BPTT&#30456;&#27604;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks trained with the backpropagation through time (BPTT) algorithm have led to astounding successes in various temporal tasks. However, BPTT introduces severe limitations, such as the requirement to propagate information backwards through time, the weight symmetry requirement, as well as update-locking in space and time. These problems become roadblocks for AI systems where online training capabilities are vital. Recently, researchers have developed biologically-inspired training algorithms, addressing a subset of those problems. In this work, we propose a novel learning algorithm called online spatio-temporal learning with target projection (OSTTP) that resolves all aforementioned issues of BPTT. In particular, OSTTP equips a network with the capability to simultaneously process and learn from new incoming data, alleviating the weight symmetry and update-locking problems. We evaluate OSTTP on two temporal tasks, showcasing competitive performance compared to BPTT
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05116</link><description>&lt;p&gt;
&#19981;&#21516;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#22312;&#22522;&#20110;&#22270;&#30340;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05116
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#35843;&#24615;&#39640;&#65292;&#25104;&#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39640;&#24230;&#28789;&#27963;&#24615;&#20276;&#38543;&#30340;&#26159;&#35299;&#37322;&#24615;&#32570;&#22833;&#21644;&#21487;&#33021;&#36829;&#21453;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#20351;&#29992;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#26469;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#21487;&#20197;&#20316;&#20026;&#19982;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#37197;&#21512;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411; MTP-GO&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#36816;&#21160;&#27169;&#22411;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#33719;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#65292;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#21487;&#20197;&#23545;&#36816;&#21160;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26032;&#38395;&#31579;&#36873;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#26032;&#38395;&#65292;&#22522;&#20110;&#27969;&#21160;&#24615;&#39537;&#21160;&#30340;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#26512;&#30456;&#20851;&#26032;&#38395;&#24773;&#32490;&#12290;</title><link>http://arxiv.org/abs/2304.05115</link><description>&lt;p&gt;
&#30446;&#26631;&#26159;&#31995;&#32479;&#21270;&#30340;&#26085;&#20869;&#26032;&#38395;&#31579;&#36873;: &#19968;&#20010;&#20851;&#27880;&#27969;&#21160;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards systematic intraday news screening: a liquidity-focused approach. (arXiv:2304.05115v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26032;&#38395;&#31579;&#36873;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#26032;&#38395;&#65292;&#22522;&#20110;&#27969;&#21160;&#24615;&#39537;&#21160;&#30340;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#26512;&#30456;&#20851;&#26032;&#38395;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#21487;&#20197;&#20256;&#36798;&#23545;&#37329;&#34701;&#36164;&#20135;&#30475;&#36300;&#25110;&#30475;&#28072;&#30340;&#35266;&#28857;&#12290;&#26426;&#26500;&#25237;&#36164;&#32773;&#38656;&#35201;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#33258;&#21160;&#35780;&#20272;&#38544;&#21547;&#30340;&#26032;&#38395;&#24773;&#32490;&#12290;&#37492;&#20110;&#27599;&#22825;&#21457;&#24067;&#30340;&#22823;&#37327;&#26032;&#38395;&#25991;&#31456;&#20013;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#20013;&#24615;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26032;&#38395;&#31579;&#36873;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#8220;&#30495;&#27491;&#8221;&#26377;&#24433;&#21709;&#21147;&#30340;&#26032;&#38395;&#65292;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#24320;&#21457;&#26032;&#38395;&#24773;&#32490;&#23398;&#20064;&#26041;&#27861;&#12290;&#22522;&#20110;&#20960;&#20010;&#30001;&#27969;&#21160;&#24615;&#39537;&#21160;&#30340;&#21464;&#37327;&#65292;&#21253;&#25324;&#27874;&#21160;&#24615;&#12289;&#25104;&#20132;&#37327;&#12289;&#20080;&#21334;&#20215;&#24046;&#21644;&#25104;&#20132;&#39069;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;5&#20998;&#38047;&#26102;&#38388;&#27573;&#19982;&#20004;&#31181;&#29305;&#23450;&#30340;&#27969;&#21160;&#24615;&#27169;&#24335;&#20043;&#19968;&#30456;&#20851;&#32852;&#12290;&#20854;&#20013;&#19968;&#20010;&#20195;&#34920;&#24066;&#22330;&#22312;&#22823;&#37096;&#20998;&#26102;&#38388;&#20572;&#30041;&#22312;&#8220;&#24179;&#38745;&#8221;&#29366;&#24577;&#65292;&#32780;&#21478;&#19968;&#20010;&#21017;&#20197;&#30456;&#23545;&#36739;&#39640;&#30340;&#27874;&#21160;&#29575;&#21644;&#20132;&#26131;&#37327;&#20026;&#29305;&#24449;&#65292;&#25551;&#36848;&#21463;&#26576;&#20123;&#22806;&#29983;&#20107;&#20214;&#39537;&#21160;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20851;&#27880;&#27969;&#21160;&#24615;&#27169;&#24335;&#20174;&#21069;&#32773;&#20999;&#25442;&#21040;&#21518;&#32773;&#30340;&#26102;&#21051;&#65292;&#24182;&#32771;&#34385;&#38468;&#36817;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#20855;&#26377;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#24212;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
News can convey bearish or bullish views on financial assets. Institutional investors need to evaluate automatically the implied news sentiment based on textual data. Given the huge amount of news articles published each day, most of which are neutral, we present a systematic news screening method to identify the ``true'' impactful ones, aiming for more effective development of news sentiment learning methods. Based on several liquidity-driven variables, including volatility, turnover, bid-ask spread, and book size, we associate each 5-min time bin to one of two specific liquidity modes. One represents the ``calm'' state at which the market stays for most of the time and the other, featured with relatively higher levels of volatility and trading volume, describes the regime driven by some exogenous events. Then we focus on the moments where the liquidity mode switches from the former to the latter and consider the news articles published nearby impactful. We apply naive Bayes on these 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#31867;&#65292;&#21033;&#29992;&#32039;&#25903;&#25745;B&#26679;&#26465;&#22522;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#31232;&#30095;&#32447;&#24615;&#20195;&#25968;&#26469;&#26174;&#33879;&#21152;&#24555;&#30697;&#38453;&#36816;&#31639;&#65292; &#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19979;&#24555;&#36895;&#39640;&#25928;&#22320;&#24314;&#27169;&#24555;&#36895;&#21464;&#21270;&#30340;&#31354;&#38388;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2304.05091</link><description>&lt;p&gt;
&#23454;&#38469;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Actually Sparse Variational Gaussian Processes. (arXiv:2304.05091v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#31867;&#65292;&#21033;&#29992;&#32039;&#25903;&#25745;B&#26679;&#26465;&#22522;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#31232;&#30095;&#32447;&#24615;&#20195;&#25968;&#26469;&#26174;&#33879;&#21152;&#24555;&#30697;&#38453;&#36816;&#31639;&#65292; &#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19979;&#24555;&#36895;&#39640;&#25928;&#22320;&#24314;&#27169;&#24555;&#36895;&#21464;&#21270;&#30340;&#31354;&#38388;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#36890;&#24120;&#22240;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#19981;&#21033;&#25193;&#23637;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#23545;&#20110;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#31232;&#30095; GP &#36890;&#36807;&#22312;&#23569;&#37327;&#24863;&#24212;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#23545;&#25968;&#25454;&#36827;&#34892;&#24635;&#32467;&#26469;&#20943;&#23569;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#35768;&#22810;&#24863;&#24212;&#21464;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#20302;&#38271;&#24230;&#23610;&#24230;&#31354;&#38388;&#25968;&#25454;&#65289;&#20013;&#65292;&#21363;&#20351;&#26159;&#31232;&#30095; GP &#20063;&#21487;&#33021;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#65292;&#21463;&#20351;&#29992;&#24863;&#24212;&#21464;&#37327;&#30340;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#31867;&#65292;&#36890;&#36807;&#23558; GP &#25237;&#24433;&#21040;&#19968;&#32452;&#32039;&#25903;&#25745; B &#26679;&#26465;&#22522;&#20989;&#25968;&#19978;&#26469;&#26500;&#24314;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#65292;B &#26679;&#26465;&#22522;&#20989;&#25968;&#30340;&#32039;&#25903;&#25745;&#20801;&#35768;&#20351;&#29992;&#31232;&#30095;&#32447;&#24615;&#20195;&#25968;&#26469;&#26174;&#33879;&#21152;&#24555;&#30697;&#38453;&#36816;&#31639;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#23384;&#20648;&#21344;&#29992;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#25968;&#20197;&#19975;&#35745;&#30340;&#24863;&#24212;&#21464;&#37327;&#38750;&#24120;&#26377;&#25928;&#22320;&#24314;&#27169;&#24555;&#36895;&#21464;&#21270;&#30340;&#31354;&#38388;&#29616;&#35937;&#65292;&#32780;&#20808;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#24863;&#24212;&#21464;&#37327;&#30340;&#20351;&#29992;&#38480;&#21046;&#32780;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) are typically criticised for their unfavourable scaling in both computational and memory requirements. For large datasets, sparse GPs reduce these demands by conditioning on a small set of inducing variables designed to summarise the data. In practice however, for large datasets requiring many inducing variables, such as low-lengthscale spatial data, even sparse GPs can become computationally expensive, limited by the number of inducing variables one can use. In this work, we propose a new class of inter-domain variational GP, constructed by projecting a GP onto a set of compactly supported B-spline basis functions. The key benefit of our approach is that the compact support of the B-spline basis functions admits the use of sparse linear algebra to significantly speed up matrix operations and drastically reduce the memory footprint. This allows us to very efficiently model fast-varying spatial phenomena with tens of thousands of inducing variables, where previo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#25105;&#20851;&#27880;&#33976;&#39311;&#27169;&#22359;&#21644;&#22810;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#25216;&#26415;&#26469;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#30340;&#40511;&#27807;&#65292;&#33258;&#21160;&#20174;&#20805;&#30005;&#26354;&#32447;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#21151;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#23558;&#30693;&#35782;&#20174;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#23436;&#25972;&#24490;&#29615;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#30340;&#27973;&#24490;&#29615;&#65292;&#23454;&#29616;&#20102;&#23545;&#21830;&#29992;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.05084</link><description>&lt;p&gt;
&#33258;&#25105;&#20851;&#27880;&#30693;&#35782;&#39046;&#22495;&#36866;&#24212;&#32593;&#32476;&#65306;&#27973;&#24490;&#29615;&#19979;&#21830;&#29992;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745; (arXiv:2304.05084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
A Self-attention Knowledge Domain Adaptation Network for Commercial Lithium-ion Batteries State-of-health Estimation under Shallow Cycles. (arXiv:2304.05084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05084
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#25105;&#20851;&#27880;&#33976;&#39311;&#27169;&#22359;&#21644;&#22810;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#25216;&#26415;&#26469;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#30340;&#40511;&#27807;&#65292;&#33258;&#21160;&#20174;&#20805;&#30005;&#26354;&#32447;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#21151;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#23558;&#30693;&#35782;&#20174;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#23436;&#25972;&#24490;&#29615;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#30340;&#27973;&#24490;&#29615;&#65292;&#23454;&#29616;&#20102;&#23545;&#21830;&#29992;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#20272;&#35745;&#23545;&#20110;&#30830;&#20445;&#30005;&#27744;&#20379;&#30005;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#65292;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;SOH&#20272;&#35745;&#26041;&#27861;&#20851;&#27880;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;0-100&#65285;&#28385;&#20805;&#29366;&#24577;&#65288;SOC&#65289;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#30005;&#27744;&#36890;&#24120;&#22312;&#37096;&#20998;SOC&#33539;&#22260;&#20869;&#22312;&#27973;&#24490;&#29615;&#26465;&#20214;&#19979;&#24037;&#20316;&#65292;&#24182;&#36981;&#24490;&#19981;&#21516;&#30340;&#36864;&#21270;&#26354;&#32447;&#65292;&#27809;&#26377;&#21487;&#29992;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#22240;&#27492;&#20351;SOH&#20272;&#35745;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20272;&#35745;&#27973;&#24490;&#29615;&#30005;&#27744;&#30340;SOH&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#25105;&#20851;&#27880;&#33976;&#39311;&#27169;&#22359;&#21644;&#22810;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#25216;&#26415;&#26469;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#30340;&#40511;&#27807;&#12290;&#26412;&#26041;&#27861;&#33258;&#21160;&#20174;&#20805;&#30005;&#26354;&#32447;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#21151;&#33021;&#65292;&#20197;&#23558;&#30693;&#35782;&#20174;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#23436;&#25972;&#24490;&#29615;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#30340;&#27973;&#24490;&#29615;&#12290;&#20351;&#29992;CALCE&#21644;SNL&#30005;&#27744;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20272;&#35745;&#27973;&#24490;&#29615;&#30005;&#27744;SOH&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate state-of-health (SOH) estimation is critical to guarantee the safety, efficiency and reliability of battery-powered applications. Most SOH estimation methods focus on the 0-100\% full state-of-charge (SOC) range that has similar distributions. However, the batteries in real-world applications usually work in the partial SOC range under shallow-cycle conditions and follow different degradation profiles with no labeled data available, thus making SOH estimation challenging. To estimate shallow-cycle battery SOH, a novel unsupervised deep transfer learning method is proposed to bridge different domains using self-attention distillation module and multi-kernel maximum mean discrepancy technique. The proposed method automatically extracts domain-variant features from charge curves to transfer knowledge from the large-scale labeled full cycles to the unlabeled shallow cycles. The CALCE and SNL battery datasets are employed to verify the effectiveness of the proposed method to estima
&lt;/p&gt;</description></item><item><title>TodyNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#21462;&#38544;&#34255;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#26426;&#21046;&#25429;&#33719;&#19981;&#21516;&#26102;&#38388;&#27133;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05078</link><description>&lt;p&gt;
TodyNet&#65306;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#26102;&#38388;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TodyNet: Temporal Dynamic Graph Neural Network for Multivariate Time Series Classification. (arXiv:2304.05078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05078
&lt;/p&gt;
&lt;p&gt;
TodyNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#21462;&#38544;&#34255;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#26426;&#21046;&#25429;&#33719;&#19981;&#21516;&#26102;&#38388;&#27133;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;(MTSC)&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#65292;&#21487;&#20197;&#36890;&#36807;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#30340;&#38544;&#34255;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#24456;&#23569;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#30340;&#29420;&#29305;&#21160;&#24577;&#29305;&#24449;&#65292;&#36825;&#32570;&#20047;&#36275;&#22815;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#26469;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;(TodyNet)&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#38544;&#34255;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#22270;&#32467;&#26500;&#12290;&#23427;&#20351;&#24471;&#38548;&#31163;&#20294;&#30456;&#20114;&#20381;&#36182;&#30340;&#21464;&#37327;&#20043;&#38388;&#33021;&#22815;&#36827;&#34892;&#20449;&#24687;&#27969;&#21160;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22270;&#26426;&#21046;&#25429;&#33719;&#19981;&#21516;&#26102;&#38388;&#27133;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;GNN&#30340;&#38480;&#21046;&#65292;&#26080;&#27861;&#23398;&#20064;&#21040;&#22270;&#30340;&#23618;&#27425;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#22270;&#27744;&#23618;&#65292;&#20197;&#27839;&#26102;&#38388;&#32500;&#24230;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#22810;&#32423;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TodyNet&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22266;&#26377;&#30340;&#21160;&#24577;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series classification (MTSC) is an important data mining task, which can be effectively solved by popular deep learning technology. Unfortunately, the existing deep learning-based methods neglect the hidden dependencies in different dimensions and also rarely consider the unique dynamic features of time series, which lack sufficient feature extraction capability to obtain satisfactory classification accuracy. To address this problem, we propose a novel temporal dynamic graph neural network (TodyNet) that can extract hidden spatio-temporal dependencies without undefined graph structure. It enables information flow among isolated but implicit interdependent variables and captures the associations between different time slots by dynamic graph mechanism, which further improves the classification performance of the model. Meanwhile, the hierarchical representations of graphs cannot be learned due to the limitation of GNNs. Thus, we also design a temporal graph pooling laye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26368;&#23567;&#20540;&#19978;&#30028;&#25552;&#20986;&#20102;&#25240;&#25187;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#30340;&#20272;&#35745;&#35823;&#24046;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#28151;&#21512;&#29305;&#24615;&#21644;&#25240;&#25187;&#22240;&#23376;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#23545;&#19968;&#32452;&#26174;&#33879;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#24212;&#30340;&#37319;&#26679;&#31243;&#24207;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.05073</link><description>&lt;p&gt;
&#25240;&#25187;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#21644;&#20272;&#35745;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
A Tale of Sampling and Estimation in Discounted Reinforcement Learning. (arXiv:2304.05073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26368;&#23567;&#20540;&#19978;&#30028;&#25552;&#20986;&#20102;&#25240;&#25187;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#30340;&#20272;&#35745;&#35823;&#24046;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#28151;&#21512;&#29305;&#24615;&#21644;&#25240;&#25187;&#22240;&#23376;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#23545;&#19968;&#32452;&#26174;&#33879;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#24212;&#30340;&#37319;&#26679;&#31243;&#24207;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25240;&#25187;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#21253;&#25324;&#22312;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#36807;&#31243;&#30340;&#31283;&#24577;&#20998;&#24067;&#19979;&#23545;&#20989;&#25968;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#20363;&#22914;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#39044;&#26399;&#22238;&#25253;&#25110;&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#20272;&#35745;&#36890;&#36807;&#26377;&#38480;&#22320;&#36827;&#34892;&#21608;&#26399;&#37319;&#26679;&#26469;&#20135;&#29983;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#24335;&#24573;&#30053;&#20102;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#29305;&#24615;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#31181;&#23454;&#38469;&#21644;&#29702;&#35770;&#35774;&#32622;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#65292;&#25991;&#29486;&#20013;&#20063;&#32570;&#20047;&#23545;&#21608;&#26399;&#37319;&#26679;&#30340;&#32570;&#38519;&#20197;&#21450;&#22914;&#20309;&#26368;&#20248;&#22320;&#36827;&#34892;&#21608;&#26399;&#37319;&#26679;&#30340;&#27491;&#24335;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25240;&#25187;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#30340;&#26368;&#23567;&#20540;&#19978;&#30028;&#65292;&#26126;&#30830;&#22320;&#23558;&#20272;&#35745;&#35823;&#24046;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#29305;&#24615;&#21644;&#25240;&#25187;&#22240;&#23376;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#26174;&#33879;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#24212;&#30340;&#37319;&#26679;&#31243;&#24207;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#24120;&#20351;&#29992;&#30340;&#26377;&#38480;&#26102;&#38388;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21452;&#26354;&#20960;&#20309;&#26377;&#25928;&#34920;&#36798;&#20102;&#22270;&#30340;&#20998;&#23618;&#23646;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#23618;&#27425;&#23646;&#24615;&#35757;&#32451;&#33410;&#28857;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05059</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Geometric Graph Representation Learning for Hierarchy-imbalance Node Classification. (arXiv:2304.05059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21452;&#26354;&#20960;&#20309;&#26377;&#25928;&#34920;&#36798;&#20102;&#22270;&#30340;&#20998;&#23618;&#23646;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#23618;&#27425;&#23646;&#24615;&#35757;&#32451;&#33410;&#28857;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#20013;&#19981;&#24179;&#34913;&#33410;&#28857;&#30340;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#26356;&#20026;&#26174;&#33879;&#21644;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22270;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#33410;&#28857;&#30340;&#25299;&#25169;&#29305;&#24615;&#65288;&#20363;&#22914;&#20301;&#32622;&#65292;&#35282;&#33394;&#65289;&#30340;&#19981;&#24179;&#34913;&#65288;&#25299;&#25169;&#19981;&#24179;&#34913;&#65289;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#26631;&#35760;&#33410;&#28857;&#30340;&#25968;&#37327;&#65288;&#25968;&#37327;&#19981;&#24179;&#34913;&#65289;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#25299;&#25169;&#19981;&#24179;&#34913;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33410;&#28857;&#30340;&#20301;&#32622;&#25110;&#32773;&#23616;&#37096;&#37051;&#22495;&#32467;&#26500;&#19978;&#65292;&#24573;&#30053;&#20102;&#22270;&#30340;&#20840;&#23616;&#24213;&#23618;&#20998;&#23618;&#23646;&#24615;&#65292;&#21363;&#23618;&#32423;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22270;&#25968;&#25454;&#30340;&#20998;&#23618;&#32467;&#26500;&#25581;&#31034;&#20102;&#22270;&#30340;&#37325;&#35201;&#25299;&#25169;&#23646;&#24615;&#24182;&#19982;&#24191;&#27867;&#30340;&#24212;&#29992;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#19981;&#21516;&#23618;&#27425;&#23646;&#24615;&#35757;&#32451;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21152;&#20197;&#35777;&#23454;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#21452;&#26354;&#20960;&#20309;&#22312;&#34920;&#31034;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning unbiased node representations for imbalanced samples in the graph has become a more remarkable and important topic. For the graph, a significant challenge is that the topological properties of the nodes (e.g., locations, roles) are unbalanced (topology-imbalance), other than the number of training labeled nodes (quantity-imbalance). Existing studies on topology-imbalance focus on the location or the local neighborhood structure of nodes, ignoring the global underlying hierarchical properties of the graph, i.e., hierarchy. In the real-world scenario, the hierarchical structure of graph data reveals important topological properties of graphs and is relevant to a wide range of applications. We find that training labeled nodes with different hierarchical properties have a significant impact on the node classification tasks and confirm it in our experiments. It is well known that hyperbolic geometry has a unique advantage in representing the hierarchical structure of graphs. Theref
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;FPGA&#19978;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#20449;&#21495;&#35299;&#26512;&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#30340;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#20449;&#21495;&#26144;&#23556;&#65292;&#20811;&#26381;&#20102;ANN&#32593;&#32476;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2304.05042</link><description>&lt;p&gt;
&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;ANN&#30340;&#21644;&#20256;&#32479;&#20449;&#21495;&#35299;&#26512;&#26041;&#27861;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#20449;&#31995;&#32479;&#20013;FPGA&#30340;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Approach combining ANN-based and Conventional Demapping in Communication for Efficient FPGA-Implementation. (arXiv:2304.05042v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;FPGA&#19978;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#20449;&#21495;&#35299;&#26512;&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#30340;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#20449;&#21495;&#26144;&#23556;&#65292;&#20811;&#26381;&#20102;ANN&#32593;&#32476;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#26159;&#25351;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26367;&#25442;&#25910;&#21457;&#22120;&#20013;&#30340;&#37096;&#20998;&#32452;&#20214;&#65292;&#36890;&#36807;&#20449;&#36947;&#27169;&#22411;&#23545;&#25972;&#20010;&#31995;&#32479;&#36827;&#34892;&#31471;&#23545;&#31471;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#36890;&#20449;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#20294;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#36866;&#29992;&#20110;&#33410;&#33021;&#30340;ANN&#23454;&#29616;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;ANN&#20013;&#30340;&#25805;&#20316;&#25968;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#24040;&#22823;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;&#36890;&#20449;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;FPGA&#19978;&#20351;&#29992;&#39640;&#25928;&#29575;&#30340;&#20256;&#32479;&#20449;&#21495;&#35299;&#26512;&#31639;&#27861;&#19982;&#20855;&#26377;&#36866;&#24212;&#24615;&#30340;AE&#30456;&#32467;&#21512;&#12290;&#22312;&#36866;&#24212;&#20449;&#36947;&#26465;&#20214;&#24182;&#35843;&#25972;&#21518;&#65292;ANN&#20250;&#38544;&#24335;&#23398;&#20064;&#21040;&#20449;&#36947;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In communication systems, Autoencoder (AE) refers to the concept of replacing parts of the transmitter and receiver by artificial neural networks (ANNs) to train the system end-to-end over a channel model. This approach aims to improve communication performance, especially for varying channel conditions, with the cost of high computational complexity for training and inference. Field-programmable gate arrays (FPGAs) have been shown to be a suitable platform for energy-efficient ANN implementation. However, the high number of operations and the large model size of ANNs limit the performance on resource-constrained devices, which is critical for low latency and high-throughput communication systems. To tackle his challenge, we propose a novel approach for efficient ANN-based remapping on FPGAs, which combines the adaptability of the AE with the efficiency of conventional demapping algorithms. After adaption to channel conditions, the channel characteristics, implicitly learned by the ANN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;SoftDTW&#65289;&#20316;&#20026;&#36830;&#32493;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#30340;&#19968;&#31181;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#24369;&#23545;&#40784;&#25968;&#25454;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#38382;&#39064;&#21644;&#23454;&#20540;&#30446;&#26631;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2304.05032</link><description>&lt;p&gt;
&#22810;&#38899;&#39640;&#20272;&#35745;&#21450;&#20854;&#20182;&#20219;&#21153;&#30340;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Soft Dynamic Time Warping for Multi-Pitch Estimation and Beyond. (arXiv:2304.05032v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;SoftDTW&#65289;&#20316;&#20026;&#36830;&#32493;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#30340;&#19968;&#31181;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#24369;&#23545;&#40784;&#25968;&#25454;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#38382;&#39064;&#21644;&#23454;&#20540;&#30446;&#26631;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;(MIR)&#20013;&#35768;&#22810;&#20219;&#21153;&#28041;&#21450;&#21040;&#24369;&#23545;&#40784;&#25968;&#25454;&#65292;&#20854;&#20013;&#30830;&#20999;&#30340;&#26102;&#38388;&#23545;&#24212;&#20851;&#31995;&#26410;&#30693;&#12290;&#36830;&#32493;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#24369;&#23545;&#40784;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;CTC&#20165;&#38480;&#20110;&#31163;&#25955;&#20540;&#30446;&#26631;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#25193;&#23637;&#21040;&#22810;&#26631;&#31614;&#38382;&#39064;&#26102;&#21487;&#33021;&#24456;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;SoftDTW&#65289;&#65292;&#19968;&#31181;&#32463;&#20856;DTW&#30340;&#21487;&#24494;&#21464;&#20307;&#65292;&#20316;&#20026;CTC&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#36807;&#22810;&#38899;&#39640;&#20272;&#35745;&#20316;&#20026;&#31034;&#20363;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SoftDTW&#19982;&#26368;&#20808;&#36827;&#30340;CTC&#22810;&#26631;&#31614;&#25193;&#23637;&#33719;&#24471;&#20102;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#38500;&#20102;&#22312;&#31639;&#27861;&#19978;&#26356;&#21152;&#20248;&#38597;&#65292;SoftDTW&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#23454;&#20540;&#30446;&#26631;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks in music information retrieval (MIR) involve weakly aligned data, where exact temporal correspondences are unknown. The connectionist temporal classification (CTC) loss is a standard technique to learn feature representations based on weakly aligned training data. However, CTC is limited to discrete-valued target sequences and can be difficult to extend to multi-label problems. In this article, we show how soft dynamic time warping (SoftDTW), a differentiable variant of classical DTW, can be used as an alternative to CTC. Using multi-pitch estimation as an example scenario, we show that SoftDTW yields results on par with a state-of-the-art multi-label extension of CTC. In addition to being more elegant in terms of its algorithmic formulation, SoftDTW naturally extends to real-valued target sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992; MIP &#25216;&#26415;&#29983;&#25104;&#20844;&#24179;&#19988;&#21487;&#35299;&#37322;&#30340;&#35780;&#20998;&#31995;&#32479;&#35299;&#20915;&#19968;&#33324;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102; SLIM &#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20135;&#29983;&#19982;&#29616;&#26377;&#26041;&#27861;&#31454;&#20105;&#24615;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05023</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#22810;&#31867;&#20998;&#31867;&#30340;&#26368;&#20248;&#20844;&#24179;&#35780;&#20998;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Fair Scoring Systems for Multi-Class Classification. (arXiv:2304.05023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992; MIP &#25216;&#26415;&#29983;&#25104;&#20844;&#24179;&#19988;&#21487;&#35299;&#37322;&#30340;&#35780;&#20998;&#31995;&#32479;&#35299;&#20915;&#19968;&#33324;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102; SLIM &#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20135;&#29983;&#19982;&#29616;&#26377;&#26041;&#27861;&#31454;&#20105;&#24615;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#29992;&#35780;&#20998;&#12289;&#21307;&#23398;&#25110;&#32047;&#29359;&#39044;&#27979;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#21644;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#25110;&#20877;&#29616;&#30340;&#19981;&#33391;&#20559;&#24046;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#31185;&#23398;&#30028;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#33324;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#27809;&#26377;&#25552;&#20986;&#20026;&#22810;&#31867;&#20998;&#31867;&#29983;&#25104;&#20844;&#24179;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#25216;&#26415;&#65292;&#22312;&#31232;&#30095;&#24615;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#65292;&#20026;&#19968;&#33324;&#30340;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#29983;&#25104;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#35780;&#20998;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#24191;&#20102; Rudin &#21644; Ustun &#25552;&#20986;&#30340; SLIM&#65288;Supersparse Linear Integer Models&#65289;&#26694;&#26550;&#65292;&#20197;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#30340;&#26368;&#20248;&#20844;&#24179;&#35780;&#20998;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#21644;&#22522;&#20934;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#24615;&#39044;&#27979;&#24615;&#33021;&#30340;&#20844;&#24179;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning models are increasingly used for decision making, in particular in high-stakes applications such as credit scoring, medicine or recidivism prediction. However, there are growing concerns about these models with respect to their lack of interpretability and the undesirable biases they can generate or reproduce. While the concepts of interpretability and fairness have been extensively studied by the scientific community in recent years, few works have tackled the general multi-class classification problem under fairness constraints, and none of them proposes to generate fair and interpretable models for multi-class classification. In this paper, we use Mixed-Integer Linear Programming (MILP) techniques to produce inherently interpretable scoring systems under sparsity and fairness constraints, for the general multi-class classification setup. Our work generalizes the SLIM (Supersparse Linear Integer Models) framework that was proposed by Rudin and Ustun to learn optimal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#20083;&#33146;&#30284;&#26816;&#27979;&#65292;&#37319;&#29992; ResNet50 &#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#29575;&#12289;AUC&#12289;&#21484;&#22238;&#29575;&#21644;&#25439;&#22833;&#29575;&#31561;&#25351;&#26631;&#26041;&#38754;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05022</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20083;&#33146;&#30284;&#26816;&#27979;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Deep Analysis of Transfer Learning Based Breast Cancer Detection Using Histopathology Images. (arXiv:2304.05022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#20083;&#33146;&#30284;&#26816;&#27979;&#65292;&#37319;&#29992; ResNet50 &#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#29575;&#12289;AUC&#12289;&#21484;&#22238;&#29575;&#21644;&#25439;&#22833;&#29575;&#31561;&#25351;&#26631;&#26041;&#38754;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#22919;&#22899;&#20013;&#26368;&#24120;&#35265;&#21644;&#26368;&#21361;&#38505;&#30340;&#30284;&#30151;&#20043;&#19968;&#65292;&#20063;&#21487;&#33021;&#24433;&#21709;&#30007;&#24615;&#12290;&#20351;&#29992;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#20083;&#33146;&#30284;&#27835;&#30103;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#36275;&#22815;&#30340;&#34920;&#22411;&#25968;&#25454;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#20083;&#33146;&#30284;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; ResNet50&#12289;ResNet101&#12289;VGG16 &#21644; VGG19 &#31561;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#21547; 2453 &#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#20083;&#33146;&#30284;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#34987;&#20998;&#20026;&#21547;&#26377;&#28024;&#28070;&#24615;&#23548;&#31649;&#30284;&#65288;IDC&#65289;&#21644;&#19981;&#21547; IDC &#20004;&#31867;&#12290;&#22312;&#23545;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616; ResNet50 &#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#29575;&#20026; 90.2%&#12289;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026; 90.0%&#12289;&#21484;&#22238;&#29575;&#20026; 94.7% &#21644;&#23567; margin &#25439;&#22833;&#29575;&#20026; 3.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is one of the most common and dangerous cancers in women, while it can also afflict men. Breast cancer treatment and detection are greatly aided by the use of histopathological images since they contain sufficient phenotypic data. A Deep Neural Network (DNN) is commonly employed to improve accuracy and breast cancer detection. In our research, we have analyzed pre-trained deep transfer learning models such as ResNet50, ResNet101, VGG16, and VGG19 for detecting breast cancer using the 2453 histopathology images dataset. Images in the dataset were separated into two categories: those with invasive ductal carcinoma (IDC) and those without IDC. After analyzing the transfer learning model, we found that ResNet50 outperformed other models, achieving accuracy rates of 90.2%, Area under Curve (AUC) rates of 90.0%, recall rates of 94.7%, and a marginal loss of 3.5%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36125;&#21494;&#26031;&#28508;&#21464;&#37327;&#8220;&#24847;&#22270;&#8221;&#65292;&#23558;&#20064;&#24815;&#24615;&#34892;&#20026;&#21644;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#39640;&#25928;&#30340;&#34892;&#20026;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2304.05008</link><description>&lt;p&gt;
&#20064;&#24815;&#19982;&#30446;&#26631;&#30340;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#34892;&#20026;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Habits and goals in synergy: a variational Bayesian framework for behavior. (arXiv:2304.05008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36125;&#21494;&#26031;&#28508;&#21464;&#37327;&#8220;&#24847;&#22270;&#8221;&#65292;&#23558;&#20064;&#24815;&#24615;&#34892;&#20026;&#21644;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#39640;&#25928;&#30340;&#34892;&#20026;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#39640;&#25928;&#12289;&#28789;&#27963;&#22320;&#34892;&#20026;&#26159;&#29702;&#35299;&#29983;&#29289;&#26426;&#20307;&#21644;&#21019;&#24314;&#26234;&#33021;&#21270;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#34892;&#20026;&#34987;&#24402;&#31867;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#24555;&#36895;&#32780;&#19981;&#28789;&#27963;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#30340;&#20064;&#24815;&#34892;&#20026;&#65292;&#21644;&#28789;&#27963;&#32780;&#24930;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#12290;&#20256;&#32479;&#19978;&#65292;&#20064;&#24815;&#21644;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#34987;&#35748;&#20026;&#26159;&#30001;&#22823;&#33041;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#31995;&#32479;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#65292;&#23558;&#36825;&#20004;&#31181;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#24341;&#20837;&#19968;&#31181;&#36125;&#21494;&#26031;&#28508;&#21464;&#37327;&#31216;&#20026;&#8220;&#24847;&#22270;&#8221;&#12290;&#20064;&#24815;&#24615;&#34892;&#20026;&#26159;&#36890;&#36807;&#20351;&#29992;&#24847;&#22270;&#30340;&#20808;&#39564;&#20998;&#24067;&#29983;&#25104;&#30340;&#65292;&#35813;&#20808;&#39564;&#20998;&#24067;&#26159;&#27809;&#26377;&#30446;&#26631;&#30340;&#65307;&#32780;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#26159;&#30001;&#24847;&#22270;&#30340;&#21518;&#39564;&#20998;&#24067;&#29983;&#25104;&#30340;&#65292;&#35813;&#21518;&#39564;&#20998;&#24067;&#21462;&#20915;&#20110;&#30446;&#26631;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34892;&#20026;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to behave efficiently and flexibly is a central problem for understanding biological agents and creating intelligent embodied AI. It has been well known that behavior can be classified as two types: reward-maximizing habitual behavior, which is fast while inflexible; and goal-directed behavior, which is flexible while slow. Conventionally, habitual and goal-directed behaviors are considered handled by two distinct systems in the brain. Here, we propose to bridge the gap between the two behaviors, drawing on the principles of variational Bayesian theory. We incorporate both behaviors in one framework by introducing a Bayesian latent variable called "intention". The habitual behavior is generated by using prior distribution of intention, which is goal-less; and the goal-directed behavior is generated by the posterior distribution of intention, which is conditioned on the goal. Building on this idea, we present a novel Bayesian framework for modeling behaviors. Our proposed framework 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#26377;&#25928;&#22320;&#35745;&#31639;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#21338;&#24328;&#31867;&#21035;&#20013;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#30340;&#31038;&#20250;&#31119;&#21033;&#65292;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05005</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#21644;&#26080;&#24724;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Bayes correlated equilibria and no-regret dynamics. (arXiv:2304.05005v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#26377;&#25928;&#22320;&#35745;&#31639;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#21338;&#24328;&#31867;&#21035;&#20013;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#30340;&#31038;&#20250;&#31119;&#21033;&#65292;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#21338;&#24328;&#30340;&#22343;&#34913;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#22522;&#26412;&#21338;&#24328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#19977;&#31181;&#29702;&#24819;&#30340;&#24179;&#34913;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22312;&#21338;&#24328;&#20013;&#24341;&#20837;&#35843;&#35299;&#32773;&#26469;&#33258;&#28982;&#22320;&#23454;&#29616;&#22343;&#34913;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#26377;&#25928;&#22320;&#35745;&#31639;&#22343;&#34913;&#12290;&#31532;&#19977;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#21338;&#24328;&#31867;&#21035;&#65292;&#35813;&#31867;&#22343;&#34913;&#36817;&#20284;&#22320;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;&#65292;&#21363;&#36890;&#36807;&#28798;&#21464;&#20195;&#20215;&#26469;&#24230;&#37327;&#12290;&#36825;&#19977;&#31181;&#23646;&#24615;&#20801;&#35768;&#29609;&#23478;&#35745;&#31639;&#22343;&#34913;&#65292;&#24182;&#36890;&#36807;&#35843;&#35299;&#32773;&#20351;&#20854;&#23454;&#29616;&#65292;&#20174;&#32780;&#22312;&#36817;&#20046;&#26368;&#20248;&#30340;&#31038;&#20250;&#31119;&#21033;&#20013;&#36798;&#25104;&#31283;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23384;&#22312;&#19968;&#20010;&#22343;&#34913;&#27010;&#24565;&#65292;&#28385;&#36275;&#36825;&#19977;&#31181;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21508;&#31181;&#65288;&#19981;&#31561;&#20215;&#30340;&#65289;&#30456;&#20851;&#22343;&#34913;&#25193;&#23637;&#65292;&#32479;&#31216;&#20026;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#20419;&#36827;&#29609;&#23478;&#20043;&#38388;&#20132;&#27969;&#30340;&#27807;&#36890;&#22343;&#34913;&#65288;&#20063;&#31216;&#20026;&#21327;&#35843;&#26426;&#21046;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24724;&#21160;&#24577;&#65292;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#25910;&#25947;&#20110;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24179;&#34913;&#27010;&#24565;&#28385;&#36275;&#19977;&#31181;&#29702;&#24819;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#24182;&#21576;&#29616;&#20102;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores equilibrium concepts for Bayesian games, which are fundamental models of games with incomplete information. We aim at three desirable properties of equilibria. First, equilibria can be naturally realized by introducing a mediator into games. Second, an equilibrium can be computed efficiently in a distributed fashion. Third, any equilibrium in that class approximately maximizes social welfare, as measured by the price of anarchy, for a broad class of games. These three properties allow players to compute an equilibrium and realize it via a mediator, thereby settling into a stable state with approximately optimal social welfare. Our main result is the existence of an equilibrium concept that satisfies these three properties.  Toward this goal, we characterize various (non-equivalent) extensions of correlated equilibria, collectively known as Bayes correlated equilibria. In particular, we focus on communication equilibria (also known as coordination mechanisms), which 
&lt;/p&gt;</description></item><item><title>NeMo&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#22810;&#32593;&#32476;&#25193;&#25955;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#65292;&#37319;&#29992;&#29983;&#25104;&#24335;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#27491;&#36127;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#26469;&#36827;&#34892;&#29992;&#25143;&#20852;&#36259;&#20256;&#25773;&#12290;NeMo&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.04994</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#22810;&#32593;&#32476;&#25193;&#25955;&#30340;&#31038;&#20132;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Neural Multi-network Diffusion towards Social Recommendation. (arXiv:2304.04994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04994
&lt;/p&gt;
&lt;p&gt;
NeMo&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#22810;&#32593;&#32476;&#25193;&#25955;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#65292;&#37319;&#29992;&#29983;&#25104;&#24335;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#27491;&#36127;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#26469;&#36827;&#34892;&#29992;&#25143;&#20852;&#36259;&#20256;&#25773;&#12290;NeMo&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#31038;&#20132;&#25512;&#33616;&#31561;&#21508;&#31867;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#21644;&#36807;&#20998;&#24179;&#28369;&#21270;&#30340;&#20005;&#37325;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21644;&#30452;&#25509;&#23884;&#20837;&#29616;&#25104;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#22810;&#32593;&#32476;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;NeMo&#65289;&#65292;&#29992;&#20110;&#31038;&#20132;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#19968;&#31181;&#29983;&#25104;&#24335;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#27491;&#36127;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#26469;&#36827;&#34892;&#29992;&#25143;&#20852;&#36259;&#20256;&#25773;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NeMo&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65288;&#20363;&#22914;&#65292;&#22312;NDCG@15&#26041;&#38754;&#25552;&#39640;&#20102;&#39640;&#36798;38.8%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely applied on a variety of real-world applications, such as social recommendation. However, existing GNN-based models on social recommendation suffer from serious problems of generalization and oversmoothness, because of the underexplored negative sampling method and the direct implanting of the off-the-shelf GNN models. In this paper, we propose a succinct multi-network GNN-based neural model (NeMo) for social recommendation. Compared with the existing methods, the proposed model explores a generative negative sampling strategy, and leverages both the positive and negative user-item interactions for users' interest propagation. The experiments show that NeMo outperforms the state-of-the-art baselines on various real-world benchmark datasets (e.g., by up to 38.8% in terms of NDCG@15).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;SDN&#25216;&#26415;&#26469;&#30417;&#27979;&#21644;&#24378;&#21046;&#27599;&#20010;IoT&#35774;&#22791;&#30340;&#39044;&#26399;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;MUD&#26631;&#20934;&#20943;&#23569;&#25915;&#20987;&#24418;&#21183;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#31867;&#20998;&#31867;&#27169;&#22411;&#65292;&#21160;&#24577;&#26816;&#27979;&#32593;&#32476;&#27963;&#21160;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#20197;&#26816;&#27979;&#22823;&#27969;&#37327;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#35823;&#25253;&#29575;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.04987</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;MUD&#27963;&#21160;&#30340;&#21160;&#24577;&#30417;&#27979;&#65292;&#26816;&#27979;IoT&#22823;&#27969;&#37327;&#25915;&#20987;&#20013;&#30340;&#24322;&#24120;&#24494;&#27969;
&lt;/p&gt;
&lt;p&gt;
Detecting Anomalous Microflows in IoT Volumetric Attacks via Dynamic Monitoring of MUD Activity. (arXiv:2304.04987v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;SDN&#25216;&#26415;&#26469;&#30417;&#27979;&#21644;&#24378;&#21046;&#27599;&#20010;IoT&#35774;&#22791;&#30340;&#39044;&#26399;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;MUD&#26631;&#20934;&#20943;&#23569;&#25915;&#20987;&#24418;&#21183;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#31867;&#20998;&#31867;&#27169;&#22411;&#65292;&#21160;&#24577;&#26816;&#27979;&#32593;&#32476;&#27963;&#21160;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#20197;&#26816;&#27979;&#22823;&#27969;&#37327;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#35823;&#25253;&#29575;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoT&#32593;&#32476;&#36234;&#26469;&#36234;&#25104;&#20026;&#26032;&#22411;&#32593;&#32476;&#25915;&#20987;&#30340;&#30446;&#26631;&#12290;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#33021;&#22815;&#21457;&#29616;&#26032;&#22411;&#25915;&#20987;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#35823;&#25253;&#35686;&#12289;&#35299;&#37322;&#22256;&#38590;&#21644;&#38590;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;IETF&#26368;&#36817;&#21457;&#24067;&#30340;&#21378;&#21830;&#20351;&#29992;&#25551;&#36848;&#65288;MUD&#65289;&#26631;&#20934;&#26377;&#26395;&#36890;&#36807;&#26126;&#30830;&#25351;&#23450;IoT&#35774;&#22791;&#39044;&#26399;&#30340;&#32593;&#32476;&#34892;&#20026;&#26469;&#38480;&#21046;&#23427;&#20204;&#30340;&#25915;&#20987;&#34920;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;SDN&#26469;&#24378;&#21046;&#25191;&#34892;&#21644;&#30417;&#27979;&#27599;&#20010;IoT&#35774;&#22791;&#30340;&#39044;&#26399;&#34892;&#20026;&#65292;&#24182;&#35757;&#32451;&#21333;&#31867;&#20998;&#31867;&#27169;&#22411;&#26469;&#26816;&#27979;&#22823;&#27969;&#37327;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#22235;&#20010;&#26041;&#38754;&#12290;&#19968;&#26159;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#32423;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;SDN&#36965;&#27979;&#21160;&#24577;&#26816;&#27979;MUD&#21512;&#35268;&#27969;&#37327;&#30340;&#32593;&#32476;&#27963;&#21160;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#65292;&#28982;&#21518;&#23545;&#24322;&#24120;&#27969;&#36827;&#34892;&#25968;&#25454;&#21253;&#26816;&#26597;&#12290;&#36825;&#25552;&#20379;&#20102;&#23545;&#20998;&#24067;&#24335;&#21644;&#30452;&#25509;&#25915;&#20987;&#30340;&#24378;&#21270;&#32454;&#31890;&#24230;&#21487;&#35265;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#38548;&#31163;&#22823;&#27969;&#37327;&#26469;&#28304;&#12290;&#20108;&#26159;&#25105;&#20204;&#21033;&#29992;MUD&#20943;&#23569;&#25915;&#20987;&#34920;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24050;&#30693;&#30340;&#27491;&#24120;&#27969;&#37327;&#27169;&#24335;&#19978;&#35757;&#32451;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#19977;&#26159;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;MUD&#22686;&#24378;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#26032;&#25216;&#26415;&#12290;&#22235;&#26159;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;IoT&#27979;&#35797;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#22823;&#27969;&#37327;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#35823;&#25253;&#29575;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT networks are increasingly becoming target of sophisticated new cyber-attacks. Anomaly-based detection methods are promising in finding new attacks, but there are certain practical challenges like false-positive alarms, hard to explain, and difficult to scale cost-effectively. The IETF recent standard called Manufacturer Usage Description (MUD) seems promising to limit the attack surface on IoT devices by formally specifying their intended network behavior. In this paper, we use SDN to enforce and monitor the expected behaviors of each IoT device, and train one-class classifier models to detect volumetric attacks.  Our specific contributions are fourfold. (1) We develop a multi-level inferencing model to dynamically detect anomalous patterns in network activity of MUD-compliant traffic flows via SDN telemetry, followed by packet inspection of anomalous flows. This provides enhanced fine-grained visibility into distributed and direct attacks, allowing us to precisely isolate volumetr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;BFReg-NN&#65289;&#65292;&#23427;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#24182;&#23558;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#29289;&#23398;&#30693;&#35782;&#65288;&#22914;&#22522;&#22240;&#35843;&#33410;&#32593;&#32476;&#21644;GO&#65289;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#27169;&#25311;&#29983;&#29289;&#31995;&#32479;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04982</link><description>&lt;p&gt;
&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Biological Factor Regulatory Neural Network. (arXiv:2304.04982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;BFReg-NN&#65289;&#65292;&#23427;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#24182;&#23558;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#29289;&#23398;&#30693;&#35782;&#65288;&#22914;&#22522;&#22240;&#35843;&#33410;&#32593;&#32476;&#21644;GO&#65289;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#27169;&#25311;&#29983;&#29289;&#31995;&#32479;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#26159;&#20998;&#26512;&#29983;&#29289;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#22522;&#22240;&#34920;&#36798;&#36827;&#34892;&#21508;&#31181;&#29983;&#29289;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#26412;&#36136;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24456;&#38590;&#20026;&#20154;&#31867;&#25552;&#20379;&#29983;&#29289;&#23398;&#27934;&#35265;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#24037;&#20316;&#23558;&#29983;&#29289;&#30693;&#35782;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#32435;&#20837;&#37096;&#20998;&#29983;&#29289;&#23398;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#20122;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;BFReg-NN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#27169;&#25311;&#32454;&#32990;&#31995;&#32479;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#20851;&#31995;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;BFReg-NN&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#24320;&#22987;&#65292;&#33021;&#22815;&#23558;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#29289;&#23398;&#30693;&#35782;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#21253;&#25324;&#22522;&#22240;&#25110;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#35843;&#33410;&#20851;&#31995;&#65288;&#20363;&#22914;&#22522;&#22240;&#35843;&#33410;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65288;PPI&#65289;&#65289;&#21644;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65292;pathways&#65289;&#12290;BFReg-NN&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#23376;&#35843;&#33410;&#24809;&#32602;&#65288;FRP&#65289;&#30340;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#27169;&#22411;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#35843;&#33410;&#20851;&#31995;&#24471;&#21040;&#34920;&#31034;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BFReg-NN&#30456;&#23545;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genes are fundamental for analyzing biological systems and many recent works proposed to utilize gene expression for various biological tasks by deep learning models. Despite their promising performance, it is hard for deep neural networks to provide biological insights for humans due to their black-box nature. Recently, some works integrated biological knowledge with neural networks to improve the transparency and performance of their models. However, these methods can only incorporate partial biological knowledge, leading to suboptimal performance. In this paper, we propose the Biological Factor Regulatory Neural Network (BFReg-NN), a generic framework to model relations among biological factors in cell systems. BFReg-NN starts from gene expression data and is capable of merging most existing biological knowledge into the model, including the regulatory relations among genes or proteins (e.g., gene regulatory networks (GRN), protein-protein interaction networks (PPI)) and the hierarc
&lt;/p&gt;</description></item><item><title>Wav2code&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;ASR&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#26080;&#22833;&#30495;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.04974</link><description>&lt;p&gt;
Wav2code&#65306;&#36890;&#36807;&#30721;&#26412;&#26597;&#25214;&#24674;&#22797;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;ASR
&lt;/p&gt;
&lt;p&gt;
Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR. (arXiv:2304.04974v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04974
&lt;/p&gt;
&lt;p&gt;
Wav2code&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;ASR&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#26080;&#22833;&#30495;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#22024;&#26434;&#29615;&#22659;&#19979;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#35821;&#38899;&#22686;&#24378;(SE)&#24341;&#20837;&#20316;&#20026;&#21069;&#31471;&#26469;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#30001;&#20110;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;&#65292;&#21487;&#33021;&#23545;&#19979;&#28216;ASR&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#26368;&#26032;&#30340;&#24037;&#20316;&#23558;SE&#21644;&#24403;&#21069;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#32467;&#21512;&#36215;&#26469;&#26469;&#32531;&#35299;&#22833;&#30495;&#38382;&#39064;&#24182;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#26377;&#25928;&#24615;&#65292;&#20294;&#20256;&#32479;SE&#24341;&#36215;&#30340;&#35821;&#38899;&#22833;&#30495;&#20173;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wav2code&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;ASR&#30340;&#26080;&#22833;&#30495;&#24191;&#20041;SE&#12290;&#39318;&#20808;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;SSL&#27169;&#22411;&#33719;&#24471;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#36890;&#36807;&#26368;&#36817;&#37051;&#29305;&#24449;&#21305;&#37197;&#26597;&#25214;&#31163;&#25955;&#30721;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#24471;&#21040;&#30340;&#20195;&#30721;&#24207;&#21015;&#26469;&#37325;&#26500;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#20197;&#33719;&#24471;&#24178;&#20928;&#30340;&#35821;&#38899;&#34920;&#24449;&#65307;&#25509;&#30528;&#65292;&#35813;&#20195;&#30721;&#24207;&#21015;&#34987;&#29992;&#20110;&#26080;&#22833;&#30495;&#22320;&#22686;&#24378;&#24102;&#22122;&#35821;&#38899;&#20197;&#20415;&#20110;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has gained a remarkable success thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be completely eliminated. In this paper, we propose a self-supervised framework named Wav2code to implement a generalized SE without distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the origin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#35757;&#32451;&#38454;&#27573;&#22312;&#20998;&#31867;&#22120;&#36755;&#20986;&#19978;&#28155;&#21152;&#20559;&#31227;&#20197;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.04972</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#31867;&#22120;&#20559;&#31227;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Classifier Shift for Class Imbalance. (arXiv:2304.04972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#35757;&#32451;&#38454;&#27573;&#22312;&#20998;&#31867;&#22120;&#36755;&#20986;&#19978;&#28155;&#21152;&#20559;&#31227;&#20197;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#21327;&#20316;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#23646;&#20110;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#19988;&#19981;&#20801;&#35768;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;IID&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#25361;&#25112;&#65292;&#22914;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#28418;&#31227;&#24182;&#26174;&#33879;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FedShift&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#35757;&#32451;&#38454;&#27573;&#22312;&#20998;&#31867;&#22120;&#36755;&#20986;&#19978;&#28155;&#21152;&#20559;&#31227;&#20197;&#20943;&#36731;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#28040;&#26497;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;FedShift&#20013;&#30340;&#20998;&#31867;&#22120;&#20559;&#31227;&#21487;&#20197;&#20351;&#26412;&#22320;&#26368;&#20248;&#35299;&#19982;&#20840;&#23616;&#26368;&#20248;&#35299;&#19968;&#33268;&#65292;&#24182;&#30830;&#20445;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;FedShift&#22312;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning aims to learn a global model collaboratively while the training data belongs to different clients and is not allowed to be exchanged. However, the statistical heterogeneity challenge on non-IID data, such as class imbalance in classification, will cause client drift and significantly reduce the performance of the global model. This paper proposes a simple and effective approach named FedShift which adds the shift on the classifier output during the local training phase to alleviate the negative impact of class imbalance. We theoretically prove that the classifier shift in FedShift can make the local optimum consistent with the global optimum and ensure the convergence of the algorithm. Moreover, our experiments indicate that FedShift significantly outperforms the other state-of-the-art federated learning approaches on various datasets regarding accuracy and communication efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04970</link><description>&lt;p&gt;
GRIL&#65306;&#19968;&#31181;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21442;&#25968;&#25345;&#20037;&#24615;&#21516;Topology Data Analysis (TDA)&#30456;&#20851;&#65292;&#21487;&#30740;&#31350;&#25968;&#25454;&#20013;&#38544;&#34255;&#30528;&#30340;&#36830;&#36890;&#20998;&#37327;&#21644;&#24490;&#29615;&#31561;&#25299;&#25169;&#29305;&#24449;&#12290;&#24050;&#24212;&#29992;&#20110;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20016;&#23500;&#25299;&#25169;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30740;&#31350;&#21452;&#36807;&#28388;&#20989;&#25968;&#35825;&#23548;&#30340;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#27169;&#22359;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#34920;&#31034;&#20449;&#24687;&#21152;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21521;&#37327;&#34920;&#31034;&#31216;&#20026;Generalized Rank Invariant Landscape \textsc{Gril}&#65292;&#24182;&#23558;&#20854;&#35777;&#26126;&#20026;&#22312;Lipschitz&#31283;&#23450;&#26465;&#20214;&#19979;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#30784;&#36807;&#28388;&#20989;&#25968;&#30340;&#32534;&#30721;&#21487;&#20197;&#23481;&#26131;&#22320;&#34701;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#21521;&#37327;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
$1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Perp-Neg&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#30446;&#21069;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#65292;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;3D&#24212;&#29992;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.04968</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#36127;&#25552;&#31034;&#31639;&#27861;&#65306;&#23558;2D&#25193;&#25955;&#36716;&#21270;&#20026;3D&#65292;&#32531;&#35299;&#8220;&#25196;&#23612;&#26031;&#38382;&#39064;&#8221;&#31561;&#31561;
&lt;/p&gt;
&lt;p&gt;
Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond. (arXiv:2304.04968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Perp-Neg&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#30446;&#21069;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#65292;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;3D&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#26377;&#26102;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#31867;&#20284;&#20110;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#30340;&#25991;&#26412;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;2D&#21644;3D&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#36127;&#38754;&#25552;&#31034;&#65292;&#20294;&#21457;&#29616;&#24403;&#21069;&#30340;&#23454;&#29616;&#26080;&#27861;&#20135;&#29983;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#20027;&#25552;&#31034;&#21644;&#36127;&#38754;&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;Perp-Neg&#65292;&#19968;&#31181;&#21033;&#29992;&#24471;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#26469;&#35299;&#20915;&#24403;&#21069;&#36127;&#24615;&#25552;&#31034;&#31639;&#27861;&#32570;&#28857;&#30340;&#26032;&#31639;&#27861;&#12290;Perp-Neg&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;Perp-Neg&#36890;&#36807;&#22312;2D&#24773;&#20917;&#19979;&#20351;&#29992;&#25143;&#33021;&#22815;&#32534;&#36753;&#25481;&#21021;&#22987;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19981;&#24819;&#35201;&#30340;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#29983;&#25104;&#22270;&#20687;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#30340;&#31639;&#27861;&#21040;3D&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Perp-Neg&#30340;3D&#36127;&#38754;&#25552;&#31034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#21387;&#32553;&#21367;&#31215;&#23618;&#30340;&#24352;&#37327;&#26684;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#22810;&#32500;&#26680;&#20026;&#19968;&#32500;&#28388;&#27874;&#22120;&#26469;&#20943;&#23569;CNN&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#26102;&#20934;&#30830;&#39044;&#27979;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04964</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27874;&#24418;&#27169;&#25311;&#22120;&#20013;&#30340;&#20808;&#39564;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
A priori compression of convolutional neural networks for wave simulators. (arXiv:2304.04964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04964
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#21387;&#32553;&#21367;&#31215;&#23618;&#30340;&#24352;&#37327;&#26684;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#22810;&#32500;&#26680;&#20026;&#19968;&#32500;&#28388;&#27874;&#22120;&#26469;&#20943;&#23569;CNN&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#26102;&#20934;&#30830;&#39044;&#27979;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29616;&#22312;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#38754;&#37096;&#21644;&#29289;&#20307;&#35782;&#21035;&#12289;&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#31561;&#12290;&#27492;&#22806;&#65292;&#20687;&#29289;&#29702;&#20449;&#24687;&#27169;&#25311;&#22120;&#36825;&#26679;&#30340;&#24212;&#29992;&#38656;&#35201;&#23454;&#26102;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21253;&#21547;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#21442;&#25968;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#26377;&#38480;&#20869;&#23384;&#30340;&#35774;&#22791;&#19978;&#23433;&#35013;&#27492;&#31867;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#21387;&#32553;&#25216;&#26415;&#21487;&#33021;&#36890;&#36807;&#20943;&#23569;&#36129;&#29486;&#21040;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#21442;&#25968;&#25968;&#37327;&#26469;&#20943;&#23567;CNN&#27169;&#22411;&#30340;&#22823;&#23567;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#30340;&#21367;&#31215;&#23618;&#24352;&#37327;&#26684;&#24335;&#65292;&#20808;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36827;&#34892;&#12290;&#21367;&#31215;&#23618;&#20013;&#30340;&#19977;&#32500;&#25110;&#20108;&#32500;&#26680;&#34987;&#19968;&#32500;&#36807;&#28388;&#22120;&#26367;&#25442;&#12290;&#36807;&#24230;&#25311;&#21512;&#29616;&#35937;&#20063;&#23558;&#20943;&#23569;&#12290;&#39044;&#27979;&#25152;&#38656;&#30340;&#26102;&#38388;&#25110;&#35745;&#31639;&#26426;&#20223;&#30495;&#21644;&#39044;&#27979;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#23558;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks are now seeing widespread use in a variety of fields, including image classification, facial and object recognition, medical imaging analysis, and many more. In addition, there are applications such as physics-informed simulators in which accurate forecasts in real time with a minimal lag are required. The present neural network designs include millions of parameters, which makes it difficult to install such complex models on devices that have limited memory. Compression techniques might be able to resolve these issues by decreasing the size of CNN models that are created by reducing the number of parameters that contribute to the complexity of the models. We propose a compressed tensor format of convolutional layer, a priori, before the training of the neural network. 3-way kernels or 2-way kernels in convolutional layers are replaced by one-way fiters. The overfitting phenomena will be reduced also. The time needed to make predictions or time required fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;spa&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.04932</link><description>&lt;p&gt;
&#37327;&#23376;&#22855;&#24322;&#20540;&#21464;&#25442;&#21450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms. (arXiv:2304.04932v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;spa&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#26377;&#20960;&#31181;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#21644;&#29305;&#21035;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#37327;&#23376;&#31639;&#27861;&#34987;&#8220;&#21435;&#37327;&#23376;&#21270;&#8221;&#12290;&#36825;&#20123;&#21435;&#37327;&#23376;&#21270;&#32467;&#26524;&#36890;&#24120;&#22312;&#32463;&#20856;&#31639;&#27861;&#36890;&#36807;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#26041;&#27861;&#35775;&#38382;&#25968;&#25454;&#26102;&#25104;&#31435;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#21435;&#37327;&#23376;&#21270;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#32463;&#20856;&#31639;&#27861;&#21482;&#33021;&#20174;&#25509;&#36817;&#29702;&#24819;&#20998;&#24067;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#34429;&#28982;&#37327;&#23376;&#31639;&#27861;&#22312;&#38754;&#23545;&#23567;&#25200;&#21160;&#26102;&#26412;&#36136;&#19978;&#26159;&#40065;&#26834;&#30340;&#65292;&#20294;&#24403;&#21069;&#30340;&#21435;&#37327;&#23376;&#21270;&#25216;&#26415;&#24182;&#19981;&#26159;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35768;&#22810;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30001;Chia&#12289;Gily\'en&#12289;Li&#12289;Lin&#12289;Tang&#21644;Wang&#65288;JACM 2022&#65289;&#25552;&#20986;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;&#29992;&#20110;spa&#30340;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several quantum algorithms for linear algebra problems, and in particular quantum machine learning problems, have been "dequantized" in the past few years. These dequantization results typically hold when classical algorithms can access the data via length-squared sampling. In this work we investigate how robust these dequantization results are. We introduce the notion of approximate length-squared sampling, where classical algorithms are only able to sample from a distribution close to the ideal distribution in total variation distance. While quantum algorithms are natively robust against small perturbations, current techniques in dequantization are not. Our main technical contribution is showing how many techniques from randomized linear algebra can be adapted to work under this weaker assumption as well. We then use these techniques to show that the recent low-rank dequantization framework by Chia, Gily\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework for spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#26469;&#38477;&#20302;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#20272;&#35745;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#20808;&#21033;&#29992;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#65292;&#28982;&#21518;&#29992;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#29366;&#24577;&#32858;&#21512;&#65292;&#26368;&#32456;&#21033;&#29992;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.04916</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#26469;&#38477;&#20302;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#20272;&#35745;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#20808;&#21033;&#29992;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#65292;&#28982;&#21518;&#29992;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#29366;&#24577;&#32858;&#21512;&#65292;&#26368;&#32456;&#21033;&#29992;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#20351;&#29992;&#20195;&#29702;&#34892;&#20026;&#25968;&#25454;&#20272;&#35745;&#20195;&#29702;&#22870;&#21169;&#20989;&#25968;&#65288;&#20063;&#31216;&#20026;&#8220;&#32467;&#26500;&#21442;&#25968;&#8221;&#65289;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#65292;&#36825;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#32858;&#21512;&#29366;&#24577;&#65292;&#38477;&#20302;&#20102;&#20272;&#35745;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28789;&#27963;&#30340;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#30340;Q&#20989;&#25968;&#65292;&#20197;&#21450;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#65292;&#36873;&#25321;&#20102;&#19968;&#20123;&#26368;&#20026;&#37325;&#35201;&#30340;&#29366;&#24577;&#65292;&#36825;&#20123;&#29366;&#24577;&#23545;&#20110;&#39537;&#21160;Q&#20989;&#25968;&#30340;&#21464;&#21270;&#26368;&#20026;&#20851;&#38190;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#36825;&#20123;&#34987;&#36873;&#25321;&#30340;&#8220;&#32858;&#21512;&#8221;&#29366;&#24577;&#65292;&#25105;&#20204;&#20351;&#29992;&#24120;&#29992;&#30340;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#38454;&#27573;&#26041;&#27861;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#20351;&#29992;CNN&#21644;Transformer&#26469;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#28072;&#36300;&#24133;&#12290;</title><link>http://arxiv.org/abs/2304.04912</link><description>&lt;p&gt;
&#20351;&#29992;CNN&#21644;Transformer&#36827;&#34892;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Financial Time Series Forecasting using CNN and Transformer. (arXiv:2304.04912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#20351;&#29992;CNN&#21644;Transformer&#26469;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#28072;&#36300;&#24133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20915;&#31574;&#20013;&#37117;&#26159;&#37325;&#35201;&#30340;&#12290;&#20854;&#20013;&#65292;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#65288;&#22914;&#32929;&#31080;&#20215;&#26684;&#65289;&#30340;&#39044;&#27979;&#24448;&#24448;&#27604;&#36739;&#22256;&#38590;&#65292;&#22240;&#20026;&#24456;&#38590;&#23545;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#24314;&#27169;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#20110;&#25429;&#25417;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#23616;&#37096;&#27169;&#24335;&#24456;&#25797;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24863;&#21463;&#37326;&#65292;CNN&#19981;&#33021;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Transformer&#21487;&#20197;&#23398;&#20064;&#20840;&#23616;&#19978;&#19979;&#25991;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;CNN&#21644;Transformer&#30340;&#20248;&#21183;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#20215;&#26684;&#30340;&#28072;&#36300;&#24133;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#24120;&#29992;&#30340;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#26631;&#26222;500&#25104;&#20998;&#32929;&#30424;&#20013;&#30424;&#32929;&#20215;&#26684;&#21464;&#21270;&#26041;&#38754;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is important across various domains for decision-making. In particular, financial time series such as stock prices can be hard to predict as it is difficult to model short-term and long-term temporal dependencies between data points. Convolutional Neural Networks (CNN) are good at capturing local patterns for modeling short-term dependencies. However, CNNs cannot learn long-term dependencies due to the limited receptive field. Transformers on the other hand are capable of learning global context and long-term dependencies. In this paper, we propose to harness the power of CNNs and Transformers to model both short-term and long-term dependencies within a time series, and forecast if the price would go up, down or remain the same (flat) in the future. In our experiments, we demonstrated the success of the proposed method in comparison to commonly adopted statistical and deep learning methods on forecasting intraday stock price change of S&amp;P 500 constituents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23454;&#26102;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#31995;&#21015;&#24377;&#24615;&#25191;&#34892;&#22120;&#30340;&#21147;&#37327;&#25511;&#21046;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#30828;&#20214;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.04911</link><description>&lt;p&gt;
&#23454;&#26102;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#31995;&#21015;&#24377;&#24615;&#25191;&#34892;&#22120;&#21147;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator. (arXiv:2304.04911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23454;&#26102;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#31995;&#21015;&#24377;&#24615;&#25191;&#34892;&#22120;&#30340;&#21147;&#37327;&#25511;&#21046;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#30828;&#20214;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#21015;&#24377;&#24615;&#25191;&#34892;&#22120;(SEAs)&#26159;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#19968;&#31181;&#38381;&#29615;&#21147;&#25511;&#21046;&#25216;&#26415;&#65292;&#21487;&#23454;&#29616;&#35832;&#22914;&#34892;&#36208;&#12289;&#20030;&#36215;&#21644;&#25805;&#20316;&#31561;&#22797;&#26434;&#20219;&#21153;&#12290;&#27169;&#22411;&#33258;&#30001;PID&#25511;&#21046;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;SEAs&#38750;&#32447;&#24615;&#30340;&#19981;&#31283;&#23450;&#24615;&#24433;&#21709;&#65292;&#32780;&#32423;&#32852;&#27169;&#22411;&#22522;&#30784;&#40065;&#26834;&#25511;&#21046;&#22120;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#24433;&#21709;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#21147;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#30784;&#26041;&#27861;&#38656;&#35201;&#35814;&#32454;&#35843;&#26597;&#20197;&#20934;&#30830;&#34920;&#24449;&#31995;&#32479;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#20013;&#20165;&#26377;&#23569;&#37327;&#24037;&#20316;&#28041;&#21450;&#30828;&#20214;&#23398;&#20064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22312;SEAs&#25670;&#31995;&#32479;&#30828;&#20214;&#19978;&#20351;&#29992;Proximal Policy Optimization&#65288;PPO&#65289;&#31639;&#27861;&#23545;DRL&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#36319;&#36394;&#21147;&#25511;&#21046;&#36712;&#36857;&#20174;0.05 ~ 0.35 Hz&#65292;&#25391;&#24133;&#20026;50 N&#12290;&#24320;&#21457;&#21644;&#20351;&#29992;&#23433;&#20840;&#26426;&#21046;&#26469;&#35757;&#32451;&#31574;&#30053;12&#23567;&#26102;(&#36807;&#22812;)&#32780;&#26080;&#38656;&#25805;&#20316;&#21592;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for 12 hours (overnight) without an operator present with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#20195;&#29702;&#20219;&#21153;&#29992;&#20110;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04907</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#20197;&#25913;&#21892;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#20195;&#29702;&#20219;&#21153;&#29992;&#20110;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20174;&#21487;&#23548;&#33322;&#20301;&#32622;&#38598;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#26469;&#36873;&#25321;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36827;&#19968;&#27493;&#25506;&#32034;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21463;&#30410;&#20110;&#22312;&#23548;&#33322;&#26399;&#38388;&#29983;&#25104;&#28508;&#22312;&#26410;&#26469;&#35270;&#22270;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#20154;&#31867;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#21608;&#22260;&#30340;&#35270;&#22270;&#20250;&#23545;&#26410;&#26469;&#30340;&#29615;&#22659;&#26377;&#19968;&#20010;&#39044;&#26399;&#65292;&#24182;&#24110;&#21161;&#27491;&#30830;&#22320;&#23548;&#33322;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32473;&#20195;&#29702;&#35013;&#22791;&#36825;&#31181;&#29983;&#25104;&#26410;&#26469;&#23548;&#33322;&#35270;&#22270;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#19977;&#31181;&#20195;&#29702;&#20219;&#21153;: &#25513;&#34109;&#20840;&#26223;&#24314;&#27169; (MPM)&#65292;&#25513;&#34109;&#36712;&#36857;&#24314;&#27169; (MTM) &#21644;&#24102;&#26377;&#22270;&#20687;&#29983;&#25104;&#30340;&#21160;&#20316;&#39044;&#27979; (APIG)&#12290;&#36825;&#19977;&#20010;&#30446;&#26631;&#25945;&#20250;&#20102;&#27169;&#22411;&#39044;&#27979;&#20840;&#26223;&#20013;&#30340;&#32570;&#23569;&#35270;&#22270; (MPM)&#12289;&#39044;&#27979;&#23436;&#25972;&#36712;&#36857;&#20013;&#30340;&#32570;&#23569;&#27493;&#39588; (MTM) &#21644;&#36827;&#34892;&#21160;&#20316;&#39044;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104; (APIG)&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35770;&#36848;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#39044;&#27979;&#26041;&#27861;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#32593;&#32476;&#36755;&#20986;&#30340;&#35757;&#32451;&#21518;&#22788;&#29702;&#65292;&#26368;&#32456;&#25506;&#35752;&#20102;&#25298;&#32477;&#36873;&#39033;&#22312;&#20943;&#23569;&#23454;&#26102;&#39044;&#27979;&#26102;&#38388;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.04906</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23454;&#29616;&#21487;&#20449;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#65306;&#25298;&#32477;&#36873;&#39033;&#21644;&#35757;&#32451;&#21518;&#22788;&#29702;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing. (arXiv:2304.04906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35770;&#36848;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#39044;&#27979;&#26041;&#27861;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#32593;&#32476;&#36755;&#20986;&#30340;&#35757;&#32451;&#21518;&#22788;&#29702;&#65292;&#26368;&#32456;&#25506;&#35752;&#20102;&#25298;&#32477;&#36873;&#39033;&#22312;&#20943;&#23569;&#23454;&#26102;&#39044;&#27979;&#26102;&#38388;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#20248;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#30001;&#20110;&#23545;&#20854;&#30693;&#35782;&#38480;&#21046;&#30340;&#32570;&#20047;&#35748;&#35782;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#36136;&#30097;&#12290;&#20026;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#32435;&#20837;&#36825;&#31181;&#35748;&#35782;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#39044;&#27979;&#26041;&#27861;&#65288;&#20063;&#31216;&#20026;&#36873;&#25321;&#24615;&#20998;&#31867;&#25110;&#24102;&#26377;&#25918;&#24323;&#26426;&#21046;&#30340;&#20998;&#31867;&#65289;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;&#25298;&#32477;&#36873;&#39033;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#19987;&#27880;&#20110;&#36825;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#25298;&#32477;&#36873;&#39033;&#30456;&#20851;&#30340;&#19981;&#21516;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#21644;&#32593;&#32476;&#36755;&#20986;&#30340;&#35757;&#32451;&#21518;&#22788;&#29702;&#65288;&#22914;&#26524;&#26377;&#30340;&#35805;&#65289;&#20197;&#29983;&#25104;&#36866;&#24403;&#30340;&#27169;&#22411;&#30693;&#35782;&#35748;&#30693;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25298;&#32477;&#36873;&#39033;&#22312;&#20943;&#23569;&#23454;&#26102;&#39044;&#27979;&#26102;&#38388;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although neural networks (especially deep neural networks) have achieved \textit{better-than-human} performance in many fields, their real-world deployment is still questionable due to the lack of awareness about the limitation in their knowledge. To incorporate such awareness in the machine learning model, prediction with reject option (also known as selective classification or classification with abstention) has been proposed in literature. In this paper, we present a systematic review of the prediction with the reject option in the context of various neural networks. To the best of our knowledge, this is the first study focusing on this aspect of neural networks. Moreover, we discuss different novel loss functions related to the reject option and post-training processing (if any) of network output for generating suitable measurements for knowledge awareness of the model. Finally, we address the application of the rejection option in reducing the prediction time for the real-time pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32435;&#31859;&#36890;&#36947;&#20013;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;MD&#27169;&#25311;&#30340;&#24555;&#36895;&#26367;&#20195;&#27169;&#22411;&#65292;&#27604;XGBoost&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04896</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32435;&#31859;&#38480;&#21046;&#19979;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Neural Network Predicts Ion Concentration Profiles under Nanoconfinement. (arXiv:2304.04896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32435;&#31859;&#36890;&#36947;&#20013;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;MD&#27169;&#25311;&#30340;&#24555;&#36895;&#26367;&#20195;&#27169;&#22411;&#65292;&#27604;XGBoost&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#36890;&#36947;&#20013;&#30340;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;&#27169;&#25311;&#22312;&#29702;&#35299;&#30005;&#21452;&#23618;&#21644;&#30005;&#28183;&#27969;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30001;&#20110;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#21644;&#31163;&#25955;&#28342;&#21058;&#20998;&#23376;&#25928;&#24212;&#65292;&#32463;&#24120;&#34987;&#29992;&#20316;&#30740;&#31350;&#22312;&#32435;&#31859;&#38480;&#21046;&#19979;&#31163;&#23376;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#28982;&#32780;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32435;&#31859;&#36890;&#36947;&#20013;&#19981;&#21516;&#26500;&#22411;&#65288;&#21253;&#25324;&#36890;&#36947;&#23485;&#24230;&#65292;&#31163;&#23376;&#27987;&#24230;&#21644;&#31163;&#23376;&#31181;&#31867;&#31561;&#65289;&#20013;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;&#12290;&#36890;&#36807;&#23558;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;&#24314;&#27169;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#27169;&#25311;&#36924;&#30495;&#30340;MD&#27169;&#25311;&#30340;&#24555;&#36895;&#26367;&#20195;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;XGBoost&#30340;&#20248;&#36234;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#31934;&#24230;&#19979;&#39044;&#27979;&#31163;&#23376;&#27987;&#24230;&#20998;&#24067;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the ion concentration profile in nanochannel plays an important role in understanding the electrical double layer and electroosmotic flow. Due to the non-negligible surface interaction and the effect of discrete solvent molecules, molecular dynamics (MD) simulation is often used as an essential tool to study the behavior of ions under nanoconfinement. Despite the accuracy of MD simulation in modeling nanoconfinement systems, it is computationally expensive. In this work, we propose neural network to predict ion concentration profiles in nanochannels with different configurations, including channel widths, ion molarity, and ion types. By modeling the ion concentration profile as a probability distribution, our neural network can serve as a much faster surrogate model for MD simulation with high accuracy. We further demonstrate the superior prediction accuracy of neural network over XGBoost. Lastly, we demonstrated that neural network is flexible in predicting ion concentration 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.04874</link><description>&lt;p&gt;
ImageCaptioner$^2$: &#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#35780;&#20272;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#23398;&#20064;&#31995;&#32479;&#37117;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#36825;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20004;&#32773;&#12290;&#34913;&#37327;&#21644;&#37327;&#21270;&#20559;&#24046;&#21450;&#20854;&#26469;&#28304;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#22312;&#21253;&#25324;&#35270;&#35273;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20559;&#24046;&#35780;&#20272;&#25351;&#26631;&#65292;&#31216;&#20026; ImageCaptioner$^2$&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#29983;&#25104;&#30340;&#23383;&#24149;&#35780;&#20272;&#22270;&#20687;&#23383;&#24149;&#31639;&#27861;&#19981;&#21516;&#65292;ImageCaptioner$^2$&#22312;&#27979;&#37327;&#20559;&#24046;&#26102;&#32771;&#34385;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24335;&#26469;&#20316;&#20026;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26469;&#27979;&#37327;&#29983;&#25104;&#23383;&#24149;&#30340;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;DASS&#30340;&#21327;&#21516;&#35774;&#35745;&#24314;&#27169;&#31995;&#32479;&#65292;&#32467;&#21512;&#20154;-&#26426;&#24490;&#29615;&#35270;&#35273;&#24341;&#23548;&#12289;&#31354;&#38388;&#25968;&#25454;&#21644;&#21487;&#35299;&#37322;&#30340;AI&#65292;&#33258;&#21160;&#25968;&#25454;&#25366;&#25496;&#22686;&#21152;&#39046;&#22495;&#30693;&#35782;&#65292;&#22312;&#22836;&#39048;&#30284;&#24739;&#32773;&#30340;&#38271;&#26399;&#27602;&#21103;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#21450;&#39046;&#22495;&#19987;&#23478;&#21453;&#39304;&#65292;&#20026;&#35299;&#20915;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#21253;&#25324;&#31354;&#38388;&#20449;&#24687;&#26102;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04870</link><description>&lt;p&gt;
DASS Good: &#35299;&#37322;&#24615;&#30340;&#31354;&#38388;&#38431;&#21015;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DASS Good: Explainable Data Mining of Spatial Cohort Data. (arXiv:2304.04870v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;DASS&#30340;&#21327;&#21516;&#35774;&#35745;&#24314;&#27169;&#31995;&#32479;&#65292;&#32467;&#21512;&#20154;-&#26426;&#24490;&#29615;&#35270;&#35273;&#24341;&#23548;&#12289;&#31354;&#38388;&#25968;&#25454;&#21644;&#21487;&#35299;&#37322;&#30340;AI&#65292;&#33258;&#21160;&#25968;&#25454;&#25366;&#25496;&#22686;&#21152;&#39046;&#22495;&#30693;&#35782;&#65292;&#22312;&#22836;&#39048;&#30284;&#24739;&#32773;&#30340;&#38271;&#26399;&#27602;&#21103;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#21450;&#39046;&#22495;&#19987;&#23478;&#21453;&#39304;&#65292;&#20026;&#35299;&#20915;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#21253;&#25324;&#31354;&#38388;&#20449;&#24687;&#26102;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#21253;&#25324;&#31354;&#38388;&#20449;&#24687;&#26102;&#65292;&#20363;&#22914;&#37051;&#36817;&#30340;&#26377;&#39118;&#38505;&#22120;&#23448;&#21608;&#22260;&#30340;&#36752;&#23556;&#21058;&#37327;&#20998;&#24067;&#65292;&#24320;&#21457;&#36866;&#29992;&#30340;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#24314;&#27169;&#31995;&#32479;DASS&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#20197;&#25903;&#25345;&#20154;&#26426;&#28151;&#21512;&#24320;&#21457;&#21644;&#39564;&#35777;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#22836;&#39048;&#30284;&#24739;&#32773;&#25918;&#30103;&#21058;&#37327;&#30456;&#20851;&#30340;&#38271;&#26399;&#27602;&#21103;&#20316;&#29992;&#12290;&#19982;&#32959;&#30244;&#23398;&#21644;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#19987;&#23478;&#21512;&#20316;&#24320;&#21457;&#30340;DASS&#65292;&#32467;&#21512;&#20154;-&#26426;&#24490;&#29615;&#35270;&#35273;&#24341;&#23548;&#12289;&#31354;&#38388;&#25968;&#25454;&#21644;&#21487;&#35299;&#37322;&#30340;AI&#65292;&#20197;&#33258;&#21160;&#25968;&#25454;&#25366;&#25496;&#22686;&#21152;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#23454;&#29992;&#30340;&#20020;&#24202;&#20998;&#23618;&#27169;&#22411;&#23637;&#31034;&#20102;DASS&#65292;&#24182;&#25253;&#21578;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20174;&#36825;&#27425;&#21327;&#20316;&#32463;&#39564;&#20013;&#23398;&#21040;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing applicable clinical machine learning models is a difficult task when the data includes spatial information, for example, radiation dose distributions across adjacent organs at risk. We describe the co-design of a modeling system, DASS, to support the hybrid human-machine development and validation of predictive models for estimating long-term toxicities related to radiotherapy doses in head and neck cancer patients. Developed in collaboration with domain experts in oncology and data mining, DASS incorporates human-in-the-loop visual steering, spatial data, and explainable AI to augment domain knowledge with automatic data mining. We demonstrate DASS with the development of two practical clinical stratification models and report feedback from domain experts. Finally, we describe the design lessons learned from this collaborative experience.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#23569;&#37327;&#39640;&#20445;&#30495;&#25968;&#25454;&#26469;&#25552;&#39640;&#22823;&#37327;&#20302;&#20445;&#30495;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#65292;&#26041;&#27861;&#21253;&#25324;&#26500;&#24314;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#35745;&#31639;&#20854;&#20302;&#33021;&#37327;&#35889;&#26469;&#23558;&#25968;&#25454;&#32858;&#31867;&#65292;&#24182;&#33719;&#21462;&#20851;&#38190;&#28857;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#36827;&#34892;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2304.04862</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#20302;&#20445;&#30495;&#25968;&#25454;&#20934;&#30830;&#24615;&#30340;&#23569;&#26679;&#26412;&#22270;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A few-shot graph Laplacian-based approach for improving the accuracy of low-fidelity data. (arXiv:2304.04862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#23569;&#37327;&#39640;&#20445;&#30495;&#25968;&#25454;&#26469;&#25552;&#39640;&#22823;&#37327;&#20302;&#20445;&#30495;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#65292;&#26041;&#27861;&#21253;&#25324;&#26500;&#24314;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#35745;&#31639;&#20854;&#20302;&#33021;&#37327;&#35889;&#26469;&#23558;&#25968;&#25454;&#32858;&#31867;&#65292;&#24182;&#33719;&#21462;&#20851;&#38190;&#28857;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20445;&#30495;&#25968;&#25454;&#36890;&#24120;&#20135;&#29983;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#19981;&#20934;&#30830;&#12290;&#30456;&#21453;&#65292;&#39640;&#20445;&#30495;&#25968;&#25454;&#20934;&#30830;&#20294;&#33719;&#21462;&#25104;&#26412;&#26114;&#36149;&#12290;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#20445;&#30495;&#25968;&#25454;&#26469;&#22686;&#24378;&#22823;&#37327;&#20302;&#20445;&#30495;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25152;&#25551;&#36848;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#20302;&#20445;&#30495;&#25968;&#25454;&#26500;&#24314;&#22270;&#25289;&#26222;&#25289;&#26031;&#24182;&#35745;&#31639;&#20854;&#20302;&#33021;&#37327;&#35889;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#27492;&#35889;&#26469;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#24182;&#35782;&#21035;&#19982;&#32858;&#31867;&#37325;&#24515;&#26368;&#36817;&#30340;&#25968;&#25454;&#28857;&#12290;&#38543;&#21518;&#65292;&#20026;&#36825;&#20123;&#20851;&#38190;&#28857;&#33719;&#21462;&#39640;&#20445;&#30495;&#25968;&#25454;&#12290;&#28982;&#21518;&#36890;&#36807;&#26368;&#23567;&#21270;&#20851;&#38190;&#28857;&#22788;&#30340;&#21452;&#20445;&#30495;&#25968;&#25454;&#19982;&#39640;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#20445;&#25345;&#20302;&#20445;&#30495;&#25968;&#25454;&#20998;&#24067;&#30340;&#22522;&#30784;&#32467;&#26500;&#26469;&#30830;&#23450;&#23558;&#27599;&#20010;&#20302;&#20445;&#30495;&#25968;&#25454;&#28857;&#26144;&#23556;&#21040;&#20854;&#21452;&#20445;&#30495;&#25968;&#25454;&#23545;&#24212;&#29289;&#30340;&#36716;&#25442;&#12290;&#21518;&#19968;&#20010;&#30446;&#26631;&#20877;&#27425;&#26159;&#36890;&#36807;&#20381;&#36182;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#35889;&#29305;&#24615;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-fidelity data is typically inexpensive to generate but inaccurate. On the other hand, high-fidelity data is accurate but expensive to obtain. Multi-fidelity methods use a small set of high-fidelity data to enhance the accuracy of a large set of low-fidelity data. In the approach described in this paper, this is accomplished by constructing a graph Laplacian using the low-fidelity data and computing its low-lying spectrum. This spectrum is then used to cluster the data and identify points that are closest to the centroids of the clusters. High-fidelity data is then acquired for these key points. Thereafter, a transformation that maps every low-fidelity data point to its bi-fidelity counterpart is determined by minimizing the discrepancy between the bi- and high-fidelity data at the key points, and to preserve the underlying structure of the low-fidelity data distribution. The latter objective is achieved by relying, once again, on the spectral properties of the graph Laplacian. This
&lt;/p&gt;</description></item><item><title>ShapeShift&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36229;&#26925;&#29699;&#30340;&#21442;&#32771;&#24418;&#29366;&#39044;&#27979;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04861</link><description>&lt;p&gt;
ShapeShift&#65306;&#22522;&#20110;&#36229;&#26925;&#29699;&#24418;&#29366;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping. (arXiv:2304.04861v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04861
&lt;/p&gt;
&lt;p&gt;
ShapeShift&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36229;&#26925;&#29699;&#30340;&#21442;&#32771;&#24418;&#29366;&#39044;&#27979;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26159;&#31934;&#30830;&#29289;&#20307;&#25805;&#32437;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#21442;&#32771;3D&#29289;&#20307;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#25193;&#23637;&#21040;&#26032;&#30340;&#29289;&#20307;&#31867;&#21035;&#21464;&#24471;&#26114;&#36149;&#12290;&#30452;&#25509;&#23039;&#24577;&#39044;&#27979;&#20063;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#20805;&#20998;&#20449;&#24687;&#32780;&#19981;&#21442;&#32771;3D&#27169;&#22411;&#12290;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20869;&#22312;&#30340;&#25551;&#36848;&#24615;&#33021;&#21147;&#32780;&#19981;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;3D&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#32570;&#20047;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36229;&#26925;&#29699;&#24418;&#29366;&#30340;ShapeShift&#26694;&#26550;&#65292;&#29992;&#20110;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#65292;&#35813;&#26694;&#26550;&#39044;&#27979;&#19982;&#36866;&#21512;&#20110;&#29289;&#20307;&#30340;&#21407;&#22987;&#24418;&#29366;&#30456;&#23545;&#30340;&#29289;&#20307;&#23039;&#24577;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20869;&#22312;&#30340;&#25551;&#36848;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36229;&#20986;&#35757;&#32451;&#38598;&#30340;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object's pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#22312;&#26089;&#26399;&#23618;&#27425;&#30340;&#32593;&#32476;&#20013;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;Tiny-ImageNet&#25968;&#25454;&#38598;&#22522;&#20934;&#27979;&#35797;&#21644;&#19968;&#31995;&#21015;&#20256;&#36882;&#23398;&#20064;&#21644;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#27604;LLF&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.04858</link><description>&lt;p&gt;
&#22312;&#26089;&#26399;&#23618;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Simulated Annealing in Early Layers Leads to Better Generalization. (arXiv:2304.04858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04858
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#22312;&#26089;&#26399;&#23618;&#27425;&#30340;&#32593;&#32476;&#20013;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;Tiny-ImageNet&#25968;&#25454;&#38598;&#22522;&#20934;&#27979;&#35797;&#21644;&#19968;&#31995;&#21015;&#20256;&#36882;&#23398;&#20064;&#21644;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#27604;LLF&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#20123;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#38752;&#26356;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#26469;&#25442;&#21462;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;LLF&#26159;&#36825;&#19968;&#31867;&#26041;&#27861;&#20013;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#23450;&#26399;&#37325;&#26032;&#21021;&#22987;&#21270;&#32593;&#32476;&#30340;&#26368;&#21518;&#20960;&#23618;&#26469;&#22686;&#24378;&#26089;&#26399;&#23618;&#27425;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#22312;&#32593;&#32476;&#30340;&#26089;&#26399;&#23618;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;(SEAL)&#20195;&#26367;&#21518;&#38754;&#23618;&#27425;&#30340;&#37325;&#26032;&#21021;&#22987;&#21270;&#12290;&#23454;&#36136;&#19978;&#65292;&#21518;&#38754;&#30340;&#23618;&#36890;&#36807;&#27491;&#24120;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#32780;&#26089;&#26399;&#23618;&#36890;&#36807;&#30701;&#26242;&#30340;&#26799;&#24230;&#19978;&#21319;&#21518;&#36319;&#30528;&#26799;&#24230;&#19979;&#38477;&#12290;&#22312;&#27969;&#34892;&#30340;Tiny-ImageNet&#25968;&#25454;&#38598;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#19968;&#31995;&#21015;&#20256;&#36882;&#23398;&#20064;&#21644;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;SEAL&#27604;LLF&#30340;&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19982;&#27491;&#24120;&#35757;&#32451;&#30456;&#27604;&#65292;&#34429;&#28982;LLF&#29305;&#24615;&#33021;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#38477;&#20302;&#21407;&#22987;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;SEAL&#19981;&#20165;&#21487;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#25913;&#21892;&#21407;&#22987;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a number of iterative learning methods have been introduced to improve generalization. These typically rely on training for longer periods of time in exchange for improved generalization. LLF (later-layer-forgetting) is a state-of-the-art method in this category. It strengthens learning in early layers by periodically re-initializing the last few layers of the network. Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers. Essentially, later layers go through the normal gradient descent process, while the early layers go through short stints of gradient ascent followed by gradient descent. Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin. We further show that, compared to normal training, LLF features, although improving on the target task, degrade
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39034;&#24207;&#23398;&#20064;&#22810;&#20010;&#29289;&#29702;&#26041;&#31243;&#65292;&#32780;&#26080;&#38656;&#20026;&#26032;&#20219;&#21153;&#28155;&#21152;&#39069;&#22806;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#27979;&#35797;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#36739;&#39640;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04854</link><description>&lt;p&gt;
iPINNs&#65306;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
iPINNs: Incremental learning for Physics-informed neural networks. (arXiv:2304.04854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39034;&#24207;&#23398;&#20064;&#22810;&#20010;&#29289;&#29702;&#26041;&#31243;&#65292;&#32780;&#26080;&#38656;&#20026;&#26032;&#20219;&#21153;&#28155;&#21152;&#39069;&#22806;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#27979;&#35797;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#36739;&#39640;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26368;&#36817;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20294;&#30001;&#20110;&#38656;&#35201;&#36941;&#21382;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#65292;&#25214;&#21040;&#19968;&#32452;&#28385;&#36275;PDE&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#21807;&#19968;&#24615;&#12290;&#34429;&#28982;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;PINNs&#30340;&#22686;&#37327;&#22521;&#35757;&#31243;&#24207;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#36825;&#20123;&#22521;&#35757;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;PINNs&#65288;iPINNs&#65289;&#65292;&#23427;&#21487;&#20197;&#39034;&#24207;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65288;&#26041;&#31243;&#65289;&#65292;&#26080;&#38656;&#20026;&#26032;&#20219;&#21153;&#28155;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#24182;&#25913;&#36827;&#24207;&#21015;&#20013;&#27599;&#20010;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26368;&#31616;&#21333;&#30340;PDE&#24320;&#22987;&#23398;&#20064;&#22810;&#20010;PDE&#65292;&#20026;&#27599;&#20010;PDE&#21019;&#24314;&#33258;&#24049;&#30340;&#23376;&#32593;&#32476;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#23376;&#32593;&#32476;&#19982;&#20043;&#21069;&#23398;&#20064;&#30340;&#23376;&#32593;&#32476;&#37325;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#23376;&#32593;&#32476;&#26159;&#26032;&#20219;&#21153;&#32593;&#32476;&#30340;&#33391;&#22909;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#27599;&#20010;&#23376;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#22810;&#20010;&#27979;&#35797;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;iPINNs&#27604;&#20854;&#20182;PINN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently become a powerful tool for solving partial differential equations (PDEs). However, finding a set of neural network parameters that lead to fulfilling a PDE can be challenging and non-unique due to the complexity of the loss landscape that needs to be traversed. Although a variety of multi-task learning and transfer learning approaches have been proposed to overcome these issues, there is no incremental training procedure for PINNs that can effectively mitigate such training challenges. We propose incremental PINNs (iPINNs) that can learn multiple tasks (equations) sequentially without additional parameters for new tasks and improve performance for every equation in the sequence. Our approach learns multiple PDEs starting from the simplest one by creating its own subnetwork for each PDE and allowing each subnetwork to overlap with previously learned subnetworks. We demonstrate that previous subnetworks are a good initialization for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;MicroTVM&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#26512;&#20026;&#21518;&#31471;&#30340;C&#28304;&#20195;&#30721;&#24211;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;Ahead-of-Time C&#36816;&#34892;&#26102;&#22312;ARM Cortex M4F&#26680;&#24515;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.04842</link><description>&lt;p&gt;
&#20351;&#29992;MicroTVM&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#21040;&#36793;&#32536;Ahead-of-Time&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;
Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM. (arXiv:2304.04842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;MicroTVM&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#26512;&#20026;&#21518;&#31471;&#30340;C&#28304;&#20195;&#30721;&#24211;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;Ahead-of-Time C&#36816;&#34892;&#26102;&#22312;ARM Cortex M4F&#26680;&#24515;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#24212;&#29992;&#21040;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#25968;&#25454;&#31185;&#23398;&#23478;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;&#22914;PyTorch&#25110;TensorFlow&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#26080;&#27861;&#26080;&#32541;&#22320;&#22312;&#36793;&#32536;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;MicroTVM&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#26512;&#20026;&#21518;&#31471;&#30340;C&#28304;&#20195;&#30721;&#24211;&#65292;MicroTVM&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#32534;&#35793;&#22120;&#26694;&#26550;&#25193;&#23637;&#65292;&#29992;&#20110;&#22788;&#29702;&#35064;&#26426;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#36816;&#31639;&#31526;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#36890;&#29992;&#27169;&#22359;&#21152;&#36895;&#22120;&#65288;UMA&#65289;&#25509;&#21475;&#21368;&#36733;&#21040;&#19987;&#29992;&#21152;&#36895;&#22120;&#19978;&#65292;&#32780;&#20854;&#20182;&#36816;&#31639;&#31526;&#21017;&#22312;CPU&#26680;&#24515;&#20013;&#22788;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;Ahead-of-Time C&#36816;&#34892;&#26102;&#65292;&#22312;ARM Cortex M4F&#26680;&#24515;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, more and more AI applications have been applied to edge devices. However, models trained by data scientists with machine learning frameworks, such as PyTorch or TensorFlow, can not be seamlessly executed on edge. In this paper, we develop an end-to-end code generator parsing a pre-trained model to C source libraries for the backend using MicroTVM, a machine learning compiler framework extension addressing inference on bare metal devices. An analysis shows that specific compute-intensive operators can be easily offloaded to the dedicated accelerator with a Universal Modular Accelerator (UMA) interface, while others are processed in the CPU cores. By using the automatically generated ahead-of-time C runtime, we conduct a hand gesture recognition experiment on an ARM Cortex M4F core.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#26469;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#20154;&#20307;&#20581;&#24247;&#34892;&#20026;&#21644;&#20581;&#36523;&#24773;&#20917;&#30340;&#26041;&#27861;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04839</link><description>&lt;p&gt;
MHfit&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#36816;&#21160;&#21592;&#20581;&#24247;&#29366;&#20917;&#30340;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning. (arXiv:2304.04839v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#26469;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#20154;&#20307;&#20581;&#24247;&#34892;&#20026;&#21644;&#20581;&#36523;&#24773;&#20917;&#30340;&#26041;&#27861;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#30005;&#35805;&#21644;&#20854;&#20182;&#30005;&#23376;&#35774;&#22791;&#24050;&#32463;&#24110;&#21161;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#25968;&#25454;&#12290;&#26412;&#25991;&#23558;&#29305;&#21035;&#20851;&#27880;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#12290;&#31227;&#21160;&#20581;&#24247;&#25968;&#25454;&#20351;&#29992;&#31227;&#21160;&#35774;&#22791;&#23454;&#26102;&#25910;&#38598;&#20020;&#24202;&#20581;&#24247;&#25968;&#25454;&#24182;&#36319;&#36394;&#24739;&#32773;&#29983;&#21629;&#20307;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#20174;&#31227;&#21160;&#35774;&#22791;&#21644;&#24739;&#32773;&#36523;&#19978;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#21644;&#20581;&#24247;&#65292;&#24182;&#20026;&#23567;&#22411;&#25110;&#22823;&#22411;&#36816;&#21160;&#38431;&#25552;&#20379;&#20851;&#20110;&#26576;&#20010;&#36816;&#21160;&#21592;&#26159;&#21542;&#36866;&#21512;&#21442;&#19982;&#29305;&#23450;&#27604;&#36187;&#30340;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#20174;&#19968;&#39033;&#31867;&#20284;&#30340;&#31227;&#21160;&#20581;&#24247;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;10&#21517;&#24535;&#24895;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#24517;&#39035;&#22312;&#36523;&#20307;&#19978;&#25918;&#32622;&#20256;&#24863;&#22120;&#24182;&#36827;&#34892;&#22810;&#39033;&#20307;&#21147;&#27963;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;5&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;XGBoost&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#65289;&#26469;&#20998;&#26512;&#21644;&#39044;&#27979;&#20154;&#31867;&#20581;&#24247;&#34892;&#20026;&#21644;&#20581;&#36523;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile phones and other electronic gadgets or devices have aided in collecting data without the need for data entry. This paper will specifically focus on Mobile health data. Mobile health data use mobile devices to gather clinical health data and track patient vitals in real-time. Our study is aimed to give decisions for small or big sports teams on whether one athlete good fit or not for a particular game with the compare several machine learning algorithms to predict human behavior and health using the data collected from mobile devices and sensors placed on patients. In this study, we have obtained the dataset from a similar study done on mhealth. The dataset contains vital signs recordings of ten volunteers from different backgrounds. They had to perform several physical activities with a sensor placed on their bodies. Our study used 5 machine learning algorithms (XGBoost, Naive Bayes, Decision Tree, Random Forest, and Logistic Regression) to analyze and predict human health behav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24207;&#27169;&#24335;&#8221;&#20316;&#20026;&#20998;&#26512;&#24847;&#20041;&#30340;&#21333;&#20301;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26684;&#19978;&#36890;&#36807;&#27491;&#24335;&#32972;&#26223;&#30340;&#20840;&#23610;&#24230;&#27979;&#37327;&#26469;&#35782;&#21035;&#36825;&#20123;&#24207;&#23376;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#20013;&#31561;&#23610;&#23544;&#24207;&#25968;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#22522;&#26412;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.04827</link><description>&lt;p&gt;
&#26684;&#19978;&#30340;&#24207;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Ordinal Motifs in Lattices. (arXiv:2304.04827v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24207;&#27169;&#24335;&#8221;&#20316;&#20026;&#20998;&#26512;&#24847;&#20041;&#30340;&#21333;&#20301;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26684;&#19978;&#36890;&#36807;&#27491;&#24335;&#32972;&#26223;&#30340;&#20840;&#23610;&#24230;&#27979;&#37327;&#26469;&#35782;&#21035;&#36825;&#20123;&#24207;&#23376;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#20013;&#31561;&#23610;&#23544;&#24207;&#25968;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#22522;&#26412;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#26159;&#34920;&#31034;&#21644;&#20998;&#26512;&#20851;&#31995;&#21644;&#26412;&#20307;&#30693;&#35782;&#30340;&#24120;&#29992;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24207;&#27169;&#24335;&#8221;&#20316;&#20026;&#20998;&#26512;&#24847;&#20041;&#30340;&#21333;&#20301;&#65292;&#30740;&#31350;&#36825;&#20123;&#24207;&#23376;&#32467;&#26500;&#65288;&#25110;&#26631;&#20934;&#23610;&#24230;&#65289;&#36890;&#36807;&#26469;&#33258;&#27491;&#24335;&#27010;&#24565;&#20998;&#26512;&#39046;&#22495;&#30340;&#27491;&#24335;&#32972;&#26223;&#30340;&#20840;&#23610;&#24230;&#27979;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24213;&#23618;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#23436;&#20840;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#36880;&#27493;&#35782;&#21035;&#24207;&#27169;&#24335;&#20197;&#33410;&#30465;&#35745;&#31639;&#37327;&#30340;&#32467;&#26524;&#12290;&#20276;&#38543;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24207;&#27169;&#24335;&#20174;&#20013;&#31561;&#23610;&#23544;&#30340;&#24207;&#25968;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#22522;&#26412;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lattices are a commonly used structure for the representation and analysis of relational and ontological knowledge. In particular, the analysis of these requires a decomposition of a large and high-dimensional lattice into a set of understandably large parts. With the present work we propose /ordinal motifs/ as analytical units of meaning. We study these ordinal substructures (or standard scales) through (full) scale-measures of formal contexts from the field of formal concept analysis. We show that the underlying decision problems are NP-complete and provide results on how one can incrementally identify ordinal motifs to save computational effort. Accompanying our theoretical results, we demonstrate how ordinal motifs can be leveraged to retrieve basic meaning from a medium sized ordinal data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23548;&#33268;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20855;&#38382;&#39064;&#24615;&#30340;&#36755;&#20837;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.04824</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#24402;&#22240;&#20110;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning. (arXiv:2304.04824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23548;&#33268;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20855;&#38382;&#39064;&#24615;&#30340;&#36755;&#20837;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#39044;&#27979;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#25200;&#21160;&#12289;&#23545;&#25239;&#25915;&#20987;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#36755;&#20837;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#65292;&#20934;&#30830;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#26377;&#24517;&#35201;&#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#28304;&#24182;&#37319;&#21462;&#25514;&#26045;&#20943;&#36731;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#19981;&#20165;&#36827;&#34892;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#32780;&#19988;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#12289;&#30830;&#23450;&#20854;&#26469;&#28304;&#24182;&#25552;&#20986;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#26368;&#20855;&#38382;&#39064;&#24615;&#30340;&#36755;&#20837;&#21306;&#22495;&#65292;&#20174;&#32780;&#23548;&#33268;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;UA-Backprop&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12289;&#26494;&#24347;&#30340;&#20551;&#35774;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions made by deep learning models are prone to data perturbations, adversarial attacks, and out-of-distribution inputs. To build a trusted AI system, it is therefore critical to accurately quantify the prediction uncertainties. While current efforts focus on improving uncertainty quantification accuracy and efficiency, there is a need to identify uncertainty sources and take actions to mitigate their effects on predictions. Therefore, we propose to develop explainable and actionable Bayesian deep learning methods to not only perform accurate uncertainty quantification but also explain the uncertainties, identify their sources, and propose strategies to mitigate the uncertainty impacts. Specifically, we introduce a gradient-based uncertainty attribution method to identify the most problematic regions of the input that contribute to the prediction uncertainty. Compared to existing methods, the proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.04819</link><description>&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques. (arXiv:2304.04819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#26159;&#19968;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#23041;&#32961;&#65292;&#32618;&#29359;&#20351;&#29992;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#24182;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#21644;&#22312;&#20854;&#21457;&#29983;&#20043;&#21069;&#38450;&#27490;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20351;&#29992;&#19978;&#36848;&#25216;&#26415;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#27599;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;150&#22810;&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#32422;50&#31687;&#26368;&#36817;&#21644;&#26368;&#30456;&#20851;&#30340;&#30740;&#31350;&#25991;&#31456;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19968;&#20123;&#32593;&#32476;&#29359;&#32618;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20363;&#22914;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#21644;&#35782;&#21035;&#28508;&#22312;&#23041;&#32961;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#36827;&#34892;CT&#25195;&#25551;&#22270;&#20687;&#30340;&#32954;&#30284;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;CNN&#24471;&#21040;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04814</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#36827;&#34892;CT&#25195;&#25551;&#22270;&#20687;&#30340;&#32954;&#30284;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
LCDctCNN: Lung Cancer Diagnosis of CT scan Images Using CNN Based Model. (arXiv:2304.04814v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#36827;&#34892;CT&#25195;&#25551;&#22270;&#20687;&#30340;&#32954;&#30284;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;CNN&#24471;&#21040;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#19990;&#30028;&#19978;&#26368;&#33268;&#21629;&#21644;&#21361;&#26426;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#26089;&#26399;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#26159;&#38477;&#20302;&#32954;&#30284;&#27515;&#20129;&#29575;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#30340;&#22270;&#20687;&#26159;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#32954;&#30284;&#26816;&#27979;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;CT&#25195;&#25551;&#22270;&#20687;&#26089;&#26399;&#26816;&#27979;&#32954;&#30284;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#20854;&#20182;&#27169;&#22411;&#65292;&#20363;&#22914;Inception V3&#12289;Xception&#21644;ResNet-50&#27169;&#22411;&#65292;&#20197;&#19982;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#26681;&#25454;&#20934;&#30830;&#29575;&#12289;&#26354;&#32447;&#19979;&#38754;&#31215;(AUC)&#12289;&#21484;&#22238;&#29575;&#21644;&#25439;&#22833;&#31561;&#25351;&#26631;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;CNN&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#23427;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;92%&#12289;AUC&#20026;98.21%&#12289;&#21484;&#22238;&#29575;&#20026;91.72%&#12289;&#25439;&#22833;&#20026;0.328&#12290;
&lt;/p&gt;
&lt;p&gt;
The most deadly and life-threatening disease in the world is lung cancer. Though early diagnosis and accurate treatment are necessary for lowering the lung cancer mortality rate. A computerized tomography (CT) scan-based image is one of the most effective imaging techniques for lung cancer detection using deep learning models. In this article, we proposed a deep learning model-based Convolutional Neural Network (CNN) framework for the early detection of lung cancer using CT scan images. We also have analyzed other models for instance Inception V3, Xception, and ResNet-50 models to compare with our proposed model. We compared our models with each other considering the metrics of accuracy, Area Under Curve (AUC), recall, and loss. After evaluating the model's performance, we observed that CNN outperformed other models and has been shown to be promising compared to traditional methods. It achieved an accuracy of 92%, AUC of 98.21%, recall of 91.72%, and loss of 0.328.
&lt;/p&gt;</description></item><item><title>Scallop&#26159;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#20248;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#33021;&#22815;&#20197;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#23427;&#21487;&#22312;AI&#20219;&#21153;&#20013;&#34920;&#36798;&#31639;&#27861;&#25512;&#29702;&#24182;&#34701;&#21512;&#36923;&#36753;&#39046;&#22495;&#30693;&#35782;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.04812</link><description>&lt;p&gt;
Scallop: &#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Scallop: A Language for Neurosymbolic Programming. (arXiv:2304.04812v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04812
&lt;/p&gt;
&lt;p&gt;
Scallop&#26159;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#20248;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#33021;&#22815;&#20197;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#23427;&#21487;&#22312;AI&#20219;&#21153;&#20013;&#34920;&#36798;&#31639;&#27861;&#25512;&#29702;&#24182;&#34701;&#21512;&#36923;&#36753;&#39046;&#22495;&#30693;&#35782;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Scallop&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#20248;&#28857;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;Scallop &#21551;&#29992;&#29992;&#25143;&#32534;&#20889;&#24191;&#27867;&#30340;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#24182;&#20197;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#21253;&#25324;&#65306;1&#65289;&#22522;&#20110;&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#30340;&#28789;&#27963;&#31526;&#21495;&#34920;&#31034;&#65307;2&#65289;&#22522;&#20110; Datalog &#30340;&#22768;&#26126;&#24615;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#25903;&#25345;&#36882;&#24402;&#12289;&#32858;&#21512;&#21644;&#21542;&#23450;&#65307;3&#65289;&#22522;&#20110;&#35777;&#26126;&#21322;&#29615;&#29702;&#35770;&#30340;&#33258;&#21160;&#39640;&#25928;&#21487;&#24494;&#25512;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#30340;&#20843;&#31181;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#22871;&#20214;&#19978;&#35780;&#20272; Scallop&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Scallop &#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; AI &#20219;&#21153;&#20013;&#34920;&#36798;&#31639;&#27861;&#25512;&#29702;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#21592;&#25552;&#20379;&#31616;&#27905;&#30340;&#25509;&#21475;&#20197;&#34701;&#21512;&#36923;&#36753;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.04807</link><description>&lt;p&gt;
&#22312;&#24658;&#26143;&#21464;&#24322;&#23384;&#22312;&#19979;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep-learning based measurement of planetary radial velocities in the presence of stellar variability. (arXiv:2304.04807v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24658;&#26143;&#21464;&#24322;&#23384;&#22312;&#19979;&#27979;&#37327;&#23567;&#34892;&#26143;&#30340;&#24452;&#21521;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#38477;&#32500;&#21644;&#25968;&#25454;&#20998;&#21106;&#26041;&#27861;&#65292;&#20197;&#21450;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#21333;&#32447;CNN&#12289;&#21333;&#32447;CNN&#38598;&#21512;&#21644;&#22810;&#32447;CNN&#12290;&#25105;&#20204;&#23558;&#31867;&#20284;&#20110;&#34892;&#26143;&#30340;&#24452;&#21521;&#36895;&#24230;&#27880;&#20837;&#20809;&#35889;&#20013;&#24182;&#20351;&#29992;&#32593;&#32476;&#24674;&#22797;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#32447;CNN&#33021;&#22815;&#24674;&#22797;0.2m/s&#21322;&#25391;&#24133;&#12289;50&#22825;&#21608;&#26399;&#30340;&#34892;&#26143;&#65292;&#20854;&#25391;&#24133;&#35823;&#24046;&#20026;8.8&#65285;&#65292;&#21608;&#26399;&#35823;&#24046;&#20026;0.7&#65285;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep-learning based approach for measuring small planetary radial velocities in the presence of stellar variability. We use neural networks to reduce stellar RV jitter in three years of HARPS-N sun-as-a-star spectra. We develop and compare dimensionality-reduction and data splitting methods, as well as various neural network architectures including single line CNNs, an ensemble of single line CNNs, and a multi-line CNN. We inject planet-like RVs into the spectra and use the network to recover them. We find that the multi-line CNN is able to recover planets with 0.2 m/s semi-amplitude, 50 day period, with 8.8% error in the amplitude and 0.7% in the period. This approach shows promise for mitigating stellar RV variability and enabling the detection of small planetary RVs with unprecedented precision.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26694;&#26550;RAPID&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#21644;&#20559;&#24046;&#20943;&#23569;&#65292;&#23398;&#20064;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#36164;&#28304;&#20849;&#20139;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04797</link><description>&lt;p&gt;
RAPID: &#22312;&#21160;&#24577;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments. (arXiv:2304.04797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04797
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26694;&#26550;RAPID&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#21644;&#20559;&#24046;&#20943;&#23569;&#65292;&#23398;&#20064;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#36164;&#28304;&#20849;&#20139;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#24037;&#20316;&#36127;&#36733;&#20043;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#24050;&#25104;&#20026;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#20043;&#38388;&#30340;&#19968;&#31181;&#31361;&#20986;&#23454;&#36341;&#65292;&#36825;&#26159;&#30001;&#38656;&#27714;&#25913;&#36827;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#38477;&#20302;&#25317;&#26377;&#25104;&#26412;&#25152;&#39537;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#20105;&#29992;&#21487;&#33021;&#20250;&#23545;&#20855;&#26377;&#20005;&#26684;&#26381;&#21153;&#36136;&#37327; (QoS) &#35201;&#27714;&#30340;&#20248;&#20808;&#32423;&#39640;&#12289;&#38754;&#21521;&#29992;&#25143;&#30340;&#36127;&#36733;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#22240;&#27492;&#26377;&#25928;&#30340;&#36164;&#28304;&#20849;&#20139;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#22312;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#20173;&#28982;&#24456;&#38590;&#23454;&#36341;&#65292;&#22240;&#20026;&#36127;&#36733;&#20107;&#20808;&#26159;&#26410;&#30693;&#30340;&#65292;&#21487;&#33021;&#20165;&#36816;&#34892;&#30701;&#26242;&#30340;&#26102;&#38388;&#65292;&#20174;&#32780;&#31105;&#27490;&#33073;&#26426;&#23398;&#20064;&#65292;&#24182;&#19988;&#26174;&#33879;&#38459;&#30861;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; RAPID&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#12289;&#23436;&#20840;&#22312;&#32447;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#23398;&#20064;&#12290;RAPID &#21033;&#29992;&#36731;&#37327;&#32423; QoS &#39044;&#27979;&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#21644;&#20559;&#24046;&#20943;&#23569;&#65292;&#20197;&#20998;&#31163;&#20105;&#29992;&#26816;&#27979;&#21644;&#20998;&#37197;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource sharing between multiple workloads has become a prominent practice among cloud service providers, motivated by demand for improved resource utilization and reduced cost of ownership. Effective resource sharing, however, remains an open challenge due to the adverse effects that resource contention can have on high-priority, user-facing workloads with strict Quality of Service (QoS) requirements. Although recent approaches have demonstrated promising results, those works remain largely impractical in public cloud environments since workloads are not known in advance and may only run for a brief period, thus prohibiting offline learning and significantly hindering online learning. In this paper, we propose RAPID, a novel framework for fast, fully-online resource allocation policy learning in highly dynamic operating environments. RAPID leverages lightweight QoS predictions, enabled by domain-knowledge-inspired techniques for sample efficiency and bias reduction, to decouple contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04795</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35780;&#20272;&#19979;&#37325;&#26032;&#23457;&#35270;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Revisiting Test Time Adaptation under Online Evaluation. (arXiv:2304.04795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#12290;TTA&#26041;&#27861;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#36866;&#24212;&#20998;&#24067;&#31227;&#20301;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24778;&#20154;&#30340;&#24615;&#33021;&#36890;&#24120;&#35201;&#20197;&#26174;&#30528;&#22686;&#21152;&#30340;&#35745;&#31639;&#39044;&#31639;&#20026;&#20195;&#20215;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#20102;&#36825;&#31181;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#65292;&#24433;&#21709;&#23427;&#20204;&#22312;&#23454;&#38469;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#29616;&#23454;&#30340;TTA&#26041;&#27861;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#36825;&#20010;&#21327;&#35758;&#20013;&#25968;&#25454;&#20197;&#24658;&#23450;&#36895;&#29575;&#20174;&#25968;&#25454;&#27969;&#20013;&#22312;&#32447;&#25509;&#25910;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#26041;&#27861;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#23545;&#22810;&#31181;TTA&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#36793;&#32536;&#21021;&#22987;&#21270;&#26102;&#30340;&#35757;&#32451;&#33021;&#21147;&#65292;&#21457;&#29616;&#39281;&#21644;&#30340;&#28608;&#27963;&#20989;&#25968;&#20250;&#22952;&#30861;&#35757;&#32451;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27839;&#28151;&#27788;&#36793;&#32536;&#21021;&#22987;&#21270;&#21482;&#26159;&#33719;&#24471;&#26368;&#20339;&#21487;&#35757;&#32451;&#24615;&#25152;&#24517;&#38656;&#20294;&#19981;&#20805;&#20998;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.04784</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20020;&#30028;&#24615;&#19982;&#22343;&#21248;&#24615;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Criticality versus uniformity in deep neural networks. (arXiv:2304.04784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#36793;&#32536;&#21021;&#22987;&#21270;&#26102;&#30340;&#35757;&#32451;&#33021;&#21147;&#65292;&#21457;&#29616;&#39281;&#21644;&#30340;&#28608;&#27963;&#20989;&#25968;&#20250;&#22952;&#30861;&#35757;&#32451;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27839;&#28151;&#27788;&#36793;&#32536;&#21021;&#22987;&#21270;&#21482;&#26159;&#33719;&#24471;&#26368;&#20339;&#21487;&#35757;&#32451;&#24615;&#25152;&#24517;&#38656;&#20294;&#19981;&#20805;&#20998;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27839;&#30528;&#28151;&#27788;&#36793;&#32536;&#21021;&#22987;&#21270;&#30340;&#28145;&#23618;&#21069;&#39304;&#32593;&#32476;&#34920;&#29616;&#20986;&#25351;&#25968;&#32423;&#20248;&#36234;&#30340;&#35757;&#32451;&#33021;&#21147;&#65292;&#20854;&#26368;&#22823;&#21487;&#35757;&#32451;&#28145;&#24230;&#21487;&#20197;&#37327;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#27839;&#28151;&#27788;&#36793;&#32536;&#39281;&#21644;tanh&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30456;&#31354;&#38388;&#20013;&#26368;&#22823;&#29109;&#30340;&#21518;&#28608;&#27963;&#20998;&#24067;&#30340;&#22343;&#21248;&#24615;&#32447;&#12290;&#35813;&#32447;&#20132;&#21449;&#20110;&#28151;&#27788;&#36793;&#32536;&#65292;&#24182;&#25351;&#31034;&#20102;&#36229;&#36807;&#28608;&#27963;&#20989;&#25968;&#39281;&#21644;&#24320;&#22987;&#22952;&#30861;&#35757;&#32451;&#25928;&#29575;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27839;&#28151;&#27788;&#36793;&#32536;&#21021;&#22987;&#21270;&#26159;&#33719;&#24471;&#26368;&#20339;&#21487;&#35757;&#32451;&#24615;&#25152;&#24517;&#38656;&#20294;&#19981;&#20805;&#20998;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep feedforward networks initialized along the edge of chaos exhibit exponentially superior training ability as quantified by maximum trainable depth. In this work, we explore the effect of saturation of the tanh activation function along the edge of chaos. In particular, we determine the line of uniformity in phase space along which the post-activation distribution has maximum entropy. This line intersects the edge of chaos, and indicates the regime beyond which saturation of the activation function begins to impede training efficiency. Our results suggest that initialization along the edge of chaos is a necessary but not sufficient condition for optimal trainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20215;&#20540;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.04782</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24847;&#22270;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20215;&#20540;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#21160;&#35266;&#23519;&#25968;&#25454;&#20016;&#23500;&#32780;&#23500;&#26377;&#20449;&#24687;&#65292;&#28982;&#32780;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24456;&#23569;&#33021;&#22815;&#21033;&#29992;&#35813;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#27169;&#24847;&#22270;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#34913;&#37327;&#24403;&#26234;&#33021;&#20307;&#20026;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#32780;&#37319;&#21462;&#34892;&#21160;&#26102;&#26410;&#26469;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#22914;&#20309;&#21464;&#21270;&#26469;&#23398;&#20064;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#24046;&#23398;&#20064;&#30446;&#26631;&#26469;&#23398;&#20064;&#24847;&#22270;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20294;&#26159;&#23436;&#20840;&#26159;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#12290;&#36890;&#36807;&#20248;&#21270;&#35813;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#21516;&#26102;&#20174;&#21407;&#22987;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#29366;&#24577;&#12289;&#31574;&#30053;&#21644;&#29615;&#22659;&#19979;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#30475;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20986;&#30340;&#29305;&#24449;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20215;&#20540;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#21387;&#32553;&#31574;&#30053;&#26469;&#21152;&#36895;&#35745;&#31639;PDE&#32422;&#26463;&#30340;&#36870;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#20869;&#23384;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.04781</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#32534;&#30721;&#22120;&#21387;&#32553;&#26041;&#27861;&#21152;&#36895;&#22823;&#35268;&#27169;&#36870;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
An autoencoder compression approach for accelerating large-scale inverse problems. (arXiv:2304.04781v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04781
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#21387;&#32553;&#31574;&#30053;&#26469;&#21152;&#36895;&#35745;&#31639;PDE&#32422;&#26463;&#30340;&#36870;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#20869;&#23384;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PDE&#32422;&#26463;&#30340;&#36870;&#38382;&#39064;&#26159;&#24403;&#20170;&#35745;&#31639;&#31185;&#23398;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#20102;&#31934;&#30830;&#35745;&#31639;PDE&#35299;&#65292;&#38656;&#35201;&#32454;&#24494;&#30340;&#32593;&#26684;&#65292;&#36825;&#24341;&#20837;&#20102;&#22823;&#37327;&#21442;&#25968;&#65292;&#24182;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22914;&#26356;&#22810;&#30340;&#22788;&#29702;&#22120;&#21644;&#26356;&#22810;&#30340;&#20869;&#23384;&#26469;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#35745;&#31639;&#26799;&#24230;&#21644;&#39640;&#38454;&#23548;&#25968;&#65292;&#36890;&#24120;&#37319;&#29992;&#20276;&#38543;&#26041;&#27861;&#26469;&#38480;&#21046;&#30001;&#26102;&#38388;&#28436;&#21270;&#30340;PDE&#32422;&#26463;&#30340;&#36870;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#35201;&#27714;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#27714;&#35299;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#30340;&#20276;&#38543;PDE&#65292;&#35813;&#20276;&#38543;PDE&#20381;&#36182;&#20110;&#21069;&#21521;PDE&#35299;&#12290;&#36825;&#38656;&#35201;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#19978;&#23384;&#20648;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#21069;&#21521;&#35299;&#21521;&#37327;&#12290;&#36825;&#31181;&#36807;&#31243;&#24555;&#36895;&#32791;&#23613;&#20102;&#21487;&#29992;&#30340;&#20869;&#23384;&#36164;&#28304;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25163;&#27573;&#65292;&#22914;&#26816;&#26597;&#28857;&#21644;&#21387;&#32553;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#20869;&#23384;&#29942;&#39048;&#30340;&#24433;&#21709;&#24182;&#19988;&#25442;&#21462;&#20102;&#39069;&#22806;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
PDE-constrained inverse problems are some of the most challenging and computationally demanding problems in computational science today. Fine meshes that are required to accurately compute the PDE solution introduce an enormous number of parameters and require large scale computing resources such as more processors and more memory to solve such systems in a reasonable time. For inverse problems constrained by time dependent PDEs, the adjoint method that is often employed to efficiently compute gradients and higher order derivatives requires solving a time-reversed, so-called adjoint PDE that depends on the forward PDE solution at each timestep. This necessitates the storage of a high dimensional forward solution vector at every timestep. Such a procedure quickly exhausts the available memory resources. Several approaches that trade additional computation for reduced memory footprint have been proposed to mitigate the memory bottleneck, including checkpointing and compression strategies
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21450;&#20854;&#27969;&#34892;&#36235;&#21183;&#65292;&#38416;&#36848;&#20102;XAI&#30340;&#20351;&#29992;&#21407;&#22240;&#12289;&#22914;&#20309;&#20351;&#29992;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#32473;&#20986;&#20102;&#22914;&#20309;&#33719;&#24471;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#26041;&#27861;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#26377;&#37325;&#35201;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.04780</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#32508;&#36848;&#65306;&#20026;&#20160;&#20040;&#12289; &#22914;&#20309;&#21644;&#20309;&#26102;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?. (arXiv:2304.04780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21450;&#20854;&#27969;&#34892;&#36235;&#21183;&#65292;&#38416;&#36848;&#20102;XAI&#30340;&#20351;&#29992;&#21407;&#22240;&#12289;&#22914;&#20309;&#20351;&#29992;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#32473;&#20986;&#20102;&#22914;&#20309;&#33719;&#24471;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#26041;&#27861;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#26377;&#37325;&#35201;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#21516;&#26102;&#20063;&#20986;&#29616;&#20102;&#20851;&#20110;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20915;&#31574;&#21487;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#31995;&#32479;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#30446;&#21069;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25353;&#29031;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;&#20803;&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#65288;PRISMA&#65289;&#26631;&#20934;&#65292;&#26816;&#32034;&#20102;2012&#24180;1&#26376;1&#26085;&#33267;2022&#24180;2&#26376;2&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#30456;&#20851;&#25991;&#31456;&#12290;&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;XAI&#30340;&#27969;&#34892;&#36235;&#21183;&#65292;&#24182;&#38416;&#36848;&#20102;&#30740;&#31350;&#30340;&#20027;&#35201;&#26041;&#21521;&#12290;&#25105;&#20204;&#35843;&#26597;&#36825;&#20123;XAI&#27169;&#22411;&#30340;&#20351;&#29992;&#21407;&#22240;&#12289;&#22914;&#20309;&#20351;&#29992;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#20197;&#21450;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;XAI&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#38416;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#25551;&#36848;&#21307;&#30103;&#39046;&#22495;&#30340;AI&#27169;&#22411;&#26469;&#33719;&#24471;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23545;&#26412;&#25991;&#30340;&#35752;&#35770;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#28145;&#20837;&#20102;&#35299;XAI&#22312;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for relevant work published from 1 January 2012 to 02 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will
&lt;/p&gt;</description></item><item><title>GraphMAE2&#26159;&#19968;&#31181;&#25513;&#30721;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#35270;&#35282;&#38543;&#26426;&#37325;&#26032;&#23631;&#34109;&#35299;&#30721;&#21644;&#28508;&#22312;&#34920;&#31034;&#39044;&#27979;&#31574;&#30053;&#23545;&#22270;SSL&#20013;&#30340;&#29305;&#24449;&#37325;&#26500;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.04779</link><description>&lt;p&gt;
GraphMAE2&#65306;&#19968;&#31181;&#35299;&#30721;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner. (arXiv:2304.04779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04779
&lt;/p&gt;
&lt;p&gt;
GraphMAE2&#26159;&#19968;&#31181;&#25513;&#30721;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#35270;&#35282;&#38543;&#26426;&#37325;&#26032;&#23631;&#34109;&#35299;&#30721;&#21644;&#28508;&#22312;&#34920;&#31034;&#39044;&#27979;&#31574;&#30053;&#23545;&#22270;SSL&#20013;&#30340;&#29305;&#24449;&#37325;&#26500;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21253;&#25324;&#23545;&#27604;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#31614;&#31232;&#32570;&#30340;&#22270;&#25968;&#25454;&#30340;&#26681;&#26412;&#38590;&#39064;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#20004;&#31181;&#22270;SSL&#25216;&#26415;&#20013;&#65292;&#25513;&#30721;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;GraphMAE&#65289;&#26159;&#26368;&#36817;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#30340;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#12290;&#20854;&#24605;&#24819;&#26159;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20174;&#36755;&#20837;&#20013;&#38543;&#26426;&#23631;&#34109;&#30340;&#33410;&#28857;&#29305;&#24449;&#65288;&#25110;&#32467;&#26500;&#65289;&#36827;&#34892;&#37325;&#26500;&#12290;&#28982;&#32780;&#65292;&#25513;&#30721;&#29305;&#24449;&#37325;&#26500;&#30340;&#24615;&#33021;&#33258;&#28982;&#21462;&#20915;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#21487;&#36776;&#21035;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#23545;&#29305;&#24449;&#20013;&#30340;&#25200;&#21160;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25513;&#30721;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;GraphMAE2&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#24819;&#27861;&#26159;&#22312;&#22270;SSL&#20013;&#23545;&#29305;&#24449;&#37325;&#26500;&#26045;&#21152;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#35270;&#35282;&#38543;&#26426;&#37325;&#26032;&#25513;&#30721;&#35299;&#30721;&#21644;&#28508;&#22312;&#34920;&#31034;&#39044;&#27979;&#30340;&#31574;&#30053;&#20197;&#23454;&#29616;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to reg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31639;&#23376;&#21644;/&#25110;&#38543;&#26426;&#32422;&#26463;&#30340;&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#24403;FCVI&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#38750;&#20809;&#28369;&#30340;&#25110;&#38543;&#26426;&#30340;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#31639;&#23376;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04778</link><description>&lt;p&gt;
&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
First-order methods for Stochastic Variational Inequality problems with Function Constraints. (arXiv:2304.04778v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31639;&#23376;&#21644;/&#25110;&#38543;&#26426;&#32422;&#26463;&#30340;&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#24403;FCVI&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#38750;&#20809;&#28369;&#30340;&#25110;&#38543;&#26426;&#30340;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#31639;&#23376;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#20276;&#38543;&#30528;&#21487;&#33021;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#20989;&#25968;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#25237;&#24433;&#31639;&#23376;&#30340;&#35745;&#31639;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#24102;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#21253;&#25324;&#20855;&#26377;&#38543;&#26426;&#31639;&#23376;&#21644;/&#25110;&#38543;&#26426;&#32422;&#26463;&#30340;&#20809;&#28369;&#25110;&#38750;&#20809;&#28369;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;{\texttt{OpConEx}}&#26041;&#27861;&#21450;&#20854;&#38543;&#26426;&#21464;&#20307;&#65292;&#23427;&#20204;&#37319;&#29992;&#31639;&#23376;&#21644;&#38480;&#21046;&#35780;&#20272;&#30340;&#22806;&#25512;&#26469;&#26356;&#26032;&#21464;&#37327;&#21644;Lagrangian&#20056;&#25968;&#12290;&#24403;FCVI&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#38750;&#20809;&#28369;&#30340;&#25110;&#38543;&#26426;&#30340;&#65288;&#21253;&#25324;&#20809;&#28369;&#25110;&#38750;&#20809;&#28369;&#30340;&#38543;&#26426;&#32422;&#26463;&#65289;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#31639;&#23376;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31616;&#21333;&#30340;&#21333;&#24490;&#29615;&#31243;&#24207;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#23601;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The monotone Variational Inequality (VI) is an important problem in machine learning. In numerous instances, the VI problems are accompanied by function constraints which can possibly be data-driven, making the projection operator challenging to compute. In this paper, we present novel first-order methods for function constrained VI (FCVI) problem under various settings, including smooth or nonsmooth problems with a stochastic operator and/or stochastic constraints. First, we introduce the~{\texttt{OpConEx}} method and its stochastic variants, which employ extrapolation of the operator and constraint evaluations to update the variables and the Lagrangian multipliers. These methods achieve optimal operator or sample complexities when the FCVI problem is either (i) deterministic nonsmooth, or (ii) stochastic, including smooth or nonsmooth stochastic constraints. Notably, our algorithms are simple single-loop procedures and do not require the knowledge of Lagrange multipliers to attain th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#26041;&#38754;&#30340;&#21010;&#26102;&#20195;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#32531;&#35299;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#24378;&#35843;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38656;&#35201;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04761</link><description>&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#19982;&#20844;&#20849;&#21355;&#29983;&#24179;&#31561;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Connecting Fairness in Machine Learning with Public Health Equity. (arXiv:2304.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04761
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#26041;&#38754;&#30340;&#21010;&#26102;&#20195;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#32531;&#35299;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#24378;&#35843;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38656;&#35201;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#20844;&#20849;&#21355;&#29983;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26377;&#26395;&#25552;&#39640;&#20154;&#21475;&#20581;&#24247;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#36873;&#25321;&#21644;&#21355;&#29983;&#31995;&#32479;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#25918;&#22823;&#29616;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;&#19981;&#24179;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#26041;&#38754;&#30340;&#21010;&#26102;&#20195;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#32531;&#35299;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#21487;&#20197;&#23558;&#20844;&#24179;&#24615;&#32435;&#20837;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#35828;&#26126;&#25968;&#25454;&#20013;&#20559;&#35265;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#31995;&#32479;&#24615;&#20559;&#35265;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#34987;&#25918;&#22823;&#12290;&#36825;&#20123;&#26696;&#20363;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#26469;&#39044;&#38450;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38656;&#35201;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has become a critical tool in public health, offering the potential to improve population health, diagnosis, treatment selection, and health system efficiency. However, biases in data and model design can result in disparities for certain protected groups and amplify existing inequalities in healthcare. To address this challenge, this study summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model. The framework provides guidance on incorporating fairness into different stages of the typical ML pipeline, such as data processing, model design, deployment, and evaluation. To illustrate the impact of biases in data on ML models, we present examples that demonstrate how systematic biases can be amplified through model predictions. These case studies suggest how the framework can be used to prevent these biases and highlight the need for fair and equitable ML models in public health. This work aims
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04759</link><description>&lt;p&gt;
&#21387;&#32553;&#32034;&#24341;&#23454;&#29616;&#30636;&#38388;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Similarity search in the blink of an eye with compressed indices. (arXiv:2304.04759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25968;&#25454;&#20197;&#21521;&#37327;&#34920;&#31034;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#23547;&#25214;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20284;&#30340;&#21521;&#37327;&#26159;&#19968;&#39033;&#24191;&#27867;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#24314;&#26356;&#24555;&#12289;&#26356;&#23567;&#30340;&#32034;&#24341;&#20197;&#36816;&#34892;&#36825;&#20123;&#25628;&#32034;&#30340;&#26032;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#65292;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#23427;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#25628;&#32034;&#24615;&#33021;&#65292;&#23545;&#25628;&#32034;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;LVQ&#34987;&#35774;&#35745;&#20026;&#19982;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#19968;&#36215;&#24037;&#20316;&#20197;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#31995;&#32479;&#20013;&#38024;&#23545;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#36827;&#34892;&#20851;&#38190;&#20248;&#21270;&#21518;&#65292;LVQ&#30340;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#22312;&#22788;&#29702;&#25968;&#21313;&#20159;&#20010;&#21521;&#37327;&#26102;&#65292;LVQ&#36229;&#36807;&#31532;&#20108;&#20339;&#26041;&#26696;&#65306;
&lt;/p&gt;
&lt;p&gt;
Nowadays, data is represented by vectors. Retrieving those vectors, among millions and billions, that are similar to a given query is a ubiquitous problem of relevance for a wide range of applications. In this work, we present new techniques for creating faster and smaller indices to run these searches. To this end, we introduce a novel vector compression method, Locally-adaptive Vector Quantization (LVQ), that simultaneously reduces memory footprint and improves search performance, with minimal impact on search accuracy. LVQ is designed to work optimally in conjunction with graph-based indices, reducing their effective bandwidth while enabling random-access-friendly fast similarity computations. Our experimental results show that LVQ, combined with key optimizations for graph-based indices in modern datacenter systems, establishes the new state of the art in terms of performance and memory footprint. For billions of vectors, LVQ outcompetes the second-best alternatives: (1) in the low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#31561;&#21516;&#24615;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#26469;&#35780;&#20272;&#31561;&#21464;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#23616;&#37096;&#20122;&#32467;&#26500;&#32534;&#30721;&#65288;LSE&#65289;&#21644;&#24103;&#36716;&#25442;&#32534;&#30721;&#65288;FTE&#65289;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#12290;LEFTNet&#26377;&#25928;&#22320;&#24212;&#29992;&#20102;&#36825;&#20123;&#27169;&#22359;&#24182;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04757</link><description>&lt;p&gt;
&#24314;&#31435;&#39640;&#25928;&#21644;&#26377;&#34920;&#29616;&#21147;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A new perspective on building efficient and expressive 3D equivariant graph neural networks. (arXiv:2304.04757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#31561;&#21516;&#24615;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#26469;&#35780;&#20272;&#31561;&#21464;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#23616;&#37096;&#20122;&#32467;&#26500;&#32534;&#30721;&#65288;LSE&#65289;&#21644;&#24103;&#36716;&#25442;&#32534;&#30721;&#65288;FTE&#65289;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#12290;LEFTNet&#26377;&#25928;&#22320;&#24212;&#29992;&#20102;&#36825;&#20123;&#27169;&#22359;&#24182;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20351;&#24471;&#22312;&#24314;&#27169;&#19977;&#32500;&#29289;&#20307;&#26102;&#21487;&#20197;&#32534;&#30721;&#29289;&#29702;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#31561;&#21516;&#24615;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#26469;&#35780;&#20272;&#31561;&#21464;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35843;&#26597;&#20174;&#23616;&#37096;&#22359;&#20195;&#34920;&#20840;&#23616;&#20960;&#20309;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#8212;&#8212;&#23616;&#37096;&#20122;&#32467;&#26500;&#32534;&#30721;&#65288;LSE&#65289;&#21644;&#24103;&#36716;&#25442;&#32534;&#30721;&#65288;FTE&#65289;&#26159;&#35774;&#35745;&#39640;&#25928;&#21644;&#26377;&#34920;&#29616;&#21147;&#20960;&#20309;GNN&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#29702;&#35770;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEFTNet&#65292;&#23427;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#36825;&#20123;&#27169;&#22359;&#65292;&#24182;&#22312;&#26631;&#37327;&#20540;&#21644;&#30690;&#37327;&#20540;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26410;&#26469;&#21457;&#23637;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. O
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#32423;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21152;&#36895;&#27773;&#36710;&#34892;&#19994;&#20013;&#22788;&#29702;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#30340;&#20840;&#21608;&#26399;&#65292;&#20174;&#32780;&#26356;&#24555;&#22320;&#38548;&#31163;&#26681;&#26412;&#21407;&#22240;&#12289;&#30830;&#23450;&#27835;&#30103;&#25514;&#26045;&#24182;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04755</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#36947;&#36335;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#35786;&#26029;&#30340;&#20004;&#23618;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Two-level Causal Inference Framework for On-road Vehicle Quality Issues Diagnosis. (arXiv:2304.04755v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04755
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#32423;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21152;&#36895;&#27773;&#36710;&#34892;&#19994;&#20013;&#22788;&#29702;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#30340;&#20840;&#21608;&#26399;&#65292;&#20174;&#32780;&#26356;&#24555;&#22320;&#38548;&#31163;&#26681;&#26412;&#21407;&#22240;&#12289;&#30830;&#23450;&#27835;&#30103;&#25514;&#26045;&#24182;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#65292;&#22788;&#29702;&#20351;&#29992;&#20013;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#30340;&#20840;&#21608;&#26399;&#21487;&#33021;&#38656;&#35201;&#25968;&#21608;&#36827;&#34892;&#35843;&#26597;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#21040;&#38548;&#31163;&#26681;&#26412;&#21407;&#22240;&#12289;&#23450;&#20041;&#21644;&#23454;&#26045;&#36866;&#24403;&#30340;&#27835;&#30103;&#25514;&#26045;&#65292;&#20197;&#21450;&#22312;&#24517;&#35201;&#26102;&#25913;&#36827;&#27835;&#30103;&#25514;&#26045;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#12289;&#35780;&#20272;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#22312;&#24403;&#21069;&#27835;&#30103;&#34987;&#35748;&#20026;&#26080;&#25928;&#26102;&#25351;&#23548;&#19979;&#19968;&#20010;&#21487;&#34892;&#30340;&#27835;&#30103;&#25514;&#26045;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21152;&#36895;&#36825;&#20123;&#36807;&#31243;&#12290;&#20351;&#29992;&#20174;&#36335;&#19978;&#36710;&#36742;&#25910;&#38598;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#36824;&#23558;&#35752;&#35770;&#36710;&#36742;&#36136;&#37327;&#24212;&#29992;&#30340;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the automotive industry, the full cycle of managing in-use vehicle quality issues can take weeks to investigate. The process involves isolating root causes, defining and implementing appropriate treatments, and refining treatments if needed. The main pain-point is the lack of a systematic method to identify causal relationships, evaluate treatment effectiveness, and direct the next actionable treatment if the current treatment was deemed ineffective. This paper will show how we leverage causal Machine Learning (ML) to speed up such processes. A real-word data set collected from on-road vehicles will be used to demonstrate the proposed framework. Open challenges for vehicle quality applications will also be discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#20351;&#29992;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65306;&#24102;&#26377;&#20013;&#22830;&#33410;&#28857;&#21644;&#19981;&#24102;&#20013;&#22830;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.04754</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#30005;&#39057;&#35889;&#21344;&#29992;&#26816;&#27979;&#26041;&#27861;&#30340;&#27604;&#36739;&#65306;&#33410;&#28857;&#21644;&#26080;&#33410;&#28857;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Por\'ownanie metod detekcji zaj\k{e}to\'sci widma radiowego z wykorzystaniem uczenia federacyjnego z oraz bez w\k{e}z{\l}a centralnego. (arXiv:2304.04754v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#20351;&#29992;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65306;&#24102;&#26377;&#20013;&#22830;&#33410;&#28857;&#21644;&#19981;&#24102;&#20013;&#22830;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#39057;&#35889;&#25509;&#20837;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#20851;&#20110;&#39057;&#35889;&#21344;&#29992;&#24773;&#20917;&#21644;&#20854;&#20182;&#29992;&#25143;&#23384;&#22312;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#20026;&#26032;&#35774;&#22791;&#20570;&#20986;&#39057;&#35889;&#20998;&#37197;&#20915;&#31574;&#12290;&#31616;&#21333;&#30340;&#39057;&#35889;&#21344;&#29992;&#26816;&#27979;&#26041;&#27861;&#32463;&#24120;&#19981;&#22815;&#21487;&#38752;&#65292;&#22240;&#27492;&#36890;&#24120;&#25104;&#21151;&#22320;&#37319;&#29992;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39057;&#35889;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#20943;&#23569;&#25511;&#21046;&#25968;&#25454;&#37327;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#20351;&#29992;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65306;&#24102;&#26377;&#20013;&#22830;&#33410;&#28857;&#21644;&#19981;&#24102;&#20013;&#22830;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic spectrum access systems typically require information about the spectrum occupancy and thus the presence of other users in order to make a spectrum al-location decision for a new device. Simple methods of spectrum occupancy detection are often far from reliable, hence spectrum occupancy detection algorithms supported by machine learning or artificial intelligence are often and successfully used. To protect the privacy of user data and to reduce the amount of control data, an interesting approach is to use federated machine learning. This paper compares two approaches to system design using federated machine learning: with and without a central node.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;e-Uber&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21033;&#29992;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36890;&#36807;V2G&#21644;BST&#20849;&#21516;&#23454;&#29616;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#12290;&#24179;&#21488;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22522;&#20110;CMAB&#31639;&#27861;&#23454;&#29616;&#20010;&#24615;&#21270;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;(CARS)&#65292;&#24182;&#21033;&#29992;&#21453;&#21521;&#25293;&#21334;&#26426;&#21046;&#36873;&#25321;&#26368;&#20248;&#20986;&#20215;&#12290;</title><link>http://arxiv.org/abs/2304.04753</link><description>&lt;p&gt;
$\textit{e-Uber}$&#65306;&#19968;&#20010;&#22522;&#20110;&#20849;&#20139;&#32463;&#27982;&#30340;&#24179;&#21488;&#65292;&#23454;&#29616;&#22522;&#20110;&#30005;&#21160;&#27773;&#36710;&#30340;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
$\textit{e-Uber}$: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing. (arXiv:2304.04753v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;e-Uber&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21033;&#29992;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36890;&#36807;V2G&#21644;BST&#20849;&#21516;&#23454;&#29616;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#12290;&#24179;&#21488;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22522;&#20110;CMAB&#31639;&#27861;&#23454;&#29616;&#20010;&#24615;&#21270;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;(CARS)&#65292;&#24182;&#21033;&#29992;&#21453;&#21521;&#25293;&#21334;&#26426;&#21046;&#36873;&#25321;&#26368;&#20248;&#20986;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20849;&#20139;&#32463;&#27982;&#30340;&#21830;&#19994;&#27169;&#24335;&#22312;&#20132;&#36890;&#21644;&#20303;&#23487;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22914;Uber&#21644;Airbnb&#12290;&#20154;&#20204;&#36234;&#26469;&#36234;&#26377;&#20852;&#36259;&#23558;&#27492;&#27169;&#24335;&#24212;&#29992;&#20110;&#33021;&#28304;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#65288;P2P&#65289;&#33021;&#28304;&#20132;&#26131;&#21644;&#22522;&#20110;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#30340;&#36710;&#23545;&#32593;&#65288;V2G&#65289;&#12289;&#36710;&#23545;&#23478;&#65288;V2H&#65289;&#12289;&#36710;&#23545;&#36710;&#65288;V2V&#65289;&#20197;&#21450;&#30005;&#27744;&#26356;&#25442;&#25216;&#26415;(BST)&#31561;&#26041;&#24335;&#12290;&#26412;&#25991;&#21033;&#29992;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;e-Uber&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#36890;&#36807;V2G&#21644;BST&#20849;&#21516;&#23454;&#29616;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#12290;e-Uber&#21033;&#29992;&#31354;&#38388;&#20247;&#21253;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#21453;&#21521;&#25293;&#21334;&#29702;&#35770;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#24179;&#21488;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20102;&#35299;&#21496;&#26426;&#23545;&#19981;&#21516;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#20219;&#21153;&#30340;&#20559;&#22909;&#12290;&#22522;&#20110;&#36825;&#20123;&#20559;&#22909;&#65292;&#36890;&#36807;&#22522;&#20110;CMAB&#30340;&#31639;&#27861;&#36827;&#34892;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;(CARS)&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#21496;&#26426;&#22312;&#24895;&#24847;&#25191;&#34892;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#31454;&#26631;&#65292;&#32780;&#21453;&#21521;&#25293;&#21334;&#26426;&#21046;&#21017;&#36873;&#25321;&#26368;&#20248;&#20986;&#20215;&#12290;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#21305;&#37197;&#25928;&#29575;&#12289;&#26102;&#38388;&#25928;&#29575;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sharing-economy-based business model has recently seen success in the transportation and accommodation sectors with companies like Uber and Airbnb. There is growing interest in applying this model to energy systems, with modalities like peer-to-peer (P2P) Energy Trading, Electric Vehicles (EV)-based Vehicle-to-Grid (V2G), Vehicle-to-Home (V2H), Vehicle-to-Vehicle (V2V), and Battery Swapping Technology (BST). In this work, we exploit the increasing diffusion of EVs to realize a crowdsourcing platform called e-Uber that jointly enables ride-sharing and energy-sharing through V2G and BST. e-Uber exploits spatial crowdsourcing, reinforcement learning, and reverse auction theory. Specifically, the platform uses reinforcement learning to understand the drivers' preferences towards different ride-sharing and energy-sharing tasks. Based on these preferences, a personalized list is recommended to each driver through CMAB-based Algorithm for task Recommendation System (CARS). Drivers bid on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20351;&#29992;Pumas&#24037;&#20316;&#27969;&#31243;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#30340;&#36125;&#21494;&#26031;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#30340;&#25152;&#26377;&#27493;&#39588;&#65292;&#20197;&#21450;&#35768;&#22810;&#37325;&#35201;&#30340;&#24819;&#27861;&#21644;&#39044;&#38450;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2304.04752</link><description>&lt;p&gt;
Pumas&#22312;&#33647;&#20195;&#21160;&#21147;&#23398;&#20013;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#23454;&#36341;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Practitioner's Guide to Bayesian Inference in Pharmacometrics using Pumas. (arXiv:2304.04752v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20351;&#29992;Pumas&#24037;&#20316;&#27969;&#31243;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#30340;&#36125;&#21494;&#26031;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#30340;&#25152;&#26377;&#27493;&#39588;&#65292;&#20197;&#21450;&#35768;&#22810;&#37325;&#35201;&#30340;&#24819;&#27861;&#21644;&#39044;&#38450;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;Pumas&#24037;&#20316;&#27969;&#31243;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#36125;&#21494;&#26031;&#23454;&#36341;&#32773;&#30340;&#23454;&#36341;&#25351;&#21335;&#12290;&#25105;&#20204;&#20174;&#31616;&#35201;&#20171;&#32461;&#33647;&#20195;&#21160;&#21147;&#23398;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21160;&#26426;&#24320;&#22987;&#65292;&#24378;&#35843;&#29616;&#26377;&#36719;&#20214;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#33647;&#20195;&#21160;&#21147;&#23398;&#26631;&#20934;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#30340;&#25152;&#26377;&#27493;&#39588;&#65292;&#21253;&#25324;&#27169;&#22411;&#23450;&#20041;&#12289;&#20808;&#39564;&#36873;&#25321;&#12289;&#20174;&#21518;&#39564;&#37319;&#26679;&#12289;&#20808;&#39564;&#21644;&#21518;&#39564;&#27169;&#25311;&#21644;&#39044;&#27979;&#12289;&#21453;&#20107;&#23454;&#27169;&#25311;&#21644;&#39044;&#27979;&#12289;&#25910;&#25947;&#35786;&#26029;&#12289;&#35270;&#35273;&#39044;&#27979;&#26816;&#26597;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#21644;&#20132;&#21449;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29992;&#31616;&#21333;&#30340;&#35821;&#35328;&#35299;&#37322;&#20102;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#20013;&#35768;&#22810;&#20808;&#36827;&#27010;&#24565;&#30340;&#32972;&#26223;&#21644;&#30452;&#35273;&#65292;&#21253;&#25324;&#29992;&#25143;&#22312;&#25191;&#34892;&#36125;&#21494;&#26031;&#20998;&#26512;&#26102;&#38656;&#35201;&#35760;&#20303;&#30340;&#35768;&#22810;&#37325;&#35201;&#30340;&#24819;&#27861;&#21644;&#39044;&#38450;&#25514;&#26045;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#35768;&#22810;&#31639;&#27861;&#12289;&#20195;&#30721;&#21644;&#24605;&#24819;&#37117;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive tutorial for Bayesian practitioners in pharmacometrics using Pumas workflows. We start by giving a brief motivation of Bayesian inference for pharmacometrics highlighting limitations in existing software that Pumas addresses. We then follow by a description of all the steps of a standard Bayesian workflow for pharmacometrics using code snippets and examples. This includes: model definition, prior selection, sampling from the posterior, prior and posterior simulations and predictions, counter-factual simulations and predictions, convergence diagnostics, visual predictive checks, and finally model comparison with cross-validation. Finally, the background and intuition behind many advanced concepts in Bayesian statistics are explained in simple language. This includes many important ideas and precautions that users need to keep in mind when performing Bayesian analysis. Many of the algorithms, codes, and ideas presented in this paper are highly applicab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04641</link><description>&lt;p&gt;
&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20854;&#20027;&#35201;&#25903;&#26609;&#20026;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#21516;&#26102;&#23454;&#29616;&#26080;&#31351;&#23567;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#22914;&#20309;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#35299;&#20915;&#26041;&#26696;&#26159;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26435;&#34913;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#32422;&#26463;&#38544;&#31169;&#27844;&#38706;&#19981;&#36229;&#36807;&#39044;&#23450;&#20540;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#38750;&#24120;&#32791;&#26102;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#23384;&#22312;&#24615;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26356;&#39640;&#25928;&#12289;&#26356;&#23481;&#26131;&#34987;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;FL&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;FL&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#37319;&#26679;&#27604;&#29575;&#65292;&#20197;&#24179;&#34913;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26368;&#21518;&#35777;&#26126;FedPAC&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FedPAC&#26694;&#26550;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#21442;&#25968;&#21270;&#21644;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#33267;&#23569;&#20108;&#27425;&#36830;&#32493;&#21487;&#24494;&#12289;&#21452;Lipschitz&#36830;&#32493;&#30340;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;&#24418;&#65292;&#33021;&#22815;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04555</link><description>&lt;p&gt;
&#31070;&#32463;&#27969;&#24418;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;
&lt;/p&gt;
&lt;p&gt;
Neural Diffeomorphic Non-uniform B-spline Flows. (arXiv:2304.04555v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04555
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#21442;&#25968;&#21270;&#21644;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#33267;&#23569;&#20108;&#27425;&#36830;&#32493;&#21487;&#24494;&#12289;&#21452;Lipschitz&#36830;&#32493;&#30340;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;&#24418;&#65292;&#33021;&#22815;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#25104;&#21151;&#22320;&#23558;&#22797;&#26434;&#30340;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#20026;&#31616;&#21333;&#22522;&#26412;&#20998;&#24067;&#30340;&#21487;&#36870;&#21464;&#25442;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#38656;&#35201;&#26356;&#22810;&#12290;&#22312;&#29289;&#29702;&#20013;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#35201;&#27714;&#21464;&#25442;&#30340;&#20108;&#38454;&#23548;&#25968;&#26159;&#33391;&#22909;&#23450;&#20041;&#21644;&#36830;&#32493;&#30340;&#65292;&#24179;&#28369;&#27491;&#21017;&#21270;&#27969;&#37319;&#29992;&#26080;&#38480;&#21487;&#24494;&#21464;&#25442;&#65292;&#20294;&#20197;&#32531;&#24930;&#30340;&#38750;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#20195;&#20215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33267;&#23569;&#20108;&#27425;&#36830;&#32493;&#21487;&#24494;&#19988;&#21452;Lipschitz&#36830;&#32493;&#30340;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;&#24418;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#24494;&#20998;&#21516;&#32986;&#30340;&#35299;&#26512;&#36870;&#21464;&#25442;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Ck-2&#24494;&#20998;&#21516;&#32986;&#30340;&#38750;&#22343;&#21248;k&#38454;B&#26679;&#26465;&#21464;&#25442;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#31526;&#21512;&#20805;&#20998;&#26465;&#20214;&#30340;&#38750;&#22343;&#21248;&#31435;&#26041;B&#26679;&#26465;&#21464;&#25442;&#30340;&#35299;&#26512;&#36870;&#21464;&#25442;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows have been successfully modeling a complex probability distribution as an invertible transformation of a simple base distribution. However, there are often applications that require more than invertibility. For instance, the computation of energies and forces in physics requires the second derivatives of the transformation to be well-defined and continuous. Smooth normalizing flows employ infinitely differentiable transformation, but with the price of slow non-analytic inverse transforms. In this work, we propose diffeomorphic non-uniform B-spline flows that are at least twice continuously differentiable while bi-Lipschitz continuous, enabling efficient parametrization while retaining analytic inverse transforms based on a sufficient condition for diffeomorphism. Firstly, we investigate the sufficient condition for Ck-2-diffeomorphic non-uniform kth-order B-spline transformations. Then, we derive an analytic inverse transformation of the non-uniform cubic B-spline tran
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.04468</link><description>&lt;p&gt;
&#23454;&#29616;&#38431;&#21015;&#26234;&#33021;&#21270;&#65306;&#19968;&#31181;&#38024;&#23545;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#30340;&#36890;&#29992;&#32676;&#20307;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#26159;&#20174;&#20020;&#24202;&#24120;&#35268;&#25252;&#29702;&#20013;&#29983;&#25104;&#30340;&#65292;&#35760;&#24405;&#20102;&#24191;&#27867;&#30340;&#30149;&#20154;&#20154;&#32676;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20026;&#25913;&#21892;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#30149;&#20154;&#31649;&#29702;&#21644;&#24178;&#39044;&#31574;&#30053;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#20998;&#26512;&#33539;&#24335;&#26159;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#39318;&#20808;&#21033;&#29992;&#21333;&#20010;&#30149;&#20154;&#30340;EHR&#25968;&#25454;&#36890;&#36807;&#19968;&#20010;&#20027;&#24178;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#25903;&#25345;&#24314;&#31435;&#22312;&#36825;&#20123;&#34920;&#31034;&#30340;&#22810;&#26679;&#21270;&#30340;&#21307;&#30103;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26080;&#27861;&#28145;&#20837;&#20998;&#26512;&#30149;&#20154;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#24120;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#34987;&#31216;&#20026;&#38431;&#21015;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21516;&#19968;&#38431;&#21015;&#20013;&#30340;&#30149;&#20154;&#20542;&#21521;&#20110;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#34920;&#26126;&#20182;&#20204;&#22312;&#21307;&#30103;&#26465;&#20214;&#65288;&#22914;&#30151;&#29366;&#25110;&#30142;&#30149;&#65289;&#26041;&#38754;&#20855;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;COhort Representation lEarning (CORE)&#26694;&#26550;&#26469;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38431;&#21015;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#32676;&#20307;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#24182;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22810;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#24178;&#39044;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;1.5&#20493;&#33267;4&#20493;&#30340;&#40065;&#26834;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#22312;AudioSet 20K&#19978;&#23454;&#29616;&#20102;44.2 mAP&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04385</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Robustness in Multimodal Learning. (arXiv:2304.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22810;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#24178;&#39044;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;1.5&#20493;&#33267;4&#20493;&#30340;&#40065;&#26834;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#22312;AudioSet 20K&#19978;&#23454;&#29616;&#20102;44.2 mAP&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#34987;&#23450;&#20041;&#20026;&#23545;&#22810;&#31181;&#24322;&#26500;&#36755;&#20837;&#27169;&#24577;&#65288;&#22914;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#31561;&#65289;&#36827;&#34892;&#23398;&#20064;&#12290;&#26412;&#25991;&#20851;&#27880;&#20102;&#35299;&#24403;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#27169;&#24577;&#31867;&#22411;&#19981;&#21516;&#26102;&#65292;&#27169;&#22411;&#22914;&#20309;&#34920;&#29616;&#65292;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#24212;&#29992;&#20110;&#30828;&#20214;&#24179;&#21488;&#26102;&#20250;&#33258;&#28982;&#21457;&#29983;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#26694;&#26550;&#26469;&#31995;&#32479;&#20998;&#26512;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#24178;&#39044;&#25216;&#26415;&#65292;&#22312;AudioSet&#12289;Kinetics-400&#21644;ImageNet-Captions&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;1.5&#20493;&#33267;4&#20493;&#30340;&#40065;&#26834;&#24615;&#25552;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#24178;&#39044;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#39069;&#22806;&#30340;&#27169;&#24577;&#65292;&#22312;AudioSet 20K&#19978;&#23454;&#29616;&#20102;44.2 mAP&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning is defined as learning over multiple heterogeneous input modalities such as video, audio, and text. In this work, we are concerned with understanding how models behave as the type of modalities differ between training and deployment, a situation that naturally arises in many applications of multimodal learning to hardware platforms. We present a multimodal robustness framework to provide a systematic analysis of common multimodal representation learning methods. Further, we identify robustness short-comings of these approaches and propose two intervention techniques leading to $1.5\times$-$4\times$ robustness improvements on three datasets, AudioSet, Kinetics-400 and ImageNet-Captions. Finally, we demonstrate that these interventions better utilize additional modalities, if present, to achieve competitive results of $44.2$ mAP on AudioSet 20K.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.04103</link><description>&lt;p&gt;
TC-VAE&#65306;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
TC-VAE: Uncovering Out-of-Distribution Data Generative Factors. (arXiv:2304.04103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#26159;&#35299;&#20915;&#35299;&#32544;&#32467;&#23398;&#20064;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;-TC-VAE&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#23398;&#30340;&#28508;&#22312;&#34920;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#24635;&#30456;&#20851;&#24615;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#21457;&#29616;&#19981;&#22312;&#25968;&#25454;&#38598;&#20013;&#26174;&#24335;&#20986;&#29616;&#30340;&#21464;&#21270;&#22240;&#32032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#20855;&#26377;&#19981;&#24179;&#34913;&#30340;&#29983;&#25104;&#22240;&#32032;&#25968;&#25454;&#38598;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#20013;&#34920;&#26126;&#20102;TC-VAE&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed
&lt;/p&gt;</description></item><item><title>StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.03853</link><description>&lt;p&gt;
StepMix: &#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03853
&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#24191;&#20041;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;(&#28508;&#22312;&#21078;&#38754;&#21644;&#28508;&#22312;&#31867;&#20998;&#26512;)&#19982;&#22806;&#37096;&#21464;&#37327;(&#21327;&#21464;&#37327;&#21644;&#36828;&#31243;&#32467;&#26524;)&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;(&#21333;&#27493;&#12289;&#20004;&#27493;&#21644;&#19977;&#27493;&#26041;&#27861;)&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;&#22312;&#35768;&#22810;&#31038;&#20250;&#31185;&#23398;&#30340;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#19981;&#20165;&#26159;&#23558;&#20010;&#20307;&#32858;&#31867;&#25104;&#28508;&#22312;&#31867;&#21035;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#26469;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#19968;&#20010;&#23558;&#28508;&#22312;&#31867;&#21035;&#19982;&#35266;&#23519;&#25351;&#26631;&#30456;&#20851;&#32852;&#30340;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#23558;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#19982;&#28508;&#22312;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#27979;&#37327;&#21644;&#32467;&#26500;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25152;&#35859;&#30340;&#19968;&#27493;&#27861;&#20849;&#21516;&#20272;&#35745;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#36880;&#27493;&#26041;&#27861;&#36880;&#27493;&#20272;&#35745;&#65292;&#23545;&#20110;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#28508;&#22312;&#31867;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#38500;&#20102;&#19968;&#27493;&#27861;&#65292;StepMix&#36824;&#23454;&#29616;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#26041;&#20415;&#27169;&#22411;&#30340;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.03344</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#21327;&#20316;&#20449;&#21495;&#21435;&#22122;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#25429;&#25417;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#38454;&#21327;&#21516;&#20449;&#21495;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;GCF&#30340;&#21452;&#21521;&#37051;&#25509;&#30697;&#38453;&#65292;&#20854;&#23450;&#20041;&#20102;&#22522;&#20110;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#32858;&#21512;&#30340;&#37051;&#23621;&#65292;&#23545;&#20110;&#26377;&#22823;&#37327;&#20132;&#20114;&#20294;&#19981;&#36275;&#30340;&#29992;&#25143;/&#39033;&#30446;&#26469;&#35828;&#21487;&#33021;&#26159;&#22024;&#26434;&#30340;&#12290;&#27492;&#22806;&#65292;&#37051;&#25509;&#30697;&#38453;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#32858;&#21512;&#30340;&#26377;&#30410;&#37051;&#23621;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#24179;&#34913;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#25968;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#26469;&#33719;&#24471;&#29992;&#25143;/&#39033;&#30446;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;&#23545;&#31216;&#30340;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30456;&#20851;&#32452;&#20214;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.02836</link><description>&lt;p&gt;
&#38271;&#26399;&#30340;&#22810;&#27169;&#24335;&#21464;&#21387;&#22120;&#25972;&#21512;EHR&#20013;&#25104;&#20687;&#21644;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#65292;&#29992;&#20110;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#22797;&#25104;&#20687;&#21644;&#21307;&#30103;&#32972;&#26223;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65289;&#32435;&#20837;&#39044;&#27979;&#24615;&#23396;&#31435;&#24615;&#32954;&#37096;&#32467;&#33410;&#65288;SPN&#65289;&#35786;&#26029;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20687;&#25104;&#20687;&#21644;&#35786;&#26029;&#20195;&#30721;&#36825;&#26679;&#30340;&#20020;&#24202;&#24120;&#35268;&#27169;&#24335;&#21487;&#33021;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#19981;&#35268;&#21017;&#37319;&#26679;&#65292;&#36825;&#26159;&#38271;&#26399;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#23558;&#37325;&#22797;&#25104;&#20687;&#19982;&#26085;&#24120;&#25910;&#38598;&#30340;EHR&#20013;&#30340;&#38271;&#26399;&#20020;&#24202;&#29305;&#24449;&#30456;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;SPN&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#35299;&#32544;&#32538;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#36317;&#31163;&#32553;&#25918;&#33258;&#27880;&#24847;&#21147;&#26469;&#32852;&#21512;&#23398;&#20064;&#20020;&#24202;&#29305;&#24449;&#34920;&#36798;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#26159;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;2,668&#20010;&#25195;&#25551;&#21644;1,149&#21517;&#24535;&#24895;&#32773;&#30340;&#38271;&#26399;&#33016;&#37096;CT&#12289;&#36134;&#21333;&#20195;&#30721;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#35760;&#24405;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#35834;&#26364;&#30340;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#20316;&#20026;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#20132;&#20114;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#36825;&#20123;&#21160;&#20316;&#38454;&#27573;&#30340;&#24037;&#20855;&#30340;&#31034;&#20363;&#12290;&#35813;&#26694;&#26550;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;-LLM&#20132;&#20114;&#30740;&#31350;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.02822</link><description>&lt;p&gt;
&#20197;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#30340;&#26041;&#24335;&#35774;&#35745;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#30340;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Approach Intelligent Writing Assistants Usability with Seven Stages of Action. (arXiv:2304.02822v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#35834;&#26364;&#30340;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#20316;&#20026;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#20132;&#20114;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#36825;&#20123;&#21160;&#20316;&#38454;&#27573;&#30340;&#24037;&#20855;&#30340;&#31034;&#20363;&#12290;&#35813;&#26694;&#26550;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;-LLM&#20132;&#20114;&#30740;&#31350;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMS )&#20855;&#22791;&#25104;&#20026;&#20889;&#20316;&#21161;&#25163;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#27169;&#22411;&#36755;&#20986;&#30340;&#36830;&#36143;&#24615;&#21644;&#27969;&#30021;&#24615;&#12289;&#21487;&#20449;&#24230;&#12289;&#29983;&#25104;&#20869;&#23481;&#30340;&#25152;&#26377;&#26435;&#20197;&#21450;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#35834;&#26364;&#30340;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#20316;&#20026;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#20132;&#20114;&#35774;&#35745;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#36719;&#20214;&#25945;&#31243;&#21019;&#20316;&#30340;&#31034;&#20363;&#65292;&#35828;&#26126;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#20889;&#20316;&#20219;&#21153;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#35813;&#26694;&#26550;&#20316;&#20026;&#32508;&#21512;&#22522;&#20110;LLMS&#24037;&#20855;&#30340;&#20132;&#20114;&#35774;&#35745;&#30740;&#31350;&#30340;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#20123;&#21160;&#20316;&#38454;&#27573;&#30340;&#24037;&#20855;&#30340;&#31034;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#20154;-LLMS&#20132;&#20114;&#30740;&#31350;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability. In this position paper, we propose to adopt Norman's seven stages of action as a framework to approach the interaction design of intelligent writing assistants. We illustrate the framework's applicability to writing tasks by providing an example of software tutorial authoring. The paper also discusses the framework as a tool to synthesize research on the interaction design of LLM-based tools and presents examples of tools that support the stages of action. Finally, we briefly outline the potential of a framework for human-LLM interaction research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02012</link><description>&lt;p&gt;
EGC: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02012
&lt;/p&gt;
&lt;p&gt;
EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30456;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#22312;&#19968;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#39033;&#20219;&#21153;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGC&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#36755;&#20986;&#32473;&#23450;&#22270;&#20687;&#30340;&#26631;&#31614;&#65288;&#21363;&#26465;&#20214;&#20998;&#24067;$p(y|\mathbf{x})$&#65289;&#19981;&#21516;&#65292;EGC&#30340;&#21069;&#21521;&#20256;&#36882;&#22120;&#26159;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;$p(\mathbf{x},y)$&#65292;&#22312;&#21518;&#21521;&#20256;&#36882;&#22120;&#20013;&#36890;&#36807;&#36793;&#32536;&#21270;&#26631;&#31614;$y$&#23454;&#29616;&#29983;&#25104;&#22120;&#12290;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#20272;&#35745;&#32473;&#23450;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#37327;&#21644;&#20998;&#31867;&#27010;&#29575;&#65292;&#32780;&#22312;&#21518;&#21521;&#20256;&#36882;&#20013;&#65292;&#36890;&#36807;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;EGC&#22312;ImageNet-1k&#12289;CelebA-HQ&#21644;LSUN Church&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1k&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#26222;&#23572;&#37329;&#27663;&#22270;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#22411;&#35774;&#22791;&#65292;&#21487;&#29992;&#20110;&#21160;&#24577;&#27880;&#35270;&#21644;&#35843;&#33410;&#27979;&#37327;&#65292;&#39044;&#27979;&#35843;&#33410;&#21487;&#20197;&#31934;&#30830;&#21040;0.25D&#65292;&#27491;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.01296</link><description>&lt;p&gt;
&#21033;&#29992;&#26222;&#23572;&#37329;&#27663;&#22270;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#21160;&#24577;&#35843;&#33410;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms. (arXiv:2304.01296v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#26222;&#23572;&#37329;&#27663;&#22270;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#22411;&#35774;&#22791;&#65292;&#21487;&#29992;&#20110;&#21160;&#24577;&#27880;&#35270;&#21644;&#35843;&#33410;&#27979;&#37327;&#65292;&#39044;&#27979;&#35843;&#33410;&#21487;&#20197;&#31934;&#30830;&#21040;0.25D&#65292;&#27491;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;4&#20010;&#26222;&#23572;&#37329;&#27663;&#21453;&#23556;&#65288;PR&#65289;&#30340;&#21407;&#22411;&#35774;&#22791;&#65292;&#29992;&#20110;&#36866;&#29992;&#20110;AR&#21644;&#30524;&#31185;&#24212;&#29992;&#30340;&#21160;&#24577;&#27880;&#35270;&#21644;&#35843;&#33410;&#27979;&#37327;&#12290; PR1&#21644;2&#20197;&#21450;PR3&#21644;4&#20998;&#21035;&#29992;&#20110;&#20934;&#30830;&#27979;&#37327;&#20957;&#35270;&#21644;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#30524;&#30555;&#27169;&#22411;&#22312;ZEMAX&#20013;&#24320;&#21457;&#65292;&#24182;&#19982;&#23454;&#39564;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290; &#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#36229;&#36807;0.25D&#30340;&#31934;&#24230;&#20174;4&#24230;&#21040;1&#24230;&#39044;&#27979;&#35843;&#33410;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#37325;&#22797;&#24615;&#27979;&#35797;&#65292;&#24182;&#20174;&#21463;&#35797;&#32773;&#36523;&#19978;&#33719;&#24471;&#20102;&#20934;&#30830;&#30340;&#20957;&#35270;&#21644;&#35843;&#33410;&#20272;&#35745;&#12290;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#29289;&#29702;&#31934;&#30830;&#30340;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We developed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR) suitable for use in AR and ophthalmology applications. PR1&amp;2 and PR3&amp;4 are used for accurate gaze and accommodation measurements, respectively. Our eye model was developed in ZEMAX and matches the experiments well. Our model predicts the accommodation from 4 diopters to 1 diopter with better than 0.25D accuracy. We performed repeatability tests and obtained accurate gaze and accommodation estimations from subjects. We are generating a large synthetic data set using physically accurate models and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2303.17218</link><description>&lt;p&gt;
HARFLOW3D&#65306;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#24335;&#26550;&#26500;&#30340;&#24037;&#20855;&#38142;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#26144;&#23556;&#21040;FPGA&#19978;&#65292;&#32771;&#34385;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#21644;&#30446;&#26631;FPGA&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;HARFLOW3D&#24037;&#20855;&#38142;&#20197;ONNX&#26684;&#24335;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;FPGA&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#38142;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21253;&#25324;i) 3D CNN&#35299;&#26512;&#22120;&#65292;ii) &#24615;&#33021;&#21644;&#36164;&#28304;&#27169;&#22411;&#65292;iii) &#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#30828;&#20214;&#19978;&#25191;&#34892;3D&#27169;&#22411;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;iv) &#38024;&#23545;3D&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#36164;&#28304;&#24863;&#30693;&#20248;&#21270;&#24341;&#25806;&#65292;v) &#33258;&#21160;&#26144;&#23556;&#21040;&#21487;&#21512;&#25104;&#30340;FPGA&#20195;&#30721;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;3D CNN&#21644;FPGA&#31995;&#32479;&#37197;&#23545;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#24037;&#20855;&#38142;&#25903;&#25345;&#24191;&#27867;&#27169;&#22411;&#21644;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;3D CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#24037;&#20855;&#38142;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#26524;&#12289;&#26102;&#19981;&#21464;&#19988;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#30340;&#27969;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#34920;&#31034;&#27969;&#20989;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#24212;&#29992;&#20110;Van der Pol&#21644;FitzHugh Nagumo&#25391;&#33633;&#22120;&#30340;&#36712;&#36857;&#37325;&#29616;&#21644;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.16656</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27969;&#20989;&#25968;&#21450;&#20854;&#22312;&#38750;&#32447;&#24615;&#25391;&#33633;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Flow Functions from Data with Applications to Nonlinear Oscillators. (arXiv:2303.16656v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#26524;&#12289;&#26102;&#19981;&#21464;&#19988;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#30340;&#27969;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#34920;&#31034;&#27969;&#20989;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#24212;&#29992;&#20110;Van der Pol&#21644;FitzHugh Nagumo&#25391;&#33633;&#22120;&#30340;&#36712;&#36857;&#37325;&#29616;&#21644;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26550;&#26500;&#26469;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#12289;&#26102;&#19981;&#21464;&#19988;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#30340;&#27969;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#25511;&#21046;&#36755;&#20837;&#30340;&#31867;&#21035;&#38480;&#21046;&#20026;&#20998;&#27573;&#24120;&#25968;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#27969;&#20989;&#25968;&#31561;&#20215;&#20110;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#26144;&#23556;&#30340;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#19968;&#20010;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#26500;&#25104;&#30340;RNN&#65292;&#20197;&#23558;&#31995;&#32479;&#29366;&#24577;&#26144;&#23556;&#21040;RNN&#30340;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#23558;&#38544;&#34255;&#29366;&#24577;&#26144;&#23556;&#22238;&#31995;&#32479;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#22240;&#26524;&#24615;&#21644;&#26102;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#22320;&#34920;&#31034;&#27969;&#20989;&#25968;&#65292;&#19988;&#21487;&#20197;&#22312;&#20219;&#24847;&#26102;&#38388;&#26597;&#35810;&#23398;&#20064;&#21040;&#30340;&#27969;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;Van der Pol&#21644;FitzHugh Nagumo&#25391;&#33633;&#22120;&#30340;&#27169;&#22411;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#22320;&#37325;&#29616;&#36825;&#20004;&#20010;&#31995;&#32479;&#30340;&#36712;&#36857;&#12290;&#23545;&#20110;Van der Pol&#25391;&#33633;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#22914;&#20309;&#29992;&#20110;&#35745;&#31639;&#23558;&#31995;&#32479;&#24341;&#23548;&#21040;&#25152;&#38656;&#36712;&#36857;&#30340;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a recurrent neural network (RNN) based architecture to learn the flow function of a causal, time-invariant and continuous-time control system from trajectory data. By restricting the class of control inputs to piecewise constant functions, we show that learning the flow function is equivalent to learning the input-to-state map of a discrete-time dynamical system. This motivates the use of an RNN together with encoder and decoder networks which map the state of the system to the hidden state of the RNN and back. We show that the proposed architecture is able to approximate the flow function by exploiting the system's causality and time-invariance. The output of the learned flow function model can be queried at any time instant. We experimentally validate the proposed method using models of the Van der Pol and FitzHugh Nagumo oscillators. In both cases, the results demonstrate that the architecture is able to closely reproduce the trajectories of these two systems. For the Va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;&#36890;&#20449;&#21407;&#29702;&#30340;&#36890;&#20449;&#39640;&#25928;&#24322;&#27493;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PAO-Fed&#65289;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24322;&#26500;&#21644;&#24310;&#36831;&#35774;&#22791;&#65292;&#23454;&#29616;&#21442;&#19982;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.15226</link><description>&lt;p&gt;
&#24102;&#26377;&#38477;&#20302;&#36890;&#20449;&#35201;&#27714;&#30340;&#24322;&#27493;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Online Federated Learning with Reduced Communication Requirements. (arXiv:2303.15226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;&#36890;&#20449;&#21407;&#29702;&#30340;&#36890;&#20449;&#39640;&#25928;&#24322;&#27493;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PAO-Fed&#65289;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24322;&#26500;&#21644;&#24310;&#36831;&#35774;&#22791;&#65292;&#23454;&#29616;&#21442;&#19982;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#22320;&#29702;&#20998;&#24067;&#30340;&#35774;&#22791;&#21487;&#20197;&#20174;&#26412;&#22320;&#30340;&#27969;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#22312;&#32447;FL&#30340;&#25991;&#29486;&#37117;&#32771;&#34385;&#20102;&#26368;&#20339;&#24773;&#20917;&#19979;&#30340;&#21442;&#19982;&#23458;&#25143;&#31471;&#21644;&#36890;&#20449;&#28192;&#36947;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#12290;&#24322;&#27493;&#35774;&#32622;&#21487;&#20197;&#21453;&#26144;&#20986;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#65292;&#20363;&#22914;&#30001;&#20110;&#21487;&#29992;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#30005;&#27744;&#38480;&#21046;&#32780;&#21457;&#29983;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#20197;&#21450;&#30001;&#36890;&#20449;&#28192;&#36947;&#25110;&#33853;&#21518;&#35774;&#22791;&#24341;&#36215;&#30340;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#65292;&#24517;&#39035;&#32771;&#34385;&#33021;&#28304;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;&#36890;&#20449;&#21407;&#29702;&#30340;&#36890;&#20449;&#39640;&#25928;&#24322;&#27493;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PAO-Fed&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#19982;&#32773;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#21442;&#19982;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#24322;&#26500;&#21644;&#24310;&#36831;&#35774;&#22791;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online federated learning (FL) enables geographically distributed devices to learn a global shared model from locally available streaming data. Most online FL literature considers a best-case scenario regarding the participating clients and the communication channels. However, these assumptions are often not met in real-world applications. Asynchronous settings can reflect a more realistic environment, such as heterogeneous client participation due to available computational power and battery constraints, as well as delays caused by communication channels or straggler devices. Further, in most applications, energy efficiency must be taken into consideration. Using the principles of partial-sharing-based communications, we propose a communication-efficient asynchronous online federated learning (PAO-Fed) strategy. By reducing the communication overhead of the participants, the proposed method renders participation in the learning task more accessible and efficient. In addition, the prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#28431;&#27934;&#22914;&#20309;&#21462;&#20915;&#20110;&#21463;&#38480;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23376;&#31354;&#38388;&#32500;&#25968;&#65292;&#21516;&#26102;&#38024;&#23545;&#26631;&#20934;PGD&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#25104;&#21151;&#29575;&#25552;&#20986;&#20102;&#21333;&#35843;&#36882;&#22686;&#20989;&#25968;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14173</link><description>&lt;p&gt;
&#25214;&#21040;&#23545;&#25239;&#26679;&#26412;&#38656;&#35201;&#22810;&#23569;&#32500;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many dimensions are required to find an adversarial example?. (arXiv:2303.14173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#28431;&#27934;&#22914;&#20309;&#21462;&#20915;&#20110;&#21463;&#38480;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23376;&#31354;&#38388;&#32500;&#25968;&#65292;&#21516;&#26102;&#38024;&#23545;&#26631;&#20934;PGD&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#25104;&#21151;&#29575;&#25552;&#20986;&#20102;&#21333;&#35843;&#36882;&#22686;&#20989;&#25968;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#25506;&#32034;&#23545;&#25239;&#24615;&#28431;&#27934;&#30340;&#30740;&#31350;&#37117;&#30528;&#30524;&#20110;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#30340;&#25152;&#26377;&#32500;&#24230;&#30340;&#24773;&#20917;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#20197;&#19979;&#24773;&#20917;&#65306;&#65288;i&#65289;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;&#21442;&#25968;&#25110;&#65288;ii&#65289;&#22810;&#27169;&#24577;&#38382;&#39064;&#20013;&#30340;&#27169;&#24577;&#23376;&#38598;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#26377;&#25928;&#22320;&#21463;&#38480;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23376;&#31354;&#38388;$V$&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#28431;&#27934;&#22914;&#20309;&#21462;&#20915;&#20110;$V$&#30340;&#32500;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;PGD&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#25104;&#21151;&#29575;&#22914;&#20309;&#34920;&#29616;&#20026;$\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$&#30340;&#21333;&#35843;&#36882;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;$\epsilon$&#26159;&#25200;&#21160;&#39044;&#31639;&#65292;$\frac{1}{p}+\frac{q}{q}=1$&#65292;&#21482;&#35201;$p&gt;1$&#65288;&#24403;$p=1$&#26102;&#20250;&#20986;&#29616;&#39069;&#22806;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65289;&#12290;&#36825;&#20010;&#20989;&#25968;&#24418;&#24335;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work exploring adversarial vulnerability have focused on situations where an adversary can perturb all dimensions of model input. On the other hand, a range of recent works consider the case where either (i) an adversary can perturb a limited number of input parameters or (ii) a subset of modalities in a multimodal problem. In both of these cases, adversarial examples are effectively constrained to a subspace $V$ in the ambient input space $\mathcal{X}$. Motivated by this, in this work we investigate how adversarial vulnerability depends on $\dim(V)$. In particular, we show that the adversarial success of standard PGD attacks with $\ell^p$ norm constraints behaves like a monotonically increasing function of $\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$ where $\epsilon$ is the perturbation budget and $\frac{1}{p} + \frac{1}{q} =1$, provided $p &gt; 1$ (the case $p=1$ presents additional subtleties which we analyze in some detail). This functional form can be easily deriv
&lt;/p&gt;</description></item><item><title>ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12364</link><description>&lt;p&gt;
ExBEHRT&#65306;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#25193;&#23637;Transformer&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12364
&lt;/p&gt;
&lt;p&gt;
ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ExBEHRT&#65292;&#23427;&#26159;BEHRT&#65288;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;BERT&#65289;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#29305;&#24449;&#31354;&#38388;&#20174;&#20165;&#32771;&#34385;&#35786;&#26029;&#21644;&#24739;&#32773;&#24180;&#40836;&#25193;&#23637;&#21040;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#29305;&#24449;&#12289;&#29983;&#21629;&#20307;&#24449;&#12289;&#21560;&#28895;&#29366;&#24577;&#12289;&#35786;&#26029;&#12289;&#25163;&#26415;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#32479;&#19968;&#19981;&#21516;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20445;&#35777;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#26399;&#26799;&#24230;&#30340;&#25913;&#36827;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#20197;&#21069;&#26410;&#24212;&#29992;&#20110;&#23558;EHR&#25968;&#25454;&#19982;Transformer&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#65292;&#22914;&#29305;&#24449;&#21644;&#20196;&#29260;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32959;&#30244;&#23398;&#24739;&#32773;&#30340;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ExBEHRT&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992; ChatGPT &#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#28508;&#21147;&#65292;&#20294;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#25928;&#26524;&#27424;&#20339;&#65292;&#21516;&#26102;&#19978;&#20256;&#24739;&#32773;&#20449;&#24687;&#20063;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992; ChatGPT &#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.04360</link><description>&lt;p&gt;
LLM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#23545;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#26377;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04360
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992; ChatGPT &#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#28508;&#21147;&#65292;&#20294;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#25928;&#26524;&#27424;&#20339;&#65292;&#21516;&#26102;&#19978;&#20256;&#24739;&#32773;&#20449;&#24687;&#20063;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992; ChatGPT &#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#25512;&#21160;&#20102;&#35832;&#22914;OpenAI&#30340;ChatGPT&#20043;&#31867;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#38382;&#31572;&#12289;&#35770;&#25991;&#20889;&#20316;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;ChatGPT&#20174;&#38750;&#32467;&#26500;&#21270;&#20581;&#24247;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#25506;&#35752;ChatGPT&#22312;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#24341;&#21457;&#19982;&#23558;&#24739;&#32773;&#20449;&#24687;&#19978;&#20256;&#21040;ChatGPT API&#26377;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#24102;&#26631;&#31614;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#20108;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;Lur'e&#31995;&#32479;&#30340;&#20108;&#32500;&#29256;&#26412;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20351;&#29992;&#40065;&#26834;&#25511;&#21046;&#29702;&#35770;&#36827;&#34892;&#40065;&#20857;&#24120;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.03042</link><description>&lt;p&gt;
&#20316;&#20026;2D&#31995;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks as 2-D systems. (arXiv:2303.03042v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#20108;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;Lur'e&#31995;&#32479;&#30340;&#20108;&#32500;&#29256;&#26412;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20351;&#29992;&#40065;&#26834;&#25511;&#21046;&#29702;&#35770;&#36827;&#34892;&#40065;&#20857;&#24120;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;&#22312;&#20108;&#32500;&#21160;&#21147;&#31995;&#32479;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#21367;&#31215;&#26680;&#30340;&#21367;&#31215;&#23618;&#30340;&#24120;&#35268;&#25551;&#36848;&#65292;&#21363;&#32447;&#24615;&#28388;&#27874;&#22120;&#30340;&#33033;&#20914;&#21709;&#24212;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#32447;&#24615;&#26102;&#19981;&#21464;&#30340;&#20108;&#32500;&#31995;&#32479;&#12290;&#28982;&#21518;&#23558;&#30001;&#21367;&#31215;&#23618;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;Lur'e&#31995;&#32479;&#30340;&#20108;&#32500;&#29256;&#26412;&#65292;&#21363;&#19982;&#38745;&#24577;&#38750;&#32447;&#24615;&#32452;&#20214;&#30456;&#20114;&#36830;&#25509;&#30340;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;&#36825;&#31181;&#23545;CNN&#30340;&#20108;&#32500;Lur'e&#31995;&#32479;&#35270;&#35282;&#30340;&#22909;&#22788;&#26159;&#25105;&#20204;&#21487;&#20197;&#27604;&#20197;&#21069;&#26356;&#26377;&#25928;&#22320;&#20351;&#29992;&#40065;&#20857;&#24120;&#25968;&#20272;&#35745;&#30340;&#40065;&#26834;&#25511;&#21046;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel representation of convolutional Neural Networks (CNNs) in terms of 2-D dynamical systems. To this end, the usual description of convolutional layers with convolution kernels, i.e., the impulse responses of linear filters, is realized in state space as a linear time-invariant 2-D system. The overall convolutional Neural Network composed of convolutional layers and nonlinear activation functions is then viewed as a 2-D version of a Lur'e system, i.e., a linear dynamical system interconnected with static nonlinear components. One benefit of this 2-D Lur'e system perspective on CNNs is that we can use robust control theory much more efficiently for Lipschitz constant estimation than previously possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#35268;&#21017;&#29366;&#24577;&#35266;&#27979;&#21644;&#26410;&#30693;&#24310;&#36831;&#30340;&#36830;&#32493;&#26102;&#38388;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12604</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24310;&#36831;&#31995;&#32479;&#30340;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Laplace Control for Continuous-time Delayed Systems. (arXiv:2302.12604v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#35268;&#21017;&#29366;&#24577;&#35266;&#27979;&#21644;&#26410;&#30693;&#24310;&#36831;&#30340;&#36830;&#32493;&#26102;&#38388;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21253;&#25324;&#20855;&#26377;&#24310;&#36831;&#30340;&#36830;&#32493;&#26102;&#38388;&#29615;&#22659;&#12290;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#65306;&#39318;&#20808;&#65292;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;x(t)&#22312;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#36827;&#34892;&#35266;&#23519;&#65307;&#20854;&#27425;&#65292;&#24403;&#21069;&#34892;&#21160;a(t)&#20165;&#22312;&#26410;&#30693;&#24310;&#36831;g &gt; 0 &#30340;&#24773;&#20917;&#19979;&#24433;&#21709;&#26410;&#26469;&#29366;&#24577;x(t+g)&#12290;&#36825;&#26679;&#30340;&#29615;&#22659;&#30340;&#19968;&#20010;&#20856;&#22411;&#20363;&#23376;&#26159;&#21355;&#26143;&#25511;&#21046;&#65292;&#20854;&#20013;&#22320;&#29699;&#21644;&#21355;&#26143;&#20043;&#38388;&#30340;&#36890;&#20449;&#38142;&#36335;&#20250;&#36896;&#25104;&#35266;&#27979;&#19981;&#35268;&#21017;&#21644;&#24310;&#36831;&#12290;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#35266;&#27979;&#25110;&#24050;&#30693;&#24310;&#36831;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#28041;&#21450;&#26102;&#38388;&#19981;&#35268;&#21017;&#35266;&#27979;&#21644;&#26410;&#30693;&#24310;&#36831;&#30340;&#29615;&#22659;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;&#65292;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#21160;&#21147;&#23398;&#27169;&#22411;&#19982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#24182;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g &gt; 0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sam
&lt;/p&gt;</description></item><item><title>&#20462;&#27491;&#30340;&#26465;&#20214;t-SNE&#31639;&#27861;&#36890;&#36807;&#23545;&#39640;&#32500;&#30456;&#20284;&#24230;&#36827;&#34892;&#26465;&#20214;&#38480;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#32463;&#36807;&#26631;&#31614;&#33391;&#22909;&#32858;&#31867;&#26102;&#65292;&#26465;&#20214;t-SNE&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#24182;&#36890;&#36807;&#23545;&#30456;&#20284;&#24230;&#30697;&#38453;&#23384;&#20648;&#30340;&#25913;&#21464;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03493</link><description>&lt;p&gt;
&#20462;&#27491;&#30340;&#26465;&#20214;t-SNE&#31639;&#27861;&#65306;&#36229;&#36234;&#26368;&#36817;&#37051;&#36817;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Revised Conditional t-SNE: Looking Beyond the Nearest Neighbors. (arXiv:2302.03493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03493
&lt;/p&gt;
&lt;p&gt;
&#20462;&#27491;&#30340;&#26465;&#20214;t-SNE&#31639;&#27861;&#36890;&#36807;&#23545;&#39640;&#32500;&#30456;&#20284;&#24230;&#36827;&#34892;&#26465;&#20214;&#38480;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#32463;&#36807;&#26631;&#31614;&#33391;&#22909;&#32858;&#31867;&#26102;&#65292;&#26465;&#20214;t-SNE&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#24182;&#36890;&#36807;&#23545;&#30456;&#20284;&#24230;&#30697;&#38453;&#23384;&#20648;&#30340;&#25913;&#21464;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;t-SNE&#65288;ct-SNE&#65289;&#26159;t-SNE&#30340;&#26368;&#26032;&#25193;&#23637;&#65292;&#23427;&#20801;&#35768;&#20174;&#23884;&#20837;&#20013;&#21024;&#38500;&#24050;&#30693;&#30340;&#32858;&#31867;&#20449;&#24687;&#65292;&#20197;&#33719;&#24471;&#25581;&#31034;&#26631;&#31614;&#20449;&#24687;&#20043;&#22806;&#30340;&#32467;&#26500;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#12290;&#36825;&#22312;&#38656;&#35201;&#28040;&#38500;&#19968;&#32452;&#31867;&#20043;&#38388;&#19981;&#24819;&#35201;&#30340;&#24046;&#24322;&#26102;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;ct-SNE&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#24403;&#25968;&#25454;&#22312;&#21407;&#22987;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#25353;&#26631;&#31614;&#36827;&#34892;&#33391;&#22909;&#30340;&#32858;&#31867;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39640;&#32500;&#30456;&#20284;&#24230;&#36827;&#34892;&#26465;&#20214;&#38480;&#21046;&#65292;&#24182;&#23558;&#22522;&#20110;&#26631;&#31614;&#30340;&#26368;&#36817;&#37051;&#21644;&#36328;&#26631;&#31614;&#30340;&#26368;&#36817;&#37051;&#23384;&#20648;&#22312;&#19981;&#21516;&#30340;&#30697;&#38453;&#20013;&#12290;&#36825;&#36824;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;t-SNE&#21152;&#36895;&#25216;&#26415;&#65292;&#25552;&#39640;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20174;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32771;&#34385;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#23884;&#20837;&#36136;&#37327;&#12290;&#20294;&#26159;&#22312;&#21253;&#21547;&#25209;&#27425;&#25928;&#24212;&#30340;&#23454;&#38469;&#25968;&#25454;&#20013;&#65292;&#39044;&#26399;&#30340;&#25913;&#36827;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#25105;&#20204;&#35748;&#20026;&#32463;&#36807;&#20462;&#35746;&#30340;ct-SNE&#36739;&#21407;&#22987;&#30340;&#31639;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional t-SNE (ct-SNE) is a recent extension to t-SNE that allows removal of known cluster information from the embedding, to obtain a visualization revealing structure beyond label information. This is useful, for example, when one wants to factor out unwanted differences between a set of classes. We show that ct-SNE fails in many realistic settings, namely if the data is well clustered over the labels in the original high-dimensional space. We introduce a revised method by conditioning the high-dimensional similarities instead of the low-dimensional similarities and storing within- and across-label nearest neighbors separately. This also enables the use of recently proposed speedups for t-SNE, improving the scalability. From experiments on synthetic data, we find that our proposed method resolves the considered problems and improves the embedding quality. On real data containing batch effects, the expected improvement is not always there. We argue revised ct-SNE is preferable ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#27969;&#27169;&#22411;&#8212;&#8212;&#20027;&#35201;&#27969;&#65288;PF&#65289;&#65292;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#29305;&#24449;&#20998;&#35299;&#30340;&#20381;&#36182;&#65292;&#25429;&#25417;&#21040;&#20102;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#21457;&#25955;&#21644;&#25391;&#33633;&#34892;&#20026;&#12289;&#36867;&#36920;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#38797;&#28857;&#30340;&#36830;&#32493;&#24615;&#27969;&#65292;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#26032;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27979;&#35797;&#38598;&#35780;&#20272;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.01952</link><description>&lt;p&gt;
&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#21644;&#28145;&#24230;&#23398;&#20064;&#19981;&#31283;&#23450;&#24615;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#27969;&#27169;&#22411;&#8212;&#8212;&#20027;&#35201;&#27969;&#65288;PF&#65289;&#65292;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#29305;&#24449;&#20998;&#35299;&#30340;&#20381;&#36182;&#65292;&#25429;&#25417;&#21040;&#20102;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#21457;&#25955;&#21644;&#25391;&#33633;&#34892;&#20026;&#12289;&#36867;&#36920;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#38797;&#28857;&#30340;&#36830;&#32493;&#24615;&#27969;&#65292;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#26032;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27979;&#35797;&#38598;&#35780;&#20272;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#31192;&#35776;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#30340;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#26799;&#24230;&#19979;&#38477;&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#20854;&#19981;&#31283;&#23450;&#24615;&#65292;&#33853;&#21518;&#20110;&#20854;&#32463;&#39564;&#25104;&#21151;&#12290;&#20026;&#20102;&#22686;&#21152;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#35201;&#27969;&#65288;PF&#65289;&#65292;&#19968;&#31181;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;PF&#26159;&#21807;&#19968;&#25429;&#25417;&#21040;&#26799;&#24230;&#19979;&#38477;&#30340;&#21457;&#25955;&#21644;&#25391;&#33633;&#34892;&#20026;&#65292;&#21253;&#25324;&#36867;&#36920;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#38797;&#28857;&#30340;&#36830;&#32493;&#24615;&#27969;&#12290;&#36890;&#36807;&#20854;&#23545;&#20110;Hessian&#29305;&#24449;&#20998;&#35299;&#30340;&#20381;&#36182;&#65292;PF&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#26032;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27979;&#35797;&#38598;&#35780;&#20272;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2301.13306</link><description>&lt;p&gt;
&#24102;&#26377;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#30340;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#65306;&#25928;&#29575;&#12289;&#21518;&#24724;&#21644;&#33410;&#22863;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics. (arXiv:2301.13306v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#22312;&#22312;&#32447;&#24191;&#21578;&#24179;&#21488;&#19978;&#36827;&#34892;&#21338;&#24328;&#30340;&#24773;&#20917;&#12290;&#27599;&#20010;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#34987;&#36171;&#20104;&#20219;&#21153;&#65292;&#22312;&#22810;&#36718;&#37325;&#22797;&#25293;&#21334;&#20013;&#65292;&#26368;&#22823;&#21270;&#20854;&#24191;&#21578;&#20027;&#30340;&#24635;&#20215;&#20540;&#65292;&#21516;&#26102;&#21463;&#21040;&#39044;&#31639;&#21644;/&#25110;&#25237;&#36164;&#22238;&#25253;&#29575;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#36798;&#21040;&#36880;&#28176;&#20943;&#23567;&#30340;&#20010;&#20307;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#33258;&#21161;&#21453;&#39304;&#65292;&#24182;&#21487;&#19982;&#31532;&#19968;&#25110;&#31532;&#20108;&#20215;&#26684;&#25293;&#21334;&#20197;&#21450;&#20219;&#20309;&#8220;&#20013;&#38388;&#8221;&#25293;&#21334;&#26684;&#24335;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24403;&#36825;&#20123;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#30456;&#20114;&#31454;&#20105;&#26102;&#65292;&#25152;&#26377;&#36718;&#27425;&#30340;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160; welfare &#37117;&#33267;&#23569;&#36798;&#21040;&#20102;&#20219;&#20309;&#20998;&#37197;&#25152;&#23454;&#29616;&#30340;&#26399;&#26395;&#26368;&#20248;&#27969;&#21160; welfare &#30340;&#19968;&#21322;&#12290;&#36825;&#22312;&#20986;&#20215;&#21160;&#24577;&#26159;&#21542;&#25910;&#25947;&#21040;&#22343;&#34913;&#20197;&#21450;&#24191;&#21578;&#20027;&#20272;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#22914;&#20309;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#22343;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any "intermediate" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.13096</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#38170;&#28857;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#12290;LAAT&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#22266;&#23450;&#30340;&#38170;&#28857;&#65288;&#24402;&#19968;&#21270;&#29305;&#24449;&#23884;&#20837;&#65289;&#65292;&#24182;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#36825;&#20123;&#38170;&#28857;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;LAAT&#21487;&#20197;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#22312;&#26032;&#31867;&#21035;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26679;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#23427;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAAT&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65288;&#22914;ResNet-50&#21644;DenseNet-121&#65289;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#29992;&#20110;&#36229;&#22768;&#25104;&#20687;&#65292;&#35813;&#34920;&#31034;&#21487;&#20197;&#20174;&#37325;&#21472;&#30340;&#36229;&#22768;&#25195;&#25551;&#20013;&#23398;&#20064;&#32452;&#32455;&#23646;&#24615;&#12290;&#21033;&#29992;&#22522;&#20110;&#20809;&#32447;&#36861;&#36394;&#30340;&#31070;&#32463;&#28210;&#26579;&#36827;&#34892;&#26032;&#35270;&#22270;&#30340;&#36229;&#22768;&#21512;&#25104;&#65292;&#23454;&#29616;&#20960;&#20309;&#31934;&#30830;&#30340;B&#27169;&#24335;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.10520</link><description>&lt;p&gt;
&#36229;&#32423;NeRF&#65306;&#29992;&#20110;&#36229;&#22768;&#25104;&#20687;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging. (arXiv:2301.10520v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#29992;&#20110;&#36229;&#22768;&#25104;&#20687;&#65292;&#35813;&#34920;&#31034;&#21487;&#20197;&#20174;&#37325;&#21472;&#30340;&#36229;&#22768;&#25195;&#25551;&#20013;&#23398;&#20064;&#32452;&#32455;&#23646;&#24615;&#12290;&#21033;&#29992;&#22522;&#20110;&#20809;&#32447;&#36861;&#36394;&#30340;&#31070;&#32463;&#28210;&#26579;&#36827;&#34892;&#26032;&#35270;&#22270;&#30340;&#36229;&#22768;&#21512;&#25104;&#65292;&#23454;&#29616;&#20960;&#20309;&#31934;&#30830;&#30340;B&#27169;&#24335;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#29992;&#20110;&#20174;&#37325;&#21472;&#30340;&#36229;&#22768;&#25195;&#25551;&#20013;&#23398;&#20064;&#32452;&#32455;&#23646;&#24615;&#30340;&#36229;&#22768;&#25104;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#20809;&#32447;&#36861;&#36394;&#30340;&#31070;&#32463;&#28210;&#26579;&#36827;&#34892;&#26032;&#35270;&#22270;&#30340;&#36229;&#22768;&#21512;&#25104;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;INR&#27169;&#22411;&#21487;&#20197;&#20174;&#19968;&#32452;&#20108;&#32500;&#36229;&#22768;&#26694;&#26550;&#20013;&#32534;&#30721;&#19977;&#32500;&#22330;&#26223;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#32771;&#34385;&#21040;&#36229;&#22768;&#25104;&#20687;&#22266;&#26377;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#35270;&#35282;&#30456;&#20851;&#21464;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22330;&#26223;&#20013;&#26041;&#21521;&#30456;&#20851;&#30340;&#21464;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#29289;&#29702;&#21551;&#21457;&#24335;&#28210;&#26579;&#22914;&#20309;&#25552;&#39640;&#36229;&#22768;&#22270;&#20687;&#21512;&#25104;&#30340;&#20445;&#30495;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20026;&#20855;&#26377;&#27169;&#31946;&#34920;&#31034;&#30340;&#21306;&#22495;&#29983;&#25104;&#20960;&#20309;&#31934;&#30830;&#30340;B&#27169;&#24335;&#22270;&#20687;&#65292;&#36825;&#26159;&#30001;&#20110;&#36229;&#22768;&#22270;&#20687;&#30340;&#35270;&#35282;&#30456;&#20851;&#24046;&#24322;&#25152;&#33268;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#32925;&#33039;B&#27169;&#24335;&#36229;&#22768;&#25195;&#25551;&#21644;&#33034;&#26609;&#24187;&#24433;&#36229;&#22768;&#25195;&#25551;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a physics-enhanced implicit neural representation (INR) for ultrasound (US) imaging that learns tissue properties from overlapping US sweeps. Our proposed method leverages a ray-tracing-based neural rendering for novel view US synthesis. Recent publications demonstrated that INR models could encode a representation of a three-dimensional scene from a set of two-dimensional US frames. However, these models fail to consider the view-dependent changes in appearance and geometry intrinsic to US imaging. In our work, we discuss direction-dependent changes in the scene and show that a physics-inspired rendering improves the fidelity of US image synthesis. In particular, we demonstrate experimentally that our proposed method generates geometrically accurate B-mode images for regions with ambiguous representation owing to view-dependent differences of the US images. We conduct our experiments using simulated B-mode US sweeps of the liver and acquired US sweeps of a spine phantom tra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#27493;HFL&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#12289;&#23618;&#27425;&#21270;&#30340;IoT&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#24322;&#27493;&#32858;&#21512;&#26469;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#65292;&#24182;&#22312;&#32593;&#20851;&#21644;&#20113;&#32423;&#21035;&#19978;&#37319;&#29992;&#35774;&#22791;&#36873;&#25321;&#21644;&#35774;&#22791;&#32593;&#20851;&#35843;&#24230;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.06646</link><description>&lt;p&gt;
&#24322;&#27493;HFL&#65306;&#22312;&#20998;&#23618;IoT&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#12289;&#40065;&#26834;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064; (arXiv:2301.06646v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks. (arXiv:2301.06646v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06646
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#27493;HFL&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#12289;&#23618;&#27425;&#21270;&#30340;IoT&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#24322;&#27493;&#32858;&#21512;&#26469;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#65292;&#24182;&#22312;&#32593;&#20851;&#21644;&#20113;&#32423;&#21035;&#19978;&#37319;&#29992;&#35774;&#22791;&#36873;&#25321;&#21644;&#35774;&#22791;&#32593;&#20851;&#35843;&#24230;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#35774;&#22791;&#23398;&#20064;&#33539;&#24335;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#30495;&#23454;&#29289;&#32852;&#32593; (IoT) &#32593;&#32476;&#20013;&#37096;&#32626;FL&#20173;&#28982;&#38754;&#20020;&#30528;&#22810;&#37325;&#25361;&#25112;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#31995;&#32479;&#24322;&#36136;&#24615;&#12289;&#24847;&#22806;&#24310;&#36831;&#32773;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#38382;&#39064;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#25552;&#20379;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#20998;&#23618;&#21644;&#19981;&#21487;&#38752;&#30340;IoT&#32593;&#32476;&#20013;&#30340;&#25152;&#26377;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24322;&#27493;&#21644;&#20998;&#23618;&#26694;&#26550; (Async-HFL)&#65292;&#29992;&#20110;&#22312;&#24120;&#35265;&#30340;&#19977;&#23618;IoT&#32593;&#32476;&#32467;&#26500;&#20013;&#25191;&#34892;FL&#12290;&#38024;&#23545;&#22823;&#37327;&#19981;&#21516;&#30340;&#24310;&#36831;&#65292;Async-HFL&#22312;&#32593;&#20851;&#21644;&#20113;&#32423;&#21035;&#22343;&#37319;&#29992;&#24322;&#27493;&#32858;&#21512;&#65292;&#20174;&#32780;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#22312;&#31995;&#32479;&#24322;&#36136;&#24615;&#21644;&#24310;&#36831;&#32773;&#19979;&#24322;&#27493;HFL&#30340;&#25910;&#25947;&#36895;&#24230;&#28508;&#21147;&#65292;&#25105;&#20204;&#20998;&#21035;&#35774;&#35745;&#20102;&#32593;&#20851;&#32423;&#21035;&#30340;&#35774;&#22791;&#36873;&#25321;&#21644;&#20113;&#32423;&#21035;&#30340;&#35774;&#22791;&#32593;&#20851;&#35843;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Async-HFL&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has gained increasing interest in recent years as a distributed on-device learning paradigm. However, multiple challenges remain to be addressed for deploying FL in real-world Internet-of-Things (IoT) networks with hierarchies. Although existing works have proposed various approaches to account data heterogeneity, system heterogeneity, unexpected stragglers and scalibility, none of them provides a systematic solution to address all of the challenges in a hierarchical and unreliable IoT network. In this paper, we propose an asynchronous and hierarchical framework (Async-HFL) for performing FL in a common three-tier IoT network architecture. In response to the largely varied delays, Async-HFL employs asynchronous aggregations at both the gateway and the cloud levels thus avoids long waiting time. To fully unleash the potential of Async-HFL in converging speed under system heterogeneities and stragglers, we design device selection at the gateway level and device-ga
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#20998;&#31867;&#22120;&#25512;&#36831;&#39044;&#27979;&#24182;&#20132;&#32473;&#20154;&#31867;&#20915;&#31574;&#32773;&#20316;&#20986;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#22312;&#21487;&#23454;&#29616;&#30340;&#35774;&#32622;&#19979;&#65292;&#33719;&#24471;&#20302;&#35823;&#24046;&#30340;&#32447;&#24615;&#32452;&#21512;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20844;&#24335;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2301.06197</link><description>&lt;p&gt;
&#35841;&#24212;&#35813;&#36827;&#34892;&#39044;&#27979;&#65311;&#23398;&#20064;&#25512;&#36831;&#35753;&#20154;&#31867;&#20915;&#31574;&#30340;&#31934;&#30830;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Should Predict? Exact Algorithms For Learning to Defer to Humans. (arXiv:2301.06197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#20998;&#31867;&#22120;&#25512;&#36831;&#39044;&#27979;&#24182;&#20132;&#32473;&#20154;&#31867;&#20915;&#31574;&#32773;&#20316;&#20986;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#22312;&#21487;&#23454;&#29616;&#30340;&#35774;&#32622;&#19979;&#65292;&#33719;&#24471;&#20302;&#35823;&#24046;&#30340;&#32447;&#24615;&#32452;&#21512;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20844;&#24335;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;AI&#20998;&#31867;&#22120;&#24212;&#35813;&#33021;&#22815;&#25512;&#36831;&#39044;&#27979;&#65292;&#35753;&#20154;&#31867;&#20915;&#31574;&#32773;&#20570;&#20986;&#20915;&#31574;&#26469;&#30830;&#20445;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#19968;&#20010;&#25298;&#32477;&#22120;&#65292;&#25298;&#32477;&#22120;&#20250;&#22312;&#27599;&#20010;&#25968;&#25454;&#28857;&#19978;&#20915;&#23450;&#20998;&#31867;&#22120;&#25110;&#20154;&#31867;&#24212;&#35813;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#20302;&#35823;&#24046;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#65292;&#21363;&#20351;&#23384;&#22312;&#20855;&#26377;&#38646;&#35823;&#24046;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#25298;&#32477;&#22120;&#65288;&#21487;&#23454;&#29616;&#30340;&#35774;&#32622;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#38382;&#39064;&#21487;&#23454;&#29616;&#65292;&#33719;&#24471;&#20302;&#35823;&#24046;&#30340;&#32447;&#24615;&#32452;&#21512;&#20063;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#20026;&#20102;&#34917;&#20805;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#20844;&#24335;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#26368;&#20248;&#22320;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;MILP&#21482;&#36866;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#26159;&#21487;&#23454;&#29616;&#19968;&#33268;&#30340;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#24191;&#27867;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated AI classifiers should be able to defer the prediction to a human decision maker to ensure more accurate predictions. In this work, we jointly train a classifier with a rejector, which decides on each data point whether the classifier or the human should predict. We show that prior approaches can fail to find a human-AI system with low misclassification error even when there exists a linear classifier and rejector that have zero error (the realizable setting). We prove that obtaining a linear pair with low error is NP-hard even when the problem is realizable. To complement this negative result, we give a mixed-integer-linear-programming (MILP) formulation that can optimally solve the problem in the linear setting. However, the MILP only scales to moderately-sized problems. Therefore, we provide a novel surrogate loss function that is realizable-consistent and performs well empirically. We test our approaches on a comprehensive set of datasets and compare to a wide range of bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65288;MIHO&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;MIHO&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#65292;&#21442;&#25968;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36710;&#36742;&#22312;&#22330;&#22320;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#21644;&#31283;&#23450;&#36991;&#38556;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01470</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#23454;&#29616;&#33258;&#20027;&#36187;&#36710;&#31995;&#32479;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems. (arXiv:2301.01470v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65288;MIHO&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;MIHO&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#65292;&#21442;&#25968;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36710;&#36742;&#22312;&#22330;&#22320;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#21644;&#31283;&#23450;&#36991;&#38556;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#65288;MIHO&#65289;&#23454;&#29616;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26469;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#35782;&#21035;&#21160;&#24577;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;MIHO&#36827;&#34892;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#20248;&#21270;&#21518;&#30340;&#21442;&#25968;&#34701;&#20837;&#25105;&#20204;&#24179;&#21488;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#35774;&#35745;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MIHO&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;MIHO&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22330;&#22320;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#27169;&#22411;&#30340;&#31995;&#32479;&#20855;&#26377;&#31283;&#23450;&#30340;&#36991;&#38556;&#24615;&#33021;&#21644;&#39640;&#36895;&#34892;&#39542;&#24615;&#33021;&#65292;&#22312;&#21360;&#31532;&#23433;&#32435;&#27874;&#21033;&#26031;&#36710;&#36895;&#20844;&#22253;&#21644;&#25289;&#26031;&#32500;&#21152;&#26031;&#36710;&#36895;&#20844;&#22253;&#30340;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we propose a model parameter identification method via a hyperparameter optimization scheme (MIHO). Our method adopts an efficient explore-exploit strategy to identify the parameters of dynamic models in a data-driven optimization manner. We utilize MIHO for model parameter identification of the AV-21, a full-scaled autonomous race vehicle. We then incorporate the optimized parameters for the design of model-based planning and control systems of our platform. In experiments, MIHO exhibits more than 13 times faster convergence than traditional parameter identification methods. Furthermore, the parametric models learned via MIHO demonstrate good fitness to the given datasets and show generalization ability in unseen dynamic scenarios. We further conduct extensive field tests to validate our model-based system, demonstrating stable obstacle avoidance and high-speed driving up to 217 km/h at the Indianapolis Motor Speedway and Las Vegas Motor Speedway. The source code for M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11870</link><description>&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#35768;&#22810;&#21487;&#20135;&#29983;&#21512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#35813;&#39046;&#22495;&#20063;&#32463;&#39564;&#24615;&#22320;&#30475;&#21040;&#20102;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#23545;&#20110;&#23454;&#36341;&#32773;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24182;&#22312;&#23427;&#20204;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65288;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#28385;&#36275;&#65289;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#21644;SHAP&#65289;&#21487;&#20197;&#34987;&#35777;&#26126;&#23545;&#20110;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#37117;&#26080;&#27861;&#32988;&#20219;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24120;&#35265;&#30340;&#26368;&#32456;&#20219;&#21153;&#65292;&#22914;&#25551;&#36848;&#23616;&#37096;&#27169;&#22411;&#34892;&#20026;&#12289;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#21644;&#31639;&#27861;&#22238;&#28335;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#21551;&#31034;&#26159;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65306;&#19968;&#26086;&#36825;&#26679;&#30340;&#26368;&#32456;&#20219;&#21153;&#34987;&#23450;&#20041;&#65292;&#19968;&#20010;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#8212;&#8212;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;&#25968;&#25454;&#34892;&#20026;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#27169;&#22411;&#25512;&#26029;&#31574;&#30053;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#23548;&#33268;&#30340;&#34892;&#20026;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2211.16078</link><description>&lt;p&gt;
&#22810;&#28304;&#25968;&#25454;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34892;&#20026;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning. (arXiv:2211.16078v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;&#25968;&#25454;&#34892;&#20026;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#27169;&#22411;&#25512;&#26029;&#31574;&#30053;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#23548;&#33268;&#30340;&#34892;&#20026;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30001;&#20110;&#25968;&#25454;&#25928;&#29575;&#39640;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#34892;&#20026;&#20272;&#35745;&#65292;&#36825;&#26159;&#35768;&#22810;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#20219;&#21153;&#12290;&#34892;&#20026;&#20272;&#35745;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#22810;&#20010;&#26469;&#28304;&#25910;&#38598;&#25968;&#25454;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24573;&#30053;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#29616;&#26377;&#30340;&#34892;&#20026;&#20272;&#35745;&#26041;&#27861;&#20250;&#20986;&#29616;&#34892;&#20026;&#38169;&#35823;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#32570;&#38519;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#19968;&#32452;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#26368;&#33021;&#25551;&#36848;&#29305;&#23450;&#36712;&#36857;&#30340;&#31574;&#30053;&#20316;&#20026;&#34892;&#20026;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#20026;&#22810;&#28304;&#25968;&#25454;&#25552;&#20379;&#20102;&#20195;&#29702;&#30340;&#31934;&#32454;&#21270;&#25551;&#36848;&#65292;&#24182;&#24110;&#21161;&#23427;&#20811;&#26381;&#34892;&#20026;&#38169;&#35823;&#12290;&#26412;&#25991;&#36824;&#20026;&#35813;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Soft Actor-Critic&#65288;SAC&#65289;&#26469;&#22788;&#29702;&#22810;&#28304;&#25968;&#25454;&#65292;&#35828;&#26126;&#20102;&#23427;&#30340;&#23454;&#38469;&#29992;&#36884;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that lays the foundation of many offline RL algorithms. Behavior estimation aims at estimating the policy with which training data are generated. In particular, this work considers a scenario where the data are collected from multiple sources. In this case, neglecting data heterogeneity, existing approaches for behavior estimation suffers from behavior misspecification. To overcome this drawback, the present study proposes a latent variable model to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. This model provides with a agent fine-grained characterization for multi-source data and helps it overcome behavior misspecification. This work also proposes a learning algorithm for this model and illustrates its practical usage via extending an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#26041;&#27861;EMQ&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#36880;&#27493;&#20559;&#31163;&#39640;&#26031;&#20998;&#24067;&#24182;&#22312;&#25552;&#21319;&#20013;&#21457;&#29616;&#26368;&#20248;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14545</link><description>&lt;p&gt;
&#38598;&#25104;&#22810;&#20998;&#20301;&#25968;&#31639;&#27861;&#65306;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification. (arXiv:2211.14545v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#26041;&#27861;EMQ&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#36880;&#27493;&#20559;&#31163;&#39640;&#26031;&#20998;&#24067;&#24182;&#22312;&#25552;&#21319;&#20013;&#21457;&#29616;&#26368;&#20248;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#28789;&#27963;&#30340;&#20998;&#24067;&#39044;&#27979;&#65292;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#26465;&#20214;&#20998;&#24067;$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#27010;&#29575;&#27700;&#24179;&#30340;&#20998;&#20301;&#25968;&#65288;&#35206;&#30422;&#21306;&#38388;$(0,1)$&#65289;&#29992;&#30001;&#25105;&#20204;&#35774;&#35745;&#30340;&#21152;&#27861;&#27169;&#22411;&#25552;&#21319;&#65292;&#26469;&#39044;&#27979;&#36825;&#20010;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#23547;&#27714;$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$&#30340;&#32467;&#26500;&#23436;&#25972;&#24615;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;&#65292;&#32780;&#39640;&#26031;&#20551;&#35774;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#39640;&#24230;&#28789;&#27963;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#27809;&#26377;&#20998;&#24067;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#20272;&#35745;&#20998;&#20301;&#25968;&#65289;&#19981;&#21487;&#36991;&#20813;&#22320;&#20855;&#26377;&#32570;&#38519;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#24456;&#22909;&#22320;&#27010;&#25324;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#38598;&#25104;&#22810;&#20998;&#20301;&#25968;&#26041;&#27861;EMQ&#23436;&#20840;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#21487;&#20197;&#36880;&#27493;&#20559;&#31163;&#39640;&#26031;&#20998;&#24067;&#24182;&#22312;&#25552;&#21319;&#20013;&#21457;&#29616;&#26368;&#20248;&#26465;&#20214;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel, succinct, and effective approach to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$ in regression tasks. For predicting this conditional distribution, its quantiles of probability levels spreading the interval $(0,1)$ are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$, while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On ex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;HypAD&#65292;&#23427;&#37319;&#29992;&#36229;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26469;&#35780;&#20272;&#24322;&#24120;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#37325;&#26032;&#24314;&#31435;&#36755;&#20837;&#20449;&#21495;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.09224</link><description>&lt;p&gt;
&#8220;&#25105;&#20204;&#26159;&#21542;&#30830;&#20449;&#36825;&#26159;&#24322;&#24120;&#65311;&#65288;arXiv:2211.09224v3 [cs.LG] UPDATED&#65289;&#8221;
&lt;/p&gt;
&lt;p&gt;
Are we certain it's anomalous?. (arXiv:2211.09224v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;HypAD&#65292;&#23427;&#37319;&#29992;&#36229;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26469;&#35780;&#20272;&#24322;&#24120;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#37325;&#26032;&#24314;&#31435;&#36755;&#20837;&#20449;&#21495;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#24207;&#21015;&#30340;&#24314;&#27169;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#22823;&#22823;&#25512;&#36827;&#20102;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#35813;&#20219;&#21153;&#29992;&#20110;&#35782;&#21035;&#37329;&#34701;&#24207;&#21015;&#12289;IT&#31995;&#32479;&#12289;&#33322;&#22825;&#27979;&#37327;&#20197;&#21450;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#26816;&#27979;&#21487;&#20197;&#24110;&#21161;&#38548;&#31163;&#20986;&#25233;&#37057;&#30151;&#21644;&#32769;&#24180;&#20154;&#30340;&#29305;&#27530;&#30149;&#20363;&#12290;&#30001;&#20110;&#38750;&#24120;&#19981;&#30830;&#23450;&#24615;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#24322;&#24120;&#30340;&#23450;&#20041;&#26377;&#26102;&#26159;&#20027;&#35266;&#30340;&#65292;&#25152;&#20197;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#8220;&#36229;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#8221;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65288;HypAD&#65289;&#12290;HypAD&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#37325;&#24314;&#36755;&#20837;&#20449;&#21495;&#12290;&#25105;&#20204;&#37319;&#29992;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;LSTM&#24314;&#31435;&#24207;&#21015;&#65292;&#21516;&#26102;&#23398;&#20064;&#35299;&#30721;&#22120;&#26469;&#37325;&#24314;&#20449;&#21495;&#65292;&#24182;&#20511;&#21161;GAN&#35780;&#35770;&#23478;&#30340;&#24110;&#21161;&#12290;&#21033;&#29992;&#36229;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#65292;HypAD&#21487;&#20197;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in modelling time series and, more generally, sequences of structured data has recently revamped research in anomaly detection. The task stands for identifying abnormal behaviors in financial series, IT systems, aerospace measurements, and the medical domain, where anomaly detection may aid in isolating cases of depression and attend the elderly. Anomaly detection in time series is a complex task since anomalies are rare due to highly non-linear temporal correlations and since the definition of anomalous is sometimes subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input signal. We adopt best practices from the state-of-the-art to encode the sequence by an LSTM, jointly learned with a decoder to reconstruct the signal, with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a hyperbolic neural network. By using uncertainty, HypAD may assess whether it is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#33021;&#21147;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2211.04339</link><description>&lt;p&gt;
&#36808;&#21521;&#33258;&#36866;&#24212;&#35821;&#20041;&#36890;&#20449;&#65306;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#28304;&#36890;&#36947;&#32534;&#30721;&#30340;&#39640;&#25928;&#25968;&#25454;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Toward Adaptive Semantic Communications: Efficient Data Transmission via Online Learned Nonlinear Transform Source-Channel Coding. (arXiv:2211.04339v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#33021;&#21147;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#35821;&#20041;&#36890;&#20449;&#39046;&#22495;&#25512;&#21160;&#20102;&#31471;&#21040;&#31471;&#25968;&#25454;&#20256;&#36755;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#65292;&#23398;&#20064;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#26696;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#30340;&#28304;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#30740;&#31350;&#37325;&#28857;&#20027;&#35201;&#38598;&#20013;&#22312;&#26550;&#26500;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#65292;&#26397;&#21521;&#38745;&#24577;&#30446;&#26631;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#30340;&#38480;&#21046;&#21644;&#19981;&#23436;&#32654;&#30340;&#20248;&#21270;&#21644;&#25512;&#24191;&#65292;&#29305;&#21035;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#25110;&#20449;&#36947;&#21709;&#24212;&#19982;&#27169;&#22411;&#35757;&#32451;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20173;&#28982;&#26159;&#27425;&#20248;&#30340;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24456;&#21487;&#33021;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#23646;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#36731;&#37327;&#32423;&#22312;&#32447;&#26041;&#24335;&#26356;&#26032;&#37096;&#32626;&#21518;&#30340;&#29616;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field semantic communication is driving the research of end-to-end data transmission. By utilizing the powerful representation ability of deep learning models, learned data transmission schemes have exhibited superior performance than the established source and channel coding methods. While, so far, research efforts mainly concentrated on architecture and model improvements toward a static target domain. Despite their successes, such learned models are still suboptimal due to the limitations in model capacity and imperfect optimization and generalization, particularly when the testing data distribution or channel response is different from that adopted for model training, as is likely to be the case in real-world. To tackle this, we propose a novel online learned joint source and channel coding approach that leverages the deep learning model's overfitting property. Specifically, we update the off-the-shelf pre-trained models after deployment in a lightweight online fashion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#24050;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#27468;&#26354;&#20013;&#25552;&#21462;&#20165;&#19982;&#38899;&#39057;&#25928;&#26524;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#23454;&#29616;&#20102;&#22810;&#36712;&#38899;&#39057;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.02247</link><description>&lt;p&gt;
&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#65306;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35299;&#24320;&#38899;&#39057;&#25928;&#26524;&#30340;&#32039;&#23494;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects. (arXiv:2211.02247v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#24050;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#27468;&#26354;&#20013;&#25552;&#21462;&#20165;&#19982;&#38899;&#39057;&#25928;&#26524;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#23454;&#29616;&#20102;&#22810;&#36712;&#38899;&#39057;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#31995;&#32479;&#65292;&#23558;&#36755;&#20837;&#22810;&#36712;&#28151;&#38899;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#20026;&#21442;&#32771;&#27468;&#26354;&#30340;&#39118;&#26684;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39044;&#20808;&#32463;&#36807;&#23545;&#27604;&#30446;&#26631;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#38899;&#20048;&#24405;&#38899;&#20013;&#25552;&#21462;&#20165;&#19982;&#38899;&#39057;&#25928;&#26524;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#27169;&#22411;&#37117;&#26159;&#33258;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#30340;&#65292;&#20351;&#29992;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#20174;&#24050;&#22788;&#29702;&#30340;&#28287;&#24230;&#22810;&#36712;&#25968;&#25454;&#38598;&#20013;&#32531;&#35299;&#20102;&#33719;&#21462;&#26410;&#22788;&#29702;&#24178;&#29157;&#25968;&#25454;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20854;&#20998;&#31163;&#38899;&#39057;&#25928;&#26524;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#39564;&#35777;&#20854;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;&#20174;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#19981;&#20165;&#21487;&#20197;&#23558;&#22810;&#36712;&#38899;&#39057;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#20026;&#21442;&#32771;&#39118;&#26684;&#65292;&#32780;&#19988;&#22312;&#20351;&#29992;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#26102;&#20063;&#20855;&#26377;&#28151;&#21512;&#39118;&#26684;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end music mixing style transfer system that converts the mixing style of an input multitrack to that of a reference song. This is achieved with an encoder pre-trained with a contrastive objective to extract only audio effects related information from a reference music recording. All our models are trained in a self-supervised manner from an already-processed wet multitrack dataset with an effective data preprocessing method that alleviates the data scarcity of obtaining unprocessed dry data. We analyze the proposed encoder for the disentanglement capability of audio effects and also validate its performance for mixing style transfer through both objective and subjective evaluations. From the results, we show the proposed system not only converts the mixing style of multitrack audio close to a reference but is also robust with mixture-wise style transfer upon using a music source separation model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20923;&#32467;&#20877;&#21464;&#25442;(FTT)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#21644;&#29305;&#24449;&#22122;&#22768;&#19979;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#20923;&#32467;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.11075</link><description>&lt;p&gt;
&#20923;&#32467;&#20877;&#35757;&#32451;&#65306;&#22312;&#34394;&#20551;&#30456;&#20851;&#21644;&#29305;&#24449;&#22122;&#22768;&#19979;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise. (arXiv:2210.11075v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20923;&#32467;&#20877;&#21464;&#25442;(FTT)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#21644;&#29305;&#24449;&#22122;&#22768;&#19979;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#20923;&#32467;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#65292;&#22914;&#22270;&#20687;&#32972;&#26223;&#65292;&#21487;&#33021;&#20351;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;(ERM)&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Kirichenko&#31561;&#20154;(2022) &#23454;&#35777;&#21457;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#65292;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#20173;&#28982;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#12290;&#36825;&#24320;&#21551;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#21363;&#39318;&#20808;&#35757;&#32451;&#29305;&#24449;&#23398;&#20064;&#22120;&#32780;&#19981;&#26159;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;(&#37325;&#35757;&#32451;&#26368;&#21518;&#19968;&#23618;)&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;&#24403;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#20851;&#32852;&#30340;&#19981;&#21487;&#23454;&#29616;&#22122;&#22768;&#23567;&#20110;&#34394;&#20551;&#29305;&#24449;&#30340;&#22122;&#22768;&#26102;&#65292;&#25165;&#33021;&#24456;&#22909;&#22320;&#23398;&#20064;&#36825;&#20123;&#29305;&#24449;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#20010;&#21457;&#29616;&#65292;&#24182;&#38416;&#36848;&#19981;&#21487;&#23454;&#29616;&#22122;&#22768;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20923;&#32467;&#20877;&#21464;&#25442;(FTT)&#30340;&#31639;&#27861;&#65292;&#39318;&#20808;&#20923;&#32467;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FTT&#22312;&#29305;&#24449;&#23398;&#20064;&#22120;&#19978;&#30340;&#19968;&#20010;&#28201;&#21644;&#26465;&#20214;&#19979;&#20445;&#35777;&#26377;&#30028;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;FTT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#34394;&#20551;&#30456;&#20851;&#20197;&#21450;&#29305;&#24449;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of spurious correlations such as image backgrounds in the training environment can make empirical risk minimization (ERM) perform badly in the test environment. To address this problem, Kirichenko et al. (2022) empirically found that the core features that are related to the outcome can still be learned well even with the presence of spurious correlations. This opens a promising strategy to first train a feature learner rather than a classifier, and then perform linear probing (last layer retraining) in the test environment. However, a theoretical understanding of when and why this approach works is lacking. In this paper, we find that core features are only learned well when their associated non-realizable noise is smaller than that of spurious features, which is not necessarily true in practice. We provide both theories and experiments to support this finding and to illustrate the importance of non-realizable noise. Moreover, we propose an algorithm called Freeze then T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;SAM&#65292;&#21457;&#29616;&#22312;&#20984;&#20108;&#27425;&#30446;&#26631;&#20013;&#23427;&#20250;&#22312;&#26368;&#23567;&#20540;&#20004;&#20391;&#26469;&#22238;&#25391;&#33633;&#65292;&#20294;&#22312;&#38750;&#20108;&#27425;&#24773;&#20917;&#20013;&#20174;&#20809;&#35889;&#33539;&#25968;&#30340;&#35282;&#24230;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#26356;&#26032;&#26041;&#24335;&#34987;&#35748;&#20026;&#26159;Hessian&#30697;&#38453;&#22312;&#39046;&#20808;&#29305;&#24449;&#21521;&#37327;&#26041;&#21521;&#19978;&#30340;&#23548;&#25968;&#65292;&#40723;&#21169;&#28418;&#31227;&#21521;&#26356;&#23485;&#30340;&#26497;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.01513</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#21160;&#24577;&#65306;&#20174;&#23777;&#35895;&#21453;&#24377;&#21040;&#28418;&#21521;&#23485;&#26497;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima. (arXiv:2210.01513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;SAM&#65292;&#21457;&#29616;&#22312;&#20984;&#20108;&#27425;&#30446;&#26631;&#20013;&#23427;&#20250;&#22312;&#26368;&#23567;&#20540;&#20004;&#20391;&#26469;&#22238;&#25391;&#33633;&#65292;&#20294;&#22312;&#38750;&#20108;&#27425;&#24773;&#20917;&#20013;&#20174;&#20809;&#35889;&#33539;&#25968;&#30340;&#35282;&#24230;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#26356;&#26032;&#26041;&#24335;&#34987;&#35748;&#20026;&#26159;Hessian&#30697;&#38453;&#22312;&#39046;&#20808;&#29305;&#24449;&#21521;&#37327;&#26041;&#21521;&#19978;&#30340;&#23548;&#25968;&#65292;&#40723;&#21169;&#28418;&#31227;&#21521;&#26356;&#23485;&#30340;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#30340;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39044;&#27979;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;SAM&#24212;&#29992;&#20110;&#20984;&#20108;&#27425;&#30446;&#26631;&#26102;&#65292;&#38024;&#23545;&#22823;&#22810;&#25968;&#38543;&#26426;&#21021;&#22987;&#21270;&#65292;&#23427;&#20250;&#25910;&#25947;&#20110;&#22312;&#27839;&#30528;&#20027;&#26354;&#29575;&#26368;&#22823;&#26041;&#21521;&#30340;&#26368;&#23567;&#20540;&#20004;&#20391;&#26469;&#22238;&#25391;&#33633;&#30340;&#24490;&#29615;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#29575;&#30340;&#30028;&#38480;&#12290;&#22312;&#38750;&#20108;&#27425;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#25391;&#33633;&#23454;&#36136;&#19978;&#26159;&#22312;Hessian&#30697;&#38453;&#30340;&#20809;&#35889;&#33539;&#25968;&#19978;&#20197;&#26356;&#23567;&#30340;&#27493;&#38271;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;SAM&#30340;&#26356;&#26032;&#21487;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19977;&#38454;&#23548;&#25968; - &#21363;Hessian&#30697;&#38453;&#22312;&#39046;&#20808;&#30340;&#29305;&#24449;&#21521;&#37327;&#26041;&#21521;&#19978;&#30340;&#23548;&#25968;&#65292;&#23427;&#40723;&#21169;&#26397;&#30528;&#26356;&#23485;&#30340;&#26497;&#23567;&#20540;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence.  In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM's update may be regarded as a third derivative -the derivative of the Hessian in the leading eigenvector direction -- that encourages drift toward wider minima.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23567;&#22411;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#19978;&#30340;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#30340;&#32852;&#37030;&#35757;&#32451;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;DCCO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#37030;&#24179;&#22343;&#21644;&#20132;&#21449;&#30456;&#20851;&#25439;&#22833;&#26469;&#35757;&#32451;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.00092</link><description>&lt;p&gt;
&#23567;&#22411;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#19978;&#30340;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#32852;&#37030;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Federated Training of Dual Encoding Models on Small Non-IID Client Datasets. (arXiv:2210.00092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23567;&#22411;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#19978;&#30340;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#30340;&#32852;&#37030;&#35757;&#32451;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;DCCO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#37030;&#24179;&#22343;&#21644;&#20132;&#21449;&#30456;&#20851;&#25439;&#22833;&#26469;&#35757;&#32451;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20013;&#24515;&#21270;&#35757;&#32451;&#25968;&#25454;&#20013;&#32534;&#30721;&#23545;&#30340;&#19968;&#33268;&#24615;&#26469;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#25968;&#25454;&#38598;&#22312;&#35768;&#22810;&#23458;&#25143;&#31471;&#65288;&#29992;&#25143;&#35774;&#22791;&#25110;&#32452;&#32455;&#65289;&#19978;&#26412;&#36136;&#19978;&#26159;&#20998;&#25955;&#30340;&#65292;&#36825;&#20419;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#35768;&#22810;&#23567;&#22411;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#23458;&#25143;&#25968;&#25454;&#19978;&#23454;&#29616;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#30340;&#32852;&#37030;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22312;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#32852;&#21512;&#24179;&#22343;&#26041;&#27861;&#36827;&#34892;&#31616;&#21333;&#25913;&#36827;&#26102;&#34920;&#29616;&#31967;&#31957;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#22522;&#20110;&#32534;&#30721;&#32479;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#21508;&#20010;&#23458;&#25143;&#31471;&#19978;&#27169;&#25311;&#22823;&#25209;&#37327;&#25439;&#22833;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#35757;&#32451;&#26041;&#27861;&#65292;&#20998;&#24067;&#24335;&#20132;&#21449;&#30456;&#20851;&#20248;&#21270;&#65288;DCCO&#65289;&#65292;&#23427;&#20351;&#29992;&#20132;&#21449;&#30456;&#20851;&#25439;&#22833;&#21644;&#23458;&#25143;&#31471;&#26356;&#26032;&#30340;&#32852;&#37030;&#24179;&#22343;&#26469;&#35757;&#32451;&#21452;&#37325;&#32534;&#30721;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual encoding models that encode a pair of inputs are widely used for representation learning. Many approaches train dual encoding models by maximizing agreement between pairs of encodings on centralized training data. However, in many scenarios, datasets are inherently decentralized across many clients (user devices or organizations) due to privacy concerns, motivating federated learning. In this work, we focus on federated training of dual encoding models on decentralized data composed of many small, non-IID (independent and identically distributed) client datasets. We show that existing approaches that work well in centralized settings perform poorly when naively adapted to this setting using federated averaging. We observe that, we can simulate large-batch loss computation on individual clients for loss functions that are based on encoding statistics. Based on this insight, we propose a novel federated training approach, Distributed Cross Correlation Optimization (DCCO), which trai
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#20998;&#26512;&#19981;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#29992;&#20840;&#25209;&#27425;&#25110;&#22823;&#25209;&#27425;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#65292;Cohen&#31561;&#20154;(2021)&#21457;&#29616;&#30340;&#26799;&#24230;&#19979;&#38477;&#36793;&#32536;&#31283;&#23450;&#24615;&#29616;&#35937;&#34920;&#26126;&#65292;&#24403;&#38160;&#24230;&#36798;&#21040;&#19981;&#31283;&#23450;&#24615;&#25130;&#27490;&#20540;$2/\eta$&#26102;&#65292;&#36845;&#20195;&#20855;&#26377;&#33258;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#38544;&#24335;&#20559;&#21521;&#31283;&#23450;&#36793;&#32536;&#35299;&#30340;&#20559;&#24046;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#36807;&#25429;&#33719;&#20108;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#30340;&#27604;&#20363;&#31995;&#25968;&#24471;&#21040;&#20102;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2209.15594</link><description>&lt;p&gt;
&#33258;&#31283;&#23450;&#24615;&#65306;&#26799;&#24230;&#19979;&#38477;&#22312;&#31283;&#23450;&#36793;&#32536;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. (arXiv:2209.15594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15594
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#20998;&#26512;&#19981;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#29992;&#20840;&#25209;&#27425;&#25110;&#22823;&#25209;&#27425;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#65292;Cohen&#31561;&#20154;(2021)&#21457;&#29616;&#30340;&#26799;&#24230;&#19979;&#38477;&#36793;&#32536;&#31283;&#23450;&#24615;&#29616;&#35937;&#34920;&#26126;&#65292;&#24403;&#38160;&#24230;&#36798;&#21040;&#19981;&#31283;&#23450;&#24615;&#25130;&#27490;&#20540;$2/\eta$&#26102;&#65292;&#36845;&#20195;&#20855;&#26377;&#33258;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#38544;&#24335;&#20559;&#21521;&#31283;&#23450;&#36793;&#32536;&#35299;&#30340;&#20559;&#24046;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#36807;&#25429;&#33719;&#20108;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#30340;&#27604;&#20363;&#31995;&#25968;&#24471;&#21040;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65292;&#20063;&#31216;&#20026;&#38160;&#24230;$S(\theta)$&#65292;&#34987;$2/\eta$&#38480;&#21046;&#26102;&#65292;&#35757;&#32451;&#26159;&#8220;&#31283;&#23450;&#30340;&#8221;&#65292;&#35757;&#32451;&#25439;&#22833;&#21333;&#35843;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#29992;&#20840;&#25209;&#37327;&#25110;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36825;&#31181;&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;&#26368;&#36817;&#65292;Cohen&#31561;&#20154;(2021)&#35266;&#23519;&#21040;&#20102;&#20004;&#20010;&#37325;&#35201;&#29616;&#35937;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#34987;&#31216;&#20026;&#28176;&#36827;&#38160;&#21270;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#38160;&#24230;&#31283;&#27493;&#22686;&#21152;&#65292;&#30452;&#21040;&#36798;&#21040;&#19981;&#31283;&#23450;&#24615;&#25130;&#27490;&#20540;$2/\eta$&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#34987;&#31216;&#20026;&#31283;&#23450;&#36793;&#32536;&#65292;&#22312;&#21097;&#20313;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#38160;&#24230;&#20572;&#30041;&#22312;$2/\eta$&#65292;&#32780;&#25439;&#22833;&#21017;&#25345;&#32493;&#19979;&#38477;&#65292;&#23613;&#31649;&#19981;&#26159;&#21333;&#35843;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#31283;&#23450;&#36793;&#32536;&#22788;&#65292;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#24577;&#21487;&#20197;&#30001;&#19968;&#20010;&#19977;&#27425;&#27888;&#21202;&#23637;&#24320;&#24335;&#25429;&#33719;:&#24403;&#36845;&#20195;&#22312;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#21521;&#37327;&#26041;&#21521;&#19978;&#21457;&#25955;&#26102;&#65292;&#38160;&#24230;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#27604;&#20363;&#31995;&#25968;&#30001;&#25439;&#22833;&#30340;&#20108;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#20915;&#23450;&#12290;&#24403;&#38160;&#24230;&#31934;&#30830;&#20026;$2/\eta$&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36845;&#20195;&#30340;&#33258;&#31283;&#23450;&#24615;&#21487;&#20197;&#27704;&#20037;&#22320;&#20445;&#25345;&#22312;&#31283;&#23450;&#36793;&#32536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#33258;&#31283;&#23450;&#29616;&#35937;&#20351;&#26799;&#24230;&#19979;&#38477;&#20855;&#26377;&#38544;&#24335;&#20559;&#21521;&#31283;&#23450;&#36793;&#32536;&#35299;&#30340;&#20559;&#24046;&#65292;&#32780;&#31283;&#23450;&#36793;&#32536;&#26159;&#25439;&#22833;&#26126;&#26174;&#20302;&#20110;&#31283;&#23450;&#21306;&#22495;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\theta)$, is bounded by $2/\eta$, training is "stable" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessi
&lt;/p&gt;</description></item><item><title>TRBoost &#26159;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#20351;&#29992;&#32422;&#26463;&#20108;&#27425;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#24182;&#24212;&#29992;&#20449;&#36182;&#22495;&#31639;&#27861;&#26469;&#33719;&#24471;&#26032;&#30340;&#23398;&#20064;&#22120;&#65292;&#20855;&#26377;&#36866;&#29992;&#20110;&#20219;&#24847;&#25439;&#22833;&#20989;&#25968;&#30340;&#36890;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.13791</link><description>&lt;p&gt;
TRBoost: &#22522;&#20110;&#20449;&#36182;&#22495;&#26041;&#27861;&#30340;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;
&lt;/p&gt;
&lt;p&gt;
TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13791
&lt;/p&gt;
&lt;p&gt;
TRBoost &#26159;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#20351;&#29992;&#32422;&#26463;&#20108;&#27425;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#24182;&#24212;&#29992;&#20449;&#36182;&#22495;&#31639;&#27861;&#26469;&#33719;&#24471;&#26032;&#30340;&#23398;&#20064;&#22120;&#65292;&#20855;&#26377;&#36866;&#29992;&#20110;&#20219;&#24847;&#25439;&#22833;&#20989;&#25968;&#30340;&#36890;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#26426; (GBMs) &#21033;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#27888;&#21202;&#23637;&#24320;&#26174;&#33879;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21508;&#31181;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#23545; GBMs &#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#65292;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340; GBMs &#20351;&#29992;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#20197;&#30830;&#20445;&#36866;&#29992;&#20110;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340; GBMs &#21033;&#29992;&#27491;&#23450;&#30340;&#40657;&#22622;&#30697;&#38453;&#33719;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20197;&#29306;&#29298;&#36890;&#29992;&#24615;&#20026;&#20195;&#20215;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#31216;&#20026; TRBoost&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;TRBoost &#20351;&#29992;&#19968;&#20010;&#32422;&#26463;&#20108;&#27425;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#24182;&#24212;&#29992;&#20449;&#36182;&#22495;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#24182;&#33719;&#24471;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#22120;&#12290;&#19982;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340; GBMs &#19981;&#21516;&#65292;TRBoost &#19981;&#35201;&#27714;&#40657;&#22622;&#30697;&#38453;&#26159;&#27491;&#23450;&#30340;&#65292;&#22240;&#27492;&#20801;&#35768;&#23427;&#29992;&#20110;&#20219;&#24847;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Boosting Machines (GBMs) have demonstrated remarkable success in solving diverse problems by utilizing Taylor expansions in functional space. However, achieving a balance between performance and generality has posed a challenge for GBMs. In particular, gradient descent-based GBMs employ the first-order Taylor expansion to ensure applicability to all loss functions, while Newton's method-based GBMs use positive Hessian information to achieve superior performance at the expense of generality. To address this issue, this study proposes a new generic Gradient Boosting Machine called Trust-region Boosting (TRBoost). In each iteration, TRBoost uses a constrained quadratic model to approximate the objective and applies the Trust-region algorithm to solve it and obtain a new learner. Unlike Newton's method-based GBMs, TRBoost does not require the Hessian to be positive definite, thereby allowing it to be applied to arbitrary loss functions while still maintaining competitive performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#23427;&#20943;&#23569;&#21040;&#20114;&#34917;&#31867;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#27493;&#39588;&#65292;&#20934;&#30830;&#30340;&#20114;&#34917;&#26631;&#31614;&#27010;&#29575;&#20272;&#35745;&#21487;&#20197;&#20135;&#29983;&#33391;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2209.09500</link><description>&lt;p&gt;
&#20174;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#21040;&#27010;&#29575;&#20272;&#35745;&#30340;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reduction from Complementary-Label Learning to Probability Estimates. (arXiv:2209.09500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#23427;&#20943;&#23569;&#21040;&#20114;&#34917;&#31867;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#27493;&#39588;&#65292;&#20934;&#30830;&#30340;&#20114;&#34917;&#26631;&#31614;&#27010;&#29575;&#20272;&#35745;&#21487;&#20197;&#20135;&#29983;&#33391;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20165;&#20174;&#20114;&#34917;&#26631;&#31614;&#20013;&#23398;&#20064;&#22810;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#25351;&#31034;&#23454;&#20363;&#19981;&#23646;&#20110;&#30340;&#31867;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#20943;&#23569;&#21040;&#26222;&#36890;&#20998;&#31867;&#30340;&#33539;&#20363;&#65292;&#23545;CLL&#36827;&#34892;&#29305;&#23450;&#36716;&#25442;&#21644;&#26367;&#20195;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#20960;&#20010;&#38480;&#21046;&#65292;&#20363;&#22914;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#25110;&#28145;&#24230;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;-&#20943;&#23569;&#21040;&#20114;&#34917;&#31867;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#35268;&#36991;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20934;&#30830;&#30340;&#20114;&#34917;&#26631;&#31614;&#27010;&#29575;&#20272;&#35745;&#36890;&#36807;&#31616;&#21333;&#30340;&#35299;&#30721;&#27493;&#39588;&#20135;&#29983;&#33391;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#35813;&#35777;&#26126;&#24314;&#31435;&#20102;&#19968;&#20010;&#20174;CLL&#21040;&#27010;&#29575;&#20272;&#35745;&#30340;&#31616;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#35299;&#37322;&#20960;&#20010;&#20851;&#38190;CLL&#26041;&#27861;&#30340;&#29305;&#27530;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#19968;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#30340;CLL&#26041;&#27861;&#26356;&#20855;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary-Label Learning (CLL) is a weakly-supervised learning problem that aims to learn a multi-class classifier from only complementary labels, which indicate a class to which an instance does not belong. Existing approaches mainly adopt the paradigm of reduction to ordinary classification, which applies specific transformations and surrogate losses to connect CLL back to ordinary classification. Those approaches, however, face several limitations, such as the tendency to overfit or be hooked on deep models. In this paper, we sidestep those limitations with a novel perspective--reduction to probability estimates of complementary classes. We prove that accurate probability estimates of complementary labels lead to good classifiers through a simple decoding step. The proof establishes a reduction framework from CLL to probability estimates. The framework offers explanations of several key CLL approaches as its special cases and allows us to design an improved algorithm that is mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#25110;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#30340;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#39062;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#29992;Broyden's&#26041;&#27861;&#36924;&#36817;&#36229;&#26799;&#24230;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.07075</link><description>&lt;p&gt;
&#20351;&#29992;Broyden&#36229;&#26799;&#24230;&#30340;PDE&#32422;&#26463;&#20248;&#21270;&#30340;&#21452;&#23618;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476; (arXiv:2209.07075v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Bi-level Physics-Informed Neural Networks for PDE Constrained Optimization using Broyden's Hypergradients. (arXiv:2209.07075v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#25110;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#30340;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#39062;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#29992;Broyden's&#26041;&#27861;&#36924;&#36817;&#36229;&#26799;&#24230;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;DeepONets&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;PDE&#32422;&#26463;&#20248;&#21270;&#65288;PDECO&#65289;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19981;&#36275;&#20197;&#22788;&#29702;&#37027;&#20123;PDE&#32422;&#26463;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#20855;&#26377;&#22797;&#26434;&#25110;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#20248;&#21270;&#26469;&#35299;&#20915;&#12290;&#23545;&#20110;&#20869;&#37096;&#24490;&#29615;&#20248;&#21270;&#65292;&#25105;&#20204;&#37319;&#29992;PINNs&#20165;&#35299;&#20915;PDE&#32422;&#26463;&#12290;&#23545;&#20110;&#22806;&#37096;&#24490;&#29615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#38544;&#24335;&#20989;&#25968;&#23450;&#29702;&#65288;IFT&#65289;&#20351;&#29992;Broyden&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36924;&#36817;&#36229;&#26799;&#24230;&#26041;&#38754;&#39640;&#25928;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36229;&#26799;&#24230;&#35745;&#31639;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#35823;&#24046;&#20998;&#26512;&#12290;&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#21644;&#38750;&#32447;&#24615;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based approaches like Physics-informed neural networks (PINNs) and DeepONets have shown promise on solving PDE constrained optimization (PDECO) problems. However, existing methods are insufficient to handle those PDE constraints that have a complicated or nonlinear dependency on optimization targets. In this paper, we present a novel bi-level optimization framework to resolve the challenge by decoupling the optimization of the targets and constraints. For the inner loop optimization, we adopt PINNs to solve the PDE constraints only. For the outer loop, we design a novel method by using Broyden's method based on the Implicit Function Theorem (IFT), which is efficient and accurate for approximating hypergradients. We further present theoretical explanations and error analysis of the hypergradients computation. Extensive experiments on multiple large-scale and nonlinear PDE constrained optimization problems demonstrate that our method achieves state-of-the-art results compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#19968;&#26234;&#33021;&#20307;&#26679;&#26412;&#36335;&#24452;&#30340;&#8220;&#27801;&#31665;&#23398;&#20064;&#8221;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#38750;&#21512;&#20316;&#30340;&#29615;&#22659;&#19979;&#20316;&#20026;&#39044;&#28909;&#24320;&#22987;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22343;&#22330;oracle&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{\mathcal{O}}(\epsilon^{-4})$&#12290;</title><link>http://arxiv.org/abs/2208.11639</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#19968;&#26679;&#26412;&#36335;&#24452;&#30340;&#22343;&#22330;&#21338;&#24328;&#26080;&#38656;oracle&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path. (arXiv:2208.11639v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#19968;&#26234;&#33021;&#20307;&#26679;&#26412;&#36335;&#24452;&#30340;&#8220;&#27801;&#31665;&#23398;&#20064;&#8221;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#38750;&#21512;&#20316;&#30340;&#29615;&#22659;&#19979;&#20316;&#20026;&#39044;&#28909;&#24320;&#22987;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22343;&#22330;oracle&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{\mathcal{O}}(\epsilon^{-4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#21033;&#29992;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#26679;&#26412;&#36335;&#24452;&#36817;&#20284;&#22343;&#22330;&#24179;&#34913;(MFE)&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22343;&#22330;oracle&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#27801;&#31665;&#23398;&#20064;&#8221;&#65292;&#22240;&#20026;&#20219;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38750;&#21512;&#20316;&#35774;&#32622;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#37117;&#21487;&#20197;&#20351;&#29992;&#23427;&#20316;&#20026;&#39044;&#28909;&#24320;&#22987;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#24930;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#22343;&#22330;&#30340;&#22312;&#32447;&#22266;&#23450;&#28857;&#36882;&#24402;&#65292;&#19982;&#26222;&#36890;&#20195;&#29702;&#30340;&#26356;&#24555;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25511;&#21046;&#31574;&#30053;&#26356;&#26032;&#19968;&#36215;&#25805;&#20316;&#12290;&#37492;&#20110;&#20195;&#29702;&#30340;&#22522;&#26412;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26159;&#36830;&#36890;&#30340;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20445;&#35777;&#22343;&#22330;&#21644;&#25511;&#21046;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#22330;&#24179;&#34913;&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#20445;&#35777;&#12290;&#27801;&#31665;&#23398;&#20064;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;$\tilde{\mathcal{O}}(\epsilon^{-4})$&#65292;&#20854;&#20013;$\epsilon$&#26159;MFE&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning in Mean-Field Games (MFGs). Unlike traditional approaches, we alleviate the need for a mean-field oracle by developing an algorithm that approximates the Mean-Field Equilibrium (MFE) using the single sample path of the generic agent. We call this {\it Sandbox Learning}, as it can be used as a warm-start for any agent learning in a multi-agent non-cooperative setting. We adopt a two time-scale approach in which an online fixed-point recursion for the mean-field operates on a slower time-scale, in tandem with a control policy update on a faster time-scale for the generic agent. Given that the underlying Markov Decision Process (MDP) of the agent is communicating, we provide finite sample convergence guarantees in terms of convergence of the mean-field and control policy to the mean-field equilibrium. The sample complexity of the Sandbox learning algorithm is $\tilde{\mathcal{O}}(\epsilon^{-4})$ where $\epsilon$ is the MFE approximation error. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#29992;&#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#35813;&#35774;&#32622;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#21333;&#27425;&#36941;&#21382;&#12289;&#32570;&#20047;&#22806;&#37096;&#30417;&#30563;&#21644;&#20808;&#39564;&#30693;&#35782;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCALE&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32431;&#31929;&#22320;&#20174;&#25968;&#25454;&#36830;&#32493;&#20307;&#20013;&#25552;&#21462;&#21644;&#35760;&#24518;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2208.11266</link><description>&lt;p&gt;
SCALE&#65306;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#29992;&#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#35813;&#35774;&#32622;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#21333;&#27425;&#36941;&#21382;&#12289;&#32570;&#20047;&#22806;&#37096;&#30417;&#30563;&#21644;&#20808;&#39564;&#30693;&#35782;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCALE&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32431;&#31929;&#22320;&#20174;&#25968;&#25454;&#36830;&#32493;&#20307;&#20013;&#25552;&#21462;&#21644;&#35760;&#24518;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#32456;&#36523;&#23398;&#20064;&#26159;&#25351;&#33021;&#22815;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#22312;&#26102;&#38388;&#19978;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#24182;&#35760;&#24518;&#20197;&#21069;&#30340;&#27169;&#24335;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#26377;&#20851;&#36755;&#20837;&#25968;&#25454;&#30340;&#24378;&#20808;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#24050;&#30693;&#31867;&#21035;&#36793;&#30028;&#65289;&#65292;&#32780;&#36825;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#29615;&#22659;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#33719;&#24471;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#26356;&#23454;&#36341;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#20026;&#20102;&#24212;&#23545;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCALE&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32431;&#31929;&#22320;&#20174;&#25968;&#25454;&#36830;&#32493;&#20307;&#20013;&#25552;&#21462;&#21644;&#35760;&#24518;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgettin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LogLG&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#20107;&#20214;&#22270;&#26500;&#24314;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#26088;&#22312;&#25506;&#32034;&#24207;&#21015;&#20013;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#24212;&#29992;&#31243;&#24207;&#26085;&#24535;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2208.10833</link><description>&lt;p&gt;
LogLG: &#36890;&#36807;&#20107;&#20214;&#22270;&#26500;&#24314;&#23454;&#29616;&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction. (arXiv:2208.10833v5 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LogLG&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#20107;&#20214;&#22270;&#26500;&#24314;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#26088;&#22312;&#25506;&#32034;&#24207;&#21015;&#20013;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#24212;&#29992;&#31243;&#24207;&#26085;&#24535;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20026;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#21322;&#30417;&#30563;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#20511;&#21161;&#35299;&#26512;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29420;&#31435;&#32771;&#34385;&#27599;&#20010;&#20851;&#38190;&#23383;&#65292;&#24573;&#30053;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26085;&#24535;&#24207;&#21015;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#21517;&#20026;LogLG&#65292;&#20197;&#25506;&#32034;&#24207;&#21015;&#20013;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#39318;&#20808;&#25552;&#21462;&#26410;&#26631;&#35760;&#26085;&#24535;&#30340;&#20851;&#38190;&#23383;&#20197;&#26500;&#24314;&#26085;&#24535;&#20107;&#20214;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23376;&#22270;&#27880;&#37322;&#22120;&#29983;&#25104;&#26410;&#26631;&#35760;&#26085;&#24535;&#24207;&#21015;&#30340;&#20266;&#26631;&#31614;&#12290;&#20026;&#20102;&#25913;&#21892;&#27880;&#37322;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#30417;&#30563;&#20219;&#21153;&#26469;&#39044;&#35757;&#32451;&#23376;&#22270;&#27880;&#37322;&#22120;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#35757;&#32451;&#26816;&#27979;&#27169;&#22411;&#12290;&#22312;&#26465;&#20214;&#160;&#19979;&#65292;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#26469;&#33258;&#24212;&#29992;&#31243;&#24207;&#26085;&#24535;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully supervised log anomaly detection methods suffer the heavy burden of annotating massive unlabeled log data. Recently, many semi-supervised methods have been proposed to reduce annotation costs with the help of parsed templates. However, these methods consider each keyword independently, which disregards the correlation between keywords and the contextual relationships among log sequences. In this paper, we propose a novel weakly supervised log anomaly detection framework, named LogLG, to explore the semantic connections among keywords from sequences. Specifically, we design an end-to-end iterative process, where the keywords of unlabeled logs are first extracted to construct a log-event graph. Then, we build a subgraph annotator to generate pseudo labels for unlabeled log sequences. To ameliorate the annotation quality, we adopt a self-supervised task to pre-train a subgraph annotator. After that, a detection model is trained with the generated pseudo labels. Conditioned on the cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;DIET&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36793;&#38469;&#29420;&#31435;&#32479;&#35745;&#37327;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22823;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#25311;&#21512;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#23548;&#33268;&#30340;&#25439;&#22833;&#21151;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.08579</link><description>&lt;p&gt;
DIET: &#21033;&#29992;&#21097;&#20313;&#20449;&#24687;&#30340;&#36793;&#38469;&#30456;&#20851;&#24230;&#37327;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
DIET: Conditional independence testing with marginal dependence measures of residual information. (arXiv:2208.08579v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;DIET&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36793;&#38469;&#29420;&#31435;&#32479;&#35745;&#37327;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#22823;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#25311;&#21512;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#23548;&#33268;&#30340;&#25439;&#22833;&#21151;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#65288;CRT&#65289;&#29992;&#20110;&#35780;&#20272;&#21464;&#37327;$x$&#22312;&#24050;&#30693;&#21327;&#21464;&#37327;$z$&#30340;&#24773;&#20917;&#19979;&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;$y$&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;CRT&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#25311;&#21512;&#65292;&#36825;&#36890;&#24120;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#35757;&#32451;&#21644;&#27979;&#35797;&#37096;&#20998;&#65292;&#25110;&#20381;&#38752;&#30456;&#20114;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20250;&#23548;&#33268;&#21151;&#29575;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32806;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;DIET&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36793;&#38469;&#29420;&#31435;&#32479;&#35745;&#37327;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#36825;&#20004;&#31181;&#38382;&#39064;&#12290;DIET&#27979;&#35797;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#36793;&#38469;&#29420;&#31435;&#24615;&#65306;$F(x \mid z)$&#21644;$F(y \mid z)$&#20854;&#20013;$F(\cdot \mid z)$&#26159;&#26465;&#20214;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#65288;CDF&#65289;&#65292;&#36825;&#20123;&#21464;&#37327;&#34987;&#31216;&#20026;&#8220;&#20449;&#24687;&#27531;&#24046;&#8221;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;DIET&#23454;&#29616;&#26377;&#38480;&#26679;&#26412;&#30340;&#31867;&#22411;1&#38169;&#35823;&#25511;&#21046;&#21644;&#21151;&#29575;&#22823;&#20110;&#31867;&#22411;1&#38169;&#35823;&#29575;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;DIET&#24212;&#29992;&#26102;&#65292;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#38656;&#35201;&#28385;&#36275;&#20219;&#20309;&#29305;&#23450;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional randomization tests (CRTs) assess whether a variable $x$ is predictive of another variable $y$, having observed covariates $z$. CRTs require fitting a large number of predictive models, which is often computationally intractable. Existing solutions to reduce the cost of CRTs typically split the dataset into a train and test portion, or rely on heuristics for interactions, both of which lead to a loss in power. We propose the decoupled independence test (DIET), an algorithm that avoids both of these issues by leveraging marginal independence statistics to test conditional independence relationships. DIET tests the marginal independence of two random variables: $F(x \mid z)$ and $F(y \mid z)$ where $F(\cdot \mid z)$ is a conditional cumulative distribution function (CDF). These variables are termed "information residuals." We give sufficient conditions for DIET to achieve finite sample type-1 error control and power greater than the type-1 error rate. We then prove that when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#35782;&#21035;&#20987;&#38190;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#20987;&#38190;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#26469;&#25913;&#36827;&#22522;&#20110;&#20987;&#38190;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#30495;&#23454;&#24615;&#24182;&#34920;&#26126;&#22312;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2207.13394</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#35782;&#21035;&#20987;&#38190;&#25968;&#25454;&#29983;&#25104;&#30340; BeCAPTCHA &#31867;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection. (arXiv:2207.13394v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#35782;&#21035;&#20987;&#38190;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#20987;&#38190;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#26469;&#25913;&#36827;&#22522;&#20110;&#20987;&#38190;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#30495;&#23454;&#24615;&#24182;&#34920;&#26126;&#22312;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;&#20987;&#38190;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#19982;&#22522;&#20110;&#36890;&#29992;&#27169;&#22411;&#21644;&#29992;&#25143;&#30456;&#20851;&#27169;&#22411;&#30340;&#20004;&#31181;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20987;&#38190;&#25968;&#25454;&#26469;&#25913;&#36827;&#22522;&#20110;&#20987;&#38190;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#20351;&#29992; 136 &#30334;&#19975;&#20010;&#26469;&#33258; 168 &#21315;&#20010;&#21463;&#35797;&#32773;&#30340;&#20987;&#38190;&#20107;&#20214;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#20998;&#26512;&#20102;&#19977;&#31181;&#21512;&#25104;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#20102;&#22522;&#20110;&#20960;&#31181;&#21463;&#30417;&#30563;&#20998;&#31867;&#22120;&#65288;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65289;&#21644;&#21253;&#25324;&#20154;&#31867;&#21644;&#21512;&#25104;&#26679;&#26412;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#19981;&#21516;&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#21512;&#25104;&#26679;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;&#20998;&#31867;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#29616;&#30456;&#24403;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a data driven learning model for the synthesis of keystroke biometric data. The proposed method is compared with two statistical approaches based on Universal and User-dependent models. These approaches are validated on the bot detection task, using the keystroke synthetic data to improve the training process of keystroke-based bot detection systems. Our experimental framework considers a dataset with 136 million keystroke events from 168 thousand subjects. We have analyzed the performance of the three synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on several supervised classifiers (Support Vector Machine, Random Forest, Gaussian Naive Bayes and a Long Short-Term Memory network) and a learning framework including human and synthetic samples. The experiments demonstrate the realism of the synthetic samples. The classification results suggest that in scenarios with large labeled data, these synthetic 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24310;&#36831;&#22870;&#21169;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20048;&#35266;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#19968;&#20010;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#38543;&#30528;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#30340;&#24809;&#32602;&#20989;&#25968;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.10786</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#22312;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#30740;&#31350;&#20877;&#35775;
&lt;/p&gt;
&lt;p&gt;
Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10786
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24310;&#36831;&#22870;&#21169;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20048;&#35266;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#19968;&#20010;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#38543;&#30528;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#30340;&#24809;&#32602;&#20989;&#25968;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#22870;&#21169;&#20960;&#20046;&#24635;&#26159;&#34987;&#24310;&#36831;&#65292;&#23548;&#33268;&#35201;&#27714;&#21363;&#26102;&#22870;&#21169;&#30340;&#27169;&#22411;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22312;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#24310;&#36831;&#22870;&#21169;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20048;&#35266;&#31639;&#27861;&#36866;&#24212;&#24310;&#36831;&#21453;&#39304;&#39046;&#22495;&#33021;&#22815;&#26377;&#19968;&#20010;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#24809;&#32602;&#20989;&#25968;&#12290;&#36825;&#27604;&#29616;&#26377;&#30340;&#24037;&#20316;&#26174;&#33879;&#30340;&#25552;&#39640;&#20102;&#65292;&#22240;&#20026;&#26368;&#20339;&#30340;&#24050;&#30693;&#30340;&#24809;&#32602;&#20989;&#25968;&#30340;&#30028;&#38480;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic generalised linear bandit is a well-understood model for sequential decision-making problems, with many algorithms achieving near-optimal regret guarantees under immediate feedback. However, the stringent requirement for immediate rewards is unmet in many real-world applications where the reward is almost always delayed. We study the phenomenon of delayed rewards in generalised linear bandits in a theoretical manner. We show that a natural adaptation of an optimistic algorithm to the delayed feedback achieves a regret bound where the penalty for the delays is independent of the horizon. This result significantly improves upon existing work, where the best known regret bound has the delay penalty increasing with the horizon. We verify our theoretical results through experiments on simulated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65288;CMCL&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#25918;&#23556;&#23398;&#23478;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#36880;&#27493;&#20248;&#21270;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;CMCL&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.14579</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65288;CMCL&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#25918;&#23556;&#23398;&#23478;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#36880;&#27493;&#20248;&#21270;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;CMCL&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#20219;&#21153;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#30340;&#36830;&#36143;&#25551;&#36848;&#12290;&#19982;&#19968;&#33324;&#30340;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#19981;&#21516;&#65292;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159; 1) &#20005;&#37325;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644; 2) &#26377;&#38480;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#20559;&#24046;&#24182;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65288;CMCL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CMCL &#27169;&#25311;&#20102;&#25918;&#23556;&#23398;&#23478;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#36880;&#27493;&#20248;&#21270;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;CMCL &#20272;&#35745;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#30340;&#38590;&#24230;&#24182;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;; &#20854;&#27425;&#65292;CMCL &#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#35757;&#32451;&#23454;&#20363;&#32452;&#21512;&#20197;&#32771;&#34385;&#24403;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36845;&#20195;&#19978;&#36848;&#20004;&#20010;&#27493;&#39588;&#65292;CMCL &#21487;&#20197;&#36880;&#28176;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#30340;IU-Xray&#21644;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CMCL &#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#32988;&#36807;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model's performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#30913;&#38236;&#38754;&#19979;&#38477;&#31639;&#27861;&#20316;&#20026;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#30456;&#24212;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#19982;CFR&#30456;&#31454;&#20105;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#8220;&#40657;&#26263;&#20845;&#35282;&#8221;&#21644;&#8220;&#24187;&#35937;&#20117;&#23383;&#8221;&#20013;&#30340;&#33258;&#25105;&#29609;&#32781;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.05825</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#12289;&#37327;&#21270;&#30456;&#24212;&#22343;&#34913;&#21644;&#20108;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games. (arXiv:2206.05825v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#30913;&#38236;&#38754;&#19979;&#38477;&#31639;&#27861;&#20316;&#20026;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#30456;&#24212;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#19982;CFR&#30456;&#31454;&#20105;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#8220;&#40657;&#26263;&#20845;&#35282;&#8221;&#21644;&#8220;&#24187;&#35937;&#20117;&#23383;&#8221;&#20013;&#30340;&#33258;&#25105;&#29609;&#32781;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#30913;&#38236;&#38754;&#19979;&#38477;&#65292;&#23427;&#21463;&#21040;&#38236;&#38754;&#19979;&#38477;&#21644;&#38750;&#27431;&#20960;&#37324;&#24503;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#30913;&#38236;&#38754;&#19979;&#38477;&#20316;&#20026;&#22343;&#34913;&#27714;&#35299;&#22120;&#20197;&#21450;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#36825;&#20123;&#20248;&#28857;&#21253;&#25324;&#65306;1) &#25104;&#20026;&#39318;&#20010;&#23545;&#20110;&#20855;&#26377;&#19968;&#38454;&#21453;&#39304;&#30340;&#25193;&#23637;&#24418;&#24335;&#28216;&#25103;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#30340;&#37327;&#21270;&#30456;&#24212;&#22343;&#34913;&#27714;&#35299;&#22120;&#65307;2) &#25104;&#20026;&#39318;&#20010;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#20013;&#19982;CFR&#23454;&#29616;&#23454;&#39564;&#24615;&#31454;&#20105;&#32467;&#26524;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65307;3) &#23454;&#29616;&#20102;&#22312;3x3&#40657;&#26263;&#20845;&#35282;&#21644;&#24187;&#35937;&#20117;&#23383;&#28216;&#25103;&#20013;&#25104;&#20026;&#33258;&#25105;&#28216;&#25103;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#21033;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#23849;&#28291;&#26159;&#24403;&#28145;&#24230;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#26102;&#20986;&#29616;&#30340;&#19968;&#31181;&#29366;&#24577;&#65292;&#23427;&#31616;&#21270;&#20102;&#26368;&#21518;&#19968;&#23618;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#26368;&#36817;&#31867;&#20013;&#24515;&#20915;&#31574;&#35268;&#21017;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#23849;&#28291;&#30340;&#24314;&#27169;&#21407;&#21017;&#65292;&#20197;&#21450;&#23427;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.04041</link><description>&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#65306;&#24314;&#27169;&#21407;&#21017;&#21644;&#27867;&#21270;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse: A Review on Modelling Principles and Generalization. (arXiv:2206.04041v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04041
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#26159;&#24403;&#28145;&#24230;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#26102;&#20986;&#29616;&#30340;&#19968;&#31181;&#29366;&#24577;&#65292;&#23427;&#31616;&#21270;&#20102;&#26368;&#21518;&#19968;&#23618;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#26368;&#36817;&#31867;&#20013;&#24515;&#20915;&#31574;&#35268;&#21017;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#23849;&#28291;&#30340;&#24314;&#27169;&#21407;&#21017;&#65292;&#20197;&#21450;&#23427;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#24182;&#19988;&#26368;&#32456;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#36235;&#36817;&#20110;&#38646;&#26102;&#65292;&#28145;&#24230;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#36827;&#20837;&#35757;&#32451;&#32456;&#31471;&#38454;&#27573;&#65292;&#24448;&#24448;&#21576;&#29616;&#20986;&#24341;&#20154;&#20837;&#32988;&#30340;&#31070;&#32463;&#23849;&#28291;&#29305;&#24615;&#12290;&#31070;&#32463;&#23849;&#28291;&#23454;&#38469;&#19978;&#20195;&#34920;&#20102;&#19968;&#20010;&#29366;&#24577;&#65292;&#20854;&#20013;&#26368;&#32456;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#31867;&#22343;&#20540;&#24418;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#12290;&#23613;&#31649;&#36825;&#31181;&#29366;&#24577;&#24456;&#31616;&#21333;&#65292;&#20294;&#36798;&#21040;&#23427;&#30340;&#21160;&#24577;&#21644;&#24433;&#21709;&#36824;&#26377;&#24453;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26377;&#21161;&#20110;&#24314;&#27169;&#31070;&#32463;&#23849;&#28291;&#30340;&#21407;&#21017;&#65292;&#25509;&#30528;&#30740;&#31350;&#20102;&#36825;&#31181;&#29366;&#24577;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#36884;&#24452;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#38024;&#23545;&#20855;&#26377;&#23616;&#37096;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#26080;&#32422;&#26463;&#20984;&#20248;&#21270;&#30340;&#21152;&#36895;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;(APG)&#20197;&#21450;&#32422;&#26463;&#20984;&#20248;&#21270;&#30340;&#19968;&#38454;&#36817;&#31471;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#23547;&#25214;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.01209</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#30340;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient. (arXiv:2206.01209v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#38024;&#23545;&#20855;&#26377;&#23616;&#37096;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#26080;&#32422;&#26463;&#20984;&#20248;&#21270;&#30340;&#21152;&#36895;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;(APG)&#20197;&#21450;&#32422;&#26463;&#20984;&#20248;&#21270;&#30340;&#19968;&#38454;&#36817;&#31471;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#23547;&#25214;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#23616;&#37096;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#21152;&#36895;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#20855;&#26377;LLCG&#30340;&#26080;&#32422;&#26463;&#20984;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#36895;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;(APG)&#26469;&#35299;&#20915;&#23427;&#12290;&#25152;&#25552;&#20986;&#30340;APG&#26041;&#27861;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#32456;&#27490;&#20934;&#21017;&#65292;&#24182;&#19988;&#22312;&#23547;&#25214;&#26080;&#32422;&#26463;&#20984;&#20248;&#21270;&#21644;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#949;-&#27531;&#24046;&#35299;&#26102;&#65292;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;${\cal O}(\varepsilon^{-1/2}\log \varepsilon^{-1})$&#21644;${\cal O}(\log \varepsilon^{-1})$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;LLCG&#30340;&#32422;&#26463;&#20984;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#36817;&#31471;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;APG&#26041;&#27861;&#24212;&#29992;&#20110;&#36817;&#20284;&#27714;&#35299;&#19968;&#31995;&#21015;&#36817;&#31471;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#23376;&#38382;&#39064;&#12290;&#30001;&#27492;&#24471;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#32456;&#27490;&#20934;&#21017;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we develop accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient (LLCG), which is beyond the well-studied class of convex optimization with Lipschitz continuous gradient. In particular, we first consider unconstrained convex optimization with LLCG and propose accelerated proximal gradient (APG) methods for solving it. The proposed APG methods are equipped with a verifiable termination criterion and enjoy an operation complexity of ${\cal O}(\varepsilon^{-1/2}\log \varepsilon^{-1})$ and ${\cal O}(\log \varepsilon^{-1})$ for finding an $\varepsilon$-residual solution of an unconstrained convex and strongly convex optimization problem, respectively. We then consider constrained convex optimization with LLCG and propose an first-order proximal augmented Lagrangian method for solving it by applying one of our proposed APG methods to approximately solve a sequence of proximal augmented Lagrangian subproblems. The resulting method is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DPSNN&#65289;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20445;&#25345;&#20102;SNN&#30340;&#24378;&#38544;&#31169;&#20445;&#25252;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;&#65288;TEP&#65289;&#26041;&#27861;&#65292;&#20351;SNN&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.12718</link><description>&lt;p&gt;
DPSNN: &#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19982;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
DPSNN: A Differentially Private Spiking Neural Network with Temporal Enhanced Pooling. (arXiv:2205.12718v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DPSNN&#65289;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20445;&#25345;&#20102;SNN&#30340;&#24378;&#38544;&#31169;&#20445;&#25252;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;&#65288;TEP&#65289;&#26041;&#27861;&#65292;&#20351;SNN&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#24120;&#24120;&#32467;&#21512;&#20256;&#32479;&#22522;&#20110;&#23454;&#25968;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#20026;&#26032;&#19968;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23545;SNN&#30340;&#38544;&#31169;&#20445;&#25252;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#31639;&#27861;&#19982;SNN&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DPSNN&#65289;&#12290;SNN&#20351;&#29992;&#31163;&#25955;&#30340;&#33033;&#20914;&#24207;&#21015;&#26469;&#20256;&#36755;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;DP&#24341;&#20837;&#30340;&#26799;&#24230;&#22122;&#22768;&#65292;&#20174;&#32780;&#20351;SNN&#20445;&#25345;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20351;SNN&#22312;&#33719;&#24471;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;&#65288;TEP&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#23558;SNN&#30340;&#26102;&#24207;&#20449;&#24687;&#19982;&#31354;&#38388;&#20449;&#24687;&#20256;&#36755;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;SNN&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#20449;&#24687;&#20256;&#36755;&#12290;&#25105;&#20204;&#23545;DPSNN&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy protection is a crucial issue in machine learning algorithms, and the current privacy protection is combined with traditional artificial neural networks based on real values. Spiking neural network (SNN), the new generation of artificial neural networks, plays a crucial role in many fields. Therefore, research on the privacy protection of SNN is urgently needed. This paper combines the differential privacy(DP) algorithm with SNN and proposes a differentially private spiking neural network (DPSNN). The SNN uses discrete spike sequences to transmit information, combined with the gradient noise introduced by DP so that SNN maintains strong privacy protection. At the same time, to make SNN maintain high performance while obtaining high privacy protection, we propose the temporal enhanced pooling (TEP) method. It fully integrates the temporal information of SNN into the spatial information transfer, which enables SNN to perform better information transfer. We conduct experiments on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30693;&#35782;&#25972;&#21512;&#30340;&#39640;&#25928;&#20869;&#23384;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30446;&#26631; Q &#32593;&#32476;&#21521;&#24403;&#21069; Q &#32593;&#32476;&#25972;&#21512;&#30693;&#35782;&#65292;&#20943;&#23569;&#36951;&#24536;&#24182;&#20445;&#25345;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#26412;&#31639;&#27861;&#22312;&#29305;&#24449;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#22823;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#24102;&#26469;&#30340;&#20869;&#23384;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2205.10868</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30693;&#35782;&#25972;&#21512;&#30340;&#39640;&#25928;&#20869;&#23384;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation. (arXiv:2205.10868v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30693;&#35782;&#25972;&#21512;&#30340;&#39640;&#25928;&#20869;&#23384;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30446;&#26631; Q &#32593;&#32476;&#21521;&#24403;&#21069; Q &#32593;&#32476;&#25972;&#21512;&#30693;&#35782;&#65292;&#20943;&#23569;&#36951;&#24536;&#24182;&#20445;&#25345;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#26412;&#31639;&#27861;&#22312;&#29305;&#24449;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#22823;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#24102;&#26469;&#30340;&#20869;&#23384;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#19978;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35757;&#32451;&#38590;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#29420;&#31435;&#25110;&#38750;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#22256;&#38590;&#12290;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#32452;&#20214;&#65292;&#36890;&#24120;&#36890;&#36807;&#23558;&#32463;&#39564;&#23384;&#20648;&#22312;&#22823;&#32531;&#23384;&#22120;&#20013;&#24182;&#24310;&#36831;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#36951;&#24536;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#37325;&#25918;&#32531;&#23384;&#22120;&#20250;&#23548;&#33268;&#37325;&#36127;&#36733;&#20869;&#23384;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20869;&#23384;&#23481;&#37327;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#21644;&#23884;&#20837;&#24335;&#35774;&#22791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230; Q &#32593;&#32476;&#31639;&#27861;&#30340;&#20869;&#23384;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#20174;&#30446;&#26631; Q &#32593;&#32476;&#21521;&#24403;&#21069; Q &#32593;&#32476;&#25972;&#21512;&#30693;&#35782;&#65292;&#20943;&#23569;&#36951;&#24536;&#24182;&#20445;&#25345;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22522;&#20110;&#29305;&#24449;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#22823;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#22270;&#25366;&#25496;&#31639;&#27861;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#22312;&#20154;&#31867;&#20013;&#24515;&#24212;&#29992;&#20013;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#32570;&#20047;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#32676;&#36896;&#25104;&#27495;&#35270;&#12290;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19981;&#21516;&#65292;&#22270;&#25366;&#25496;&#20013;&#30340;&#20844;&#24179;&#24615;&#20855;&#26377;&#29420;&#29305;&#30340;&#20998;&#31867;&#21644;&#23454;&#29616;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20844;&#24179;&#27010;&#24565;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#25216;&#26415;&#20197;&#20419;&#36827;&#22270;&#25366;&#25496;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.09888</link><description>&lt;p&gt;
&#22270;&#25366;&#25496;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Fairness in Graph Mining: A Survey. (arXiv:2204.09888v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#22270;&#25366;&#25496;&#31639;&#27861;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#22312;&#20154;&#31867;&#20013;&#24515;&#24212;&#29992;&#20013;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#32570;&#20047;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#32676;&#36896;&#25104;&#27495;&#35270;&#12290;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19981;&#21516;&#65292;&#22270;&#25366;&#25496;&#20013;&#30340;&#20844;&#24179;&#24615;&#20855;&#26377;&#29420;&#29305;&#30340;&#20998;&#31867;&#21644;&#23454;&#29616;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20844;&#24179;&#27010;&#24565;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#25216;&#26415;&#20197;&#20419;&#36827;&#22270;&#25366;&#25496;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22270;&#25366;&#25496;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#22312;&#21508;&#31181;&#22270;&#20998;&#26512;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#32570;&#20047;&#20844;&#24179;&#24615;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#20154;&#31867;&#20013;&#24515;&#24212;&#29992;&#20013;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20154;&#32676;&#36896;&#25104;&#27495;&#35270;&#12290;&#26368;&#36817;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#22312;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#25968;&#25454;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#30456;&#27604;&#65292;&#22312;&#22270;&#25366;&#25496;&#20013;&#30340;&#20844;&#24179;&#24615;&#20855;&#26377;&#29420;&#29305;&#30340;&#32972;&#26223;&#12289;&#20998;&#31867;&#21644;&#23454;&#29616;&#25216;&#26415;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22312;&#20844;&#24179;&#22270;&#25366;&#25496;&#30340;&#32972;&#26223;&#19979;&#65292;&#20840;&#38754;&#32780;&#26368;&#26032;&#22320;&#20171;&#32461;&#29616;&#26377;&#25991;&#29486;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20844;&#24179;&#27010;&#24565;&#20998;&#31867;&#27861;&#65292;&#26469;&#38416;&#26126;&#20854;&#32852;&#31995;&#21644;&#21306;&#21035;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#20419;&#36827;&#22270;&#25366;&#25496;&#20844;&#24179;&#24615;&#30340;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#26377;&#32452;&#32455;&#30340;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph mining algorithms have been playing a significant role in myriad fields over the years. However, despite their promising performance on various graph analytical tasks, most of these algorithms lack fairness considerations. As a consequence, they could lead to discrimination towards certain populations when exploited in human-centered applications. Recently, algorithmic fairness has been extensively studied in graph-based applications. In contrast to algorithmic fairness on independent and identically distributed (i.i.d.) data, fairness in graph mining has exclusive backgrounds, taxonomies, and fulfilling techniques. In this survey, we provide a comprehensive and up-to-date introduction of existing literature under the context of fair graph mining. Specifically, we propose a novel taxonomy of fairness notions on graphs, which sheds light on their connections and differences. We further present an organized summary of existing techniques that promote fairness in graph mining. Final
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#23458;&#35266;&#30446;&#26631;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#26032;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2203.07120</link><description>&lt;p&gt;
&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#29992;&#20110;&#23458;&#35266;&#30446;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design. (arXiv:2203.07120v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#23458;&#35266;&#30446;&#26631;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#26032;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#31185;&#23398;&#24212;&#29992;&#28041;&#21450;&#23545;&#20855;&#26377;&#35768;&#22810;&#26410;&#30693;&#21442;&#25968;&#30340;&#22797;&#26434;&#19981;&#30830;&#23450;&#31995;&#32479;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#36890;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#30340;&#25104;&#26412;&#21487;&#33021;&#24456;&#39640;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#35774;&#35745;&#24378;&#22823;&#30340;&#25805;&#20316;&#31526;&#65292;&#24182;&#35774;&#35745;&#26368;&#20248;&#23454;&#39564;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#36825;&#20123;&#25805;&#20316;&#31526;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various real-world scientific applications involve the mathematical modeling of complex uncertain systems with numerous unknown parameters. Accurate parameter estimation is often practically infeasible in such systems, as the available training data may be insufficient and the cost of acquiring additional data may be high. In such cases, based on a Bayesian paradigm, we can design robust operators retaining the best overall performance across all possible models and design optimal experiments that can effectively reduce uncertainty to enhance the performance of such operators maximally. While objective-based uncertainty quantification (objective-UQ) based on MOCU (mean objective cost of uncertainty) provides an effective means for quantifying uncertainty in complex systems, the high computational cost of estimating MOCU has been a challenge in applying it to real-world scientific/engineering problems. In this work, we propose a novel scheme to reduce the computational cost for objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;SNN&#30340;&#28145;&#24230;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#20013;&#36755;&#20986;&#20108;&#36827;&#21046;&#21644;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;Atari&#28216;&#25103;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.07211</link><description>&lt;p&gt;
&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#25511;&#21046;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Human-Level Control through Directly-Trained Deep Spiking Q-Networks. (arXiv:2201.07211v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;SNN&#30340;&#28145;&#24230;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#20013;&#36755;&#20986;&#20108;&#36827;&#21046;&#21644;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;Atari&#28216;&#25103;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#31532;&#19977;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30001;&#20110;&#20854;&#39640;&#33021;&#25928;&#32780;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;SNN&#30340;&#28145;&#24230;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#65288;DSRL&#65289;&#30001;&#20110;&#33033;&#20914;&#20989;&#25968;&#36755;&#20986;&#30340;&#20108;&#36827;&#21046;&#20197;&#21450;&#19981;&#21487;&#24494;&#24615;&#31561;&#38382;&#39064;&#65292;&#20173;&#22788;&#20110;&#21021;&#27493;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#65288;DSQN&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#28431;&#30005;&#25972;&#27969;&#19982;&#28779;&#65288;LIF&#65289;&#31070;&#32463;&#20803;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#36866;&#24212;&#20102;&#19968;&#31181;&#30452;&#25509;&#33033;&#20914;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20174;&#29702;&#35770;&#19978;&#35770;&#35777;&#20102;&#22312;DSQN&#20013;&#20351;&#29992;LIF&#31070;&#32463;&#20803;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;17&#20010;&#22312;Atari&#28216;&#25103;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#28216;&#25103;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the third-generation neural networks, Spiking Neural Networks (SNNs) have great potential on neuromorphic hardware because of their high energy-efficiency. However, Deep Spiking Reinforcement Learning (DSRL), i.e., the Reinforcement Learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the non-differentiable property of the spiking function. To address these issues, we propose a Deep Spiking Q-Network (DSQN) in this paper. Specifically, we propose a directly-trained deep spiking reinforcement learning architecture based on the Leaky Integrate-and-Fire (LIF) neurons and Deep Q-Network (DQN). Then, we adapt a direct spiking learning algorithm for the Deep Spiking Q-Network. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiori
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#32593;&#32476;&#26041;&#27861;&#65288;DPMN&#65289;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#21487;&#38752;&#30340;&#23618;&#32423;&#20449;&#24687;&#26102;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20445;&#35777;&#23618;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#30340;&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.13179</link><description>&lt;p&gt;
&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#23618;&#32423;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. (arXiv:2110.13179v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#32593;&#32476;&#26041;&#27861;&#65288;DPMN&#65289;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#21487;&#38752;&#30340;&#23618;&#32423;&#20449;&#24687;&#26102;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20445;&#35777;&#23618;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#30340;&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#33258;&#28982;&#30340;&#20998;&#32452;&#32467;&#26500;&#26102;&#20986;&#29616;&#65292;&#24182;&#38656;&#35201;&#22312;&#19981;&#21516;&#23618;&#27425;&#30340;&#32858;&#21512;&#21644;&#20998;&#35299;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#65292;&#36890;&#24120;&#24076;&#26395;&#22312;&#32473;&#23450;&#30340;&#23618;&#27425;&#32467;&#26500;&#20013;&#28385;&#36275;&#32858;&#21512;&#32422;&#26463;&#65292;&#31216;&#20026;&#23618;&#32423;&#19968;&#33268;&#24615;&#12290;&#22312;&#27010;&#29575;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25345;&#19968;&#33268;&#24615;&#21516;&#26102;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#21487;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24403;&#23384;&#22312;&#21487;&#38752;&#30340;&#23618;&#32423;&#20449;&#24687;&#26102;&#65292;&#21487;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#32593;&#32476;&#65288;DPMN&#65289;&#12290;&#23427;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#31181;&#32479;&#35745;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#23618;&#27425;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#36890;&#36807;&#26500;&#24314;&#65292;&#35813;&#27169;&#22411;&#20445;&#35777;&#20102;&#23618;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#39044;&#27979;&#20998;&#24067;&#32858;&#21512;&#21644;&#20998;&#35299;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#8212;M4&#31454;&#36187;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22312;&#38750;&#38598;&#25104;&#26041;&#27861;&#20013;&#65292;DPMN&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical forecasting problems arise when time series have a natural group structure, and predictions at multiple levels of aggregation and disaggregation across the groups are needed. In such problems, it is often desired to satisfy the aggregation constraints in a given hierarchy, referred to as hierarchical coherence in the literature. Maintaining coherence while producing accurate forecasts can be a challenging problem, especially in the case of probabilistic forecasting. We present a novel method capable of accurate and coherent probabilistic forecasts for time series when reliable hierarchical information is present. We call it Deep Poisson Mixture Network (DPMN). It relies on the combination of neural networks and a statistical model for the joint distribution of the hierarchical multivariate time series structure. By construction, the model guarantees hierarchical coherence and provides simple rules for aggregation and disaggregation of the predictive distributions. We perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#40654;&#26364;&#20248;&#21270;&#38382;&#39064;&#30340;&#23884;&#20837;&#21644;&#21830;&#20960;&#20309;&#30340;&#20960;&#20309;&#26223;&#35266;&#32852;&#31995;&#30340;&#36890;&#29992;&#31243;&#24207;&#65292;&#24212;&#29992;&#20110;&#22266;&#23450;&#31209;&#30697;&#38453;&#20248;&#21270;&#21487;&#20197;&#22312;&#40654;&#26364;FOSP&#38598;&#12289;SOSP&#38598;&#21644;&#20005;&#26684;&#38797;&#28857;&#38598;&#19978;&#24314;&#31435;&#31561;&#21516;&#12290;</title><link>http://arxiv.org/abs/2110.12121</link><description>&lt;p&gt;
&#20851;&#20110;&#40654;&#26364;&#22266;&#23450;&#31209;&#30697;&#38453;&#20248;&#21270;&#20013;&#23884;&#20837;&#20960;&#20309;&#21644;&#21830;&#20960;&#20309;&#30340;&#20960;&#20309;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On Geometric Connections of Embedded and Quotient Geometries in Riemannian Fixed-rank Matrix Optimization. (arXiv:2110.12121v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#40654;&#26364;&#20248;&#21270;&#38382;&#39064;&#30340;&#23884;&#20837;&#21644;&#21830;&#20960;&#20309;&#30340;&#20960;&#20309;&#26223;&#35266;&#32852;&#31995;&#30340;&#36890;&#29992;&#31243;&#24207;&#65292;&#24212;&#29992;&#20110;&#22266;&#23450;&#31209;&#30697;&#38453;&#20248;&#21270;&#21487;&#20197;&#22312;&#40654;&#26364;FOSP&#38598;&#12289;SOSP&#38598;&#21644;&#20005;&#26684;&#38797;&#28857;&#38598;&#19978;&#24314;&#31435;&#31561;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#40654;&#26364;&#20248;&#21270;&#38382;&#39064;&#30340;&#23884;&#20837;&#21644;&#21830;&#20960;&#20309;&#30340;&#20960;&#20309;&#26223;&#35266;&#32852;&#31995;&#30340;&#36890;&#29992;&#31243;&#24207;&#12290;&#36890;&#36807;&#23558;&#36890;&#29992;&#31243;&#24207;&#24212;&#29992;&#20110;&#22266;&#23450;&#31209;&#27491;&#23450;&#21322;&#23450;&#65288;PSD&#65289;&#30697;&#38453;&#21644;&#19968;&#33324;&#30697;&#38453;&#20248;&#21270;&#65292;&#25105;&#20204;&#22312;&#27969;&#24418;&#19978;&#27599;&#19968;&#28857;&#37117;&#24314;&#31435;&#20102;&#20004;&#31181;&#20960;&#20309;&#19979;&#30340;&#31934;&#30830;&#40654;&#26364;&#26799;&#24230;&#36830;&#25509;&#65292;&#24182;&#22312;&#40654;&#26364;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#22788;&#30340;&#40654;&#26364;&#40657;&#22622;&#30697;&#38453;&#30340;&#35889;&#20043;&#38388;&#24314;&#31435;&#20102;&#22841;&#26434;&#19981;&#31561;&#24335;&#12290;&#36825;&#20123;&#32467;&#26524;&#31435;&#21363;&#26263;&#31034;&#22266;&#23450;&#31209;&#30697;&#38453;&#20248;&#21270;&#22312;&#23884;&#20837;&#20960;&#20309;&#21644;&#21830;&#20960;&#20309;&#19979;&#40654;&#26364;FOSP&#38598;&#65292;&#40654;&#26364;&#20108;&#38454;&#31283;&#23450;&#28857;&#65288;SOSP&#65289;&#38598;&#21644;&#20005;&#26684;&#38797;&#28857;&#38598;&#30340;&#31561;&#21516;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#22266;&#23450;&#31209;&#30697;&#38453;&#20248;&#21270;&#20013;&#23884;&#20837;&#21644;&#21830;&#20960;&#20309;&#30340;&#39318;&#20010;&#20960;&#20309;&#26223;&#35266;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#20960;&#20309;&#22312;&#40654;&#26364;&#20248;&#21270;&#20013;&#22914;&#20309;&#32852;&#31995;&#30340;&#20855;&#20307;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a general procedure for establishing the geometric landscape connections of a Riemannian optimization problem under the embedded and quotient geometries. By applying the general procedure to the fixed-rank positive semidefinite (PSD) and general matrix optimization, we establish an exact Riemannian gradient connection under two geometries at every point on the manifold and sandwich inequalities between the spectra of Riemannian Hessians at Riemannian first-order stationary points (FOSPs). These results immediately imply an equivalence on the sets of Riemannian FOSPs, Riemannian second-order stationary points (SOSPs), and strict saddles of fixed-rank matrix optimization under the embedded and the quotient geometries. To the best of our knowledge, this is the first geometric landscape connection between the embedded and the quotient geometries for fixed-rank matrix optimization and it provides a concrete example of how these two geometries are connected in Riema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#24314;&#31435;&#20102;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#20013;&#26368;&#20248;&#31574;&#30053;&#21644;&#26368;&#20248;&#21160;&#24577;&#30340;&#31934;&#30830;&#35299;&#26512;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#26512;&#21644;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2106.03931</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#24314;&#31435;&#20102;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#20013;&#26368;&#20248;&#31574;&#30053;&#21644;&#26368;&#20248;&#21160;&#24577;&#30340;&#31934;&#30830;&#35299;&#26512;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#26512;&#21644;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#29289;&#29702;&#23398;&#20013;&#30340;&#27010;&#24565;&#20063;&#20026;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#20248;&#21270;&#30340;&#35299;&#26512;&#35299;&#30446;&#21069;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#32597;&#35265;&#20107;&#20214;&#26465;&#20214;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#20013;&#26368;&#20248;&#31574;&#30053;&#21644;&#26368;&#20248;&#21160;&#24577;&#30340;&#31934;&#30830;&#35299;&#26512;&#32467;&#26524;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#32463;&#36807;&#27169;&#25311;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#8220;&#21033;&#29992;&#8221;&#19982;&#8220;&#35880;&#24910;&#8221;(EvC)&#30340;&#33539;&#24335;&#65292;&#36873;&#21462;&#26435;&#37325;&#20540;&#26368;&#22823;&#21270;&#19988;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2105.13431</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#36125;&#21494;&#26031;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes. (arXiv:2105.13431v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#8220;&#21033;&#29992;&#8221;&#19982;&#8220;&#35880;&#24910;&#8221;(EvC)&#30340;&#33539;&#24335;&#65292;&#36873;&#21462;&#26435;&#37325;&#20540;&#26368;&#22823;&#21270;&#19988;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#30340;&#31163;&#32447;&#27169;&#22411;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#30456;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#33719;&#24471;&#30340;&#31574;&#30053;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#24403;&#37096;&#32626;&#38169;&#35823;&#30340;&#31574;&#30053;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#26102;&#12290;&#20026;&#27492;&#65292;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#35823;&#24046;&#65288;&#25110;&#23398;&#20064;&#27169;&#22411;&#19982;&#23454;&#38469;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65289;&#24182;&#24191;&#27867;&#22320;&#33719;&#24471;&#39118;&#38505;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27491;&#22312;&#37319;&#21462;&#22810;&#31181;&#36884;&#24452;&#12290;&#20294;&#22312;&#26368;&#32456;&#24212;&#29992;&#20013;&#65292;&#20174;&#21738;&#20010;&#35282;&#24230;&#20986;&#21457;&#36873;&#25321;&#22522;&#32447;&#21602;&#65311;&#22312;&#35745;&#31639;&#26102;&#38388;&#19981;&#26159;&#38382;&#39064;&#32780;&#40065;&#26834;&#24615;&#26159;&#39318;&#35201;&#32771;&#34385;&#22240;&#32032;&#30340;&#31163;&#32447;&#24773;&#24418;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#21033;&#29992;&#8221;&#19982;&#8220;&#35880;&#24910;&#8221;(EvC)&#30340;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;(1)&#21033;&#29992;&#36125;&#21494;&#26031;&#24418;&#24335;&#20027;&#24352;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;(2)&#36873;&#25321;&#26368;&#22823;&#21270;&#39118;&#38505;&#26435;&#37325;&#20540;&#21644;&#38271;&#26399;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Offline Model Learning for Planning and in Offline Reinforcement Learning, the limited data set hinders the estimate of the Value function of the relative Markov Decision Process (MDP). Consequently, the performance of the obtained policy in the real world is bounded and possibly risky, especially when the deployment of a wrong policy can lead to catastrophic consequences. For this reason, several pathways are being followed with the scope of reducing the model error (or the distributional shift between the learned model and the true one) and, more broadly, obtaining risk-aware solutions with respect to model uncertainty. But when it comes to the final application which baseline should a practitioner choose? In an offline context where computational time is not an issue and robustness is the priority we propose Exploitation vs Caution (EvC), a paradigm that (1) elegantly incorporates model uncertainty abiding by the Bayesian formalism, and (2) selects the policy that maximizes a ris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#30340;&#21333;&#35843;&#945;&#25955;&#24230;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21487;&#20248;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#20998;&#37327;&#21442;&#25968;&#65292;&#33719;&#24471;&#20102;&#22312;&#22810;&#23792;&#30446;&#26631;&#20998;&#24067;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#19978;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.05684</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#21333;&#35843;&#945;&#25955;&#24230;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Monotonic Alpha-divergence Minimisation for Variational Inference. (arXiv:2103.05684v4 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#30340;&#21333;&#35843;&#945;&#25955;&#24230;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21487;&#20248;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#20998;&#37327;&#21442;&#25968;&#65292;&#33719;&#24471;&#20102;&#22312;&#22810;&#23792;&#30446;&#26631;&#20998;&#24067;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#19978;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31639;&#27861;&#31995;&#21015;&#65292;&#29992;&#20110;&#22312;&#21464;&#20998;&#25512;&#26029;&#19978;&#36827;&#34892;&#945;&#25955;&#24230;&#26368;&#23567;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#30830;&#20445;&#22312;&#27599;&#19968;&#27493;&#20013;&#21464;&#20998;&#20998;&#24067;&#19982;&#21518;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#945;&#25955;&#24230;&#31995;&#32479;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#12290;&#22312;&#20854;&#26368;&#19968;&#33324;&#30340;&#24418;&#24335;&#19979;&#65292;&#21464;&#20998;&#20998;&#24067;&#26159;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#21516;&#26102;&#20248;&#21270;&#27492;&#28151;&#21512;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#20998;&#37327;&#21442;&#25968;&#12290;&#26412;&#25991;&#22312;&#24050;&#26377;&#30340;&#22522;&#20110;&#945;&#25955;&#24230;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#22914;&#26799;&#24230;&#25110;&#24130;&#19979;&#38477;&#26041;&#26696;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#38416;&#36848;&#20102;&#19968;&#31181;&#38598;&#25104;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23454;&#35777;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#23792;&#30446;&#26631;&#20998;&#24067;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#19978;&#37117;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel family of iterative algorithms which carry out $\alpha$-divergence minimisation in a Variational Inference context. They do so by ensuring a systematic decrease at each step in the $\alpha$-divergence between the variational and the posterior distributions. In its most general form, the variational distribution is a mixture model and our framework allows us to simultaneously optimise the weights and components parameters of this mixture model. Our approach permits us to build on various methods previously proposed for $\alpha$-divergence minimisation such as Gradient or Power Descent schemes and we also shed a new light on an integrated Expectation Maximization algorithm. Lastly, we provide empirical evidence that our methodology yields improved results on several multimodal target distributions and on a real data example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2011.05001</link><description>&lt;p&gt;
MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MMD-regularized Unbalanced Optimal Transport. (arXiv:2011.05001v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.05001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#36793;&#38469;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;UOT&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;$\phi$-&#25955;&#24230;&#65288;&#20363;&#22914;KL&#65289;&#30340;&#27491;&#21017;&#21270;&#12290;MMD&#20316;&#20026;&#20114;&#34917;&#30340;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#23478;&#26063;&#20043;&#19968;&#65292;&#22312;UOT&#19978;&#30340;&#20316;&#29992;&#20284;&#20046;&#19981;&#22826;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#65292;&#21033;&#29992;&#23427;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;MMD&#27491;&#21017;&#21270;&#30340;UOT&#65288;MMD-UOT&#65289;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#23545;&#20598;&#32467;&#26524;&#30340;&#19968;&#20010;&#26377;&#36259;&#32467;&#26524;&#26159;MMD-UOT&#35825;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20063;&#23646;&#20110;IPM&#23478;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#65292;&#29992;&#20110;&#20272;&#31639;MMD-UOT&#21644;&#30456;&#24212;&#30340;&#37325;&#24515;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#20272;&#35745;&#35823;&#24046;&#20197;$\mathcal{O}(m^{-1/2})$&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our study is motivated by the observation that existing works on UOT have mainly focused on regularization based on $\phi$-divergence (e.g., KL). The role of MMD, which belongs to the complementary family of integral probability metrics (IPMs), as a regularizer in the context of UOT seems to be less understood. Our main result is based on Fenchel duality, using which we are able to study the properties of MMD-regularized UOT (MMD-UOT). One interesting outcome of this duality result is that MMD-UOT induces a novel metric over measures, which again belongs to the IPM family. Further, we present finite-sample-based convex programs for estimating MMD-UOT and the corresponding barycenter. Under mild conditions, we prove that our convex-program-based estimators are consistent, and the estimation error decays at a rate $\mathcal{O}\left(m^{-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCRP&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#35745;&#31639;&#29305;&#24449;&#30456;&#20851;&#24615;&#20998;&#24067;&#65292;&#20197;&#35780;&#20272;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2008.01468</link><description>&lt;p&gt;
&#29305;&#24449;&#30456;&#20851;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#33945;&#29305;&#21345;&#32599;Dropout&#37319;&#26679;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Feature Relevance Uncertainty: A Monte Carlo Dropout Sampling Approach. (arXiv:2008.01468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCRP&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#35745;&#31639;&#29305;&#24449;&#30456;&#20851;&#24615;&#20998;&#24067;&#65292;&#20197;&#35780;&#20272;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#26159;&#23454;&#29616;&#26234;&#33021;&#31995;&#32479;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#22312;&#38656;&#35201;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21033;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#20837;&#20102;&#35768;&#22810;&#29305;&#24449;&#35299;&#37322;&#25216;&#26415;&#65292;&#24182;&#25104;&#20026;&#39564;&#35777;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#20801;&#35768;&#20851;&#20110;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#38472;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#30340;&#29305;&#24449;&#30456;&#20851;&#24615;&#20998;&#24067;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#21363;Monte Carlo Relevance Propagation (MCRP)&#29992;&#20110;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#35745;&#31639;&#29305;&#24449;&#30456;&#20851;&#24615;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#65292;&#20174;&#32780;&#28145;&#20837;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding decisions made by neural networks is key for the deployment of intelligent systems in real world applications. However, the opaque decision making process of these systems is a disadvantage where interpretability is essential. Many feature-based explanation techniques have been introduced over the last few years in the field of machine learning to better understand decisions made by neural networks and have become an important component to verify their reasoning capabilities. However, existing methods do not allow statements to be made about the uncertainty regarding a feature's relevance for the prediction. In this paper, we introduce Monte Carlo Relevance Propagation (MCRP) for feature relevance uncertainty estimation. A simple but powerful method based on Monte Carlo estimation of the feature relevance distribution to compute feature relevance uncertainty scores that allow a deeper understanding of a neural network's perception and reasoning.
&lt;/p&gt;</description></item><item><title>FLUID&#26159;&#19968;&#20010;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#20102;&#23569;&#26679;&#26412;&#12289;&#25345;&#32493;&#12289;&#36716;&#31227;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#28857;&#65292;&#25903;&#25345;&#22312;&#32447;&#35780;&#20272;&#21644;&#36229;&#20986;&#20998;&#24067;&#35780;&#20272;&#12290;&#23427;&#26377;&#21161;&#20110;&#21457;&#23637;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2007.02519</link><description>&lt;p&gt;
FLUID&#65306;&#19968;&#31181;&#29992;&#20110;&#28789;&#27963;&#24207;&#21015;&#25968;&#25454;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FLUID: A Unified Evaluation Framework for Flexible Sequential Data. (arXiv:2007.02519v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.02519
&lt;/p&gt;
&lt;p&gt;
FLUID&#26159;&#19968;&#20010;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#20102;&#23569;&#26679;&#26412;&#12289;&#25345;&#32493;&#12289;&#36716;&#31227;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#28857;&#65292;&#25903;&#25345;&#22312;&#32447;&#35780;&#20272;&#21644;&#36229;&#20986;&#20998;&#24067;&#35780;&#20272;&#12290;&#23427;&#26377;&#21161;&#20110;&#21457;&#23637;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#26159;IID&#12289;&#22823;&#35268;&#27169;&#21644;&#26631;&#35760;&#20805;&#20998;&#26102;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;&#22312;&#19981;&#29702;&#24819;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#23569;&#26679;&#26412;&#12289;&#25345;&#32493;&#12289;&#36716;&#31227;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#23376;&#39046;&#22495;&#22312;&#22312;&#19981;&#33391;&#26465;&#20214;&#19979;&#30340;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#65292;&#27599;&#20010;&#23376;&#39046;&#22495;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#27934;&#35265;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#26041;&#27861;&#35299;&#20915;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#25353;&#39034;&#24207;&#21040;&#36798;&#25110;&#35757;&#32451;&#31034;&#20363;&#26377;&#38480;&#65292;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#24448;&#24448;&#26080;&#27861;&#39044;&#35265;ML&#31995;&#32479;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23558;&#38754;&#20020;&#30340;&#22256;&#38590;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#22788;&#29702;&#23398;&#20064;&#23454;&#38469;&#35774;&#32622;&#20013;&#20247;&#22810;&#25361;&#25112;&#30340;&#36890;&#29992;ML&#31995;&#32479;&#12290;&#20026;&#20102;&#20419;&#36827;&#36890;&#29992;ML&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#8212;&#8212;FLUID&#65288;&#28789;&#27963;&#24207;&#21015;&#25968;&#25454;&#65289;&#12290;FLUID&#25972;&#21512;&#20102;&#23569;&#26679;&#26412;&#12289;&#25345;&#32493;&#12289;&#36716;&#31227;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36825;&#20123;&#23376;&#39046;&#22495;&#25216;&#26415;&#30340;&#27604;&#36739;&#21644;&#25972;&#21512;&#12290;&#23427;&#25903;&#25345;&#39034;&#24207;&#25968;&#25454;&#12289;&#22312;&#32447;&#35780;&#20272;&#21644;&#36229;&#20986;&#20998;&#24067;&#35780;&#20272;&#65292;&#21516;&#26102;&#20801;&#35768;&#35780;&#20272;&#19981;&#21516;&#30340;&#35821;&#24577;&#12289;&#20219;&#21153;&#21644;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;FLUID&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20351;&#29992;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern ML methods excel when training data is IID, large-scale, and well labeled. Learning in less ideal conditions remains an open challenge. The sub-fields of few-shot, continual, transfer, and representation learning have made substantial strides in learning under adverse conditions; each affording distinct advantages through methods and insights. These methods address different challenges such as data arriving sequentially or scarce training examples, however often the difficult conditions an ML system will face over its lifetime cannot be anticipated prior to deployment. Therefore, general ML systems which can handle the many challenges of learning in practical settings are needed. To foster research towards the goal of general ML methods, we introduce a new unified evaluation framework - FLUID (Flexible Sequential Data). FLUID integrates the objectives of few-shot, continual, transfer, and representation learning while enabling comparison and integration of techniques across thes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#36827;&#34892;iEEG&#30340;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#25552;&#21462;&#25163;&#24037;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/1811.00915</link><description>&lt;p&gt;
&#29992;&#20110;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks for Epileptic Seizure Prediction. (arXiv:1811.00915v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1811.00915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#36827;&#34892;iEEG&#30340;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#25552;&#21462;&#25163;&#24037;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#20934;&#30830;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#23558;&#26377;&#21161;&#20110;&#20811;&#26381;&#24739;&#32773;&#30340;&#19981;&#30830;&#23450;&#21644;&#26080;&#21161;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39045;&#20869;&#33041;&#30005;&#22270;&#65288;iEEG&#65289;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25299;&#25169;&#32467;&#26500;&#36827;&#34892;&#20449;&#21495;&#29305;&#24449;&#30340;&#30830;&#23450;&#21644;&#39044;ictal&#21644;interictal&#27573;&#30340;&#20108;&#20803;&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#22235;&#21482;&#29399;&#21644;&#19977;&#21517;&#24739;&#32773;&#30340;&#38271;&#26399;&#35760;&#24405;&#36827;&#34892;&#20102;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#26041;&#27861;&#30340;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is the most common neurological disorder and an accurate forecast of seizures would help to overcome the patient's uncertainty and helplessness. In this contribution, we present and discuss a novel methodology for the classification of intracranial electroencephalography (iEEG) for seizure prediction. Contrary to previous approaches, we categorically refrain from an extraction of hand-crafted features and use a convolutional neural network (CNN) topology instead for both the determination of suitable signal characteristics and the binary classification of preictal and interictal segments. Three different models have been evaluated on public datasets with long-term recordings from four dogs and three patients. Overall, our findings demonstrate the general applicability. In this work we discuss the strengths and limitations of our methodology.
&lt;/p&gt;</description></item></channel></rss>