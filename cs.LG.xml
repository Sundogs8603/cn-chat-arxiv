<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.06720</link><description>&lt;p&gt;
&#23500;&#25991;&#26412;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#25991;&#23383;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#27969;&#34892;&#30028;&#38754;&#12290;&#20294;&#26159;&#65292;&#23427;&#30340;&#23450;&#21046;&#36873;&#39033;&#26377;&#38480;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#31934;&#30830;&#25551;&#36848;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25903;&#25345;&#23383;&#20307;&#26679;&#24335;&#12289;&#22823;&#23567;&#12289;&#39068;&#33394;&#21644;&#33050;&#27880;&#31561;&#26684;&#24335;&#30340;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#12290;&#25105;&#20204;&#20174;&#23500;&#25991;&#26412;&#20013;&#25552;&#21462;&#27599;&#20010;&#23383;&#30340;&#23646;&#24615;&#65292;&#20197;&#21551;&#29992;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#26041;&#27861;&#26356;&#22909;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word's attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word's region based on cross-attention maps of a vanilla diffusion process using plain text. For each region, we enforce its text attributes by creating region-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06715</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#22320;&#25551;&#36848;&#25152;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;&#65292;&#35299;&#37322;&#26041;&#27861;&#25165;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#39044;&#27979;&#22312;&#29305;&#23450;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#36825;&#21253;&#25324;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26550;&#26500;&#12290;&#20219;&#20309;&#24544;&#23454;&#25551;&#36848;&#36825;&#31181;&#31867;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#37117;&#38656;&#35201;&#19982;&#35813;&#19981;&#21464;&#24615;&#23646;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#12290;&#36890;&#36807;&#36825;&#31181;&#20005;&#26684;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#65288;1&#65289;&#20004;&#20010;&#24230;&#37327;&#26469;&#34913;&#37327;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#23545;&#31216;&#32676;&#30340;&#20581;&#22766;&#24615;;&#65288;2&#65289;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#65307;&#65288;3&#65289;&#25552;&#39640;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#23545;&#31216;&#32676;&#30340;&#19981;&#21464;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#19982;&#19981;&#21516;&#23545;&#31216;&#32676;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#32463;&#39564;&#22320;&#27979;&#37327;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Zip-NeRF &#25216;&#26415;&#65292;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25239;&#38191;&#40831;&#12289;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#24182;&#38477;&#20302;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06706</link><description>&lt;p&gt;
Zip-NeRF&#65306;&#25239;&#38191;&#40831;&#32593;&#26684;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. (arXiv:2304.06706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Zip-NeRF &#25216;&#26415;&#65292;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25239;&#38191;&#40831;&#12289;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#24182;&#38477;&#20302;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#32593;&#26684;&#21270;&#34920;&#31034;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#65292;&#20294;&#32570;&#20047;&#23545;&#27604;&#20363;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#23481;&#26131;&#24341;&#20837;&#38191;&#40831;&#25110;&#20002;&#22833;&#22330;&#26223;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28210;&#26579;&#21644;&#20449;&#21495;&#22788;&#29702;&#24605;&#24819;&#29992;&#20110;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35823;&#24046;&#29575;&#27604;&#20808;&#21069;&#30340;&#25216;&#26415;&#20302;8%&#21040;76%&#65292;&#24182;&#27604; mip-NeRF 360 &#24555;22&#20493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8% - 76% lower than either prior technique, and that trains 22x faster than mip-NeRF 360.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.06701</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Decision Support Policies. (arXiv:2304.06701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#20915;&#31574;&#32773;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#26469;&#25552;&#39640;&#20915;&#31574;&#32467;&#26524;&#65292;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#65292;&#21738;&#31181;&#24418;&#24335;&#30340;&#25903;&#25345;&#20250;&#22312;&#20302;&#25104;&#26412;&#19979;&#23548;&#33268;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#32473;&#23450;&#36755;&#20837;&#26102;&#36873;&#25321;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#30340;&#20915;&#31574;&#32773;&#65292;&#24182;&#23558;&#23398;&#20064;&#21508;&#33258;&#30340;&#31574;&#30053;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20010;&#38382;&#39064;&#26435;&#34913;&#20102;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#12290;&#20351;&#29992;&#38543;&#26426;&#29615;&#22659;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; $\texttt{THREAD}$&#65292;&#36825;&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#26469;&#30830;&#23450;&#25104;&#26412;-&#24615;&#33021;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#35745;&#31639;&#23454;&#39564;&#26469;&#35777;&#26126; $\texttt{THREAD}$ &#30456;&#23545;&#20110;&#32447;&#19979;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855; $\texttt{Modiste}$&#65292;&#23427;&#20026;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#35786;&#26029;&#25552;&#20379;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#12290;$\texttt{Modiste}$ &#20351;&#29992; $\texttt{THREAD}$ &#20026;&#27599;&#20301;&#21307;&#29983;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#24182;&#25512;&#33616;&#20010;&#24615;&#21270;&#30740;&#31350;&#20197;&#20248;&#21270;&#24739;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#24182;&#23558;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#38477;&#33267;&#26368;&#20302;&#12290;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; $\texttt{Modiste}$ &#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual human decision-makers may benefit from different forms of support to improve decision outcomes. However, a key question is which form of support will lead to accurate decisions at a low cost. In this work, we propose learning a decision support policy that, for a given input, chooses which form of support, if any, to provide. We consider decision-makers for whom we have no prior information and formalize learning their respective policies as a multi-objective optimization problem that trades off accuracy and cost. Using techniques from stochastic contextual bandits, we propose $\texttt{THREAD}$, an online algorithm to personalize a decision support policy for each decision-maker, and devise a hyper-parameter tuning strategy to identify a cost-performance trade-off using simulated human behavior. We provide computational experiments to demonstrate the benefits of $\texttt{THREAD}$ compared to offline baselines. We then introduce $\texttt{Modiste}$, an interactive tool that pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Control3Diff&#30340;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;3D GANs&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#30340;&#22810;&#21151;&#33021;&#12289;&#21487;&#25511;&#30340;&#19977;&#32500;&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.06700</link><description>&lt;p&gt;
&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#23398;&#20064;&#21487;&#25511;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Control3Diff&#30340;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;3D GANs&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#30340;&#22810;&#21151;&#33021;&#12289;&#21487;&#25511;&#30340;&#19977;&#32500;&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;2D&#39046;&#22495;&#29983;&#25104;&#24314;&#27169;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#19977;&#32500;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22256;&#38590;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#19977;&#32500;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#38544;&#24335;&#19977;&#32500;&#34920;&#31034;&#38598;&#25104;&#21040;GANs&#20013;&#30340;3D GANs&#22312;&#20165;&#35757;&#32451;&#21333;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;3D GANs&#27809;&#26377;&#25552;&#20379;&#31934;&#30830;&#25511;&#21046;&#22270;&#20687;&#21512;&#25104;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Control3Diff&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;3D GANs&#20248;&#28857;&#30340;&#19977;&#32500;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#30340;&#22810;&#21151;&#33021;&#65292;&#21487;&#25511;&#30340;&#19977;&#32500;&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;&#12290;Control3Diff&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#28508;&#22312;&#30340;&#28508;&#22312;&#20998;&#24067;&#65288;&#21487;&#20197;&#26159;&#22806;&#37096;&#36755;&#20837;&#26465;&#20214;&#19979;&#30340;&#28508;&#22312;&#20998;&#24067;&#65289;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#30452;&#25509;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#29992;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#25511;&#21046;&#36755;&#20837;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#30784;&#20307;&#31995;&#32467;&#26500;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#19979;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#35782;&#21035;&#26410;&#30693;&#25163;&#21183;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#29983;&#25104;&#36136;&#37327;&#21644;&#20998;&#31867;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06696</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25913;&#36827;&#25163;&#21183;&#25968;&#25454;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving novelty detection with generative adversarial networks on hand gesture data. (arXiv:2304.06696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#19979;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#35782;&#21035;&#26410;&#30693;&#25163;&#21183;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#29983;&#25104;&#36136;&#37327;&#21644;&#20998;&#31867;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26694;&#26550;&#19979;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26469;&#35299;&#20915;&#35782;&#21035;&#26410;&#30693;&#25163;&#21183;&#30340;&#38382;&#39064;&#12290;&#29983;&#25104;&#27169;&#22411;&#20250;&#21160;&#24577;&#22320;&#22686;&#21152;&#22312;&#32447;&#25968;&#25454;&#65292;&#24182;&#29992;&#38543;&#26426;&#30446;&#26631;&#21521;&#37327;&#36827;&#34892;&#35757;&#32451;&#65292;&#37492;&#21035;&#27169;&#22411;&#20915;&#23450;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;UC2017 SG&#21644;UC2018 DualMyo&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;&#20351;&#29992;&#36317;&#31163;&#24230;&#37327;&#30596;&#20934;&#29983;&#25104;&#21644;&#30495;&#23454;&#26679;&#26412;&#30340;&#24046;&#36317;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#21644;&#26410;&#30693;&#26679;&#26412;&#19978;&#30340;&#20934;&#30830;&#29575;&#35780;&#20272;&#37492;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26679;&#26412;&#30340;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#65292;&#25152;&#26377;&#31867;&#21035;&#30340;&#24179;&#22343;&#36317;&#31163;&#19978;&#65292;GAN&#29983;&#25104;&#30340;&#26679;&#26412;&#22343;&#26126;&#26174;&#20248;&#20110;&#38543;&#26426;&#20998;&#24067;&#65288;&#22122;&#22768;&#65289;&#12290;&#22312;&#20998;&#31867;&#27979;&#35797;&#20013;&#65292;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#35782;&#21035;&#26410;&#35757;&#32451;&#30340;&#25163;&#21183;&#12290;&#24403;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#35782;&#21035;&#24050;&#35757;&#32451;&#25163;&#21183;&#21644;&#26410;&#35757;&#32451;&#25163;&#21183;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel way of solving the issue of classification of out-of-vocabulary gestures using Artificial Neural Networks (ANNs) trained in the Generative Adversarial Network (GAN) framework. A generative model augments the data set in an online fashion with new samples and stochastic target vectors, while a discriminative model determines the class of the samples. The approach was evaluated on the UC2017 SG and UC2018 DualMyo data sets. The generative models performance was measured with a distance metric between generated and real samples. The discriminative models were evaluated by their accuracy on trained and novel classes. In terms of sample generation quality, the GAN is significantly better than a random distribution (noise) in mean distance, for all classes. In the classification tests, the baseline neural network was not capable of identifying untrained gestures. When the proposed methodology was implemented, we found that there is a trade-off between the detection of trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; OKRidge &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24615;&#65292;&#21644;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06686</link><description>&lt;p&gt;
OKRidge: &#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637; k &#31232;&#30095;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; OKRidge &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24615;&#65292;&#21644;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#30528;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#65292;&#30830;&#23450;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31232;&#30095;&#25511;&#21046;&#26041;&#31243;&#65292;&#36890;&#36807;&#27714;&#35299;&#31232;&#30095;&#23725;&#22238;&#24402;&#38382;&#39064;&#21487;&#20197;&#35777;&#26126;&#26368;&#20248;&#24615;&#65292;&#20197;&#30830;&#23450;&#39537;&#21160;&#22522;&#30784;&#21160;&#24577;&#30340;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; OKRidge &#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#19979;&#30028;&#35745;&#31639;&#26041;&#27861;&#65292;&#28041;&#21450;&#38797;&#28857;&#20844;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#32447;&#24615;&#31995;&#32479;&#25110;&#22522;&#20110; ADMM &#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21478;&#19968;&#20010;&#32447;&#24615;&#31995;&#32479;&#21644;&#21333;&#35843;&#22238;&#24402;&#38382;&#39064;&#26469;&#26377;&#25928;&#22320;&#35745;&#31639;&#36817;&#31471;&#31639;&#23376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21160;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27874;&#26463;&#25628;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#27604;&#21830;&#19994;&#27714;&#35299;&#22120; Gurobi &#35299;&#20915;&#30340;&#29616;&#26377; MIP&#20844;&#24335;&#36816;&#34892;&#26102;&#38388;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20989;&#25968;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#29616;&#26469;&#28304;&#65292;&#32467;&#26524;&#34920;&#26126;DNNs&#20043;&#25152;&#20197;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#23427;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#20855;&#22791;&#19968;&#31181;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36275;&#20197;&#25269;&#28040;&#20989;&#25968;&#25968;&#37327;&#21450;&#22797;&#26434;&#24230;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2304.06670</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#22791;&#20869;&#32622;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do deep neural networks have an inbuilt Occam's razor?. (arXiv:2304.06670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20989;&#25968;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#29616;&#26469;&#28304;&#65292;&#32467;&#26524;&#34920;&#26126;DNNs&#20043;&#25152;&#20197;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#23427;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#20855;&#22791;&#19968;&#31181;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36275;&#20197;&#25269;&#28040;&#20989;&#25968;&#25968;&#37327;&#21450;&#22797;&#26434;&#24230;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#21331;&#36234;&#24615;&#33021;&#24517;&#39035;&#28304;&#33258;&#20110;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#21306;&#20998;&#36825;&#19977;&#20010;&#37096;&#20998;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;DNN&#25152;&#34920;&#36798;&#30340;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;&#32463;&#36807;&#32593;&#32476;&#30830;&#23450;&#30340;&#20989;&#25968;&#20808;&#39564;&#36890;&#36807;&#21033;&#29992;&#26377;&#24207;&#21644;&#28151;&#27788;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#32780;&#21464;&#21270;&#12290;&#23545;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#31867;&#65292;&#25105;&#20204;&#21033;&#29992;&#20989;&#25968;&#30340;&#35823;&#24046;&#35889;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#33021;&#24615;&#30340;&#36817;&#20284;&#12290;&#24403;&#19982;&#20808;&#39564;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#22320;&#39044;&#27979;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;DNN&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#20998;&#26512;&#25581;&#31034;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#36275;&#20197;&#25269;&#28040;&#22797;&#26434;&#24230;&#38543;&#20989;&#25968;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#32780;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#26159;DNNs&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#38142;&#25509;&#26465;&#20214;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#30446;&#26631;&#20248;&#21270;&#32422;&#26463;&#30340;&#19968;&#31181;&#21160;&#24577;&#24179;&#34913;&#26377;&#21521;&#32593;&#32476;&#30340;&#22855;&#25968;&#25159;&#21306;&#38480;&#21046;&#38750;&#32447;&#24615;&#26144;&#23556;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06667</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#29702;&#24819;&#38142;&#25509;&#30340;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;D-SVM
&lt;/p&gt;
&lt;p&gt;
D-SVM over Networked Systems with Non-Ideal Linking Conditions. (arXiv:2304.06667v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#38142;&#25509;&#26465;&#20214;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#30446;&#26631;&#20248;&#21270;&#32422;&#26463;&#30340;&#19968;&#31181;&#21160;&#24577;&#24179;&#34913;&#26377;&#21521;&#32593;&#32476;&#30340;&#22855;&#25968;&#25159;&#21306;&#38480;&#21046;&#38750;&#32447;&#24615;&#26144;&#23556;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36890;&#36807;&#20998;&#24067;&#24335;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;D-SVM&#65289;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#19968;&#20123;&#38142;&#25509;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#12290;&#20195;&#29702;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21327;&#21516;&#35299;&#20915;&#20849;&#35782;&#32422;&#26463;&#20998;&#24067;&#24335;&#20248;&#21270;&#65292;&#32780;&#38142;&#25509;&#21017;&#21463;&#21040;&#24378;&#28872;&#30340;&#31526;&#21495;&#20445;&#25345;&#22855;&#24322;&#38750;&#32447;&#24615;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#23545;&#25968;&#37327;&#21270;&#21644;&#25130;&#21462;&#65288;&#39281;&#21644;&#65289;&#26159;&#36825;&#26679;&#30340;&#38750;&#32447;&#24615;&#20363;&#23376;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#32771;&#34385;&#29702;&#24819;&#38142;&#25509;&#21644;&#36890;&#36807;&#32447;&#24615;&#36890;&#36947;&#36827;&#34892;&#23436;&#32654;&#20449;&#24687;&#20132;&#25442;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#33324;&#30340;&#25159;&#21306;&#38480;&#21046;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;&#21040;&#22312;&#21160;&#24577;&#24179;&#34913;&#30340;&#26377;&#21521;&#32593;&#32476;&#19978;&#25910;&#25947;&#21040;&#20248;&#21270;&#22120;&#65288;&#21363;SVM&#20998;&#31867;&#22120;&#65289;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#20219;&#20309;&#22855;&#25968;&#25159;&#21306;&#38480;&#21046;&#38750;&#32447;&#24615;&#26144;&#23556;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#21160;&#21147;&#23398;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21160;&#21147;&#23398;&#22987;&#32456;&#20855;&#26377;&#19968;&#20010;&#38646;&#29305;&#24449;&#20540;&#65288;&#19982;&#20849;&#35782;&#30456;&#20851;&#65289;&#21644;&#20854;&#20182;&#29305;&#24449;&#20540;&#37117;&#26159;&#36127;&#23454;&#25968;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers distributed optimization algorithms, with application in binary classification via distributed support-vector-machines (D-SVM) over multi-agent networks subject to some link nonlinearities. The agents solve a consensus-constraint distributed optimization cooperatively via continuous-time dynamics, while the links are subject to strongly sign-preserving odd nonlinear conditions. Logarithmic quantization and clipping (saturation) are two examples of such nonlinearities. In contrast to existing literature that mostly considers ideal links and perfect information exchange over linear channels, we show how general sector-bounded models affect the convergence to the optimizer (i.e., the SVM classifier) over dynamic balanced directed networks. In general, any odd sector-bounded nonlinear mapping can be applied to our dynamics. The main challenge is to show that the proposed system dynamics always have one zero eigenvalue (associated with the consensus) and the other eigen
&lt;/p&gt;</description></item><item><title>G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06653</link><description>&lt;p&gt;
G2T: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06653
&lt;/p&gt;
&lt;p&gt;
G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#32858;&#31867;&#30340;&#20027;&#39064;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#30340;&#35789;&#35821;&#31579;&#36873;&#26041;&#27861;&#32858;&#31867;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#29983;&#25104;&#27604;&#29983;&#25104;&#24335;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#26356;&#22909;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#36873;&#25321;&#21512;&#36866;&#21442;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#24573;&#30053;&#21333;&#35789;&#19982;&#20027;&#39064;&#21450;&#20027;&#39064;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22270;&#20027;&#39064;&#65288;G2T&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#25945;&#24072;&#35780;&#20272;&#35777;&#26126;&#36825;&#20123;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#26377;&#26497;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25945;&#32946;&#38382;&#39064;&#26377;&#22810;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Useful are Educational Questions Generated by Large Language Models?. (arXiv:2304.06638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#25945;&#24072;&#35780;&#20272;&#35777;&#26126;&#36825;&#20123;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#26377;&#26497;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#23545;&#20110;&#25945;&#24072;&#21644;&#23398;&#29983;&#26469;&#35828;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#29983;&#25104;&#21487;&#20197;&#22823;&#24133;&#20943;&#36731;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#20182;&#20204;&#25945;&#23398;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#33021;&#34920;&#26126;&#30495;&#27491;&#30340;&#25945;&#24072;&#35780;&#21028;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#26159;&#21542;&#36275;&#22815;&#26377;&#29992;&#65292;&#25110;&#32773;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#38169;&#35823;&#21644;/&#25110;&#25945;&#23398;&#20869;&#23481;&#30340;&#24110;&#21161;&#19981;&#22823;&#12290;&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#25945;&#24072;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#65288;Bloom's&#21644;&#38590;&#24230;&#20998;&#31867;&#65289;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#26377;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#23637;&#31034;&#20102;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation (CTG) by large language models has a huge potential to transform education for teachers and students alike. Specifically, high quality and diverse question generation can dramatically reduce the load on teachers and improve the quality of their educational content. Recent work in this domain has made progress with generation, but fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content. We conduct a human evaluation with teachers to assess the quality and usefulness of outputs from combining CTG and question taxonomies (Bloom's and a difficulty taxonomy). The results demonstrate that the questions generated are high quality and sufficiently useful, showing their promise for widespread use in the classroom setting.
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06634</link><description>&lt;p&gt;
PGTask&#65306;&#20171;&#32461;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#26723;&#26696;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06634
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26469;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#20449;&#24687;&#31232;&#23569;&#19988;&#38590;&#20197;&#33719;&#21462;&#65292;&#36825;&#20351;&#24471;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#25104;&#20026;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#12290;&#25105;&#20204;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#30456;&#20851;&#35805;&#35821;&#23545;&#40784;&#30340;&#26723;&#26696;&#21477;&#23376;&#65292;&#20174;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26723;&#26696;&#29983;&#25104;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26723;&#26696;&#29983;&#25104;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoSDA&#30340;&#25345;&#32493;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#21452;&#36895;&#24230;&#20248;&#21270;&#30340;&#24072;&#29983;&#27169;&#22411;&#23545;&#24182;&#37197;&#22791;&#26377;&#19968;&#33268;&#24615;&#23398;&#20064;&#33021;&#21147;&#65292;&#26088;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.06627</link><description>&lt;p&gt;
CoSDA: &#25345;&#32493;&#30340;&#26080;&#28304;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
CoSDA: Continual Source-Free Domain Adaptation. (arXiv:2304.06627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoSDA&#30340;&#25345;&#32493;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#21452;&#36895;&#24230;&#20248;&#21270;&#30340;&#24072;&#29983;&#27169;&#22411;&#23545;&#24182;&#37197;&#22791;&#26377;&#19968;&#33268;&#24615;&#23398;&#20064;&#33021;&#21147;&#65292;&#26088;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#22495;&#36866;&#24212;&#65288;SFDA&#65289;&#26159;&#25351;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#28304;&#22495;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#30446;&#26631;&#22495;&#12290;&#30001;&#20110;&#38656;&#35201;&#20445;&#25252;&#28304;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;SFDA&#26368;&#36817;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#65292;&#23427;&#20250;&#22312;&#28304;&#22495;&#19978;&#20986;&#29616;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#29616;&#35937;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#37325;&#26032;&#23454;&#29616;&#20102;&#20197;&#24448;&#30340;SFDA&#26041;&#27861;&#65292;&#24182;&#22312;&#22235;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#36866;&#24212;&#24615;&#22686;&#30410;&#21644;&#36951;&#24536;&#25439;&#22833;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26435;&#34913;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoSDA&#30340;&#25345;&#32493;&#26080;&#28304;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#37319;&#29992;&#21452;&#36895;&#24230;&#20248;&#21270;&#30340;&#24072;&#29983;&#27169;&#22411;&#23545;&#65292;&#24182;&#37197;&#22791;&#26377;&#19968;&#33268;&#24615;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CoSDA&#22312;&#25345;&#32493;&#36866;&#24212;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without access to the source data, source-free domain adaptation (SFDA) transfers knowledge from a source-domain trained model to target domains. Recently, SFDA has gained popularity due to the need to protect the data privacy of the source domain, but it suffers from catastrophic forgetting on the source domain due to the lack of data. To systematically investigate the mechanism of catastrophic forgetting, we first reimplement previous SFDA approaches within a unified framework and evaluate them on four benchmarks. We observe that there is a trade-off between adaptation gain and forgetting loss, which motivates us to design a consistency regularization to mitigate forgetting. In particular, we propose a continual source-free domain adaptation approach named CoSDA, which employs a dual-speed optimized teacher-student model pair and is equipped with consistency learning capability. Our experiments demonstrate that CoSDA outperforms state-of-the-art approaches in continuous adaptation. N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#26080;&#25439;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#65292;&#24182;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06600</link><description>&lt;p&gt;
&#26080;&#25439;&#35843;&#25972;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#20197;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation. (arXiv:2304.06600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#26080;&#25439;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#65292;&#24182;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#24120;&#29992;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20026;&#21508;&#31181;&#19987;&#38376;&#30340;&#24863;&#30693;&#38382;&#39064;&#20197;&#21450;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#20102;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#20840;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#24102;&#26469;&#26174;&#30528;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24494;&#35843;&#20250;&#30772;&#22351;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#23548;&#33268;&#20195;&#34920;&#24615;&#21521;&#24494;&#35843;&#20219;&#21153;&#30340;&#28418;&#31227;&#65292;&#20174;&#32780;&#23548;&#33268;&#21407;&#22987;&#27169;&#22411;&#30340;&#22810;&#29992;&#24615;&#20002;&#22833;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#26080;&#25439;&#36866;&#24212;&#8221;&#26469;&#35299;&#20915;&#32463;&#20856;&#24494;&#35843;&#30340;&#36825;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36866;&#24403;&#25918;&#32622;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#37197;&#22120;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#24615;&#33021;&#24046;&#36317;&#65292;&#20174;&#32780;&#20351;&#20923;&#32467;&#39044;&#35757;&#32451;&#34920;&#31034;&#19982;&#20840;&#31471;&#21040;&#31471;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#32780;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large models pretrained on common visual learning tasks can provide useful representations for a wide range of specialized perception problems, as well as a variety of robotic manipulation tasks. While prior work on robotic manipulation has predominantly used frozen pretrained features, we demonstrate that in robotics this approach can fail to reach optimal performance, and that fine-tuning of the full model can lead to significantly better results. Unfortunately, fine-tuning disrupts the pretrained visual representation, and causes representational drift towards the fine-tuned task thus leading to a loss of the versatility of the original model. We introduce "lossless adaptation" to address this shortcoming of classical fine-tuning. We demonstrate that appropriate placement of our parameter efficient adapters can significantly reduce the performance gap between frozen pretrained representations and full end-to-end fine-tuning without changes to the origina
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#19968;&#31995;&#21015;&#36229;&#20986;&#27425;&#27169;&#24615;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#32780;&#19981;&#29306;&#29298;&#22826;&#22810;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06596</link><description>&lt;p&gt;
&#36229;&#36234;&#27425;&#27169;&#24615;&#65306;&#24102;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#38598;&#21512;&#36873;&#25321;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beyond Submodularity: A Unified Framework of Randomized Set Selection with Group Fairness Constraints. (arXiv:2304.06596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#19968;&#31995;&#21015;&#36229;&#20986;&#27425;&#27169;&#24615;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#32780;&#19981;&#29306;&#29298;&#22826;&#22810;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20010;&#37325;&#35201;&#20915;&#31574;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#23450;&#21521;&#24191;&#21578;&#23637;&#31034;&#12289;&#23478;&#24237;&#36151;&#27454;&#25209;&#20934;&#21644;&#29359;&#32618;&#34892;&#20026;&#39044;&#27979;&#31561;&#12290;&#37492;&#20110;&#36825;&#20123;&#31639;&#27861;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#20851;&#38190;&#22312;&#20110;&#23427;&#20204;&#36816;&#20316;&#24212;&#35813;&#20844;&#24179;&#65292;&#27809;&#26377;&#20559;&#35265;&#25110;&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#23545;&#20110;&#20419;&#36827;&#24179;&#31561;&#21644;&#36991;&#20813;&#27495;&#35270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#28041;&#21450;&#20840;&#23616;&#25928;&#29992;&#20989;&#25968;&#20197;&#21450;&#27599;&#20010;&#32452;&#30340;&#19968;&#32452;&#25928;&#29992;&#20989;&#25968;&#65292;&#20854;&#20013;&#19968;&#20010;&#32452;&#25351;&#20849;&#20139;&#30456;&#21516;&#23646;&#24615;&#65288;&#20363;&#22914;&#24615;&#21035;&#65289;&#30340;&#20010;&#20307;&#32452;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36328;&#21487;&#34892;&#23376;&#38598;&#30340;&#20998;&#24067;&#65292;&#25351;&#23450;&#27599;&#20010;&#21487;&#34892;&#38598;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#25928;&#29992;&#20989;&#25968;&#24182;&#28385;&#36275;&#39044;&#23450;&#37197;&#39069;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms play an important role in a variety of important decision-making processes, including targeted advertisement displays, home loan approvals, and criminal behavior predictions. Given the far-reaching impact of these algorithms, it is crucial that they operate fairly, free from bias or prejudice towards certain groups in the population. Ensuring impartiality in these algorithms is essential for promoting equality and avoiding discrimination. To this end we introduce a unified framework for randomized subset selection that incorporates group fairness constraints. Our problem involves a global utility function and a set of group utility functions for each group, here a group refers to a group of individuals (e.g., people) sharing the same attributes (e.g., gender). Our aim is to generate a distribution across feasible subsets, specifying the selection probability of each feasible set, to maximize the global utility function while meeting a predetermined quota for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#35299;&#20915;&#24352;&#37327;&#20302;&#21608;&#26399;&#31209;&#36817;&#20284;&#38382;&#39064;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#37325;&#35201;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.06594</link><description>&lt;p&gt;
&#35299;&#20915;&#24352;&#37327;&#20302;&#21608;&#26399;&#31209;&#36817;&#20284;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Tensor Low Cycle Rank Approximation. (arXiv:2304.06594v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#35299;&#20915;&#24352;&#37327;&#20302;&#21608;&#26399;&#31209;&#36817;&#20284;&#38382;&#39064;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#37325;&#35201;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20107;&#29289;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#31361;&#30772;&#24615;&#24037;&#20316;[Zhao, Panigrahi, Ge, and Arora Arxiv 2023]&#20174;&#27010;&#29575;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(PCFG)&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#22312;&#35745;&#31639;PCFG&#27010;&#29575;&#30340;&#26680;&#24515;&#35745;&#31639;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#29305;&#23450;&#30340;&#24352;&#37327;&#20302;&#21608;&#26399;&#31209;&#36817;&#20284;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24352;&#37327;&#21608;&#26399;&#31209;&#12290;&#32473;&#23450;&#19968;&#20010;$n\times n\times n$&#30340;&#19977;&#38454;&#24352;&#37327;$A$&#65292;&#22914;&#26524;&#23384;&#22312;&#19977;&#20010;$n\times k^2$&#22823;&#23567;&#30340;&#30697;&#38453;$U,V,W$&#65292;&#28385;&#36275;&#23545;&#20110;&#27599;&#20010;&#26465;&#30446;&#20013;&#30340;&#27599;&#20010;$A_{a,b,c}$&#65292;&#37117;&#26377;\begin{align*} A_{a,b,c} = \sum_{i=1}^k \sum_{j=1}^k \sum_{l=1}^k U_{a,i+k(j-1)} \otimes V_{b, j + k(l-1)} \otimes W_{c, l + k(i-1) } \end{align*},&#23545;&#20110;&#25152;&#26377;&#30340;$a \in [n], b \in [n], c \in [n]$&#65292;&#21017;&#31216;$A$&#20855;&#26377;&#21608;&#26399;&#31209;-$k$&#12290;&#23545;&#20110;&#24352;&#37327;&#32463;&#20856;&#31209;&#12289;Tucker&#31209;&#21644;Train&#31209;&#31561;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;[SOD
&lt;/p&gt;
&lt;p&gt;
Large language models have become ubiquitous in modern life, finding applications in various domains such as natural language processing, language translation, and speech recognition. Recently, a breakthrough work [Zhao, Panigrahi, Ge, and Arora Arxiv 2023] explains the attention model from probabilistic context-free grammar (PCFG). One of the central computation task for computing probability in PCFG is formulating a particular tensor low rank approximation problem, we can call it tensor cycle rank. Given an $n \times n \times n$ third order tensor $A$, we say that $A$ has cycle rank-$k$ if there exists three $n \times k^2$ size matrices $U , V$, and $W$ such that for each entry in each \begin{align*} A_{a,b,c} = \sum_{i=1}^k \sum_{j=1}^k \sum_{l=1}^k U_{a,i+k(j-1)} \otimes V_{b, j + k(l-1)} \otimes W_{c, l + k(i-1) } \end{align*} for all $a \in [n], b \in [n], c \in [n]$. For the tensor classical rank, tucker rank and train rank, it has been well studied in [Song, Woodruff, Zhong SOD
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#26679;&#26412;&#20135;&#29983;&#30340;&#21407;&#22240;&#26159;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#36817;&#20284;&#21452;&#23556;&#26144;&#23556;&#25152;&#35268;&#23450;&#30340;&#32500;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#30340;&#36817;&#20284;&#19981;&#36830;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06575</link><description>&lt;p&gt;
&#20855;&#26377;&#32500;&#24230;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples from Dimensional Invariance. (arXiv:2304.06575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06575
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#20135;&#29983;&#30340;&#21407;&#22240;&#26159;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#36817;&#20284;&#21452;&#23556;&#26144;&#23556;&#25152;&#35268;&#23450;&#30340;&#32500;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#30340;&#36817;&#20284;&#19981;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#24050;&#32463;&#34987;&#21457;&#29616;&#38024;&#23545;&#21508;&#31181;&#28145;&#24230;&#21644;&#27973;&#23618;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#26102;&#34987;&#24314;&#35758;&#26159;&#21487;&#20462;&#22797;&#30340;&#27169;&#22411;&#29305;&#23450;&#38169;&#35823;&#65292;&#25110;&#32773;&#26159;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#65292;&#25110;&#32773;&#20004;&#32773;&#20860;&#22791;&#12290;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;&#23545;&#25239;&#26679;&#26412;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#23450;&#20102;&#36817;&#20284;&#21452;&#23556;&#26144;&#23556;$f:\Bbb R^n \to \Bbb R^m; n \neq m$&#65292;&#20854;&#36755;&#20837;&#30340;&#36817;&#20284;&#19981;&#36830;&#32493;&#24615;&#20135;&#29983;&#30340;&#65292;&#32780;&#36825;&#31181;&#19981;&#36830;&#32493;&#24615;&#26159;&#30001;&#20110;&#32500;&#24230;&#30340;&#25299;&#25169;&#19981;&#21464;&#24615;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples have been found for various deep as well as shallow learning models, and have at various times been suggested to be either fixable model-specific bugs, or else inherent dataset feature, or both. We present theoretical and empirical results to show that adversarial examples are approximate discontinuities resulting from models that specify approximately bijective maps $f: \Bbb R^n \to \Bbb R^m; n \neq m$ over their inputs, and this discontinuity follows from the topological invariance of dimension.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#31639;&#27861;&#26469;&#23398;&#20064;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.06574</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#26080;&#27861;&#20174;&#20855;&#26377;&#26410;&#30693;&#22122;&#22768;&#29575;&#30340;&#22024;&#26434;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;(arXiv:2304.06574v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Bayes classifier cannot be learned from noisy responses with unknown noise rates. (arXiv:2304.06574v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#31639;&#27861;&#26469;&#23398;&#20064;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#22024;&#26434;&#30340;&#26631;&#31614;&#35757;&#32451;&#20998;&#31867;&#22120;&#36890;&#24120;&#38656;&#35201;&#23398;&#20064;&#32773;&#25351;&#23450;&#26631;&#31614;&#22122;&#38899;&#30340;&#20998;&#24067;&#65292;&#20294;&#22312;&#23454;&#38469;&#20013;&#24448;&#24448;&#26159;&#26410;&#30693;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#23581;&#35797;&#25918;&#23485;&#35813;&#35201;&#27714;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#22024;&#26434;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#26159;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;&#36825;&#34920;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#32469;&#36807;&#25110;&#25918;&#23485;&#35813;&#35201;&#27714;&#12290;&#22312;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#34987;&#35782;&#21035;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#36125;&#21494;&#26031;&#20915;&#31574;&#35268;&#21017;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a classifier with noisy labels typically requires the learner to specify the distribution of label noise, which is often unknown in practice. Although there have been some recent attempts to relax that requirement, we show that the Bayes decision rule is unidentified in most classification problems with noisy labels. This suggests it is generally not possible to bypass/relax the requirement. In the special cases in which the Bayes decision rule is identified, we develop a simple algorithm to learn the Bayes decision rule, that does not require knowledge of the noise distribution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#27169;&#22359;&#21270;&#30340; R6 &#25509;&#21475;&#65292;&#29992;&#20110;&#20855;&#20307;&#23454;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#29616;&#19977;&#31181;&#26041;&#27861;&#24182;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#32467;&#21512;&#30495;&#23454;&#29992;&#20363;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#24471;&#20986;&#26377;&#20851;&#22914;&#20309;&#26356;&#25913;&#21333;&#20010;&#35266;&#27979;&#20540;&#30340;&#29305;&#24449;&#20540;&#20197;&#33719;&#24471;&#25152;&#38656;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.06569</link><description>&lt;p&gt;
counterfactuals: &#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340; R &#21253;
&lt;/p&gt;
&lt;p&gt;
counterfactuals: An R Package for Counterfactual Explanation Methods. (arXiv:2304.06569v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#27169;&#22359;&#21270;&#30340; R6 &#25509;&#21475;&#65292;&#29992;&#20110;&#20855;&#20307;&#23454;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#29616;&#19977;&#31181;&#26041;&#27861;&#24182;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#32467;&#21512;&#30495;&#23454;&#29992;&#20363;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#24471;&#20986;&#26377;&#20851;&#22914;&#20309;&#26356;&#25913;&#21333;&#20010;&#35266;&#27979;&#20540;&#30340;&#29305;&#24449;&#20540;&#20197;&#33719;&#24471;&#25152;&#38656;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#26356;&#25913;&#21333;&#20010;&#35266;&#27979;&#20540;&#30340;&#29305;&#24449;&#20540;&#20197;&#33719;&#24471;&#25152;&#38656;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#20855;&#26377;&#24191;&#27867;&#21464;&#21270;&#30340;&#25509;&#21475;&#21644;&#35201;&#27714;&#30340;&#23454;&#29616;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461; counterfactuals R &#21253;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110; R6 &#30340;&#27169;&#22359;&#21270;&#21644;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#29616;&#20102;&#19977;&#31181;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#36873;&#30340;&#26041;&#27861;&#23398;&#25193;&#23637;&#65292;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#24182;&#20351;&#20854;&#26356;&#20855;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#29992;&#20363;&#35299;&#37322;&#20102;&#21253;&#30340;&#32467;&#26500;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#20182;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#38598;&#25104;&#21040;&#21253;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#23454;&#26045;&#30340;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanation methods provide information on how feature values of individual observations must be changed to obtain a desired prediction. Despite the increasing amount of proposed methods in research, only a few implementations exist whose interfaces and requirements vary widely. In this work, we introduce the counterfactuals R package, which provides a modular and unified R6-based interface for counterfactual explanation methods. We implemented three existing counterfactual explanation methods and propose some optional methodological extensions to generalize these methods to different scenarios and to make them more comparable. We explain the structure and workflow of the package using real use cases and show how to integrate additional counterfactual explanation methods into the package. In addition, we compared the implemented methods for a variety of models and datasets with regard to the quality of their counterfactual explanations and their runtime behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#38382;&#39064;&#65292;&#24341;&#20837;&#21442;&#25968;&#34892;&#21160;&#20197;&#25552;&#39640;&#35757;&#32451;&#26102;&#38388;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#31181;&#26368;&#24378;&#22823;&#30340;&#28145;&#24230;RL&#26041;&#27861;&#65292;A2C&#12289;DQN&#21644;Rainbow&#37117;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06567</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning applied to an assembly sequence planning problem with user preferences. (arXiv:2304.06567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#38382;&#39064;&#65292;&#24341;&#20837;&#21442;&#25968;&#34892;&#21160;&#20197;&#25552;&#39640;&#35757;&#32451;&#26102;&#38388;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#31181;&#26368;&#24378;&#22823;&#30340;&#28145;&#24230;RL&#26041;&#27861;&#65292;A2C&#12289;DQN&#21644;Rainbow&#37117;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#21046;&#36896;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#38543;&#30528;&#23454;&#38469;&#25805;&#20316;&#32780;&#36827;&#34892;&#31995;&#32479;&#23398;&#20064;&#30340;&#29615;&#22659;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;DRL&#26041;&#27861;&#24212;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#65288;ASP&#65289;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;RL&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#21442;&#25968;&#34892;&#21160;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26102;&#38388;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22870;&#21169;&#20449;&#21495;&#65306;&#65288;1&#65289;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#65288;2&#65289;&#24635;&#35013;&#37197;&#26102;&#38388;&#12290;&#29992;&#25143;&#30340;&#20559;&#22909;&#20449;&#21495;&#35299;&#20915;&#20102;&#35013;&#37197;&#36807;&#31243;&#20013;&#20154;&#31867;&#38754;&#20020;&#30340;&#22256;&#38590;&#21644;&#38750;&#20154;&#20307;&#24037;&#25928;&#29305;&#24615;&#65292;&#32780;&#24635;&#35013;&#37197;&#26102;&#38388;&#20449;&#21495;&#21017;&#24378;&#21046;&#25191;&#34892;&#35013;&#37197;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#26368;&#24378;&#22823;&#30340;&#28145;&#24230;RL&#26041;&#27861; A2C&#12289;DQN &#21644; Rainbow&#65292;&#24182;&#22312;&#20004;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has demonstrated its potential in solving complex manufacturing decision-making problems, especially in a context where the system learns over time with actual operation in the absence of training data. One interesting and challenging application for such methods is the assembly sequence planning (ASP) problem. In this paper, we propose an approach to the implementation of DRL methods in ASP. The proposed approach introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: (1) user's preferences and (2) total assembly time duration. The user's preferences signal addresses the difficulties and non-ergonomic properties of the assembly faced by the human and the total assembly time signal enforces the optimization of the assembly. Three of the most powerful deep RL methods were studied, Advantage Actor-Critic (A2C), Deep Q-Learning (DQN), and Rainbow, in two different scenarios:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65306;&#20132;&#25442;&#24459;&#32852;&#37030;&#23398;&#20064;&#21644;&#36718;&#27969;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#33021;&#32791;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.06551</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#21644;&#33021;&#32791;
&lt;/p&gt;
&lt;p&gt;
Decentralized federated learning methods for reducing communication cost and energy consumption in UAV networks. (arXiv:2304.06551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65306;&#20132;&#25442;&#24459;&#32852;&#37030;&#23398;&#20064;&#21644;&#36718;&#27969;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#33021;&#32791;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#29616;&#20195;&#26234;&#24935;&#22478;&#24066;&#20013;&#25198;&#28436;&#30528;&#24456;&#22810;&#35282;&#33394;&#65292;&#22914;&#29289;&#21697;&#37197;&#36865;&#12289;&#23454;&#26102;&#36947;&#36335;&#20132;&#36890;&#22320;&#22270;&#21046;&#20316;&#21644;&#27745;&#26579;&#30417;&#27979;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26080;&#20154;&#26426;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#33021;&#37327;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#35753;&#26080;&#20154;&#26426;&#22312;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#38656;&#35201;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#32858;&#21512;&#26080;&#20154;&#26426;&#30340;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#12290;&#22914;&#26524;&#20013;&#22830;&#26381;&#21153;&#22120;&#21457;&#29983;&#25925;&#38556;&#65292;&#23558;&#20250;&#23545;&#35757;&#32451;&#36896;&#25104;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20132;&#25442;&#24459;&#32852;&#37030;&#23398;&#20064;&#21644;&#36718;&#27969;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22522;&#20110;&#24050;&#26377;&#30340;&#21435;&#20013;&#24515;&#21270;&#26080;&#20154;&#26426;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#22312;&#27599;&#20010;&#26080;&#20154;&#26426;&#30340;&#27169;&#22411;&#21442;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#21644;&#33021;&#32791;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicles (UAV) or drones play many roles in a modern smart city such as the delivery of goods, mapping real-time road traffic and monitoring pollution. The ability of drones to perform these functions often requires the support of machine learning technology. However, traditional machine learning models for drones encounter data privacy problems, communication costs and energy limitations. Federated Learning, an emerging distributed machine learning approach, is an excellent solution to address these issues. Federated learning (FL) allows drones to train local models without transmitting raw data. However, existing FL requires a central server to aggregate the trained model parameters of the UAV. A failure of the central server can significantly impact the overall training. In this paper, we propose two aggregation methods: Commutative FL and Alternate FL, based on the existing architecture of decentralised Federated Learning for UAV Networks (DFL-UN) by adding a unique
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#37325;&#23545;&#27604;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;IMU&#23450;&#21521;&#20272;&#35745;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06548</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26680;&#37325;&#23545;&#27604;&#25439;&#22833;&#30340;IMU&#23450;&#21521;&#20272;&#35745;&#65306;&#26799;&#24230;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-kernel Correntropy-based Orientation Estimation of IMUs: Gradient Descent Methods. (arXiv:2304.06548v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#37325;&#23545;&#27604;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;IMU&#23450;&#21521;&#20272;&#35745;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMUs&#65289;&#30340;&#23450;&#21521;&#20272;&#35745;&#65306;&#37325;&#23545;&#27604;&#26799;&#24230;&#19979;&#38477;&#65288;CGD&#65289;&#21644;&#37325;&#23545;&#27604;&#35299;&#32806;&#23450;&#21521;&#20272;&#35745;&#65288;CDOE&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#35299;&#32806;&#23450;&#21521;&#20272;&#35745;&#65288;DOE&#65289;&#65292;&#20381;&#36182;&#20110;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20934;&#21017;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#22806;&#37096;&#21152;&#36895;&#24230;&#21644;&#30913;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22122;&#22768;&#36981;&#24490;&#19968;&#31181;&#37325;&#23614;&#20998;&#24067;&#26102;&#65292;&#22810;&#26680;&#37325;&#23545;&#27604;&#25439;&#22833;&#65288;MKCL&#65289;&#26159;&#26368;&#20248;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#20219;&#24847;&#22823;&#30340;&#31163;&#32676;&#20540;&#65292;MKCL&#30340;&#20272;&#35745;&#35823;&#24046;&#20063;&#26159;&#26377;&#30028;&#30340;&#12290;&#36890;&#36807;&#29992;MKCL&#26367;&#25442;&#26631;&#20934;MSE&#25104;&#26412;&#20989;&#25968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;CGD&#21644;CDOE&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#20256;&#24863;&#22120;&#35774;&#32622;&#21644;&#36816;&#21160;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CGD&#21644;CDOE&#22312;&#31934;&#24230;&#12289;&#20581;&#22766;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two computationally efficient algorithms for the orientation estimation of inertial measurement units (IMUs): the correntropy-based gradient descent (CGD) and the correntropy-based decoupled orientation estimation (CDOE). Traditional methods, such as gradient descent (GD) and decoupled orientation estimation (DOE), rely on the mean squared error (MSE) criterion, making them vulnerable to external acceleration and magnetic interference. To address this issue, we demonstrate that the multi-kernel correntropy loss (MKCL) is an optimal objective function for maximum likelihood estimation (MLE) when the noise follows a type of heavy-tailed distribution. In certain situations, the estimation error of the MKCL is bounded even in the presence of arbitrarily large outliers. By replacing the standard MSE cost function with MKCL, we develop the CGD and CDOE algorithms. We evaluate the effectiveness of our proposed methods by comparing them with existing algorithms in various s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65288;TKS&#65289;&#65292;&#22312;&#19981;&#21516;&#26102;&#21051;&#20043;&#38388;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#36741;&#21161;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;SNN&#23545;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;TKS&#21487;&#20197;&#33719;&#24471;&#24403;&#21069;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06540</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#23454;&#29616;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Sharing enable Spiking Neural Network Learning from Past and Future. (arXiv:2304.06540v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65288;TKS&#65289;&#65292;&#22312;&#19981;&#21516;&#26102;&#21051;&#20043;&#38388;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#36741;&#21161;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;SNN&#23545;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;TKS&#21487;&#20197;&#33719;&#24471;&#24403;&#21069;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22240;&#20854;&#31867;&#20284;&#20110;&#22823;&#33041;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#32780;&#21560;&#24341;&#20102;&#35768;&#22810;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26367;&#20195;&#26799;&#24230;&#30340;&#25552;&#20986;&#20351;&#24471;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36801;&#31227;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#24182;&#36880;&#27493;&#32553;&#23567;&#19982;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30446;&#21069;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#25152;&#26377;&#26102;&#21051;&#30340;&#36755;&#20986;&#26469;&#20135;&#29983;&#26368;&#32456;&#30340;&#39044;&#27979;&#65292;&#36825;&#29306;&#29298;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#29305;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65288;TKS&#65289;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#26102;&#21051;&#30340;&#36755;&#20986;&#26469;&#26500;&#25104;&#25945;&#24072;&#20449;&#21495;&#65292;&#20351;&#24471;&#20449;&#24687;&#21487;&#20197;&#22312;&#19981;&#21516;&#26102;&#21051;&#20043;&#38388;&#20132;&#20114;&#65292;&#36741;&#21161;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;CIFAR10&#12289;CIFAR100&#12289;ImageNet-1k&#20197;&#21450;&#31070;&#32463;&#24418;&#24577;&#23398;&#25968;&#25454;&#38598;DVS-CIFAR10&#12289;NCALTECH101&#19978;&#39564;&#35777;&#20102;TKS&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;TKS&#65292;&#25105;&#20204;&#24050;&#32463;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#21462;&#24471;&#20102;&#24403;&#21069;&#26368;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;TKS&#30340;NCALTECH101&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks have attracted extensive attention from researchers in many fields due to their brain-like information processing mechanism. The proposal of surrogate gradient enables the spiking neural networks to migrate to more complex tasks, and gradually close the gap with the conventional artificial neural networks. Current spiking neural networks utilize the output of all moments to produce the final prediction, which compromises their temporal characteristics and causes a reduction in performance and efficiency. We propose a temporal knowledge sharing approach (TKS) that enables the interaction of information between different moments, by selecting the output of specific moments to compose teacher signals to guide the training of the network along with the real labels. We have validated TKS on both static datasets CIFAR10, CIFAR100, ImageNet-1k and neuromorphic datasets DVS-CIFAR10, NCALTECH101. Our experimental results indicate that we have achieved the current optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#33529;&#26524;&#21494;&#30149;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;EfficientNetV2S&#26550;&#26500;&#25552;&#21462;&#29305;&#24449;&#24182;&#32467;&#21512;&#20998;&#31867;&#22120;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#20180;&#32454;&#35843;&#26597;&#65292;&#20026;&#33529;&#26524;&#26641;&#20892;&#25552;&#20379;&#20102;&#26356;&#21152;&#24555;&#36895;&#39640;&#25928;&#30340;&#21494;&#30149;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06520</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#33529;&#26524;&#21494;&#30149;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Transfer Learning-based Approach for Apple Leaf Disease Classification. (arXiv:2304.06520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#33529;&#26524;&#21494;&#30149;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;EfficientNetV2S&#26550;&#26500;&#25552;&#21462;&#29305;&#24449;&#24182;&#32467;&#21512;&#20998;&#31867;&#22120;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#20180;&#32454;&#35843;&#26597;&#65292;&#20026;&#33529;&#26524;&#26641;&#20892;&#25552;&#20379;&#20102;&#26356;&#21152;&#24555;&#36895;&#39640;&#25928;&#30340;&#21494;&#30149;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#35782;&#21035;&#21644;&#20998;&#31867;&#26893;&#29289;&#30142;&#30149;&#23545;&#20110;&#20445;&#38556;&#20840;&#29699;&#31918;&#39135;&#20379;&#24212;&#30340;&#23433;&#20840;&#21644;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#32463;&#27982;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#19981;&#21516;&#30340;&#20027;&#35201;&#20316;&#29289;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22320;&#21306;&#26159;&#26368;&#37325;&#35201;&#30340;&#21830;&#19994;&#20316;&#29289;&#20043;&#19968;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#20998;&#31867;&#33529;&#26524;&#21494;&#30149;&#30340;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#33529;&#26524;&#21494;&#30149;&#35782;&#21035;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;EfficientNetV2S&#26550;&#26500;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20256;&#36882;&#21040;&#20998;&#31867;&#22120;&#22359;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#36816;&#34892;&#26102;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#24050;&#32463;&#20180;&#32454;&#35843;&#26597;&#20102;&#21508;&#31181;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#22914;&#36755;&#20837;&#20998;&#36776;&#29575;&#12289;&#23398;&#20064;&#29575;&#12289;&#32426;&#20803;&#25968;&#31561;&#12290;&#35813;&#26041;&#26696;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correct identification and categorization of plant diseases are crucial for ensuring the safety of the global food supply and the overall financial success of stakeholders. In this regard, a wide range of solutions has been made available by introducing deep learning-based classification systems for different staple crops. Despite being one of the most important commercial crops in many parts of the globe, research proposing a smart solution for automatically classifying apple leaf diseases remains relatively unexplored. This study presents a technique for identifying apple leaf diseases based on transfer learning. The system extracts features using a pretrained EfficientNetV2S architecture and passes to a classifier block for effective prediction. The class imbalance issues are tackled by utilizing runtime data augmentation. The effect of various hyperparameters, such as input resolution, learning rate, number of epochs, etc., has been investigated carefully. The competence of the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#19979;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#39057;&#35889;&#24863;&#30693;&#65288;SS&#65289;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;FL&#22312;SS&#20013;&#30340;&#21160;&#26426;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2304.06519</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#19979;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#39057;&#35889;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Secure Federated Learning for Cognitive Radio Sensing. (arXiv:2304.06519v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#19979;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#39057;&#35889;&#24863;&#30693;&#65288;SS&#65289;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;FL&#22312;SS&#20013;&#30340;&#21160;&#26426;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#20013;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#39057;&#35889;&#24863;&#30693;&#65288;SS&#65289;&#12290;&#35752;&#35770;&#20102;FL&#22312;SS&#20013;&#30340;&#21160;&#26426;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;&#27010;&#36848;&#20102;&#36825;&#20123;&#31639;&#27861;&#38754;&#20020;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#21487;&#33021;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20123;&#22270;&#31034;&#20363;&#65292;&#20197;&#21450;&#38024;&#23545;&#26410;&#26469;CR&#20013;&#22522;&#20110;FL&#30340;SS&#30340;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers reliable and secure Spectrum Sensing (SS) based on Federated Learning (FL) in the Cognitive Radio (CR) environment. Motivation, architectures, and algorithms of FL in SS are discussed. Security and privacy threats on these algorithms are overviewed, along with possible countermeasures to such attacks. Some illustrative examples are also provided, with design recommendations for FL-based SS in future CRs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;5G NR&#31995;&#32479;&#20013;&#20351;&#29992;&#19978;&#34892;SRS&#20449;&#36947;&#20272;&#35745;&#36827;&#34892;&#29992;&#25143;&#23450;&#20301;&#65292;&#35777;&#26126;&#20102;&#22312;&#21830;&#29992;5G&#29615;&#22659;&#20013;&#21363;&#20351;&#20351;&#29992;&#24456;&#23569;&#30340;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#29992;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#31859;&#32423;&#31934;&#24230;&#30340;&#23460;&#22806;&#29992;&#25143;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2304.06514</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#34892;SRS&#36890;&#36947;&#20272;&#35745;&#22312;5G NR&#31995;&#32479;&#20013;&#23454;&#29616;&#30340;ML&#36741;&#21161;&#23460;&#22806;&#29992;&#25143;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ML-Enabled Outdoor User Positioning in 5G NR Systems via Uplink SRS Channel Estimates. (arXiv:2304.06514v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;5G NR&#31995;&#32479;&#20013;&#20351;&#29992;&#19978;&#34892;SRS&#20449;&#36947;&#20272;&#35745;&#36827;&#34892;&#29992;&#25143;&#23450;&#20301;&#65292;&#35777;&#26126;&#20102;&#22312;&#21830;&#29992;5G&#29615;&#22659;&#20013;&#21363;&#20351;&#20351;&#29992;&#24456;&#23569;&#30340;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#29992;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#31859;&#32423;&#31934;&#24230;&#30340;&#23460;&#22806;&#29992;&#25143;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26426;&#29992;&#25143;&#30340;&#23450;&#20301;&#26159;5G NR&#32593;&#32476;&#25552;&#20379;&#30340;&#19968;&#39033;&#37325;&#35201;&#26381;&#21153;&#65292;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#39044;&#35265;&#23558;&#25104;&#20026;5G NR&#31995;&#32479;&#30340;&#38598;&#25104;&#37096;&#20998;&#65292;&#21487;&#25552;&#39640;&#26080;&#32447;&#30005;&#24615;&#33021;&#24182;&#20943;&#23569;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26469;&#33258;&#29289;&#29702;&#23618;&#36890;&#36947;&#30340;&#19978;&#34892;&#20449;&#36947;&#20272;&#35745;&#26500;&#25104;&#30340;5G NR&#25351;&#32441;&#36827;&#34892;&#23450;&#20301;&#30340;ML&#25216;&#26415;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25506;&#27979;&#21442;&#32771;&#20449;&#21495;&#65288;SRS&#65289;&#20449;&#36947;&#25351;&#32441;&#25552;&#20379;&#36275;&#22815;&#30340;&#25968;&#25454;&#25512;&#26029;&#29992;&#25143;&#20301;&#32622;&#26159;&#21487;&#33021;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#21363;&#20351;&#22312;&#38750;&#24120;&#31232;&#30095;&#30340;SRS&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#23567;&#22411;&#20840;&#36830;&#25509;&#36739;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#22312;&#21830;&#29992;5G&#29615;&#22659;&#20013;&#23454;&#29616;&#20855;&#26377;&#31859;&#32423;&#31934;&#24230;&#30340;&#25104;&#21151;&#23460;&#22806;&#29992;&#25143;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular user positioning is a promising service provided by Fifth Generation New Radio (5G NR) networks. Besides, Machine Learning (ML) techniques are foreseen to become an integrated part of 5G NR systems improving radio performance and reducing complexity. In this paper, we investigate ML techniques for positioning using 5G NR fingerprints consisting of uplink channel estimates from the physical layer channel. We show that it is possible to use Sounding Reference Signals (SRS) channel fingerprints to provide sufficient data to infer user position. Furthermore, we show that small fully-connected moderately Deep Neural Networks, even when applied to very sparse SRS data, can achieve successful outdoor user positioning with meter-level accuracy in a commercial 5G environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#30340;&#21746;&#23398;&#22522;&#30784;&#65292;&#20998;&#21035;&#20174;&#21487;&#25345;&#32493;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12289;&#27169;&#24335;&#30693;&#35782;&#30340;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#20013;&#30340;&#20013;&#31435;&#24615;&#32570;&#22833;&#31561;&#35282;&#24230;&#20986;&#21457;&#65292;&#20026;&#25105;&#20204;&#35774;&#35745;&#12289;&#22521;&#35757;&#21644;&#37096;&#32626;&#22522;&#20110;GeoAI&#30340;&#31995;&#32479;&#25552;&#20379;&#20102;&#24110;&#21161;&#65292;&#20063;&#20026;&#25105;&#20204;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21033;&#30410;&#21644;&#28508;&#22312;&#21361;&#38505;&#25552;&#20379;&#20102;&#20849;&#21516;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.06508</link><description>&lt;p&gt;
GeoAI&#30340;&#21746;&#23398;&#22522;&#30784;&#65306;&#25506;&#32034;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science. (arXiv:2304.06508v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#30340;&#21746;&#23398;&#22522;&#30784;&#65292;&#20998;&#21035;&#20174;&#21487;&#25345;&#32493;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12289;&#27169;&#24335;&#30693;&#35782;&#30340;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#20013;&#30340;&#20013;&#31435;&#24615;&#32570;&#22833;&#31561;&#35282;&#24230;&#20986;&#21457;&#65292;&#20026;&#25105;&#20204;&#35774;&#35745;&#12289;&#22521;&#35757;&#21644;&#37096;&#32626;&#22522;&#20110;GeoAI&#30340;&#31995;&#32479;&#25552;&#20379;&#20102;&#24110;&#21161;&#65292;&#20063;&#20026;&#25105;&#20204;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21033;&#30410;&#21644;&#28508;&#22312;&#21361;&#38505;&#25552;&#20379;&#20102;&#20849;&#21516;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#20171;&#32461;&#20102;&#21487;&#33021;&#26500;&#25104;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#21746;&#23398;&#22522;&#30784;&#30340;&#19968;&#20123;&#22522;&#26412;&#20551;&#35774;&#21644;&#21407;&#21017;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#21487;&#25345;&#32493;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12289;&#27169;&#24335;&#30693;&#35782;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26469;&#33258;&#32479;&#19968;&#20262;&#29702;&#35270;&#35282;&#30340;GeoAI&#31995;&#32479;&#20013;&#65288;&#28508;&#22312;&#30340;&#65289;&#20013;&#31435;&#24615;&#32570;&#22833;&#31561;&#20027;&#39064;&#65292;&#32780;&#38750;&#23457;&#26597;&#31354;&#38388;&#25968;&#25454;&#65288;&#20998;&#26512;&#65289;&#30340;&#25104;&#29087;&#29305;&#24449;&#65292;&#22914;&#20132;&#20114;&#12289;&#37051;&#22495;&#21644;&#33258;&#30456;&#20851;&#24615;&#12290;&#21453;&#24605;&#25105;&#20204;&#32844;&#19994;&#36947;&#24503;&#30340;&#24433;&#21709;&#23558;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#36127;&#36131;&#22320;&#36827;&#34892;&#28508;&#22312;&#30340;&#30740;&#31350;&#65292;&#35782;&#21035;&#35774;&#35745;&#12289;&#22521;&#35757;&#21644;&#37096;&#32626;&#22522;&#20110;GeoAI&#30340;&#31995;&#32479;&#20013;&#30340;&#38519;&#38449;&#65292;&#24182;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20849;&#21516;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21033;&#30410;&#21644;&#28508;&#22312;&#21361;&#38505;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#21516;&#26102;&#19982;&#20182;&#20154;&#20998;&#20139;&#25105;&#20204;&#29420;&#29305;&#30340;&#65288;&#22320;&#29702;&#65289;&#31354;&#38388;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter presents some of the fundamental assumptions and principles that could form the philosophical foundation of GeoAI and spatial data science. Instead of reviewing the well-established characteristics of spatial data (analysis), including interaction, neighborhoods, and autocorrelation, the chapter highlights themes such as sustainability, bias in training data, diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI systems from a unifying ethical perspective. Reflecting on our profession's ethical implications will assist us in conducting potentially disruptive research more responsibly, identifying pitfalls in designing, training, and deploying GeoAI-based systems, and developing a shared understanding of the benefits but also potential dangers of artificial intelligence and machine learning research across academic fields, all while sharing our unique (geo)spatial perspective with others.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Squeeze and Excitation&#32593;&#32476;&#30340;&#21464;&#20307;&#26469;&#25913;&#36827;&#37325;&#35201;&#29305;&#24449;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#21464;&#20307;&#22312;&#27531;&#24046;&#32593;&#32476;&#19978;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06502</link><description>&lt;p&gt;
Squeeze and Excitation&#32593;&#32476;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Variations of Squeeze and Excitation networks. (arXiv:2304.06502v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Squeeze and Excitation&#32593;&#32476;&#30340;&#21464;&#20307;&#26469;&#25913;&#36827;&#37325;&#35201;&#29305;&#24449;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#21464;&#20307;&#22312;&#27531;&#24046;&#32593;&#32476;&#19978;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;&#20869;&#26680;&#20013;&#32039;&#23494;&#30456;&#36830;&#12290;SE&#27169;&#22359;&#25171;&#30772;&#20102;&#31070;&#32463;&#32593;&#32476;&#20256;&#36882;&#25972;&#20307;&#32467;&#26524;&#33267;&#19979;&#19968;&#23618;&#30340;&#20256;&#32479;&#36335;&#32447;&#12290;&#30456;&#21453;&#65292;SE&#20165;&#20256;&#36882;&#21253;&#21547;&#20854;&#25380;&#21387;&#21644;&#28608;&#21169;&#27169;&#22359;&#30340;&#37325;&#35201;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SE&#27169;&#22359;&#30340;&#21464;&#20307;&#65292;&#25913;&#36827;&#20102;&#25380;&#21387;&#21644;&#28608;&#21169;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#25380;&#21387;&#25110;&#28608;&#21169;&#23618;&#20351;&#24471;&#23618;&#26435;&#37325;&#30340;&#36716;&#25442;&#21464;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;&#36825;&#20123;&#21464;&#21270;&#36824;&#20445;&#30041;&#20102;SE&#27169;&#22359;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#27531;&#24046;&#32593;&#32476;&#19978;&#36827;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#34920;&#26684;&#21270;&#30340;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks learns spatial features and are heavily interlinked within kernels. The SE module have broken the traditional route of neural networks passing the entire result to next layer. Instead SE only passes important features to be learned with its squeeze and excitation (SE) module. We propose variations of the SE module which improvises the process of squeeze and excitation and enhances the performance. The proposed squeezing or exciting the layer makes it possible for having a smooth transition of layer weights. These proposed variations also retain the characteristics of SE module. The experimented results are carried out on residual networks and the results are tabulated.
&lt;/p&gt;</description></item><item><title>EEGMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#22522;&#20110;EEG-Mixup&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#22810;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06496</link><description>&lt;p&gt;
EEGMatch: &#23398;&#20064;&#19981;&#23436;&#25972;&#26631;&#35760;&#30340;&#21322;&#30417;&#30563;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EEGMatch: Learning with Incomplete Labels for Semi-Supervised EEG-based Cross-Subject Emotion Recognition. (arXiv:2304.06496v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06496
&lt;/p&gt;
&lt;p&gt;
EEGMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#22522;&#20110;EEG-Mixup&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#22810;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#24773;&#32490;&#35782;&#21035;&#30340;&#23458;&#35266;&#24037;&#20855;&#65292;&#24182;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#35782;&#21035;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;EEGMatch&#65289;&#65292;&#20197;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;EEG&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;EEG-Mixup&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#26356;&#22810;&#29992;&#20110;&#27169;&#22411;&#23398;&#20064;&#30340;&#26377;&#25928;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20004;&#27493;&#25104;&#23545;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21407;&#22411;&#24335;&#21644;&#23454;&#20363;&#21270;&#24335;&#25104;&#23545;&#23398;&#20064;&#36830;&#25509;&#36215;&#26469;&#65292;&#20854;&#20013;&#21407;&#22411;&#24335;&#25104;&#23545;&#23398;&#20064;&#34913;&#37327;EEG&#25968;&#25454;&#19982;&#27599;&#20010;&#24773;&#24863;&#31867;&#21035;&#30340;&#21407;&#22411;&#34920;&#31034;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#32780;&#23454;&#20363;&#21270;&#24335;&#25104;&#23545;&#23398;&#20064;&#25429;&#25417;EEG&#25968;&#25454;&#20043;&#38388;&#30340;&#23616;&#37096;&#20869;&#22312;&#20851;&#31995;&#12290;&#31532;&#19977;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#22810;&#22495;&#33258;&#36866;&#24212;&#65292;&#20197;&#23545;&#40784;&#22810;&#20010;&#22495;&#65288;&#26631;&#35760;&#30340;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65289;&#20043;&#38388;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EEGMatch&#22312;&#24773;&#32490;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) is an objective tool for emotion recognition and shows promising performance. However, the label scarcity problem is a main challenge in this field, which limits the wide application of EEG-based emotion recognition. In this paper, we propose a novel semi-supervised learning framework (EEGMatch) to leverage both labeled and unlabeled EEG data. First, an EEG-Mixup based data augmentation method is developed to generate more valid samples for model learning. Second, a semi-supervised two-step pairwise learning method is proposed to bridge prototype-wise and instance-wise pairwise learning, where the prototype-wise pairwise learning measures the global relationship between EEG data and the prototypical representation of each emotion class and the instance-wise pairwise learning captures the local intrinsic relationship among EEG data. Third, a semi-supervised multi-domain adaptation is introduced to align the data representation among multiple domains (labeled
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#23398;&#20064;&#30340;EEG&#20449;&#21495;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;EEG&#20449;&#21495;&#30340;&#20302;&#32500;&#24230;&#21644;&#21487;&#36716;&#31227;&#34920;&#31034;&#65292;&#24182;&#22312;&#20844;&#24320;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06495</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#23398;&#20064;&#30340;EEG&#20449;&#21495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
An embedding for EEG signals learned using a triplet loss. (arXiv:2304.06495v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#23398;&#20064;&#30340;EEG&#20449;&#21495;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;EEG&#20449;&#21495;&#30340;&#20302;&#32500;&#24230;&#21644;&#21487;&#36716;&#31227;&#34920;&#31034;&#65292;&#24182;&#22312;&#20844;&#24320;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25110;&#23616;&#37096;&#22330;&#30005;&#20301;&#30340;&#31070;&#32463;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#35760;&#24405;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#30721;&#65292;&#20197;&#20272;&#35745;&#24739;&#32773;&#25110;&#20581;&#24247;&#29992;&#25143;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#33041;&#29366;&#24577;&#12290;&#22312;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#65292;&#36825;&#31181;&#35299;&#30721;&#30340;&#33041;&#29366;&#24577;&#20449;&#24687;&#21487;&#20197;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;&#36890;&#20449;&#25110;&#20013;&#39118;&#21518;&#24247;&#22797;&#65292;&#25110;&#32773;&#34987;&#21160;&#30417;&#27979;&#21463;&#35797;&#32773;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#33041;&#29366;&#24577;&#65292;&#20363;&#22914;&#22312;&#33499;&#21051;&#30340;&#24037;&#20316;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#35299;&#30721;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#19968;&#20010;&#29305;&#27530;&#25361;&#25112;&#26159;BCI&#20013;&#30456;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#22312;BCI&#20013;&#35299;&#20915;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#33021;&#24615;&#26159;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20854;&#20182;&#20250;&#35805;&#12289;&#20027;&#39064;&#29978;&#33267;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#20351;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#31070;&#32463;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#22791;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31070;&#32463;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;EEG&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;EEG&#20449;&#21495;&#30340;&#20302;&#32500;&#24230;&#21644;&#21487;&#36716;&#31227;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#23398;&#20064;&#30340;&#23884;&#20837;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#24739;&#32773;&#12289;&#21516;&#19968;&#24739;&#32773;&#30340;&#19981;&#21516;&#20250;&#35805;&#29978;&#33267;&#19981;&#21516;EEG&#25968;&#25454;&#38598;&#20043;&#38388;&#36716;&#31227;&#65292;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurophysiological time series recordings like the electroencephalogram (EEG) or local field potentials are obtained from multiple sensors. They can be decoded by machine learning models in order to estimate the ongoing brain state of a patient or healthy user. In a brain-computer interface (BCI), this decoded brain state information can be used with minimal time delay to either control an application, e.g., for communication or for rehabilitation after stroke, or to passively monitor the ongoing brain state of the subject, e.g., in a demanding work environment. A specific challenge in such decoding tasks is posed by the small dataset sizes in BCI compared to other domains of machine learning like computer vision or natural language processing. A possibility to tackle classification or regression problems in BCI despite small training data sets is through transfer learning, which utilizes data from other sessions, subjects or even datasets to train a model. In this exploratory study, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PV&#38453;&#21015;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#28784;&#23576;&#24433;&#21709;&#24182;&#36890;&#36807;&#23558;Is-Voc normalized Gramian angular difference field&#26041;&#27861;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;CBAM&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#31181;&#22797;&#26434;&#25925;&#38556;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20934;&#30830;&#24615;&#19982;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06493</link><description>&lt;p&gt;
&#32771;&#34385;&#28784;&#23576;&#24433;&#21709;&#30340;&#20809;&#20239;&#38453;&#21015;&#25925;&#38556;&#35786;&#26029;&#65306;&#22522;&#20110;&#29305;&#24449;&#26354;&#32447;&#30340;&#21464;&#25442;&#22270;&#24418;&#29305;&#24449;&#21644;&#24102;CBAM&#27169;&#22359;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis for PV arrays considering dust impact based on transformed graphical feature of characteristic curves and convolutional neural network with CBAM modules. (arXiv:2304.06493v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PV&#38453;&#21015;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#28784;&#23576;&#24433;&#21709;&#24182;&#36890;&#36807;&#23558;Is-Voc normalized Gramian angular difference field&#26041;&#27861;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;CBAM&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#31181;&#22797;&#26434;&#25925;&#38556;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20934;&#30830;&#24615;&#19982;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#38453;&#21015;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#22810;&#31181;&#25925;&#38556;&#65292;&#32780;&#28784;&#23576;&#21644;&#19981;&#21516;&#30340;&#20108;&#26497;&#31649;&#37197;&#32622;&#20351;&#25925;&#38556;&#26356;&#21152;&#22797;&#26434;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#22522;&#20110;I-V&#29305;&#24615;&#26354;&#32447;&#30340;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#20165;&#21033;&#29992;&#37096;&#20998;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#24120;&#20381;&#36182;&#20110;&#23558;&#22330;&#29305;&#24615;&#26354;&#32447;&#26657;&#20934;&#21040;&#26631;&#20934;&#27979;&#35797;&#26465;&#20214;&#65288;STC&#65289;&#12290;&#36825;&#24456;&#38590;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#65292;&#24182;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#20010;&#22797;&#26434;&#25925;&#38556;&#65292;&#29305;&#21035;&#26159;&#22312;&#28784;&#23576;&#24433;&#21709;&#19979;PV&#38453;&#21015;&#20013;&#20855;&#26377;&#30456;&#20284;&#24615;&#30340;&#19981;&#21516;&#38459;&#22622;&#20108;&#26497;&#31649;&#37197;&#32622;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#28784;&#23576;&#24433;&#21709;&#30340;PV&#38453;&#21015;&#25925;&#38556;&#35786;&#26029;&#26032;&#26041;&#27861;&#12290;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#20013;&#65292;&#25552;&#20986;&#20102;Isc-Voc&#24402;&#19968;&#21270;&#21387;&#32553;&#22270;&#27491;&#24358;&#35282;&#24046;&#65288;GADF&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#22330;&#29305;&#24615;&#26354;&#32447;&#21253;&#25324;I-V&#21644;P-V&#30340;&#37325;&#37319;&#26679;PV&#38453;&#21015;&#29305;&#24615;&#26354;&#32447;&#24402;&#19968;&#21270;&#21644;&#36716;&#25442;&#65292;&#20197;&#33719;&#21462;&#29305;&#24449;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#22312;&#25925;&#38556;&#35786;&#26029;&#38454;&#27573;&#20013;&#65292;&#23558;&#36716;&#25442;&#21518;&#30340;&#22270;&#24418;&#29305;&#24449;&#30697;&#38453;&#36755;&#20837;&#24102;&#26377;&#36890;&#36947;&#27880;&#24847;&#21147;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#21363;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CBAM&#65289;&#65292;&#20197;&#20934;&#30830;&#35782;&#21035;&#25925;&#38556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#21253;&#25324;&#37096;&#20998;&#36974;&#34109;&#12289;&#24320;&#36335;&#25925;&#38556;&#21644;&#28784;&#23576;&#23548;&#33268;&#30340;&#24322;&#24120;&#24037;&#20316;&#26465;&#20214;&#22312;&#20869;&#30340;&#25925;&#38556;&#12290;&#24635;&#20307;&#35786;&#26029;&#20934;&#30830;&#29575;&#36798;&#21040;100&#65285;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various faults can occur during the operation of PV arrays, and both the dust-affected operating conditions and various diode configurations make the faults more complicated. However, current methods for fault diagnosis based on I-V characteristic curves only utilize partial feature information and often rely on calibrating the field characteristic curves to standard test conditions (STC). It is difficult to apply it in practice and to accurately identify multiple complex faults with similarities in different blocking diodes configurations of PV arrays under the influence of dust. Therefore, a novel fault diagnosis method for PV arrays considering dust impact is proposed. In the preprocessing stage, the Isc-Voc normalized Gramian angular difference field (GADF) method is presented, which normalizes and transforms the resampled PV array characteristic curves from the field including I-V and P-V to obtain the transformed graphical feature matrices. Then, in the fault diagnosis stage, the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35823;&#24046;&#21521;&#37327;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38656;&#35774;&#22791;&#30340;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;RFO&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#29289;&#29702;&#23618;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#20449;&#36947;&#29305;&#24449;&#36827;&#34892;&#20154;&#21592;&#20301;&#32622;&#20998;&#31867;&#65292;&#24615;&#33021;&#26356;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.06490</link><description>&lt;p&gt;
&#22522;&#20110;&#35823;&#24046;&#21521;&#37327;&#35889;&#30340;Wi-Fi&#26080;&#38656;&#35774;&#22791;&#23460;&#20869;&#23450;&#20301;&#30340;&#26032;&#33539;&#24335;&#65306;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A New Paradigm for Device-free Indoor Localization: Deep Learning with Error Vector Spectrum in Wi-Fi Systems. (arXiv:2304.06490v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35823;&#24046;&#21521;&#37327;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38656;&#35774;&#22791;&#30340;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;RFO&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#29289;&#29702;&#23618;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#20449;&#36947;&#29305;&#24449;&#36827;&#34892;&#20154;&#21592;&#20301;&#32622;&#20998;&#31867;&#65292;&#24615;&#33021;&#26356;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20854;&#20415;&#21033;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#21830;&#29992;Wi-Fi&#35774;&#22791;&#36827;&#34892;&#26080;&#38656;&#35774;&#22791;&#30340;&#23460;&#20869;&#23450;&#20301;&#30340;&#38656;&#27714;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36805;&#36895;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26080;&#32447;&#20449;&#36947;&#20013;&#30340;&#38543;&#26426;&#39057;&#29575;&#20559;&#31227;&#65288;RFO&#65289;&#20250;&#24433;&#21709;&#23460;&#20869;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#65292;&#24403;&#20351;&#29992;&#27874;&#21160;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26102;&#26356;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#20943;&#36731;RFO&#38382;&#39064;&#65292;&#36890;&#36807;&#35823;&#24046;&#21521;&#37327;&#35889;&#65288;EVS&#65289;&#26469;&#25552;&#39640;&#20449;&#21495;&#30340;&#20998;&#36776;&#29575;&#24182;&#22686;&#24378;&#20854;&#23545;RFO&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#36182;&#20110;&#35823;&#24046;&#21521;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;EVAL&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26080;&#38656;&#35774;&#22791;&#30340;&#23460;&#20869;&#23450;&#20301;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#20174;&#29289;&#29702;&#23618;&#20449;&#21495;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#20449;&#36947;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#30340;&#26041;&#24335;&#26469;&#30830;&#23450;&#20154;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#22522;&#20110;OpenWiFi&#39033;&#30446;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#65292;&#20197;&#25552;&#21462;EVS&#21644;CSI&#24182;&#26816;&#39564;&#19981;&#21516;&#26080;&#38656;&#35774;&#22791;&#30340;&#23450;&#20301;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;EVAL&#26041;&#26696;&#22312;&#21508;&#39033;&#24615;&#33021;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20301;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for device-free indoor localization using commercial Wi-Fi devices has rapidly increased in various fields due to its convenience and versatile applications. However, random frequency offset (RFO) in wireless channels poses challenges to the accuracy of indoor localization when using fluctuating channel state information (CSI). To mitigate the RFO problem, an error vector spectrum (EVS) is conceived thanks to its higher resolution of signal and robustness to RFO. To address these challenges, this paper proposed a novel error vector assisted learning (EVAL) for device-free indoor localization. The proposed EVAL scheme employs deep neural networks to classify the location of a person in the indoor environment by extracting ample channel features from the physical layer signals. We conducted realistic experiments based on OpenWiFi project to extract both EVS and CSI to examine the performance of different device-free localization techniques. Experimental results show that our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.06489</link><description>&lt;p&gt;
&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Inertial Measurement Unit-based Human Activity Recognition: A Survey. (arXiv:2304.06489v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#31359;&#25140;&#24335;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#20197;&#24320;&#21457;&#21508;&#31181;&#26234;&#33021;&#31038;&#21306;&#24212;&#29992;&#65292;&#22914;&#30561;&#30496;&#27169;&#24335;&#30417;&#27979;&#12289;&#33647;&#29289;&#25552;&#37266;&#12289;&#35748;&#30693;&#20581;&#24247;&#35780;&#20272;&#12289;&#36816;&#21160;&#20998;&#26512;&#31561;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25918;&#32622;&#22312;&#19981;&#21516;&#30340;&#36523;&#20307;&#20301;&#32622;&#12289;&#35774;&#22791;&#22266;&#26377;&#20559;&#24046;&#21644;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#20010;&#20154;&#21644;&#29615;&#22659;&#30340;&#24046;&#24322;&#31561;&#36896;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#65292;&#36825;&#20123;WHAR&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#24433;&#21709;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#36817;&#24180;&#26469;&#24191;&#21463;&#27426;&#36814;&#30340;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#23545;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based wearable human activity recognition (WHAR) models enable the development of various smart and connected community applications such as sleep pattern monitoring, medication reminders, cognitive health assessment, sports analytics, etc. However, the widespread adoption of these WHAR models is impeded by their degraded performance in the presence of data distribution heterogeneities caused by the sensor placement at different body positions, inherent biases and heterogeneities across devices, and personal and environmental diversities. Various traditional machine learning algorithms and transfer learning techniques have been proposed in the literature to address the underpinning challenges of handling such data heterogeneities. Domain adaptation is one such transfer learning techniques that has gained significant popularity in recent literature. In this paper, we survey the recent progress of domain adaptation techniques in the Inertial Measurement Unit (IMU)-based 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;&#20063;&#26159;AGI&#30340;&#19968;&#22823;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23545;&#20854;&#22914;&#20309;&#28436;&#21464;&#20026;AIGC&#23637;&#26395;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#36890;&#29992;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.06488</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;AGI&#30340;&#19968;&#22823;&#27493;&#65306;AIGC&#26102;&#20195;&#20013;ChatGPT&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. (arXiv:2304.06488v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06488
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;&#20063;&#26159;AGI&#30340;&#19968;&#22823;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23545;&#20854;&#22914;&#20309;&#28436;&#21464;&#20026;AIGC&#23637;&#26395;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#36890;&#29992;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#26368;&#36817;&#21457;&#24067;&#20102;GPT-4&#65288;&#21448;&#31216;&#20026;ChatGPT plus&#65289;&#65292;&#35813;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#29983;&#25104;&#24335;AI&#65288;GAI&#65289;&#36808;&#20986;&#30340;&#19968;&#23567;&#27493;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26469;&#35828;&#21017;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#39134;&#36291;&#12290;&#33258;2022&#24180;11&#26376;&#27491;&#24335;&#21457;&#24067;&#20197;&#26469;&#65292;ChatGPT&#20415;&#36805;&#36895;&#21560;&#24341;&#20102;&#20247;&#22810;&#29992;&#25143;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23186;&#20307;&#20851;&#27880;&#65292;&#30456;&#20851;&#30340;&#23398;&#26415;&#25991;&#31456;&#20063;&#36229;&#36807;&#20102;500&#31687;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#36827;&#34892;&#19968;&#27425;&#32508;&#36848;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23601;&#26159;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#19977;&#20010;&#26041;&#38754;&#20840;&#38754;&#35843;&#26597;ChatGPT&#30340;&#22242;&#38431;&#65292;&#24182;&#23637;&#26395;&#20102;ChatGPT&#22914;&#20309;&#28436;&#21464;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#65292;&#36825;&#23558;&#26159;AGI&#21457;&#23637;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.
&lt;/p&gt;</description></item><item><title>CoRe-Sleep&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#25552;&#39640;&#23545;&#19981;&#23436;&#21892;&#25968;&#25454;&#30340;&#20449;&#21495;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#36890;&#36807;&#36866;&#24403;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#23481;&#24525;&#22122;&#22768;&#25110;&#20002;&#22833;&#30340;&#27169;&#24577;&#29255;&#27573;&#65292;&#23637;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06485</link><description>&lt;p&gt;
CoRe-Sleep: &#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19981;&#23436;&#21892;&#27169;&#24577;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities. (arXiv:2304.06485v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06485
&lt;/p&gt;
&lt;p&gt;
CoRe-Sleep&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#25552;&#39640;&#23545;&#19981;&#23436;&#21892;&#25968;&#25454;&#30340;&#20449;&#21495;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#36890;&#36807;&#36866;&#24403;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#23481;&#24525;&#22122;&#22768;&#25110;&#20002;&#22833;&#30340;&#27169;&#24577;&#29255;&#27573;&#65292;&#23637;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#24322;&#24120;&#21487;&#33021;&#20250;&#23545;&#20581;&#24247;&#20135;&#29983;&#20005;&#37325;&#30340;&#21518;&#26524;&#12290;&#33258;&#21160;&#21270;&#30561;&#30496;&#20998;&#26399;&#21487;&#20197;&#31616;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;&#20197;&#24448;&#30340;&#33258;&#21160;&#21270;&#30561;&#30496;&#20998;&#26399;&#24037;&#20316;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110; EEG &#20449;&#21495;&#12290;&#20294;&#26159;&#65292;&#36890;&#24120;&#22312; EEG &#20043;&#22806;&#36824;&#26377;&#22810;&#20010;&#20449;&#24687;&#28304;&#21487;&#29992;&#12290;&#24403; EEG &#35760;&#24405;&#23384;&#22312;&#22122;&#22768;&#29978;&#33267;&#23436;&#20840;&#32570;&#22833;&#26102;&#65292;&#36825;&#21487;&#33021;&#23588;&#20026;&#26377;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; CoRe-Sleep&#65292;&#19968;&#31181;&#21327;&#35843;&#34920;&#31034;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#23427;&#29305;&#21035;&#20851;&#27880;&#20110;&#25552;&#39640;&#23545;&#19981;&#23436;&#21892;&#25968;&#25454;&#30340;&#20449;&#21495;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21487;&#20197;&#26159;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;CoRe-Sleep &#23481;&#24525;&#22122;&#22768;&#25110;&#32570;&#22833;&#30340;&#27169;&#24577;&#29255;&#27573;&#65292;&#20801;&#35768;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#20351;&#29992;&#21333;&#20010;&#27169;&#24335;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#27979;&#35797;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#27979;&#35797;&#26102;&#37117;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep abnormalities can have severe health consequences. Automated sleep staging, i.e. labelling the sequence of sleep stages from the patient's physiological recordings, could simplify the diagnostic process. Previous work on automated sleep staging has achieved great results, mainly relying on the EEG signal. However, often multiple sources of information are available beyond EEG. This can be particularly beneficial when the EEG recordings are noisy or even missing completely. In this paper, we propose CoRe-Sleep, a Coordinated Representation multimodal fusion network that is particularly focused on improving the robustness of signal analysis on imperfect data. We demonstrate how appropriately handling multimodal information can be the key to achieving such robustness. CoRe-Sleep tolerates noisy or missing modalities segments, allowing training on incomplete data. Additionally, it shows state-of-the-art performance when testing on both multimodal and unimodal data using a single mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102; CryptoPunks &#21644; NFT&#24066;&#22330;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#12290;&#32467;&#26524;&#21457;&#29616;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#32780;&#38750;&#24615;&#21035;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#21035;&#26631;&#31614;&#30340;NFT&#33402;&#26415;&#21697;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#36825;&#20010;&#26032;&#20852;&#24066;&#22330;&#30340;&#31038;&#20250;&#24179;&#31561;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.06484</link><description>&lt;p&gt;
&#25506;&#32034;NFT&#24066;&#22330;&#20013;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploring Gender and Race Biases in the NFT Market. (arXiv:2304.06484v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102; CryptoPunks &#21644; NFT&#24066;&#22330;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#12290;&#32467;&#26524;&#21457;&#29616;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#32780;&#38750;&#24615;&#21035;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#21035;&#26631;&#31614;&#30340;NFT&#33402;&#26415;&#21697;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#36825;&#20010;&#26032;&#20852;&#24066;&#22330;&#30340;&#31038;&#20250;&#24179;&#31561;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;(NFTs)&#26159;&#25351;&#37027;&#20123;&#36890;&#24120;&#23384;&#20648;&#22312;&#21306;&#22359;&#38142;&#19978;&#30340;&#38750;&#21487;&#26367;&#20195;&#24615;&#36164;&#20135;&#65292;&#36890;&#24120;&#26159;&#25968;&#23383;&#33402;&#26415;&#12290;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22899;&#24615;&#21644;&#36739;&#28145;&#32932;&#33394;&#30340;NFTs&#30340;&#20215;&#20540;&#20302;&#20110;&#30007;&#24615;&#21644;&#36739;&#27973;&#32932;&#33394;&#30340;&#20316;&#21697;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#20998;&#26512;&#20102;CryptoPunks&#33402;&#26415;&#21697;&#25910;&#34255;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;CryptoPunks&#20215;&#26684;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#24182;&#39318;&#27425;&#30740;&#31350;&#20102;NFT&#24066;&#22330;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#35777;&#25454;&#25903;&#25345;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#32780;&#38750;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#21035;&#26631;&#31614;&#30340;NFT&#33402;&#26415;&#21697;&#25968;&#25454;&#38598;&#65292;&#20197;&#25512;&#36827;&#23545;&#20110;&#36825;&#20010;&#26032;&#20852;&#24066;&#22330;&#30340;&#31038;&#20250;&#24179;&#31561;&#24615;&#30340;&#26356;&#24191;&#27867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-Fungible Tokens (NFTs) are non-interchangeable assets, usually digital art, which are stored on the blockchain. Preliminary studies find that female and darker-skinned NFTs are valued less than their male and lighter-skinned counterparts. However, these studies analyze only the CryptoPunks collection. We test the statistical significance of race and gender biases in the prices of CryptoPunks and present the first study of gender bias in the broader NFT market. We find evidence of racial bias but not gender bias. Our work also introduces a dataset of gender-labeled NFT collections to advance the broader study of social equity in this emerging market.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$\beta$-VAE&#20316;&#20026;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#32852;&#21512;&#20248;&#21270;&#20449;&#21495;&#37325;&#24314;&#21644;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#12290;&#22312;7255&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06476</link><description>&lt;p&gt;
&#22522;&#20110;$\beta$-VAE&#30340;&#24515;&#30005;&#22270;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#25552;&#21462;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Joint optimization of a $\beta$-VAE for ECG task-specific feature extraction. (arXiv:2304.06476v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$\beta$-VAE&#20316;&#20026;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#32852;&#21512;&#20248;&#21270;&#20449;&#21495;&#37325;&#24314;&#21644;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#12290;&#22312;7255&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#26159;&#30740;&#31350;&#24515;&#33039;&#24773;&#20917;&#30340;&#26368;&#24120;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#24515;&#33039;&#33410;&#24459;&#21644;&#30005;&#27963;&#21160;&#36827;&#34892;&#35786;&#26029;&#21644;&#30417;&#27979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$\beta$-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20449;&#21495;&#37325;&#24314;&#21644;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#26469;&#25913;&#36827;&#20854;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;7255&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function. In this work we study the use of $\beta$-variational autoencoders (VAEs) as an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function prediction. The extracted features are then used for cardiac function prediction using logistic regression. The method is trained and tested on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021. The results show that our method significantly improved prediction and explainability compared to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;EEG&#25968;&#25454;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#36873;&#25321;&#21644;&#26102;&#38388;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06471</link><description>&lt;p&gt;
&#20004;&#20010;&#22836;&#32988;&#36807;&#19968;&#20010;&#65306;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#23545;EEG-ET&#25968;&#25454;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Two Heads are Better than One: A Bio-inspired Method for Improving Classification on EEG-ET Data. (arXiv:2304.06471v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;EEG&#25968;&#25454;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#36873;&#25321;&#21644;&#26102;&#38388;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;EEG&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#23545;&#20110;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#21644;&#20854;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#29983;&#29289;&#24615;&#36136;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#22806;&#37096;&#22122;&#22768;&#24120;&#24120;&#24178;&#25200;EEG&#25968;&#25454;&#12290;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#26631;&#20934;EEG&#39044;&#22788;&#29702;&#26041;&#27861;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#20107;&#20214;&#21644;&#29305;&#24449;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24179;&#31561;&#22320;&#22788;&#29702;&#25152;&#26377;&#30456;&#20851;&#30340;&#35748;&#30693;&#20107;&#20214;&#65292;&#24182;&#24573;&#35270;&#20102;&#22823;&#33041;&#38543;&#26102;&#38388;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#36873;&#25321;&#21644;EEG&#25968;&#25454;&#30340;&#26102;&#38388;&#20998;&#21106;&#30456;&#32467;&#21512;&#12290;&#22312;EEGEyeNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying EEG data is integral to the performance of Brain Computer Interfaces (BCI) and their applications. However, external noise often obstructs EEG data due to its biological nature and complex data collection process. Especially when dealing with classification tasks, standard EEG preprocessing approaches extract relevant events and features from the entire dataset. However, these approaches treat all relevant cognitive events equally and overlook the dynamic nature of the brain over time. In contrast, we are inspired by neuroscience studies to use a novel approach that integrates feature selection and time segmentation of EEG data. When tested on the EEGEyeNet dataset, our proposed method significantly increases the performance of Machine Learning classifiers while reducing their respective computational complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#23450;&#20041;&#20102;&#19968;&#32452;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#65292;&#22522;&#20110;&#36712;&#36857;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#29109;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#20381;&#38752;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#34920;&#31034;&#23398;&#20064;&#38477;&#20302;&#29992;&#25143;&#30340;&#37325;&#26032;&#35782;&#21035;&#29575;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#20248;&#21183;&#21644;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06469</link><description>&lt;p&gt;
&#20998;&#26512;&#38544;&#31169;-&#25928;&#29992;&#31227;&#21160;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysing Fairness of Privacy-Utility Mobility Models. (arXiv:2304.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#23450;&#20041;&#20102;&#19968;&#32452;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#65292;&#22522;&#20110;&#36712;&#36857;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#29109;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#20381;&#38752;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#34920;&#31034;&#23398;&#20064;&#38477;&#20302;&#29992;&#25143;&#30340;&#37325;&#26032;&#35782;&#21035;&#29575;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#20248;&#21183;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20849;&#20139;&#26102;&#31354;&#25968;&#25454;&#38598;&#20013;&#65292;&#20445;&#25252;&#20010;&#20154;&#30340;&#38544;&#31169;&#23545;&#20110;&#38450;&#27490;&#22522;&#20110;&#21807;&#19968;&#36712;&#36857;&#30340;&#37325;&#26032;&#35782;&#21035;&#25915;&#20987;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#25216;&#26415;&#24448;&#24448;&#25552;&#20986;&#29702;&#24819;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#20294;&#22522;&#26412;&#24573;&#30053;&#20102;&#31227;&#21160;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#24433;&#21709;&#65292;&#20197;&#21450;&#36825;&#20123;&#25216;&#26415;&#23545;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#26159;&#21542;&#21516;&#31561;&#36866;&#29992;&#12290;&#22312;&#26102;&#31354;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#19982;&#38544;&#31169;&#24847;&#35782;&#27169;&#22411;&#20043;&#38388;&#30340;&#24230;&#37327;&#20173;&#28982;&#19981;&#28165;&#26224;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#23384;&#22312;&#20219;&#20309;&#23450;&#20041;&#30340;&#20844;&#24179;&#24230;&#37327;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#32452;&#19987;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#32780;&#35774;&#35745;&#30340;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#65292;&#22522;&#20110;&#36712;&#36857;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#29109;&#12290;&#22312;&#36825;&#20123;&#23450;&#20041;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#22312;&#25968;&#25454;&#20849;&#20139;&#20013;&#20381;&#38752;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#34920;&#31034;&#23398;&#20064;&#38477;&#20302;&#29992;&#25143;&#30340;&#37325;&#26032;&#35782;&#21035;&#29575;&#65292;&#26816;&#26597;&#20102;&#20854;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#20445;&#35777;&#22312;&#32452;&#19978;&#20844;&#24179;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#22312;&#20010;&#21035;&#20844;&#24179;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving the individuals' privacy in sharing spatial-temporal datasets is critical to prevent re-identification attacks based on unique trajectories. Existing privacy techniques tend to propose ideal privacy-utility tradeoffs, however, largely ignore the fairness implications of mobility models and whether such techniques perform equally for different groups of users. The quantification between fairness and privacy-aware models is still unclear and there barely exists any defined sets of metrics for measuring fairness in the spatial-temporal context. In this work, we define a set of fairness metrics designed explicitly for human mobility, based on structural similarity and entropy of the trajectories. Under these definitions, we examine the fairness of two state-of-the-art privacy-preserving models that rely on GAN and representation learning to reduce the re-identification rate of users for data sharing. Our results show that while both models guarantee group fairness in terms of de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;</title><link>http://arxiv.org/abs/2304.06446</link><description>&lt;p&gt;
SpectFormer: &#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#26159;&#35270;&#35273;Transformer&#25152;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#20854;&#31181;&#31867;&#21253;&#25324;&#22522;&#20110;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#22914;ViT&#12289;DeIT&#65289;&#21644;&#22522;&#20110;&#35889;&#23618;&#65288;&#22914;Fnet&#12289;GFNet&#12289;AFNO&#65289;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#37117;&#23545;Transformer&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22240;&#27492;&#25552;&#20986;&#20102;&#26032;&#30340;Spectformer&#26550;&#26500;&#65292;&#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#34701;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Spectformer&#21487;&#24688;&#24403;&#22320;&#25429;&#25417;&#29305;&#24449;&#34920;&#31034;&#65292;&#19982;&#20854;&#20182;Transformer&#34920;&#24449;&#30456;&#27604;&#65292;&#21487;&#20197;&#25552;&#39640;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;</title><link>http://arxiv.org/abs/2304.06427</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;ECG&#34920;&#24449;&#23398;&#20064;&#22312;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65306;&#20998;&#24067;&#20998;&#26512;&#21450;&#23454;&#39564;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24515;&#30005;&#22270;(ECG)&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;(Self-Supervised Learning, SSL)&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#23454;&#39564;&#65292;&#20351;&#29992;&#19981;&#21516;&#22686;&#24378;&#21644;&#21442;&#25968;&#35780;&#20272;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#65288;&#22914;SimCRL&#12289;BYOL&#21644;SwAV&#65289;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#38024;&#23545;In-Distribution (ID)&#21644;Out-of-Distribution (OOD) ECG&#25968;&#25454;&#30340;&#20132;&#21449;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#27979;&#35797;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SSL&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;SwAV&#65292;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#31181;&#31867;&#30340;ECG&#25968;&#25454;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#29983;&#25104;&#21306;&#38388;&#39044;&#27979;&#26469;&#35299;&#20915;&#39044;&#27979;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;SHapley&#21487;&#21152;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06412</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#35299;&#37322;&#39044;&#27979;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#65306;&#36816;&#31609;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective. (arXiv:2304.06412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#29983;&#25104;&#21306;&#38388;&#39044;&#27979;&#26469;&#35299;&#20915;&#39044;&#27979;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;SHapley&#21487;&#21152;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#20449;&#24687;&#31995;&#32479;&#21644;&#20154;&#24037;&#26234;&#33021;&#34701;&#21512;&#65292;&#20197;&#22686;&#24378;&#36816;&#31609;&#23398;&#39046;&#22495;&#20869;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#27604;&#22914;&#24573;&#30053;&#20851;&#38190;&#29983;&#20135;&#21442;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#20272;&#35745;&#12289;&#20165;&#29983;&#25104;&#28857;&#39044;&#27979;&#32780;&#19981;&#32771;&#34385;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#32570;&#20047;&#20851;&#20110;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#29983;&#25104;&#21306;&#38388;&#39044;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#21464;&#20307;&#30340;SHapley&#21487;&#21152;&#35299;&#37322;&#26469;&#35299;&#20915;&#30740;&#31350;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#29983;&#20135;&#35745;&#21010;&#26696;&#20363;&#20013;&#24471;&#21040;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#35268;&#23450;&#24615;&#20998;&#26512;&#22312;&#31934;&#32454;&#21270;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#24378;&#35843;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a comprehensive, multi-stage machine learning methodology that effectively integrates information systems and artificial intelligence to enhance decision-making processes within the domain of operations research. The proposed framework adeptly addresses common limitations of existing solutions, such as the neglect of data-driven estimation for vital production parameters, exclusive generation of point forecasts without considering model uncertainty, and lacking explanations regarding the sources of such uncertainty. Our approach employs Quantile Regression Forests for generating interval predictions, alongside both local and global variants of SHapley Additive Explanations for the examined predictive process monitoring problem. The practical applicability of the proposed methodology is substantiated through a real-world production planning case study, emphasizing the potential of prescriptive analytics in refining decision-making procedures. This paper accentuates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;&#26694;&#26550;SeerNet&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#20934;&#30830;&#24615;&#33021;&#39044;&#27979;&#22120;&#30340;&#21387;&#32553;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#35745;&#31639;&#25104;&#26412;&#32422;&#26463;&#30340;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2304.06393</link><description>&lt;p&gt;
&#23398;&#20064;&#20934;&#30830;&#30340;&#24615;&#33021;&#39044;&#27979;&#22120;&#29992;&#20110;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Learning Accurate Performance Predictors for Ultrafast Automated Model Compression. (arXiv:2304.06393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;&#26694;&#26550;SeerNet&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#20934;&#30830;&#24615;&#33021;&#39044;&#27979;&#22120;&#30340;&#21387;&#32553;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#35745;&#31639;&#25104;&#26412;&#32422;&#26463;&#30340;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeerNet&#30340;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;&#26694;&#26550;&#65292;&#29992;&#20110;&#28789;&#27963;&#30340;&#32593;&#32476;&#37096;&#32626;&#12290;&#19982;&#20256;&#32479;&#30340;&#38750;&#21487;&#24494;&#20998;&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#20934;&#30830;&#24615;&#33021;&#39044;&#27979;&#22120;&#30340;&#21387;&#32553;&#31574;&#30053;&#26469;&#33719;&#24471;&#26368;&#20248;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#21387;&#32553;&#31574;&#30053;&#25628;&#32034;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#35745;&#31639;&#25104;&#26412;&#32422;&#26463;&#30340;&#36229;&#24555;&#36895;&#33258;&#21160;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an ultrafast automated model compression framework called SeerNet for flexible network deployment. Conventional non-differen-tiable methods discretely search the desirable compression policy based on the accuracy from exhaustively trained lightweight models, and existing differentiable methods optimize an extremely large supernet to obtain the required compressed model for deployment. They both cause heavy computational cost due to the complex compression policy search and evaluation process. On the contrary, we obtain the optimal efficient networks by directly optimizing the compression policy with an accurate performance predictor, where the ultrafast automated model compression for various computational cost constraint is achieved without complex compression policy search and evaluation. Specifically, we first train the performance predictor based on the accuracy from uncertain compression policies actively selected by efficient evolutionary search, so that
&lt;/p&gt;</description></item><item><title>VISION DIFFMASK&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#35782;&#21035;&#26368;&#23567;&#36755;&#20837;&#23376;&#38598;&#26469;&#39044;&#27979;&#23545;&#20854;&#26368;&#32456;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.06391</link><description>&lt;p&gt;
VISION DIFFMASK&#65306;&#20855;&#26377;&#21487;&#24494;&#20998;&#34917;&#19969;&#25513;&#30721;&#30340;&#35270;&#35273;Transformer&#30340;&#24544;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking. (arXiv:2304.06391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06391
&lt;/p&gt;
&lt;p&gt;
VISION DIFFMASK&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#35782;&#21035;&#26368;&#23567;&#36755;&#20837;&#23376;&#38598;&#26469;&#39044;&#27979;&#23545;&#20854;&#26368;&#32456;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#20294;Vision Transformer&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#20851;&#38190;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VISION DIFFMASK&#30340;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#27169;&#22411;&#38544;&#34255;&#23618;&#30340;&#28608;&#27963;&#26469;&#39044;&#27979;&#23545;&#20854;&#26368;&#32456;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#26469;&#35782;&#21035;&#20445;&#30041;&#39044;&#27979;&#31867;&#21035;&#20998;&#24067;&#30340;&#26368;&#23567;&#21407;&#22987;&#36755;&#20837;&#23376;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24544;&#23454;&#24230;&#20219;&#21153;&#24182;&#22312;CIFAR-10&#21644;ImageNet-1K&#19978;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#24037;&#20316;&#30340;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#65306;https://github.com/AngelosNal/Vision-DiffMask
&lt;/p&gt;
&lt;p&gt;
The lack of interpretability of the Vision Transformer may hinder its use in critical real-world applications despite its effectiveness. To overcome this issue, we propose a post-hoc interpretability method called VISION DIFFMASK, which uses the activations of the model's hidden layers to predict the relevant parts of the input that contribute to its final predictions. Our approach uses a gating mechanism to identify the minimal subset of the original input that preserves the predicted distribution over classes. We demonstrate the faithfulness of our method, by introducing a faithfulness task, and comparing it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K, achieving compelling results. To aid reproducibility and further extension of our work, we open source our implementation: https://github.com/AngelosNal/Vision-DiffMask
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23376;&#38598;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#29305;&#24449;&#23376;&#38598;&#30340;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#65292;&#21487;&#22312;ICU&#24739;&#32773;&#20013;&#26089;&#26399;&#39044;&#27979;&#24863;&#26579;&#24615;&#20241;&#20811;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2304.06384</link><description>&lt;p&gt;
&#26089;&#26399;&#24863;&#26579;&#24615;&#20241;&#20811;&#39044;&#27979;&#30340;&#22810;&#23376;&#38598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Subset Approach to Early Sepsis Prediction. (arXiv:2304.06384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23376;&#38598;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#29305;&#24449;&#23376;&#38598;&#30340;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#65292;&#21487;&#22312;ICU&#24739;&#32773;&#20013;&#26089;&#26399;&#39044;&#27979;&#24863;&#26579;&#24615;&#20241;&#20811;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sepsis&#26159;&#30001;&#20110;&#23487;&#20027;&#26080;&#27861;&#25269;&#24481;&#24863;&#26579;&#32780;&#24341;&#36215;&#30340;&#21361;&#21450;&#29983;&#21629;&#30340;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#65292;&#22914;&#26524;&#19981;&#36827;&#34892;&#36866;&#24403;&#21644;&#21450;&#26102;&#30340;&#27835;&#30103;&#65292;&#20250;&#23548;&#33268;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#22312;&#39640;&#21361;&#20154;&#32676;&#20013;&#21450;&#26102;&#35786;&#26029;&#21644;&#27835;&#30103;&#24863;&#26579;&#24615;&#20241;&#20811;&#23545;&#25552;&#20379;&#24739;&#32773;&#24555;&#36895;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#24863;&#26579;&#24615;&#20241;&#20811;&#26816;&#27979;&#25552;&#21069;6&#23567;&#26102;&#21487;&#20197;&#25552;&#39640;&#25239;&#29983;&#32032;&#30340;&#26089;&#26399;&#20351;&#29992;&#29575;&#65292;&#20174;&#32780;&#25913;&#21892;&#30149;&#27515;&#29575;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#24207;&#36143;&#22120;&#23448;&#34928;&#31469;&#35780;&#20998;&#65288;SOFA&#65289;&#20043;&#31867;&#30340;&#20020;&#24202;&#35780;&#20998;&#19981;&#36866;&#29992;&#20110;&#26089;&#26399;&#39044;&#27979;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24110;&#21161;&#25429;&#25417;&#26089;&#26399;&#39044;&#27979;&#30340;&#21457;&#23637;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20020;&#24202;&#24576;&#30097;&#24863;&#26579;&#24615;&#20241;&#20811;&#20043;&#21069;6&#23567;&#26102;&#39044;&#27979;&#20854;&#21457;&#29983;&#12290;&#23613;&#31649;&#24050;&#32463;&#23558;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#24863;&#26579;&#24615;&#20241;&#20811;&#39044;&#27979;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#31639;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;6&#23567;&#26102;&#24182;&#19981;&#26159;&#19968;&#20010;&#23567;&#30340;&#26102;&#38388;&#31383;&#21475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23376;&#38598;&#26041;&#27861;&#65292;&#23558;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#23376;&#38598;&#30340;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#65292;&#20174;&#32780;&#21487;&#20197;&#26089;&#26399;&#39044;&#27979;ICU&#24739;&#32773;&#24863;&#26579;&#24615;&#20241;&#20811;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a life-threatening organ malfunction caused by the host's inability to fight infection, which can lead to death without proper and immediate treatment. Therefore, early diagnosis and medical treatment of sepsis in critically ill populations at high risk for sepsis and sepsis-associated mortality are vital to providing the patient with rapid therapy. Studies show that advancing sepsis detection by 6 hours leads to earlier administration of antibiotics, which is associated with improved mortality. However, clinical scores like Sequential Organ Failure Assessment (SOFA) are not applicable for early prediction, while machine learning algorithms can help capture the progressing pattern for early prediction. Therefore, we aim to develop a machine learning algorithm that predicts sepsis onset 6 hours before it is suspected clinically. Although some machine learning algorithms have been applied to sepsis prediction, many of them did not consider the fact that six hours is not a small
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#36125;&#21494;&#26031;&#27010;&#29575;&#35774;&#32622;&#20026;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#24577;&#31354;&#38388;&#27169;&#22411;&#31995;&#32479;&#35782;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#32473;&#20986;&#20102;&#21518;&#39564;&#20998;&#24067;&#12289;&#21487;&#20449;&#21306;&#38388;&#21450;&#24778;&#24322;&#25351;&#25968;&#65292;&#20197;&#26377;&#25928;&#29956;&#21035;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#24773;&#20917;&#19979;&#30340;&#20351;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06349</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural State-Space Models: Empirical Evaluation of Uncertainty Quantification. (arXiv:2304.06349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#36125;&#21494;&#26031;&#27010;&#29575;&#35774;&#32622;&#20026;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#24577;&#31354;&#38388;&#27169;&#22411;&#31995;&#32479;&#35782;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#32473;&#20986;&#20102;&#21518;&#39564;&#20998;&#24067;&#12289;&#21487;&#20449;&#21306;&#38388;&#21450;&#24778;&#24322;&#25351;&#25968;&#65292;&#20197;&#26377;&#25928;&#29956;&#21035;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#24773;&#20917;&#19979;&#30340;&#20351;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#26159;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#20851;&#38190;&#20219;&#21153;&#20013;&#26356;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#19988;&#20173;&#28982;&#32570;&#22833;&#30340;&#27493;&#39588;&#12290;&#23588;&#20854;&#26159;&#25551;&#36848;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#30446;&#21069;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#31070;&#32463;&#24577;&#31354;&#38388;&#27169;&#22411;&#31995;&#32479;&#35782;&#21035;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#38382;&#39064;&#26694;&#26550;&#25918;&#22312;&#36125;&#21494;&#26031;&#27010;&#29575;&#35774;&#32622;&#20013;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#25512;&#29702;&#25216;&#26415;&#33719;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#26681;&#25454;&#21518;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#26500;&#24314;&#36755;&#20986;&#30340;&#21487;&#20449;&#21306;&#38388;&#65292;&#24182;&#23450;&#20041;&#19968;&#31181;&#24778;&#24322;&#25351;&#25968;&#65292;&#35813;&#25351;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#35786;&#26029;&#27169;&#22411;&#22312;&#28508;&#22312;&#21361;&#38505;&#30340;&#20998;&#24067;&#22806;&#24773;&#20917;&#19979;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#36825;&#26102;&#30340;&#39044;&#27979;&#26159;&#19981;&#33021;&#34987;&#20449;&#20219;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective quantification of uncertainty is an essential and still missing step towards a greater adoption of deep-learning approaches in different applications, including mission-critical ones. In particular, investigations on the predictive uncertainty of deep-learning models describing non-linear dynamical systems are very limited to date. This paper is aimed at filling this gap and presents preliminary results on uncertainty quantification for system identification with neural state-space models. We frame the learning problem in a Bayesian probabilistic setting and obtain posterior distributions for the neural network's weights and outputs through approximate inference techniques. Based on the posterior, we construct credible intervals on the outputs and define a surprise index which can effectively diagnose usage of the model in a potentially dangerous out-of-distribution regime, where predictions cannot be trusted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#25463;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#26680;&#24515;&#32452;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#20351;&#24471;&#26032;&#25968;&#25454;&#38598;&#33021;&#22815;&#24555;&#36895;&#21644;&#31283;&#20581;&#22320;&#38598;&#25104;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25214;&#21040;&#26368;&#36866;&#21512;&#19981;&#21516;&#24212;&#29992;&#30340;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#24212;&#29992;&#22312;&#24211;&#23384;&#31649;&#29702;&#29615;&#22659;&#20013;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06344</link><description>&lt;p&gt;
&#25935;&#25463;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#30340;&#31616;&#21270;&#26694;&#26550;&#25552;&#39640;&#24211;&#23384;&#31649;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Streamlined Framework for Agile Forecasting Model Development towards Efficient Inventory Management. (arXiv:2304.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#25463;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#26680;&#24515;&#32452;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#20351;&#24471;&#26032;&#25968;&#25454;&#38598;&#33021;&#22815;&#24555;&#36895;&#21644;&#31283;&#20581;&#22320;&#38598;&#25104;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25214;&#21040;&#26368;&#36866;&#21512;&#19981;&#21516;&#24212;&#29992;&#30340;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#24212;&#29992;&#22312;&#24211;&#23384;&#31649;&#29702;&#29615;&#22659;&#20013;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#24320;&#21457;&#36807;&#31243;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#26469;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20351;&#24471;&#26032;&#25968;&#25454;&#38598;&#33021;&#22815;&#24555;&#36895;&#21644;&#31283;&#20581;&#22320;&#38598;&#25104;&#65292;&#23454;&#39564;&#19981;&#21516;&#31639;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#20837;&#25163;&#65292;&#24182;&#24212;&#29992;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#28165;&#29702;&#21644;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#20026;&#20102;&#30830;&#23450;&#31283;&#20581;&#30340;&#35757;&#32451;&#37197;&#32622;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#37325;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#26426;&#21046;&#12290;&#25105;&#20204;&#24212;&#29992;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#25214;&#21040;&#26368;&#36866;&#21512;&#19981;&#21516;&#24212;&#29992;&#30340;&#27169;&#22411;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25105;&#20204;&#21442;&#21152;&#20102;&#32654;&#22269;&#22269;&#38469;&#24320;&#21457;&#32626;&#65288;USAID&#65289;&#32452;&#32455;&#30340;&#26234;&#33021;&#39044;&#27979;&#31454;&#36187;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#65292;&#24212;&#29992;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#22312;&#24211;&#23384;&#31649;&#29702;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework for developing forecasting models by streamlining the connections between core components of the developmental process. The proposed framework enables swift and robust integration of new datasets, experimentation on different algorithms, and selection of the best models. We start with the datasets of different issues and apply pre-processing steps to clean and engineer meaningful representations of time-series data. To identify robust training configurations, we introduce a novel mechanism of multiple cross-validation strategies. We apply different evaluation metrics to find the best-suited models for varying applications. One of the referent applications is our participation in the intelligent forecasting competition held by the United States Agency of International Development (USAID). Finally, we leverage the flexibility of the framework by applying different evaluation metrics to assess the performance of the models in inventory management settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.06336</link><description>&lt;p&gt;
&#22810;&#23646;&#24615;&#22810;&#38454;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#20174;&#22810;&#20851;&#31995;&#32593;&#32476;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#20803;&#36335;&#24452;&#65292;&#23427;&#26174;&#30528;&#22320;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#23646;&#24615;&#30340;&#22810;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AMOGCN&#65289;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#33410;&#28857;&#36830;&#25509;&#20013;&#26500;&#24314;&#19981;&#21516;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#20043;&#21518;&#65292;&#20174;&#21508;&#31181;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#21160;&#34701;&#21512;&#20013;&#38468;&#21152;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#12290;&#36825;&#20010;&#36807;&#31243;&#30001;&#20174;&#33410;&#28857;&#21516;&#36136;&#24615;&#36890;&#36807;&#23646;&#24615;&#35780;&#20215;&#25552;&#21462;&#30340;&#33410;&#28857;&#35821;&#20041;&#20449;&#24687;&#30417;&#30563;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#19968;&#23618;&#31616;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31895;&#32454;CNN&#21644;GRU&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#21487;&#38752;&#30340;&#36300;&#20498;&#26816;&#27979;&#65292;&#33021;&#22815;&#36824;&#21407;&#19981;&#21516;&#30340;&#31354;&#38388;&#29305;&#24449;&#31890;&#24230;&#65292;&#24182;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.06335</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38598;&#25104;&#27169;&#22411;&#31895;&#32454;CNN&#21644;GRU&#32593;&#32476;&#30340;&#36300;&#20498;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Fall Detection Algorithm Using Ensemble Model of Coarse-fine CNN and GRU Networks. (arXiv:2304.06335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31895;&#32454;CNN&#21644;GRU&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#21487;&#38752;&#30340;&#36300;&#20498;&#26816;&#27979;&#65292;&#33021;&#22815;&#36824;&#21407;&#19981;&#21516;&#30340;&#31354;&#38388;&#29305;&#24449;&#31890;&#24230;&#65292;&#24182;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36300;&#20498;&#26159;&#20840;&#29699;&#32769;&#24180;&#20154;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#36300;&#20498;&#25152;&#23548;&#33268;&#30340;&#20260;&#23475;&#19982;&#22823;&#37327;&#21307;&#30103;&#25104;&#26412;&#26377;&#20851;&#12290;&#36300;&#20498;&#21487;&#20197;&#36896;&#25104;&#20005;&#37325;&#30340;&#20260;&#23475;&#65292;&#29978;&#33267;&#23548;&#33268;&#27515;&#20129;&#65292;&#22914;&#26524;&#32769;&#24180;&#20154;&#38271;&#26102;&#38388;&#36538;&#30528;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#36300;&#20498;&#26816;&#27979; (FD) &#31995;&#32479;&#65292;&#20197;&#25552;&#20379;&#32039;&#24613;&#27714;&#21161;&#35686;&#25253;&#12290;&#30001;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#19968;&#20123;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;&#24050;&#32463;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#26512;&#20174;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#25910;&#38598;&#30340;&#20449;&#21495;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#36300;&#20498;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#31895;&#32454;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20013;&#25152;&#37319;&#29992;&#30340;&#24182;&#34892;&#32467;&#26500;&#35774;&#35745;&#36824;&#21407;&#20102;&#19981;&#21516;&#31354;&#38388;&#29305;&#24449;&#30340;&#31890;&#24230;&#65292;&#24182;&#25429;&#33719;&#20102;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#23558;FallAllD&#20844;&#20849;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls are the public health issue for the elderly all over the world since the fall-induced injuries are associated with a large amount of healthcare cost. Falls can cause serious injuries, even leading to death if the elderly suffers a "long-lie". Hence, a reliable fall detection (FD) system is required to provide an emergency alarm for first aid. Due to the advances in wearable device technology and artificial intelligence, some fall detection systems have been developed using machine learning and deep learning methods to analyze the signal collected from accelerometer and gyroscopes. In order to achieve better fall detection performance, an ensemble model that combines a coarse-fine convolutional neural network and gated recurrent unit is proposed in this study. The parallel structure design used in this model restores the different grains of spatial characteristics and capture temporal dependencies for feature representation. This study applies the FallAllD public dataset to valida
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26694;&#26550;&#20869;&#23558;&#26377;&#20851;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#35814;&#32454;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#35813;&#20808;&#39564;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26448;&#26009;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06333</link><description>&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Priors for symbolic regression. (arXiv:2304.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26694;&#26550;&#20869;&#23558;&#26377;&#20851;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#35814;&#32454;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#35813;&#20808;&#39564;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26448;&#26009;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#25968;&#25454;&#38598;&#36873;&#25321;&#31526;&#21495;&#27169;&#22411;&#26102;&#65292;&#20154;&#20204;&#33258;&#28982;&#20542;&#21521;&#20110;&#36873;&#25321;&#8220;&#31616;&#21333;&#8221;&#30340;&#34920;&#36798;&#24335;&#25110;&#26356;&#25509;&#36817;&#20043;&#21069;&#22312;&#31867;&#20284;&#24773;&#20917;&#19979;&#30475;&#21040;&#30340;&#26041;&#31243;&#24335;&#12290;&#36825;&#34920;&#26126;&#20989;&#25968;&#24212;&#35813;&#20855;&#26377;&#38750;&#22343;&#21248;&#20808;&#39564;&#30693;&#35782;&#65292;&#28982;&#32780;&#65292;&#22312;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26694;&#26550;&#20869;&#24456;&#23569;&#32771;&#34385;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#26377;&#20851;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#35814;&#32454;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;SR&#20013;&#12290;&#25105;&#20204;&#23545;&#20989;&#25968;&#32467;&#26500;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;&#21508;&#20010;&#36816;&#31639;&#31526;&#30340;&#25490;&#21015;&#26041;&#24335;&#20197;&#21450;&#27599;&#20010;&#36816;&#31639;&#31526;&#30340;&#20986;&#29616;&#39057;&#29575;&#37117;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24335;&#36125;&#21494;&#26031;&#22240;&#23376;&#30340;&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#22788;&#29702;&#25968;&#20540;&#21442;&#25968;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#36125;&#21494;&#26031;&#35777;&#25454;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#65292;&#21516;&#26102;&#26126;&#30830;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#12289;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#20197;&#21450;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#20808;&#39564;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When choosing between competing symbolic models for a data set, a human will naturally prefer the "simpler" expression or the one which more closely resembles equations previously seen in a similar context. This suggests a non-uniform prior on functions, which is, however, rarely considered within a symbolic regression (SR) framework. In this paper we develop methods to incorporate detailed prior information on both functions and their parameters into SR. Our prior on the structure of a function is based on a $n$-gram language model, which is sensitive to the arrangement of operators relative to one another in addition to the frequency of occurrence of each operator. We also develop a formalism based on the Fractional Bayes Factor to treat numerical parameter priors in such a way that models may be fairly compared though the Bayesian evidence, and explicitly compare Bayesian, Minimum Description Length and heuristic methods for model selection. We demonstrate the performance of our pri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#21457;&#29616;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#20294;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06326</link><description>&lt;p&gt;
&#29702;&#35299;&#26680;&#22238;&#24402;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding Overfitting in Adversarial Training in Kernel Regression. (arXiv:2304.06326v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#21457;&#29616;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#20294;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#27491;&#21017;&#21270;&#22238;&#24402;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#24102;&#22122;&#22768;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#24403;&#25915;&#20987;&#21644;&#22122;&#22768;&#22823;&#23567;&#20197;&#21450;&#27491;&#21017;&#21270;&#21442;&#25968;&#36235;&#21521;&#20110;&#38646;&#26102;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#26497;&#38480;&#20844;&#24335;&#12290;&#26681;&#25454;&#35813;&#26497;&#38480;&#20844;&#24335;&#65292;&#20998;&#26512;&#20102;&#29305;&#23450;&#24773;&#20917;&#24182;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#22823;&#20110;&#26631;&#20934;&#26680;&#22238;&#24402;&#30340;&#24191;&#20041;&#35823;&#24046;&#21644;Lipschitz&#24120;&#25968;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26631;&#20934;&#26680;&#22238;&#24402;&#65292;&#36798;&#21040;&#26356;&#23567;&#30340;&#24191;&#20041;&#35823;&#24046;&#21644;Lipschitz&#24120;&#25968;&#12290;&#36825;&#20123;&#21457;&#29616;&#25903;&#25345;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#33021;&#22815;&#32531;&#35299;&#36825;&#31181;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training and data augmentation with noise are widely adopted techniques to enhance the performance of neural networks. This paper investigates adversarial training and data augmentation with noise in the context of regularized regression in a reproducing kernel Hilbert space (RKHS). We establish the limiting formula for these techniques as the attack and noise size, as well as the regularization parameter, tend to zero. Based on this limiting formula, we analyze specific scenarios and demonstrate that, without appropriate regularization, these two methods may have larger generalization error and Lipschitz constant than standard kernel regression. However, by selecting the appropriate regularization parameter, these two methods can outperform standard kernel regression and achieve smaller generalization error and Lipschitz constant. These findings support the empirical observations that adversarial training can lead to overfitting, and appropriate regularization methods, suc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#36827;&#34892;&#34394;&#25311;&#27969;&#37327;&#35745;&#26657;&#20934;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36830;&#32493;&#22320;&#32852;&#21512;&#26657;&#20934;VFMs&#65292;&#21363;&#20351;&#21333;&#29420;&#20117;&#27969;&#37327;&#39640;&#24230;&#19981;&#24179;&#34913;&#20063;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.06310</link><description>&lt;p&gt;
&#24212;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#36827;&#34892;&#34394;&#25311;&#27969;&#37327;&#35745;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Sequential Monte Carlo applied to virtual flow meter calibration. (arXiv:2304.06310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#36827;&#34892;&#34394;&#25311;&#27969;&#37327;&#35745;&#26657;&#20934;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36830;&#32493;&#22320;&#32852;&#21512;&#26657;&#20934;VFMs&#65292;&#21363;&#20351;&#21333;&#29420;&#20117;&#27969;&#37327;&#39640;&#24230;&#19981;&#24179;&#34913;&#20063;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#27979;&#37327;&#30001;&#20110;&#20854;&#22312;&#35774;&#22791;&#19978;&#30340;&#24178;&#39044;&#36739;&#23569;&#19988;&#25104;&#26412;&#20302;&#24265;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#29983;&#20135;&#20013;&#65292;&#34394;&#25311;&#27969;&#37327;&#35745;&#65288;VFM&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#36719;&#27979;&#37327;&#26041;&#27861;&#65292;&#35797;&#22270;&#23454;&#26102;&#20272;&#35745;&#22797;&#30456;&#27969;&#37327;&#12290;VFM&#22522;&#20110;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26657;&#20934;&#12290;&#26657;&#20934;&#39640;&#24230;&#20381;&#36182;&#24212;&#29992;&#31243;&#24207;&#65292;&#26082;&#22240;&#20026;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20063;&#22240;&#20026;&#21487;&#29992;&#27979;&#37327;&#30340;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#20180;&#32454;&#35843;&#25972;VFM&#21442;&#25968;&#20197;&#28385;&#36275;&#20117;&#27979;&#35797;&#21487;&#23454;&#29616;&#26368;&#31934;&#30830;&#30340;&#26657;&#20934;&#65292;&#20294;&#36825;&#21487;&#33021;&#26159;&#32321;&#37325;&#30340;&#65292;&#32780;&#19988;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20117;&#37117;&#26377;&#39057;&#32321;&#30340;&#20117;&#27979;&#35797;&#25968;&#25454;&#21487;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#20135;&#20998;&#31163;&#22120;&#25552;&#20379;&#30340;&#27979;&#37327;&#65292;&#20197;&#21450;&#35266;&#23519;&#21040;&#30340;&#27969;&#37327;&#24212;&#35813;&#31561;&#20110;&#27599;&#20010;&#21333;&#29420;&#20117;&#30340;&#27969;&#37327;&#20043;&#21644;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#32852;&#21512;&#26657;&#20934;VFMs&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#65288;SMC&#65289;&#26041;&#27861;&#26469;&#20272;&#35745;VFM&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#29420;&#20117;&#27969;&#37327;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#19988;&#36830;&#32493;&#30340;VFM&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft-sensors are gaining popularity due to their ability to provide estimates of key process variables with little intervention required on the asset and at a low cost. In oil and gas production, virtual flow metering (VFM) is a popular soft-sensor that attempts to estimate multiphase flow rates in real time. VFMs are based on models, and these models require calibration. The calibration is highly dependent on the application, both due to the great diversity of the models, and in the available measurements. The most accurate calibration is achieved by careful tuning of the VFM parameters to well tests, but this can be work intensive, and not all wells have frequent well test data available. This paper presents a calibration method based on the measurement provided by the production separator, and the assumption that the observed flow should be equal to the sum of flow rates from each individual well. This allows us to jointly calibrate the VFMs continuously. The method applies Sequenti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#26469;&#22788;&#29702;&#35823;&#26631;&#31614;&#25968;&#25454;&#65292;&#36890;&#36807;&#25351;&#23450;&#35823;&#26631;&#31614;&#30340;&#29983;&#25104;&#26426;&#21046;&#65292;&#20351;&#29992;EM&#31639;&#27861;&#36845;&#20195;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06292</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#22788;&#29702;&#35823;&#26631;&#31614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Improved Naive Bayes with Mislabeled Data. (arXiv:2304.06292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#26469;&#22788;&#29702;&#35823;&#26631;&#31614;&#25968;&#25454;&#65292;&#36890;&#36807;&#25351;&#23450;&#35823;&#26631;&#31614;&#30340;&#29983;&#25104;&#26426;&#21046;&#65292;&#20351;&#29992;EM&#31639;&#27861;&#36845;&#20195;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26631;&#31614;&#38169;&#35823;&#26159;&#32463;&#24120;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;&#22914;&#26524;&#26410;&#24471;&#21040;&#22949;&#21892;&#22788;&#29702;&#65292;&#26631;&#31614;&#38169;&#35823;&#20250;&#20005;&#37325;&#24433;&#21709;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;&#23427;&#22312;&#20998;&#26512;&#19978;&#24456;&#31616;&#21333;&#65292;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#27491;&#30830;&#21644;&#38169;&#35823;&#26631;&#31614;&#30340;&#20027;&#35266;&#21028;&#26029;&#12290;&#36890;&#36807;&#25351;&#23450;&#38169;&#35823;&#26631;&#31614;&#30340;&#29983;&#25104;&#26426;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;EM&#31639;&#27861;&#36845;&#20195;&#20248;&#21270;&#30456;&#24212;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22788;&#29702;&#35823;&#26631;&#31614;&#25968;&#25454;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling mistakes are frequently encountered in real-world applications. If not treated well, the labeling mistakes can deteriorate the classification performances of a model seriously. To address this issue, we propose an improved Naive Bayes method for text classification. It is analytically simple and free of subjective judgements on the correct and incorrect labels. By specifying the generating mechanism of incorrect labels, we optimize the corresponding log-likelihood function iteratively by using an EM algorithm. Our simulation and experiment results show that the improved Naive Bayes method greatly improves the performances of the Naive Bayes method with mislabeled data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#39537;&#21160;&#30340;&#21160;&#24577;&#23631;&#34109;&#35774;&#35745;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#23631;&#34109;&#22120;&#33021;&#22815;&#21160;&#24577;&#20998;&#21106;&#12289;&#21512;&#24182;&#21644;&#37325;&#26032;&#35745;&#31639;&#26234;&#33021;&#20307;&#29366;&#24577;&#65292;&#21516;&#26102;&#25903;&#25345;&#26356;&#21152;&#39640;&#25928;&#30340;&#21512;&#25104;&#23631;&#34109;&#22120;&#20197;&#30417;&#25511;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.06281</link><description>&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#21160;&#24577;&#30462;&#22411;&#20445;&#38556;&#29992;&#20110;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#39537;&#21160;&#30340;&#21160;&#24577;&#23631;&#34109;&#35774;&#35745;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#23631;&#34109;&#22120;&#33021;&#22815;&#21160;&#24577;&#20998;&#21106;&#12289;&#21512;&#24182;&#21644;&#37325;&#26032;&#35745;&#31639;&#26234;&#33021;&#20307;&#29366;&#24577;&#65292;&#21516;&#26102;&#25903;&#25345;&#26356;&#21152;&#39640;&#25928;&#30340;&#21512;&#25104;&#23631;&#34109;&#22120;&#20197;&#30417;&#25511;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#21457;&#29616;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#31574;&#30053;&#65292;&#20294;&#22312;&#23398;&#20064;&#21644;&#37096;&#32626;&#38454;&#27573;&#27809;&#26377;&#23433;&#20840;&#20445;&#38556;&#12290;&#34429;&#28982;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#30340;&#23631;&#34109;&#26159;&#30830;&#20445;&#21333;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(RL)&#23433;&#20840;&#30340;&#26377;&#21069;&#36884;&#30340;&#27491;&#24335;&#26041;&#27861;&#65292;&#20294;&#23427;&#22312;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#26102;&#20250;&#23548;&#33268;&#20445;&#23432;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21512;&#25104;&#23631;&#34109;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MBDS&#20197;&#25903;&#25345;MARL&#31639;&#27861;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21512;&#25104;&#20998;&#24067;&#24335;&#23631;&#34109;&#22120;&#65292;&#36825;&#20123;&#23631;&#34109;&#22120;&#26159;&#19982;&#27599;&#20010;MARL&#26234;&#33021;&#20307;&#24182;&#34892;&#36816;&#34892;&#30340;&#21453;&#24212;&#31995;&#32479;&#65292;&#29992;&#20110;&#30417;&#25511;&#21644;&#32416;&#27491;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#35774;&#35745;&#20351;&#24471;&#22312;&#27809;&#26377;&#21327;&#35843;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#21512;&#25104;&#23631;&#34109;&#22120;&#20197;&#30417;&#35270;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#19981;&#30693;&#36947;&#29615;&#22659;&#30340;&#23436;&#25972;&#36716;&#25442;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#23631;&#38556;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#21644;&#26080;&#20154;&#26426;&#24033;&#36923;&#20219;&#21153;&#20013;&#20248;&#20110;LTL&#23631;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents' states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields witho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#39046;&#22495;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#20445;&#35777;&#39640;&#25928;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06277</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#25913;&#36827;&#31574;&#30053;&#20248;&#21270;&#22810;&#39046;&#22495;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Optimizing Multi-Domain Performance with Active Learning-based Improvement Strategies. (arXiv:2304.06277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#39046;&#22495;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#20445;&#35777;&#39640;&#25928;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#22810;&#39046;&#22495;&#24615;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#12290;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#20351;&#27169;&#22411;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23454;&#29616;&#39640;&#24615;&#33021;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#20010;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#36845;&#20195;&#22320;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#25913;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#29289;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#24456;&#23567;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#23601;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving performance in multiple domains is a challenging task, and often requires significant amounts of data to train and test models. Active learning techniques provide a promising solution by enabling models to select the most informative samples for labeling, thus reducing the amount of labeled data required to achieve high performance. In this paper, we present an active learning-based framework for improving performance across multiple domains. Our approach consists of two stages: first, we use an initial set of labeled data to train a base model, and then we iteratively select the most informative samples for labeling to refine the model. We evaluate our approach on several multi-domain datasets, including image classification, sentiment analysis, and object recognition. Our experiments demonstrate that our approach consistently outperforms baseline methods and achieves state-of-the-art performance on several datasets. We also show that our method is highly efficient, requirin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#30041;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#33391;&#22909;&#30340;&#20445;&#23432;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2304.06262</link><description>&lt;p&gt;
&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31639;&#23376;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Canonical and Noncanonical Hamiltonian Operator Inference. (arXiv:2304.06262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#30041;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#33391;&#22909;&#30340;&#20445;&#23432;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#38750;&#20405;&#20837;&#24335;&#21644;&#32467;&#26500;&#20445;&#30041;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#12290;&#22522;&#20110;&#31639;&#23376;&#25512;&#29702;&#30340;&#24605;&#24819;&#65292;&#36825;&#31181;&#25216;&#26415;&#26159;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;&#24555;&#29031;&#25968;&#25454;&#21644;&#31995;&#32479;&#21704;&#23494;&#39039;&#30340;&#28784;&#30418;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#32447;&#24615;&#27714;&#35299;&#38382;&#39064;&#12290;&#36890;&#36807;&#20960;&#20010;&#28041;&#21450;&#22810;&#20010;&#21452;&#26354;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20363;&#23376;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#31616;&#21270;&#27169;&#22411;&#38500;&#20102;&#20934;&#30830;&#19988;&#31283;&#23450;&#22320;&#22788;&#29702;&#22522;&#30784;&#27169;&#24335;&#30340;&#28155;&#21152;&#22806;&#65292;&#36824;&#21487;&#20197;&#24456;&#22909;&#22320;&#20445;&#30041;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20043;&#22806;&#30340;&#23432;&#24658;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method for the nonintrusive and structure-preserving model reduction of canonical and noncanonical Hamiltonian systems is presented. Based on the idea of operator inference, this technique is provably convergent and reduces to a straightforward linear solve given snapshot data and gray-box knowledge of the system Hamiltonian. Examples involving several hyperbolic partial differential equations show that the proposed method yields reduced models which, in addition to being accurate and stable with respect to the addition of basis modes, preserve conserved quantities well outside the range of their training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;MProtoNet&#65292;&#29992;&#20110;&#24102;&#26377;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#30340;&#27880;&#24847;&#27169;&#22359;&#65292;&#27604;&#22235;&#20010;&#29366;&#24577;-of-the-art&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.06258</link><description>&lt;p&gt;
MProtoNet&#65306;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#29992;&#20110;&#24102;&#26377;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging. (arXiv:2304.06258v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;MProtoNet&#65292;&#29992;&#20110;&#24102;&#26377;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#30340;&#27880;&#24847;&#27169;&#22359;&#65292;&#27604;&#22235;&#20010;&#29366;&#24577;-of-the-art&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21307;&#30103;&#21407;&#22411;&#32593;&#32476;&#65288;MProtoNet&#65289;&#65292;&#29992;&#20110;&#23558;ProtoPNet&#25193;&#23637;&#21040;&#20351;&#29992;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;2D&#33258;&#28982;&#22270;&#20687;&#21644;3D mpMRI&#20043;&#38388;&#22312;&#26412;&#22320;&#21270;&#27880;&#24847;&#21306;&#22495;&#26041;&#38754;&#30340;&#19981;&#21516;&#35201;&#27714;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25513;&#33180;&#21644;&#22312;&#32447;CAM&#25439;&#22833;&#30340;&#26032;&#22411;&#27880;&#24847;&#27169;&#22359;&#12290;MProtoNet&#36890;&#36807;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#21407;&#22411;&#21487;&#35270;&#21270;&#32500;&#25345;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#19968;&#20010;&#21253;&#21547;&#33041;&#33014;&#36136;&#30244;&#21644;&#33041;&#33180;&#30244;&#30340;mpMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent applications of deep convolutional neural networks in medical imaging raise concerns about their interpretability. While most explainable deep learning applications use post hoc methods (such as GradCAM) to generate feature attribution maps, there is a new type of case-based reasoning models, namely ProtoPNet and its variants, which identify prototypes during training and compare input image patches with those prototypes. We propose the first medical prototype network (MProtoNet) to extend ProtoPNet to brain tumor classification with 3D multi-parametric magnetic resonance imaging (mpMRI) data. To address different requirements between 2D natural images and 3D mpMRIs especially in terms of localizing attention regions, a new attention module with soft masking and online-CAM loss is introduced. Soft masking helps sharpen attention maps, while online-CAM loss directly utilizes image-level labels when training the attention module. MProtoNet achieves statistically significant improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#22270;&#34920;&#31034;&#22686;&#24378;&#21270;&#21512;&#29289;&#23646;&#24615;&#21644;&#27963;&#24615;&#39044;&#27979;&#20013;&#30340;&#27169;&#22411;&#23398;&#20064;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#65292;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#26377;&#37325;&#35201;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2304.06253</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#22270;&#34920;&#31034;&#22686;&#24378;&#21270;&#21512;&#29289;&#24615;&#36136;&#21644;&#27963;&#24615;&#39044;&#27979;&#20013;&#30340;&#27169;&#22411;&#23398;&#20064;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Model Learning and Interpretation Using Multiple Molecular Graph Representations for Compound Property and Activity Prediction. (arXiv:2304.06253v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#22270;&#34920;&#31034;&#22686;&#24378;&#21270;&#21512;&#29289;&#23646;&#24615;&#21644;&#27963;&#24615;&#39044;&#27979;&#20013;&#30340;&#27169;&#22411;&#23398;&#20064;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#65292;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#26377;&#37325;&#35201;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#20998;&#23376;&#32467;&#26500;&#32780;&#22312;&#21270;&#21512;&#29289;&#24615;&#36136;&#21644;&#27963;&#24615;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#21270;&#21512;&#29289;&#30340;&#34920;&#31034;&#21644;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#34429;&#28982;&#22522;&#20110;&#21407;&#23376;&#32423;&#21035;&#30340;&#20998;&#23376;&#22270;&#34920;&#31034;&#36890;&#24120;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#33258;&#28982;&#25299;&#25169;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#19981;&#33021;&#23436;&#20840;&#34920;&#36798;&#37325;&#35201;&#30340;&#20122;&#32467;&#26500;&#25110;&#21151;&#33021;&#22522;&#22242;&#65292;&#36825;&#20123;&#22522;&#22242;&#23545;&#20998;&#23376;&#24615;&#36136;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#29992;&#38477;&#32500;&#25216;&#26415;&#34701;&#21512;&#39640;&#32423;&#20449;&#24687;&#30340;&#26367;&#20195;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#20004;&#31181;&#34920;&#31034;&#36827;&#34892;&#27169;&#22411;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#20998;&#23376;&#22270;&#34920;&#31034;&#23545;&#27169;&#22411;&#23398;&#20064;&#21644;&#35299;&#37322;&#30340;&#30740;&#31350;&#20173;&#19981;&#22815;&#20805;&#20998;&#12290;&#35299;&#37322;&#24615;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21270;&#23398;&#27934;&#23519;&#21147;&#21644;&#20248;&#21270;&#28789;&#24863;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;GNN&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) demonstrate great performance in compound property and activity prediction due to their capability to efficiently learn complex molecular graph structures. However, two main limitations persist including compound representation and model interpretability. While atom-level molecular graph representations are commonly used because of their ability to capture natural topology, they may not fully express important substructures or functional groups which significantly influence molecular properties. Consequently, recent research proposes alternative representations employing reduction techniques to integrate higher-level information and leverages both representations for model learning. However, there is still a lack of study about different molecular graph representations on model learning and interpretation. Interpretability is also crucial for drug discovery as it can offer chemical insights and inspiration for optimization. Numerous studies attempt to inclu
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#27973;&#23618;&#25110;&#32447;&#24615;&#35299;&#30721;&#36716;&#25442;&#26469;&#32553;&#23567;&#35299;&#30721;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#21033;&#29992;&#36890;&#24120;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#39044;&#31639;&#12289;&#26356;&#24378;&#30340;&#32534;&#30721;&#32593;&#32476;&#21644;&#36845;&#20195;&#32534;&#30721;&#26469;&#20445;&#25345;&#21387;&#32553;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#35299;&#30721;&#22797;&#26434;&#24230;&#38477;&#20302;80%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.06244</link><description>&lt;p&gt;
&#20855;&#26377;&#27973;&#23618;&#35299;&#30721;&#22120;&#30340;&#19981;&#23545;&#31216;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Asymmetrically-powered Neural Image Compression with Shallow Decoders. (arXiv:2304.06244v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06244
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#27973;&#23618;&#25110;&#32447;&#24615;&#35299;&#30721;&#36716;&#25442;&#26469;&#32553;&#23567;&#35299;&#30721;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#21033;&#29992;&#36890;&#24120;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#39044;&#31639;&#12289;&#26356;&#24378;&#30340;&#32534;&#30721;&#32593;&#32476;&#21644;&#36845;&#20195;&#32534;&#30721;&#26469;&#20445;&#25345;&#21387;&#32553;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#35299;&#30721;&#22797;&#26434;&#24230;&#38477;&#20302;80%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#34920;&#29616;&#36234;&#26469;&#36234;&#24378;&#12290;&#20294;&#19982;&#20256;&#32479;&#32534;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#20986;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36825;&#25104;&#20026;&#23454;&#38469;&#37096;&#32626;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#27973;&#23618;&#29978;&#33267;&#32447;&#24615;&#35299;&#30721;&#36716;&#25442;&#26469;&#32553;&#23567;&#35299;&#30721;&#22797;&#26434;&#24230;&#30340;&#24046;&#36317;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#20026;&#20102;&#24357;&#34917;&#30001;&#27492;&#23548;&#33268;&#30340;&#21387;&#32553;&#24615;&#33021;&#19979;&#38477;&#65292;&#20316;&#32773;&#21033;&#29992;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#36890;&#24120;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#37319;&#29992;&#26356;&#24378;&#22823;&#30340;&#32534;&#30721;&#32593;&#32476;&#21644;&#36845;&#20195;&#32534;&#30721;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#36895;&#29575;&#22833;&#30495;&#21644;&#35299;&#30721;&#22797;&#26434;&#24230;&#30340;&#26435;&#34913;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#21069;&#27839;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;Minnen&#31561;&#20154;&#65288;2018&#65289;&#30340;&#24179;&#22343;&#27604;&#20363;&#36229;&#20808;&#39564;&#20307;&#31995;&#32467;&#26500;&#30456;&#31454;&#20105;&#30340;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#24635;&#20307;&#35299;&#30721;&#22797;&#26434;&#24230;80&#65285;&#65292;&#25110;&#32773;&#36229;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural image compression methods have seen increasingly strong performance in recent years. However, they suffer orders of magnitude higher computational complexity compared to traditional codecs, which stands in the way of real-world deployment. This paper takes a step forward in closing this gap in decoding complexity by adopting shallow or even linear decoding transforms. To compensate for the resulting drop in compression performance, we exploit the often asymmetrical computation budget between encoding and decoding, by adopting more powerful encoder networks and iterative encoding. We theoretically formalize the intuition behind, and our experimental results establish a new frontier in the trade-off between rate-distortion and decoding complexity for neural image compression. Specifically, we achieve rate-distortion performance competitive with the established mean-scale hyperprior architecture of Minnen et al. (2018), while reducing the overall decoding complexity by 80 %, or ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06237</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation. (arXiv:2304.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21010;&#20998;ECG&#20013;&#30340;&#20851;&#38190;&#27874;&#24418;&#26159;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#20197;&#25903;&#25345;&#35786;&#26029;&#21644;&#27835;&#30103;&#24515;&#33039;&#30142;&#30149;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#34429;&#28982;&#21033;&#29992;&#20998;&#21106;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23450;&#20301;P&#12289;QRS&#21644;T&#27874;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22788;&#29702;&#21576;&#29616;&#24515;&#24459;&#22833;&#24120;&#30340;&#20449;&#21495;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#21010;&#20998;&#20855;&#26377;&#24191;&#27867;&#24515;&#24459;&#22833;&#24120;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#65292;&#23558;&#20998;&#21106;&#19982;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#21253;&#21547;&#21508;&#31181;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#38598;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20934;&#30830;&#21010;&#20998;&#20102;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#21516;&#26102;&#32467;&#21512;&#20998;&#31867;&#25351;&#23548;&#30340;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate delineation of key waveforms in an ECG is a critical initial step in extracting relevant features to support the diagnosis and treatment of heart conditions. Although deep learning based methods using a segmentation model to locate P, QRS and T waves have shown promising results, their ability to handle signals exhibiting arrhythmia remains unclear. In this study, we propose a novel approach that leverages a deep learning model to accurately delineate signals with a wide range of arrhythmia. Our approach involves training a segmentation model using a hybrid loss function that combines segmentation with the task of arrhythmia classification. In addition, we use a diverse training set containing various arrhythmia types, enabling our model to handle a wide range of challenging cases. Experimental results show that our model accurately delineates signals with a broad range of abnormal rhythm types, and the combined training with classification guidance can effectively reduce fals
&lt;/p&gt;</description></item><item><title>PIRBN&#26159;&#19968;&#31181;&#23616;&#37096;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#27714;&#35299;&#20855;&#26377;&#39640;&#39057;&#29305;&#24449;&#21644;&#19981;&#36866;&#23450;&#35745;&#31639;&#22495;&#30340;PDE&#26041;&#31243;&#65292;&#30456;&#27604;PINN&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;PIRBN&#21487;&#20197;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.06234</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#24452;&#21521;&#22522;&#32593;&#32476;&#65288;PIRBN&#65289;&#65306;&#29992;&#20110;&#27714;&#35299;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#23616;&#37096;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed radial basis network (PIRBN): A local approximation neural network for solving nonlinear PDEs. (arXiv:2304.06234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06234
&lt;/p&gt;
&lt;p&gt;
PIRBN&#26159;&#19968;&#31181;&#23616;&#37096;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#27714;&#35299;&#20855;&#26377;&#39640;&#39057;&#29305;&#24449;&#21644;&#19981;&#36866;&#23450;&#35745;&#31639;&#22495;&#30340;PDE&#26041;&#31243;&#65292;&#30456;&#27604;PINN&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;PIRBN&#21487;&#20197;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26368;&#36817;&#30340;&#28145;&#20837;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#35757;&#32451;&#21518;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24448;&#24448;&#26159;&#23616;&#37096;&#36924;&#36817;&#22120;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24341;&#21457;&#20102;&#36825;&#31181;&#26032;&#22411;&#30340;&#29289;&#29702;&#20449;&#24687;&#24452;&#21521;&#22522;&#32593;&#32476;&#65288;PIRBN&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#23616;&#37096;&#29305;&#24615;&#12290;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;PIRBN&#21482;&#21253;&#21547;&#19968;&#20010;&#38544;&#34255;&#23618;&#21644;&#19968;&#20010;&#24452;&#21521;&#22522;&#8220;&#28608;&#27963;&#8221;&#20989;&#25968;&#12290;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;PIRBN&#21487;&#20197;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#30740;&#31350;&#20102;PIRBN&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;PIRBN&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#22522;&#20110;&#25968;&#20540;&#20363;&#23376;&#65292;PIRBN&#24050;&#34987;&#35777;&#26126;&#27604;PINN&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#39057;&#29305;&#24449;&#21644;&#19981;&#36866;&#23450;&#35745;&#31639;&#22495;&#30340;PDE&#26041;&#31243;&#26041;&#38754;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;PINN&#25968;&#23383;&#25216;&#26415;&#65292;&#20363;&#22914;ad...
&lt;/p&gt;
&lt;p&gt;
Our recent intensive study has found that physics-informed neural networks (PINN) tend to be local approximators after training. This observation leads to this novel physics-informed radial basis network (PIRBN), which can maintain the local property throughout the entire training process. Compared to deep neural networks, a PIRBN comprises of only one hidden layer and a radial basis "activation" function. Under appropriate conditions, we demonstrated that the training of PIRBNs using gradient descendent methods can converge to Gaussian processes. Besides, we studied the training dynamics of PIRBN via the neural tangent kernel (NTK) theory. In addition, comprehensive investigations regarding the initialisation strategies of PIRBN were conducted. Based on numerical examples, PIRBN has been demonstrated to be more effective and efficient than PINN in solving PDEs with high-frequency features and ill-posed computational domains. Moreover, the existing PINN numerical techniques, such as ad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;GPS&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#28304;&#30340;&#20986;&#34892;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;SA-MGCRN&#30340;&#24773;&#22659;&#24863;&#30693;&#22810;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#20197;&#23454;&#29616;&#37326;&#28779;&#30095;&#25955;&#26102;&#20132;&#36890;&#38656;&#27714;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06233</link><description>&lt;p&gt;
&#22522;&#20110;SA-MGCRN&#30340;&#37326;&#28779;&#30095;&#25955;&#24773;&#20917;&#19979;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Situational-Aware Multi-Graph Convolutional Recurrent Network (SA-MGCRN) for Travel Demand Forecasting During Wildfires. (arXiv:2304.06233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;GPS&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#28304;&#30340;&#20986;&#34892;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;SA-MGCRN&#30340;&#24773;&#22659;&#24863;&#30693;&#22810;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#20197;&#23454;&#29616;&#37326;&#28779;&#30095;&#25955;&#26102;&#20132;&#36890;&#38656;&#27714;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#28779;&#30095;&#25955;&#26399;&#38388;&#23454;&#26102;&#20934;&#30830;&#22320;&#39044;&#27979;&#20132;&#36890;&#38656;&#27714;&#23545;&#20110;&#24212;&#24613;&#31649;&#29702;&#20154;&#21592;&#21644;&#20132;&#36890;&#35268;&#21010;&#32773;&#20570;&#20986;&#21450;&#26102;&#21644;&#26126;&#26234;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#22823;&#35268;&#27169;&#32039;&#24613;&#30095;&#25955;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#31227;&#21160;&#35774;&#22791;&#29983;&#25104;&#30340;GPS&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#28304;&#30340;&#20986;&#34892;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#24320;&#23637;&#20102;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#12290;&#22522;&#20110;GPS&#25968;&#25454;&#25512;&#26029;&#20986;&#30340;&#20132;&#36890;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#22522;&#20110;SA-MGCRN&#30340;&#24773;&#22659;&#24863;&#30693;&#22810;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26356;&#26032;&#26041;&#26696;&#20197;&#23454;&#29616;&#37326;&#28779;&#30095;&#25955;&#26399;&#38388;&#20132;&#36890;&#38656;&#27714;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#22312;&#30495;&#23454;&#26696;&#20363;&#8212;&#8212;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#32034;&#35834;&#29595;&#21439;2019&#24180;Kincade&#28779;&#28798;&#20013;&#23545;&#25552;&#20986;&#30340;&#26041;&#27861;&#26694;&#26550;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time forecasting of travel demand during wildfire evacuations is crucial for emergency managers and transportation planners to make timely and better-informed decisions. However, few studies focus on accurate travel demand forecasting in large-scale emergency evacuations. Therefore, this study develops and tests a new methodological framework for modeling trip generation in wildfire evacuations by using (a) large-scale GPS data generated by mobile devices and (b) state-of-the-art AI technologies. The proposed methodology aims at forecasting evacuation trips and other types of trips. Based on the travel demand inferred from the GPS data, we develop a new deep learning model, i.e., Situational-Aware Multi-Graph Convolutional Recurrent Network (SA-MGCRN), along with a model updating scheme to achieve real-time forecasting of travel demand during wildfire evacuations. The proposed methodological framework is tested in this study for a real-world case study: the 2019 Kincade Fire in So
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#20844;&#31435;&#23398;&#26657;&#24341;&#20837;&#30340;&#39044;&#27979;&#31639;&#27861;&#65288;EWS&#65289;&#26410;&#33021;&#25552;&#39640;&#27605;&#19994;&#29575;&#65292;EWS&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#29615;&#22659;&#22240;&#32032;&#24433;&#21709;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2304.06205</link><description>&lt;p&gt;
&#23041;&#26031;&#24247;&#26143;&#20844;&#31435;&#23398;&#26657;&#31038;&#20132;&#39044;&#27979;&#30340;&#22256;&#38590;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Difficult Lessons on Social Prediction from Wisconsin Public Schools. (arXiv:2304.06205v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06205
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20844;&#31435;&#23398;&#26657;&#24341;&#20837;&#30340;&#39044;&#27979;&#31639;&#27861;&#65288;EWS&#65289;&#26410;&#33021;&#25552;&#39640;&#27605;&#19994;&#29575;&#65292;EWS&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#29615;&#22659;&#22240;&#32032;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20844;&#31435;&#23398;&#26657;&#36817;&#26399;&#24341;&#20837;&#20102;&#39044;&#27979;&#31639;&#27861;&#65288;EWS&#65289;&#26469;&#25552;&#39640;&#27605;&#19994;&#29575;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#36807;&#39044;&#27979;&#21738;&#20123;&#23398;&#29983;&#21487;&#33021;&#36864;&#23398;&#65292;&#24110;&#21161;&#23545;&#20010;&#20307;&#23398;&#29983;&#36827;&#34892;&#24178;&#39044;&#12290;&#34429;&#28982;&#25237;&#36164;&#24040;&#22823;&#65292;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;EWS&#26377;&#25928;&#24615;&#30340;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#23041;&#26031;&#24247;&#26143;&#20840;&#21306;&#30340;&#36817;&#21313;&#24180;&#25968;&#25454;&#65292;&#39318;&#27425;&#23545;EWS&#23545;&#27605;&#19994;&#29575;&#30340;&#38271;&#26399;&#24433;&#21709;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#39044;&#27979;&#31995;&#32479;&#25152;&#20570;&#30340;&#39118;&#38505;&#35780;&#20272;&#38750;&#24120;&#20934;&#30830;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#36793;&#32536;&#21270;&#32972;&#26223;&#30340;&#23398;&#29983;&#12290;&#23613;&#31649;&#35813;&#31995;&#32479;&#20934;&#30830;&#24615;&#39640;&#24182;&#19988;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#23427;&#20250;&#23548;&#33268;&#27605;&#19994;&#29575;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#35265;&#35299;&#23384;&#22312;&#65292;&#21363;&#29615;&#22659;&#22240;&#32032;&#65292;&#20363;&#22914;&#23398;&#29983;&#25152;&#22312;&#23398;&#26657;&#25110;&#31038;&#21306;&#30340;&#36136;&#37327;&#65292;&#28153;&#27809;&#20102;EWS&#23545;&#27605;&#19994;&#29575;&#21487;&#33021;&#20135;&#29983;&#30340;&#20219;&#20309;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warning systems (EWS) are prediction algorithms that have recently taken a central role in efforts to improve graduation rates in public schools across the US. These systems assist in targeting interventions at individual students by predicting which students are at risk of dropping out. Despite significant investments and adoption, there remain significant gaps in our understanding of the efficacy of EWS. In this work, we draw on nearly a decade's worth of data from a system used throughout Wisconsin to provide the first large-scale evaluation of the long-term impact of EWS on graduation outcomes.  We present evidence that risk assessments made by the prediction system are highly accurate, including for students from marginalized backgrounds. Despite the system's accuracy and widespread use, we find no evidence that it has led to improved graduation rates. We surface a robust statistical pattern that can explain why these seemingly contradictory insights hold. Namely, environmen
&lt;/p&gt;</description></item><item><title>SURFSUP&#37319;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#36830;&#32493;&#34920;&#31034;&#23545;&#35937;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27969;&#20307;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65307;&#19988;&#33021;&#22815;&#36866;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#22797;&#26434;&#30495;&#23454;&#22330;&#26223;&#21644;&#23545;&#35937;&#65292;&#21487;&#20197;&#21453;&#28436;&#36866;&#29992;&#20110;&#29289;&#20307;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.06197</link><description>&lt;p&gt;
SURFSUP&#65306;&#23398;&#20064;&#26032;&#39062;&#34920;&#38754;&#27969;&#20307;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
SURFSUP: Learning Fluid Simulation for Novel Surfaces. (arXiv:2304.06197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06197
&lt;/p&gt;
&lt;p&gt;
SURFSUP&#37319;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#36830;&#32493;&#34920;&#31034;&#23545;&#35937;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27969;&#20307;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65307;&#19988;&#33021;&#22815;&#36866;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#22797;&#26434;&#30495;&#23454;&#22330;&#26223;&#21644;&#23545;&#35937;&#65292;&#21487;&#20197;&#21453;&#28436;&#36866;&#29992;&#20110;&#29289;&#20307;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#12289;&#22270;&#24418;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#27169;&#25311;&#22797;&#26434;&#22330;&#26223;&#20013;&#27969;&#20307;&#30340;&#21147;&#23398;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#21644;&#21487;&#24494;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#65292;&#20294;&#26159;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#19981;&#33021;&#31934;&#30830;&#22320;&#27169;&#25311;&#27969;&#20307;&#22914;&#20309;&#19982;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#39062;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SURFSUP&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#38544;&#24335;&#34920;&#31034;&#23545;&#35937;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#26174;&#24335;&#34920;&#31034;&#32593;&#26684;&#25110;&#31890;&#23376;&#12290;&#20960;&#20309;&#20307;&#30340;&#36825;&#31181;&#36830;&#32493;&#34920;&#31034;&#20351;&#24471;&#22312;&#38271;&#26102;&#38388;&#27573;&#20869;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#27969;&#20307;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20351;&#35745;&#31639;&#26356;&#21152;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;SURFSUP&#22312;&#31616;&#21333;&#30340;&#24418;&#29366;&#22522;&#20803;&#19978;&#35757;&#32451;&#65292;&#33021;&#22815;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#65292;&#29978;&#33267;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#30495;&#23454;&#22330;&#26223;&#21644;&#23545;&#35937;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#21453;&#28436;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#35774;&#35745;&#31616;&#21333;&#30340;&#29289;&#20307;&#26469;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the mechanics of fluid in complex scenes is vital to applications in design, graphics, and robotics. Learning-based methods provide fast and differentiable fluid simulators, however most prior work is unable to accurately model how fluids interact with genuinely novel surfaces not seen during training. We introduce SURFSUP, a framework that represents objects implicitly using signed distance functions (SDFs), rather than an explicit representation of meshes or particles. This continuous representation of geometry enables more accurate simulation of fluid-object interactions over long time periods while simultaneously making computation more efficient. Moreover, SURFSUP trained on simple shape primitives generalizes considerably out-of-distribution, even to complex real-world scenes and objects. Finally, we show we can invert our model to design simple objects to manipulate fluid flow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#35266;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#33021;&#22815;&#33258;&#21160;&#28385;&#36275;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#29992;&#25143;&#21487;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#26465;&#20214;&#65292;&#22312;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06193</link><description>&lt;p&gt;
&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#25910;&#25947;&#21644;Lipschitz&#38381;&#29615;&#23398;&#20064;&#31574;&#30053;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems. (arXiv:2304.06193v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#35266;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#33021;&#22815;&#33258;&#21160;&#28385;&#36275;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#29992;&#25143;&#21487;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#26465;&#20214;&#65292;&#22312;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#35266;&#27979;&#21160;&#24577;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Youla &#21442;&#25968;&#21270;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340; REN &#27169;&#22411;&#30340;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35813;&#31574;&#30053;&#21442;&#25968;&#21270;&#33258;&#21160;&#28385;&#36275;&#20102;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65288;&#25910;&#25947;&#65289;&#21644;&#29992;&#25143;&#21487;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#65288;Lipschitz&#65289;&#26465;&#20214;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#29992;&#20110;&#23433;&#20840;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32422;&#26463;&#25110;&#25237;&#24433;&#26469;&#24378;&#21046;&#31283;&#23450;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#25311;&#20013;&#27979;&#35797;&#20102;&#26032;&#30340;&#31574;&#30053;&#31867;&#65306;1&#65289;&#30913;&#24748;&#28014;&#65292;2&#65289;&#20498;&#32622;&#26059;&#36716;&#33218;&#25670;&#12290;&#25105;&#20204;&#21457;&#29616; Youla-REN &#22312;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#23637;&#31034;&#23545;&#25932;&#23545;&#25200;&#21160;&#25552;&#39640;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#25511;&#21046;&#21644;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a policy parameterization for learning-based control on nonlinear, partially-observed dynamical systems. The parameterization is based on a nonlinear version of the Youla parameterization and the recently proposed Recurrent Equilibrium Network (REN) class of models. We prove that the resulting Youla-REN parameterization automatically satisfies stability (contraction) and user-tunable robustness (Lipschitz) conditions on the closed-loop system. This means it can be used for safe learning-based control with no additional constraints or projections required to enforce stability or robustness. We test the new policy class in simulation on two reinforcement learning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum. We find that the Youla-REN performs similarly to existing learning-based and optimal control methods while also ensuring stability and exhibiting improved robustness to adversarial disturbances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#22320;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;&#35745;&#31639;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#65292;&#20854;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06174</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#29289;&#20307;&#24863;&#30693;&#31561;&#21464;&#22522;&#20803;&#21453;&#24212;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#30830;&#36807;&#28193;&#24577;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. (arXiv:2304.06174v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#22320;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;&#35745;&#31639;&#26102;&#38388;&#22823;&#22823;&#32553;&#30701;&#65292;&#20854;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#28193;&#24577;&#25628;&#32034;&#22312;&#21270;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#29992;&#20110;&#38416;&#26126;&#21453;&#24212;&#26426;&#29702;&#21644;&#25506;&#32034;&#21453;&#24212;&#32593;&#32476;&#12290;&#20294;&#25628;&#32034;&#31934;&#30830;&#30340;&#19977;&#32500;&#36807;&#28193;&#24577;&#32467;&#26500;&#38656;&#35201;&#22823;&#37327;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#65292;&#22240;&#20026;&#21183;&#33021;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#20307;&#24863;&#30693; SE(3) &#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#65292;&#28385;&#36275;&#29983;&#25104;&#21453;&#24212;&#29289;&#12289;&#36807;&#28193;&#24577;&#21644;&#29983;&#25104;&#29289;&#19977;&#31181;&#32467;&#26500;&#30340;&#25152;&#26377;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#24050;&#30693;&#21453;&#24212;&#29289;&#21644;&#29983;&#25104;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#36807;&#28193;&#24577;&#32467;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#32553;&#30701;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#29983;&#25104;&#30340;&#36807;&#28193;&#24577;&#32467;&#26500;&#19982;&#30495;&#23454;&#32467;&#26500;&#30340;&#24179;&#22343;&#35823;&#24046;&#20026; 0.13 A &#26681;&#22343;&#26041;&#24046;&#12290;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21487;&#20197;&#23454;&#29616;&#21453;&#24212;&#36895;&#29575;&#20272;&#35745;&#25152;&#38656;&#30340;&#31934;&#24230; (2.6 kcal/mol)&#65292;&#24182;&#19988;&#21482;&#38656;&#23545; 14% &#30340;&#32467;&#26524;&#36827;&#34892;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transition state (TS) search is key in chemistry for elucidating reaction mechanisms and exploring reaction networks. The search for accurate 3D TS structures, however, requires numerous computationally intensive quantum chemistry calculations due to the complexity of potential energy surfaces. Here, we developed an object-aware SE(3) equivariant diffusion model that satisfies all physical symmetries and constraints for generating pairs of structures, i.e., reactant, TS, and product, in an elementary reaction. Provided reactant and product, this model generates a TS structure in seconds instead of the hours required when performing quantum chemistry-based optimizations. The generated TS structures achieve an average error of 0.13 A root mean square deviation compared to true TS. With a confidence scoring model for uncertainty quantification, we approach an accuracy required for reaction rate estimation (2.6 kcal/mol) by only performing quantum chemistry-based optimizations on 14% of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#26041;&#27861;NP-Free, &#23454;&#26102;&#22788;&#29702;&#36895;&#24230;&#19988;&#26080;&#38656;&#24402;&#19968;&#21270;&#21644;&#21442;&#25968;&#35843;&#33410;&#12290;</title><link>http://arxiv.org/abs/2304.06168</link><description>&lt;p&gt;
NP-Free&#65306;&#19968;&#31181;&#23454;&#26102;&#12289;&#38750;&#24402;&#19968;&#21270;&#12289;&#20813;&#21442;&#25968;&#35843;&#33410;&#30340;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NP-Free: A Real-Time Normalization-free and Parameter-tuning-free Representation Approach for Open-ended Time Series. (arXiv:2304.06168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#26041;&#27861;NP-Free, &#23454;&#26102;&#22788;&#29702;&#36895;&#24230;&#19988;&#26080;&#38656;&#24402;&#19968;&#21270;&#21644;&#21442;&#25968;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#32852;&#32593;&#35774;&#22791;&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#65292;&#24182;&#26399;&#26395;&#22312;&#23454;&#26102;&#25910;&#38598;&#21644;&#22788;&#29702;&#25968;&#25454;&#65292;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#20197;&#24110;&#21161;&#22312;&#25968;&#25454;&#25366;&#25496;&#24212;&#29992;&#31243;&#24207;&#20013;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#25552;&#21069;&#30693;&#36947;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#30340;&#24635;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#24402;&#19968;&#21270;&#26041;&#27861;&#23545;&#25972;&#20010;&#24207;&#21015;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#34920;&#31034;&#26041;&#27861;&#38656;&#35201;&#29992;&#25143;&#39044;&#20808;&#37197;&#32622;&#21644;&#35843;&#25972;&#26576;&#20123;&#21442;&#25968;&#20197;&#33719;&#24471;&#28385;&#24847;&#30340;&#34920;&#31034;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#38750;&#24402;&#19968;&#21270;&#12289;&#20813;&#21442;&#25968;&#35843;&#33410;&#30340;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#26041;&#27861;NP-Free&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36866;&#24212;&#24615;&#22320;&#22788;&#29702;&#26410;&#30693;&#30340;&#25968;&#25454;&#38271;&#24230;&#24182;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24402;&#19968;&#21270;&#25110;&#21442;&#25968;&#35843;&#33410;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23454;&#26102;&#22788;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As more connected devices are implemented in a cyber-physical world and data is expected to be collected and processed in real time, the ability to handle time series data has become increasingly significant. To help analyze time series in data mining applications, many time series representation approaches have been proposed to convert a raw time series into another series for representing the original time series. However, existing approaches are not designed for open-ended time series (which is a sequence of data points being continuously collected at a fixed interval without any length limit) because these approaches need to know the total length of the target time series in advance and pre-process the entire time series using normalization methods. Furthermore, many representation approaches require users to configure and tune some parameters beforehand in order to achieve satisfactory representation results. In this paper, we propose NP-Free, a real-time Normalization-free and Pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#65292;&#26500;&#36896;&#20986;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#24378;&#21046;&#23454;&#29616;STL&#20013;&#26576;&#19968;&#29255;&#27573;&#30340;&#35268;&#33539;&#28385;&#36275;&#65292;&#21516;&#26102;&#20351;&#29992;BarrierNet&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#26368;&#21518;&#19968;&#23618;&#65292;&#20445;&#35777;&#35268;&#33539;&#30340;&#28385;&#36275;&#65292;&#25552;&#39640;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06160</link><description>&lt;p&gt;
&#20174;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#20013;&#20351;&#29992;BarrierNet&#23398;&#20064;&#20581;&#22766;&#19988;&#27491;&#30830;&#30340;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Robust and Correct Controllers from Signal Temporal Logic Specifications Using BarrierNet. (arXiv:2304.06160v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#65292;&#26500;&#36896;&#20986;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#24378;&#21046;&#23454;&#29616;STL&#20013;&#26576;&#19968;&#29255;&#27573;&#30340;&#35268;&#33539;&#28385;&#36275;&#65292;&#21516;&#26102;&#20351;&#29992;BarrierNet&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#26368;&#21518;&#19968;&#23618;&#65292;&#20445;&#35777;&#35268;&#33539;&#30340;&#28385;&#36275;&#65292;&#25552;&#39640;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#31995;&#32479;&#38656;&#35201;&#28385;&#36275;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;(STL)&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;STL&#30340;&#23450;&#37327;&#35821;&#20041;&#26469;&#23450;&#20041;&#40065;&#26834;&#28385;&#36275;&#30340;&#27010;&#24565;&#12290;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#27491;&#30830;&#24615;&#65292;&#21363;&#20445;&#35777;&#34987;&#25511;&#21046;&#30340;&#31995;&#32479;&#28385;&#36275;&#35268;&#33539;&#65292;&#26159;&#19968;&#20010;&#36817;&#26399;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#26469;&#26500;&#36896;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#39640;&#38454;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;(HOCBFs)&#65292;&#24378;&#21046;&#25191;&#34892;STL&#20013;&#26576;&#19968;&#29255;&#27573;&#30340;&#20844;&#24335;&#30340;&#28385;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;BarrierNet&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#38480;&#21046;&#30340;&#21487;&#24494;&#20998;&#20108;&#27425;&#35268;&#21010;(dQP)&#65292;&#20197;&#20445;&#35777;STL&#20844;&#24335;&#30340;&#28385;&#36275;&#12290;&#25105;&#20204;&#19968;&#36215;&#35757;&#32451;HOCBF&#21644;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;STL&#35268;&#33539;&#30340;&#28385;&#36275;&#65292;&#24182;&#23545;&#25200;&#21160;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning a neural network controller for a system required to satisfy a Signal Temporal Logic (STL) specification. We exploit STL quantitative semantics to define a notion of robust satisfaction. Guaranteeing the correctness of a neural network controller, i.e., ensuring the satisfaction of the specification by the controlled system, is a difficult problem that received a lot of attention recently. We provide a general procedure to construct a set of trainable High Order Control Barrier Functions (HOCBFs) enforcing the satisfaction of formulas in a fragment of STL. We use the BarrierNet, implemented by a differentiable Quadratic Program (dQP) with HOCBF constraints, as the last layer of the neural network controller, to guarantee the satisfaction of the STL formulas. We train the HOCBFs together with other neural network parameters to further improve the robustness of the controller. Simulation results demonstrate that our approach ensures sati
&lt;/p&gt;</description></item><item><title>R&#35821;&#35328;&#30340;growclusters&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#22686;&#24378;&#29256;&#30340;k-means&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#22810;&#32452;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#32858;&#31867;&#25110;&#20998;&#21306;&#65292;&#20989;&#25968;&#21253;&#21547;&#20272;&#35745;&#22810;&#20803;&#25968;&#25454;&#20998;&#21306;&#32467;&#26500;&#30340;&#21151;&#33021;&#65292;&#20351;&#29992;&#24809;&#32602;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#12290;&#24182;&#21487;&#21019;&#24314;&#21487;&#35270;&#21270;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20854;&#25805;&#20316;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06145</link><description>&lt;p&gt;
R&#35821;&#35328;&#30340;growclusters&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
The growclusters Package for R. (arXiv:2304.06145v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06145
&lt;/p&gt;
&lt;p&gt;
R&#35821;&#35328;&#30340;growclusters&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#22686;&#24378;&#29256;&#30340;k-means&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#22810;&#32452;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#32858;&#31867;&#25110;&#20998;&#21306;&#65292;&#20989;&#25968;&#21253;&#21547;&#20272;&#35745;&#22810;&#20803;&#25968;&#25454;&#20998;&#21306;&#32467;&#26500;&#30340;&#21151;&#33021;&#65292;&#20351;&#29992;&#24809;&#32602;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#12290;&#24182;&#21487;&#21019;&#24314;&#21487;&#35270;&#21270;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20854;&#25805;&#20316;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
growclusters&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#19968;&#20010;&#22686;&#24378;&#29256;k-means&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#22810;&#32452;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#32858;&#31867;&#25110;&#20998;&#21306;&#65292;&#27599;&#32452;&#25968;&#25454;&#30340;&#32858;&#31867;&#20013;&#24515;&#37117;&#26469;&#28304;&#20110;&#19968;&#20010;&#20840;&#23616;&#20998;&#21306;&#12290;&#35813;&#36719;&#20214;&#21253;&#21253;&#21547;&#19968;&#20123;&#20272;&#35745;&#22810;&#20803;&#25968;&#25454;&#20998;&#21306;&#32467;&#26500;&#30340;&#20989;&#25968;&#12290;&#20272;&#35745;&#26159;&#22522;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#34920;&#36848;&#25512;&#23548;&#20986;&#30340;&#19968;&#31181;&#24809;&#32602;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;growclusters&#36719;&#20214;&#21253;&#30340;&#19968;&#20123;&#21151;&#33021;&#21644;&#33021;&#21147;&#65292;&#21253;&#25324;&#21019;&#24314;R Shiny&#24212;&#29992;&#31243;&#24207;&#20197;&#21487;&#35270;&#21270;&#23637;&#31034;growclusters&#36719;&#20214;&#21253;&#30340;&#25805;&#20316;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growclusters package for R implements an enhanced version of k-means clustering that allows discovery of local clusterings or partitions for a collection of data sets that each draw their cluster means from a single, global partition. The package contains functions to estimate a partition structure for multivariate data. Estimation is performed under a penalized optimization derived from Bayesian non-parametric formulations. This paper describes some of the functions and capabilities of the growclusters package, including the creation of R Shiny applications designed to visually illustrate the operation and functionality of the growclusters package.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#32534;&#36753;&#21451;&#22909;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06140</link><description>&lt;p&gt;
&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65306;&#21453;&#28436;&#19982;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
An Edit Friendly DDPM Noise Space: Inversion and Manipulations. (arXiv:2304.06140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#32534;&#36753;&#30340;DDPM&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#32534;&#36753;&#21451;&#22909;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21033;&#29992;&#19968;&#31995;&#21015;&#30333;&#22122;&#22768;&#26679;&#26412;&#29983;&#25104;&#22270;&#20687;&#12290;&#31867;&#20284;&#20110;GAN&#65292;&#36825;&#20123;&#22122;&#22768;&#22270;&#21487;&#20197;&#30475;&#20316;&#26159;&#29983;&#25104;&#22270;&#20687;&#30456;&#20851;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21407;&#22987;&#22122;&#22768;&#31354;&#38388;&#27809;&#26377;&#26041;&#20415;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#22312;&#32534;&#36753;&#20219;&#21153;&#20013;&#24456;&#38590;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;DDPM&#30340;&#28508;&#22312;&#22122;&#22768;&#31354;&#38388;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#25163;&#27573;&#36827;&#34892;&#24191;&#27867;&#30340;&#32534;&#36753;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#20219;&#20309;&#32473;&#23450;&#22270;&#20687;&#65288;&#30495;&#23454;&#25110;&#21512;&#25104;&#29983;&#25104;&#65289;&#30340;&#26131;&#20110;&#32534;&#36753;&#22122;&#22768;&#22270;&#30340;&#21453;&#28436;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g., shifting, color edits). Moreover, in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31181;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Transformer&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35828;&#26126;&#36880;&#23618;&#30456;&#20851;&#24615;&#20256;&#25773;&#27604;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#21644;&#20851;&#27880;&#21487;&#35270;&#21270;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;ViT&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.06133</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#23398;&#22270;&#20687;&#35270;&#35273;&#36716;&#25442;&#22120;&#35299;&#37322;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Evaluating Explanations of Vision Transformers for Medical Imaging. (arXiv:2304.06133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31181;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Transformer&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35828;&#26126;&#36880;&#23618;&#30456;&#20851;&#24615;&#20256;&#25773;&#27604;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#21644;&#20851;&#27880;&#21487;&#35270;&#21270;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;ViT&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#36879;&#26126;&#21644;&#21487;&#20449;&#30340;&#20915;&#31574;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#35299;&#37322;&#26041;&#27861;&#36890;&#36807;&#23558;&#37325;&#35201;&#24615;&#24402;&#22240;&#20110;&#36755;&#20837;&#29305;&#24449;&#26469;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20379;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;&#38543;&#30528;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViT&#65289;&#25104;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20854;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31181;&#35299;&#37322;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#23545;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;ViT&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;ViT&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#12289;&#25935;&#24863;&#24230;&#21644;&#22797;&#26434;&#24230;&#30340;&#27010;&#24565;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#36880;&#23618;&#30456;&#20851;&#24615;&#20256;&#25773;&#20248;&#20110;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#21644;&#20851;&#27880;&#21487;&#35270;&#21270;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;ViT&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models increasingly find applications in critical domains such as medical imaging, the need for transparent and trustworthy decision-making becomes paramount. Many explainability methods provide insights into how these models make predictions by attributing importance to input features. As Vision Transformer (ViT) becomes a promising alternative to convolutional neural networks for image classification, its interpretability remains an open research question. This paper investigates the performance of various interpretation methods on a ViT applied to classify chest X-ray images. We introduce the notion of evaluating faithfulness, sensitivity, and complexity of ViT explanations. The obtained results indicate that Layerwise relevance propagation for transformers outperforms Local interpretable model-agnostic explanations and Attention visualization, providing a more accurate and reliable representation of what a ViT has actually learned. Our findings provide insights int
&lt;/p&gt;</description></item><item><title>UniverSeg&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#26410;&#35265;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20132;&#21449;&#22359;&#26426;&#21046;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#30340;53&#20010;&#24320;&#25918;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2304.06131</link><description>&lt;p&gt;
UniverSeg: &#36890;&#29992;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
UniverSeg: Universal Medical Image Segmentation. (arXiv:2304.06131v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06131
&lt;/p&gt;
&lt;p&gt;
UniverSeg&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#26410;&#35265;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20132;&#21449;&#22359;&#26426;&#21046;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#30340;53&#20010;&#24320;&#25918;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#28041;&#21450;&#26032;&#35299;&#21078;&#23398;&#12289;&#22270;&#20687;&#27169;&#24335;&#25110;&#26631;&#31614;&#30340;&#26410;&#35265;&#20998;&#21106;&#20219;&#21153;&#12290;&#38024;&#23545;&#26032;&#30340;&#20998;&#21106;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#24517;&#39035;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26500;&#25104;&#20102;&#23454;&#36136;&#24615;&#30340;&#38556;&#30861;&#65292;&#20182;&#20204;&#24448;&#24448;&#32570;&#20047;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#36164;&#28304;&#21644;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniverSeg&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26410;&#35265;&#30340;&#21307;&#23398;&#20998;&#21106;&#20219;&#21153;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#22270;&#20687;&#21644;&#19968;&#32452;&#23450;&#20041;&#26032;&#20998;&#21106;&#20219;&#21153;&#30340;&#22270;&#20687;-&#26631;&#31614;&#23545;&#31034;&#20363;&#38598;&#65292;UniverSeg&#20351;&#29992;&#26032;&#30340;&#20132;&#21449;&#22359;&#26426;&#21046;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#26631;&#20934;&#21270;&#20102;53&#20010;&#24320;&#25918;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;22,000&#20010;&#25195;&#25551;&#22270;&#20687;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;MegaMedical&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have become the predominant method for medical image segmentation, they are typically not capable of generalizing to unseen segmentation tasks involving new anatomies, image modalities, or labels. Given a new segmentation task, researchers generally have to train or fine-tune models, which is time-consuming and poses a substantial barrier for clinical researchers, who often lack the resources and expertise to train neural networks. We present UniverSeg, a method for solving unseen medical segmentation tasks without additional training. Given a query image and example set of image-label pairs that define a new segmentation task, UniverSeg employs a new Cross-Block mechanism to produce accurate segmentation maps without the need for additional training. To achieve generalization to new tasks, we have gathered and standardized a collection of 53 open-access medical segmentation datasets with over 22,000 scans, which we refer to as MegaMedical. We used this colle
&lt;/p&gt;</description></item><item><title>&#26080;&#26631;&#31614;CBM&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;CBM&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06129</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Label-Free Concept Bottleneck Models. (arXiv:2304.06129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06129
&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#31614;CBM&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;CBM&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBM)&#26159;&#19968;&#31181;&#21019;&#24314;&#26356;&#26131;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#23545;&#24212;&#20110;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CBM&#21450;&#20854;&#21464;&#20307;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#20010;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#25910;&#38598;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#65307;&#20854;&#27425;&#65292;&#22312;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;CBM&#30340;&#20934;&#30830;&#24615;&#36890;&#24120;&#26126;&#26174;&#20302;&#20110;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#26679;&#30340;&#34920;&#29616;&#20026;&#20854;&#22312;&#23454;&#38469;&#19990;&#30028;&#24212;&#29992;&#20013;&#36896;&#25104;&#19968;&#23450;&#30340;&#38556;&#30861;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#26631;&#31614;CBM&#65292;&#23427;&#26159;&#19968;&#31181;&#23558;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;CBM&#30340;&#26032;&#26694;&#26550;&#65292;&#26080;&#38656;&#26631;&#35760;&#27010;&#24565;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26080;&#26631;&#31614;CBM&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#23427;&#26159;&#21487;&#25193;&#23637;&#30340;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#25193;&#23637;&#21040;ImageNet&#30340;CBM&#65292;&#39640;&#25928;&#30340;&#8212;&#8212;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;CBM&#20165;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#65292;&#32780;&#19988;&#21487;&#20197;&#33258;&#21160;&#21270;&#36827;&#34892;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automate
&lt;/p&gt;</description></item><item><title>AutoShot &#21457;&#24067;&#20102;&#19968;&#20221;&#26032;&#30340;&#30701;&#35270;&#39057;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#21517;&#20026; AutoShot &#30340;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#22312; F1 &#20998;&#25968;&#19978;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06116</link><description>&lt;p&gt;
AutoShot&#65306;&#19968;&#20221;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#26368;&#26032;&#30340;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection. (arXiv:2304.06116v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06116
&lt;/p&gt;
&lt;p&gt;
AutoShot &#21457;&#24067;&#20102;&#19968;&#20221;&#26032;&#30340;&#30701;&#35270;&#39057;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#21517;&#20026; AutoShot &#30340;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#22312; F1 &#20998;&#25968;&#19978;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35270;&#39057;&#22312;&#26032;&#30340;&#31038;&#20132;&#23186;&#20307;&#36235;&#21183;&#20013;&#29190;&#21457;&#24615;&#22320;&#27969;&#34892;&#36215;&#26469;&#12290;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#65288;SBD&#65289;&#26159;&#21508;&#31181;&#22330;&#26223;&#20013;&#26368;&#22522;&#26412;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#23545;&#20110;&#35270;&#39057;&#20869;&#23481;&#30340;&#21019;&#24314;&#21644;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;SHOT&#30340;&#26032;&#30340;&#20844;&#20849;&#30701;&#35270;&#39057;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;853&#20010;&#23436;&#25972;&#30340;&#30701;&#35270;&#39057;&#21644;11,606&#20010;&#38236;&#22836;&#27880;&#37322;&#65292;&#20854;&#20013;&#21253;&#25324;200&#20010;&#27979;&#35797;&#35270;&#39057;&#20013;&#30340;2,716&#20010;&#39640;&#36136;&#37327;&#30340;&#38236;&#22836;&#36793;&#30028;&#27880;&#37322;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#25968;&#25454;&#36130;&#23500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoShot&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21253;&#21547;&#21508;&#31181;&#20808;&#36827;&#30340;3D ConvNets&#21644;Transformers&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#26469;&#20248;&#21270;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#27169;&#22411;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#22312;&#36229;&#36807;TransNetV2 4.2&#65285;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The short-form videos have explosive popularity and have dominated the new social media trends. Prevailing short-video platforms,~\textit{e.g.}, Kuaishou (Kwai), TikTok, Instagram Reels, and YouTube Shorts, have changed the way we consume and create content. For video content creation and understanding, the shot boundary detection (SBD) is one of the most essential components in various scenarios. In this work, we release a new public Short video sHot bOundary deTection dataset, named SHOT, consisting of 853 complete short videos and 11,606 shot annotations, with 2,716 high quality shot boundary annotations in 200 test videos. Leveraging this new data wealth, we propose to optimize the model design for video SBD, by conducting neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. Our proposed approach, named AutoShot, achieves higher F1 scores than previous state-of-the-art approaches, e.g., outperforming TransNetV2 by 4.2%, when bein
&lt;/p&gt;</description></item><item><title>PATMAT&#20351;&#29992;&#21442;&#32771;&#22270;&#20687;&#21644;&#24494;&#35843;&#25216;&#26415;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#38754;&#37096;&#20462;&#34917;&#20013;&#30340;&#32454;&#33410;&#21644;&#20154;&#29289;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2304.06107</link><description>&lt;p&gt;
PATMAT: &#38754;&#21521;&#20010;&#20307;&#21270;&#30340;&#38754;&#37096;&#20462;&#22797;&#30340;&#36974;&#32617;&#24863;&#30693;Transformer
&lt;/p&gt;
&lt;p&gt;
PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting. (arXiv:2304.06107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06107
&lt;/p&gt;
&lt;p&gt;
PATMAT&#20351;&#29992;&#21442;&#32771;&#22270;&#20687;&#21644;&#24494;&#35843;&#25216;&#26415;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#38754;&#37096;&#20462;&#34917;&#20013;&#30340;&#32454;&#33410;&#21644;&#20154;&#29289;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#29983;&#25104;&#27169;&#22411;&#22914;StyleGAN2&#21644;Stable Diffusion&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22270;&#20687;&#21512;&#25104;&#12289;&#20462;&#22797;&#21644;&#21435;&#22122;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#29992;&#20110;&#38754;&#37096;&#20462;&#34917;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20445;&#30041;&#32454;&#33410;&#21644;&#20010;&#20154;&#30340;&#36523;&#20221;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#21019;&#24314;&#20986;&#23457;&#32654;&#19978;&#21487;&#20449;&#30340;&#22270;&#20687;&#32467;&#26500;&#21644;&#32441;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Person Aware Tuning (PAT)&#30340;Mask-Aware Transformer (MAT)&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;PATMAT&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#20154;&#30340;&#21442;&#32771;&#22270;&#20687;&#21644;&#24494;&#35843;&#22312;&#38754;&#23380;&#19978;&#35757;&#32451;&#36807;&#30340;MAT&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#36523;&#20221;&#12290;PATMAT&#20351;&#29992;&#32422;40&#20010;&#21442;&#32771;&#22270;&#20687;&#22312;MAT&#30340;&#26679;&#24335;&#27169;&#22359;&#20013;&#21019;&#24314;&#38170;&#28857;&#65292;&#24182;&#20351;&#29992;&#38170;&#28857;&#26469;&#36866;&#24212;&#26032;&#30340;&#38754;&#37096;&#36523;&#20221;&#12290;&#27492;&#22806;&#65292;PATMAT&#21033;&#29992;&#35757;&#32451;&#26399;&#38388;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#22270;&#20687;&#20351;&#27169;&#22411;&#20351;&#29992;&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#32771;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PATMAT&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#29616;&#26377;&#30340;&#38754;&#37096;&#20462;&#34917;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#32454;&#33410;&#21644;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models such as StyleGAN2 and Stable Diffusion have achieved state-of-the-art performance in computer vision tasks such as image synthesis, inpainting, and de-noising. However, current generative models for face inpainting often fail to preserve fine facial details and the identity of the person, despite creating aesthetically convincing image structures and textures. In this work, we propose Person Aware Tuning (PAT) of Mask-Aware Transformer (MAT) for face inpainting, which addresses this issue. Our proposed method, PATMAT, effectively preserves identity by incorporating reference images of a subject and fine-tuning a MAT architecture trained on faces. By using ~40 reference images, PATMAT creates anchor points in MAT's style module, and tunes the model using the fixed anchors to adapt the model to a new face identity. Moreover, PATMAT's use of multiple images per anchor during training allows the model to use fewer reference images than competing methods. We demonstrate th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.06104</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24102;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#30340;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06104
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#22806;&#29983;&#26102;&#38388;&#21464;&#21270;&#19978;&#19979;&#25991;&#24178;&#25200;&#30340;&#26410;&#30693;&#40657;&#30418;&#20989;&#25968;&#30340;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#26368;&#20248;&#35299;&#30340;&#20122;&#32447;&#24615;&#32047;&#31215;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#38646;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#36829;&#35268;&#65292;&#30830;&#20445;&#20102;&#32422;&#26463;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#37319;&#26679;&#23454;&#20363;&#21644;&#36830;&#32493;&#25605;&#25292;&#27133;&#21453;&#24212;&#22120;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#21644;&#24179;&#22343;&#20445;&#25345;&#32422;&#26463;&#21487;&#34892;&#24615;&#65292;&#36825;&#19982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#35201;&#20040;&#36973;&#21463;&#22823;&#37327;&#32047;&#31215;&#36951;&#25022;&#65292;&#35201;&#20040;&#23384;&#22312;&#20005;&#37325;&#32422;&#26463;&#36829;&#35268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#32500;&#21644;&#30417;&#30563;&#24335;&#22238;&#24402;&#65292;&#23545;&#26263;&#29289;&#36136;&#23494;&#24230;&#22330;&#36827;&#34892;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#20223;&#30495;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#21442;&#25968;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06099</link><description>&lt;p&gt;
&#22522;&#20110;&#38477;&#32500;&#21644;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23431;&#23449;&#23494;&#24230;&#22330;&#24555;&#36895;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Fast emulation of cosmological density fields based on dimensionality reduction and supervised machine-learning. (arXiv:2304.06099v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#32500;&#21644;&#30417;&#30563;&#24335;&#22238;&#24402;&#65292;&#23545;&#26263;&#29289;&#36136;&#23494;&#24230;&#22330;&#36827;&#34892;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#20223;&#30495;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24212;&#23545;&#22797;&#26434;&#21442;&#25968;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-body&#27169;&#25311;&#26159;&#30740;&#31350;&#22823;&#23610;&#24230;&#32467;&#26500;&#38750;&#32447;&#24615;&#28436;&#21270;&#30340;&#26368;&#26377;&#21147;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22312;&#38656;&#35201;&#24191;&#27867;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#37319;&#29992;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24555;&#36895;&#30340;&#26263;&#29289;&#36136;&#23494;&#24230;&#22330;&#20223;&#30495;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#22522;&#20110;&#38477;&#32500;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#12290;&#38024;&#23545;&#21333;&#20010;&#33258;&#30001;&#21442;&#25968;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#26263;&#29289;&#36136;&#23494;&#24230;&#21442;&#25968;$\Omega_m$&#65292;&#32780;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#33258;&#30001;&#21442;&#25968;&#30340;&#20223;&#30495;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;$\Omega_m$&#21644;&#32418;&#31227;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#37319;&#29992;&#32473;&#23450;&#22522;&#24213;&#30340;&#27169;&#25311;&#32593;&#26684;&#30340;&#25237;&#24433;&#65307;&#28982;&#21518;&#65292;&#22312;&#35813;&#25237;&#24433;&#32593;&#26684;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#38024;&#23545;&#19981;&#21516;&#21442;&#25968;&#30340;&#26032;&#23494;&#24230;&#31435;&#26041;&#20307;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-body simulations are the most powerful method to study the non-linear evolution of large-scale structure. However, they require large amounts of computational resources, making unfeasible their direct adoption in scenarios that require broad explorations of parameter spaces. In this work, we show that it is possible to perform fast dark matter density field emulations with competitive accuracy using simple machine-learning approaches. We build an emulator based on dimensionality reduction and machine learning regression combining simple Principal Component Analysis and supervised learning methods. For the estimations with a single free parameter, we train on the dark matter density parameter, $\Omega_m$, while for emulations with two free parameters, we train on a range of $\Omega_m$ and redshift. The method first adopts a projection of a grid of simulations on a given basis; then, a machine learning regression is trained on this projected grid. Finally, new density cubes for differe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06094</link><description>&lt;p&gt;
&#33021;&#37327;&#24341;&#23548;&#30340;&#29109;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#21644;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#30784;&#27169;&#22411;&#65288;EBMs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24050;&#32463;&#26377;&#25968;&#21313;&#24180;&#30340;&#21382;&#21490;&#12290;&#33258;&#20004;&#21315;&#24180;&#20195;&#36215;&#65292;&#19968;&#30452;&#26377;&#24456;&#22810;&#39640;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#33021;&#37327;&#21183;&#65288;&#38750;&#24402;&#19968;&#21270;&#30340;&#20284;&#28982;&#20989;&#25968;&#65289;&#26469;&#35299;&#20915;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;OT&#27714;&#35299;&#22120;&#65292;&#21463;&#21040;&#30340;&#25506;&#32034;&#35201;&#23569;&#24471;&#22810;&#65292;&#20165;&#26377;&#19968;&#20123;&#36817;&#26399;&#30340;&#30740;&#31350;&#65288;&#19981;&#21253;&#25324;&#21033;&#29992;OT&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;WGAN&#26041;&#27861;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;EBMs&#21644;&#29109;&#27491;&#21017;&#21270;OT&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#21033;&#29992;&#21069;&#32773;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#25216;&#26415;&#25913;&#36827;&#26469;&#20016;&#23500;&#21518;&#32773;&#12290;&#25105;&#20204;&#22312;2D&#24773;&#26223;&#21644;&#26631;&#20934;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#31616;&#21333;&#36215;&#35265;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#31616;&#30701;&#21644;&#38271;&#36305;&#30340;EBMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20845;&#31181;&#19981;&#21516;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25506;&#32034;&#33719;&#24471;&#20102;&#23500;&#26377;&#25104;&#25928;&#30340; Pareto &#26368;&#20248;&#35299;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#25104;&#26412;&#21516;&#26102;&#28385;&#36275;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.06059</link><description>&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#19978;&#20445;&#25252;&#38544;&#31169;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Learning Models for Privacy-preserving People Counting on Low-resolution Infrared Arrays. (arXiv:2304.06059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20845;&#31181;&#19981;&#21516;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25506;&#32034;&#33719;&#24471;&#20102;&#23500;&#26377;&#25104;&#25928;&#30340; Pareto &#26368;&#20248;&#35299;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#25104;&#26412;&#21516;&#26102;&#28385;&#36275;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;(IR)&#38453;&#21015;&#20256;&#24863;&#22120;&#20026;&#20154;&#27969;&#37327;&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#33021;&#25928;&#39640;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#21344;&#29992;&#30417;&#27979;&#31561;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#33719;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#19968;&#20010;&#23545;&#22522;&#20110;IR&#38453;&#21015;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#21508;&#31181;&#39640;&#25928;DL&#26550;&#26500;&#36827;&#34892;&#24191;&#27867;&#27604;&#36739;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#26550;&#26500;&#19981;&#20165;&#32771;&#34385;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#36824;&#32771;&#34385;&#20102;&#23558;&#20854;&#37096;&#32626;&#22312;&#35760;&#24518;&#21644;&#33021;&#37327;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;(IoT)&#36793;&#32536;&#33410;&#28857;&#19978;&#30340;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;6&#31181;&#19981;&#21516;&#30340;DL&#26550;&#26500;&#65292;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#30001;&#21830;&#19994;8x8&#38453;&#21015;&#25910;&#38598;&#30340;IR&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#65292;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#27599;&#31181;&#27169;&#22411;&#31867;&#22411;&#30340;&#24191;&#27867;&#26550;&#26500;&#25506;&#32034;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#32452;&#23500;&#26377;&#25104;&#25928;&#30340;Pareto&#26368;&#20248;&#35299;&#65292;&#20854;&#20132;&#21449;&#39564;&#35777;&#24179;&#34913;&#20934;&#30830;&#24615;&#24471;&#20998;&#33539;&#22260;&#20026;55.70%-82.70%&#12290;&#24403;&#22312;S&#21830;&#19994;&#24494;&#25511;&#21046;&#22120;(MCU)&#19978;&#37096;&#32626;&#26102;&#65292;&#25152;&#26377;DL&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#22343;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#65292;&#24182;&#22312;&#30465;&#30005;&#27169;&#24335;&#19979;&#28040;&#32791;&#26497;&#23569;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultra-low-resolution Infrared (IR) array sensors offer a low-cost, energy-efficient, and privacy-preserving solution for people counting, with applications such as occupancy monitoring. Previous work has shown that Deep Learning (DL) can yield superior performance on this task. However, the literature was missing an extensive comparative analysis of various efficient DL architectures for IR array-based people counting, that considers not only their accuracy, but also the cost of deploying them on memory- and energy-constrained Internet of Things (IoT) edge nodes. In this work, we address this need by comparing 6 different DL architectures on a novel dataset composed of IR images collected from a commercial 8x8 array, which we made openly available. With a wide architectural exploration of each model type, we obtain a rich set of Pareto-optimal solutions, spanning cross-validated balanced accuracy scores in the 55.70-82.70% range. When deployed on a commercial Microcontroller (MCU) by S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#30340;&#24120;&#29992;&#20844;&#24179;&#24615;&#25514;&#26045;&#30340;&#26368;&#22823;&#38598;&#65292;&#22635;&#34917;&#20102;&#8220;&#19981;&#21487;&#33021;&#23450;&#29702;&#8221;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;12&#20010;&#26368;&#22823;&#38598;&#26159;&#21487;&#33021;&#30340;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.06057</link><description>&lt;p&gt;
&#26368;&#22823;&#20844;&#24179;&#24615;(Maximal Fairness)
&lt;/p&gt;
&lt;p&gt;
Maximal Fairness. (arXiv:2304.06057v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#30340;&#24120;&#29992;&#20844;&#24179;&#24615;&#25514;&#26045;&#30340;&#26368;&#22823;&#38598;&#65292;&#22635;&#34917;&#20102;&#8220;&#19981;&#21487;&#33021;&#23450;&#29702;&#8221;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;12&#20010;&#26368;&#22823;&#38598;&#26159;&#21487;&#33021;&#30340;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20844;&#24179;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#21644;&#31038;&#20250;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#25152;&#35859;&#30340;&#8220;&#19981;&#21487;&#33021;&#23450;&#29702;&#8221;&#26159;&#19968;&#20010;&#26356;&#20026;&#26174;&#33879;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#23427;&#20855;&#26377;&#29702;&#35770;&#21644;&#23454;&#36341;&#21518;&#26524;&#65292;&#22240;&#20026;&#23427;&#35748;&#20026;&#28385;&#36275;&#19968;&#23450;&#32452;&#21512;&#30340;&#20844;&#24179;&#24615;&#25514;&#26045;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#36127;&#38754;&#32467;&#26524;&#36824;&#27809;&#26377;&#24471;&#21040;&#31215;&#26497;&#30340;&#34917;&#20805;:&#21363;&#23545;&#21738;&#20123;&#20844;&#24179;&#24615;&#35266;&#24565;&#30340;&#32452;&#21512;&#26159;&#21487;&#33021;&#30340;&#36827;&#34892;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#35782;&#21035;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#30340;&#24120;&#29992;&#20844;&#24179;&#24615;&#25514;&#26045;&#30340;&#26368;&#22823;&#38598;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#25514;&#26045;&#26159;&#20154;&#21475;&#27604;&#20363;&#24179;&#31561;&#12289;&#24179;&#31561;&#26426;&#20250;&#12289;&#35823;&#21028;&#29575;&#24179;&#31561;&#12289;&#39044;&#27979;&#20934;&#30830;&#24615;&#24179;&#31561;&#12289;&#24635;&#20307;&#20934;&#30830;&#24615;&#24179;&#31561;&#21644;&#27835;&#30103;&#24179;&#31561;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#24635;&#20849;&#26377;12&#20010;&#36825;&#20123;&#20844;&#24179;&#24615;&#25514;&#26045;&#30340;&#26368;&#22823;&#38598;&#26159;&#21487;&#33021;&#30340;&#65292;&#20854;&#20013;&#26377;&#19971;&#31181;&#20004;&#20010;&#25514;&#26045;&#30340;&#32452;&#21512;&#21644;&#20116;&#31181;&#19977;&#20010;&#25514;&#26045;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#36215;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI has garnered quite some attention in research, and increasingly also in society. The so-called "Impossibility Theorem" has been one of the more striking research results with both theoretical and practical consequences, as it states that satisfying a certain combination of fairness measures is impossible. To date, this negative result has not yet been complemented with a positive one: a characterization of which combinations of fairness notions are possible. This work aims to fill this gap by identifying maximal sets of commonly used fairness measures that can be simultaneously satisfied. The fairness measures used are demographic parity, equal opportunity, false positive parity, predictive parity, predictive equality, overall accuracy equality and treatment equality. We conclude that in total 12 maximal sets of these fairness measures are possible, among which seven combinations of two measures, and five combinations of three measures. Our work raises interest questions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#31579;&#36873;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#31579;&#36873;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06054</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#31579;&#36873;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Landslide Susceptibility Prediction Modeling Based on Self-Screening Deep Learning Model. (arXiv:2304.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#31579;&#36873;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#31579;&#36873;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#26131;&#21457;&#24615;&#24314;&#27169;&#23384;&#22312;&#19968;&#20123;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#22914;&#28369;&#22369;&#26679;&#26412;&#35823;&#24046;&#21644;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#31579;&#36873;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;SGCN-LSTM&#65289;&#27169;&#22411;&#65292;&#20197;&#20811;&#26381;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#20013;&#30340;&#19978;&#36848;&#38382;&#39064;&#12290;SGCN-LSTM&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#24615;&#21644;&#33391;&#22909;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#33258;&#31579;&#36873;&#32593;&#32476;&#21487;&#20197;&#28040;&#38500;&#19968;&#23450;&#38408;&#20540;&#21306;&#38388;&#22806;&#20855;&#26377;&#22823;&#35823;&#24046;&#30340;&#28369;&#22369;&#26679;&#26412;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#31354;&#38388;&#33410;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#29615;&#22659;&#22240;&#32032;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;SGCN-LSTM&#27169;&#22411;&#24212;&#29992;&#20110;&#27743;&#35199;&#30465;&#23433;&#36828;&#21439;&#30340;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#65292;&#24182;&#19982;&#22810;&#20803;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26497;&#31471;&#23398;&#20064;&#26426;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#31561;&#22235;&#31181;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SGCN-LSTM&#27169;&#22411;&#22312;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20540;&#65292;&#34920;&#26126;&#20854;&#22312;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslide susceptibility prediction has always been an important and challenging content. However, there are some uncertain problems to be solved in susceptibility modeling, such as the error of landslide samples and the complex nonlinear relationship between environmental factors. A self-screening graph convolutional network and long short-term memory network (SGCN-LSTM) is proposed int this paper to overcome the above problems in landslide susceptibility prediction. The SGCN-LSTM model has the advantages of wide width and good learning ability. The landslide samples with large errors outside the set threshold interval are eliminated by self-screening network, and the nonlinear relationship between environmental factors can be extracted from both spatial nodes and time series, so as to better simulate the nonlinear relationship between environmental factors. The SGCN-LSTM model was applied to landslide susceptibility prediction in Anyuan County, Jiangxi Province, China, and compared w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26500;&#24314;&#21487;&#38752;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#38081;&#36335;&#20449;&#21495;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26377;&#28508;&#21147;&#20026;&#23454;&#29616;&#27491;&#24335;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#25552;&#20379;&#23454;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2304.06052</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#21644;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26377;&#20449;&#24515;&#29289;&#20307;&#26816;&#27979;&#65306;&#38081;&#36335;&#20449;&#21495;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling. (arXiv:2304.06052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26500;&#24314;&#21487;&#38752;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#38081;&#36335;&#20449;&#21495;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26377;&#28508;&#21147;&#20026;&#23454;&#29616;&#27491;&#24335;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#25552;&#20379;&#23454;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#35748;&#35777;&#31995;&#32479;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#25552;&#20379;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#19981;&#30830;&#23450;&#24615;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26500;&#24314;&#21487;&#38752;&#30340;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#26816;&#27979;&#38081;&#36335;&#20449;&#21495;&#30340;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#28779;&#36710;&#25805;&#20316;&#21592;&#35270;&#35282;&#19979;&#30340;&#22270;&#20687;&#21644;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20960;&#31181;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#39118;&#38505;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#25552;&#20379;&#27491;&#24335;&#20445;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#20855;&#26377;&#28508;&#21147;&#65292;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#25552;&#20379;&#20102;&#23454;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying deep learning models in real-world certified systems requires the ability to provide confidence estimates that accurately reflect their uncertainty. In this paper, we demonstrate the use of the conformal prediction framework to construct reliable and trustworthy predictors for detecting railway signals. Our approach is based on a novel dataset that includes images taken from the perspective of a train operator and state-of-the-art object detectors. We test several conformal approaches and introduce a new method based on conformal risk control. Our findings demonstrate the potential of the conformal prediction framework to evaluate model performance and provide practical guidance for achieving formally guaranteed uncertainty bounds.
&lt;/p&gt;</description></item><item><title>Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06051</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#26234;&#33021;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598; - Open-TransMind
&lt;/p&gt;
&lt;p&gt;
Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06051
&lt;/p&gt;
&lt;p&gt;
Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#35745;&#31639;&#33021;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25216;&#26415;&#34987;&#36234;&#26469;&#36234;&#22810;&#30340;&#34892;&#19994;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;&#22312;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#20197;&#19979;&#20856;&#22411;&#25361;&#25112;&#65306;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#65292;&#26088;&#22312;&#22686;&#21152;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#24230;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#35813;&#25361;&#25112;&#20998;&#20026;&#20004;&#20010;&#36187;&#36947;&#65306;&#20840;&#33021;&#22411;&#21644;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20004;&#20010;&#36187;&#36947;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#32447;&#21644;&#22522;&#20934;&#25968;&#25454;&#65292;&#31216;&#20026;Open-TransMind&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#36719;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21253;&#25324;ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24418;&#24335;&#39564;&#35777;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06049</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21040;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#30340;&#31934;&#30830;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#33258;&#21160;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Exact and Cost-Effective Automated Transformation of Neural Network Controllers to Decision Tree Controllers. (arXiv:2304.06049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#36719;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21253;&#25324;ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24418;&#24335;&#39564;&#35777;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#25511;&#21046;&#22120;&#22312;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#21644;&#24847;&#22806;&#34892;&#20026;&#21644;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#30340;&#39118;&#38505;&#23545;&#20110;&#22312;&#20855;&#26377;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#24378;&#20445;&#35777;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#37096;&#32626;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#23558;&#22522;&#20110;NN&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#30340;&#36719;&#20915;&#31574;&#26641;&#65288;SDT&#65289;&#25511;&#21046;&#22120;&#21450;&#20854;&#23545;&#21487;&#39564;&#35777;&#24615;&#30340;&#24433;&#21709;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;argmax&#25805;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#30830;&#20294;&#33410;&#30465;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33258;&#21160;&#21024;&#38500;&#22810;&#20313;&#30340;&#20998;&#25903;&#12290;&#25105;&#20204;&#20351;&#29992;OpenAI Gym&#29615;&#22659;&#30340;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SDT&#36716;&#25442;&#21487;&#20197;&#20351;&#24418;&#24335;&#39564;&#35777;&#21463;&#30410;&#65292;&#26174;&#31034;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, neural network (NN)-based controllers have demonstrated remarkable efficacy in a variety of decision-making tasks. However, their black-box nature and the risk of unexpected behaviors and surprising results pose a challenge to their deployment in real-world systems with strong guarantees of correctness and safety. We address these limitations by investigating the transformation of NN-based controllers into equivalent soft decision tree (SDT)-based controllers and its impact on verifiability. Differently from previous approaches, we focus on discrete-output NN controllers including rectified linear unit (ReLU) activation functions as well as argmax operations. We then devise an exact but cost-effective transformation algorithm, in that it can automatically prune redundant branches. We evaluate our approach using two benchmarks from the OpenAI Gym environment. Our results indicate that the SDT transformation can benefit formal verification, showing runtime improveme
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RELS-DQN&#30340;&#36731;&#37327;&#32423;DQN&#26694;&#26550;&#65292;&#21487;&#20197;&#23637;&#29616;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#24182;&#25552;&#20379;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20854;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#20540;&#35201;&#39640;&#20110;&#25110;&#31561;&#20110;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06048</link><description>&lt;p&gt;
RELS-DQN&#65306;&#32452;&#21512;&#20248;&#21270;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#23616;&#37096;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RELS-DQN: A Robust and Efficient Local Search Framework for Combinatorial Optimization. (arXiv:2304.06048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RELS-DQN&#30340;&#36731;&#37327;&#32423;DQN&#26694;&#26550;&#65292;&#21487;&#20197;&#23637;&#29616;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#24182;&#25552;&#20379;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20854;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#20540;&#35201;&#39640;&#20110;&#25110;&#31561;&#20110;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#26088;&#22312;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#28041;&#21450;&#20174;&#32479;&#35745;&#29289;&#29702;&#21040;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#30340;NP&#38590;&#38382;&#39064;&#12290;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#24212;&#29992;&#21487;&#20197;&#20174;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#22312;&#36138;&#23146;&#31574;&#30053;&#19978;&#36827;&#34892;&#21487;&#36870;&#25805;&#20316;&#12290;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#65288;DQN&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#22797;&#21046;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#21644;&#33719;&#24471;&#19982;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#26041;&#38754;&#24456;&#26377;&#21069;&#36884;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#24687;&#20256;&#36882;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;&#36807;&#20110;&#24179;&#28369;&#21644;&#20449;&#24687;&#20002;&#22833;&#38480;&#21046;&#20102;&#20854;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22823;&#30340;&#28040;&#24687;&#21521;&#37327;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;RELS-DQN&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;DQN&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#21516;&#26102;&#25552;&#20379;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20351;&#29992;&#22312;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#35757;&#32451;&#30340;RELS-DQN&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#20110;&#25110;&#31561;&#20110;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#20540;&#26469;&#25512;&#24191;&#21040;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) aims to efficiently find the best solution to NP-hard problems ranging from statistical physics to social media marketing. A wide range of CO applications can benefit from local search methods because they allow reversible action over greedy policies. Deep Q-learning (DQN) using message-passing neural networks (MPNN) has shown promise in replicating the local search behavior and obtaining comparable results to the local search algorithms. However, the over-smoothing and the information loss during the iterations of message passing limit its robustness across applications, and the large message vectors result in memory inefficiency. Our paper introduces RELS-DQN, a lightweight DQN framework that exhibits the local search behavior while providing practical scalability. Using the RELS-DQN model trained on one application, it can generalize to various applications by providing solution values higher than or equal to both the local search algorithms and the e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06044</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38750;&#32447;&#24615;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#65306;COMM-PINN&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN. (arXiv:2304.06044v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06044
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#36335;&#24452;&#30456;&#20851;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#12290;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#19981;&#20165;&#28385;&#36275;&#25152;&#26377;&#28909;&#21147;&#23398;&#32422;&#26463;&#65292;&#32780;&#19988;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#21152;&#36733;&#24773;&#20917;&#19979;&#65292;&#31435;&#21363;&#25552;&#20379;&#20851;&#20110;&#24403;&#21069;&#26448;&#26009;&#29366;&#24577;&#65288;&#21363;&#33258;&#30001;&#33021;&#65292;&#24212;&#21147;&#21644;&#20869;&#37096;&#21464;&#37327;&#30340;&#28436;&#21464;&#65289;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#21021;&#22987;&#25968;&#25454;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#23427;&#35268;&#36991;&#20102;&#27714;&#35299;&#22797;&#26448;&#26009;&#27169;&#22411;&#20013;&#38750;&#32447;&#24615;&#26041;&#31243;&#25152;&#38656;&#30340;&#37325;&#22797;&#29275;&#39039;&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20943;&#23569;&#33719;&#21462;&#20999;&#21521;&#31639;&#23376;&#25152;&#38656;&#30340;&#23548;&#25968;&#27425;&#24207;&#30340;&#31574;&#30053;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#20219;&#20309;&#26377;&#38480;&#20803;&#31243;&#24207;&#65288;&#25110;&#20854;&#20182;&#25968;&#20540;&#26041;&#27861;&#65289;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23450;&#20041;&#37197;&#28857;&#21644;&#25972;&#21512;&#21516;&#26102;&#28608;&#27963;&#25110;&#38750;&#28608;&#27963;&#30340;&#22810;&#20010;&#38750;&#30456;&#31561;&#32422;&#26463;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivation for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#39550;&#39542;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#29289;&#20256;&#24863;&#31995;&#32479;&#26469;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#27880;&#24847;&#21147;&#29366;&#24577;&#65292;&#35813;&#25506;&#38024;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20998;&#26512;&#21644;&#22788;&#29702;&#33719;&#24471;&#30340;PPG&#20449;&#21495;&#65292;&#20197;&#35782;&#21035;&#19982;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#30456;&#19968;&#33268;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.06041</link><description>&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Systems for Advanced Driving Assistance. (arXiv:2304.06041v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#39550;&#39542;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#29289;&#20256;&#24863;&#31995;&#32479;&#26469;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#27880;&#24847;&#21147;&#29366;&#24577;&#65292;&#35813;&#25506;&#38024;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20998;&#26512;&#21644;&#22788;&#29702;&#33719;&#24471;&#30340;PPG&#20449;&#21495;&#65292;&#20197;&#35782;&#21035;&#19982;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#30456;&#19968;&#33268;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#27773;&#36710;&#36890;&#36807;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#26234;&#33021;&#35780;&#20272;&#36710;&#36742;&#39550;&#39542;&#23433;&#20840;&#12290;&#36825;&#39033;&#23433;&#20840;&#39550;&#39542;&#30417;&#27979;&#21487;&#20197;&#20351;&#29992;&#35768;&#22810;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#24191;&#27867;&#35752;&#35770;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#29305;&#23450;&#30340;&#29983;&#29289;&#20256;&#24863;&#31995;&#32479;&#26469;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#27880;&#24847;&#21147;&#29366;&#24577;&#12290;&#20026;&#20102;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#29289;&#20256;&#24863;&#25506;&#38024;&#30340;&#26041;&#27861;&#65292;&#35813;&#25506;&#38024;&#30001;&#36817;&#32418;&#22806;(NiR)&#20809;&#35889;&#19979;&#30340;&#32806;&#21512;LED&#21644;&#20809;&#30005;&#26816;&#27979;&#22120;&#32452;&#25104;&#12290;&#35813;&#25506;&#38024;&#25918;&#32622;&#22312;&#34987;&#30417;&#27979;&#30340;&#23545;&#35937;&#19978;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#19968;&#31181;&#31216;&#20026;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;(PPG)&#30340;&#29983;&#29702;&#20449;&#21495;&#12290;PPG&#20449;&#21495;&#30340;&#24418;&#25104;&#30001;&#34987;&#30417;&#27979;&#23545;&#35937;&#34880;&#28082;&#20013;&#27687;&#21512;&#21644;&#38750;&#27687;&#21512;&#34880;&#32418;&#34507;&#30333;&#27987;&#24230;&#30340;&#21464;&#21270;&#35843;&#33410;&#65292;&#36825;&#23558;&#30452;&#25509;&#19982;&#30001;&#33258;&#20027;&#31070;&#32463;&#31995;&#32479;(ANS)&#35843;&#33410;&#30340;&#24515;&#33039;&#27963;&#21160;&#30456;&#36830;&#25509;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22788;&#29702;&#21644;&#20998;&#26512;&#33719;&#24471;&#30340;PPG&#20449;&#21495;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#19982;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#19968;&#33268;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next generation cars embed intelligent assessment of car driving safety through innovative solutions often based on usage of artificial intelligence. The safety driving monitoring can be carried out using several methodologies widely treated in scientific literature. In this context, the author proposes an innovative approach that uses ad-hoc bio-sensing system suitable to reconstruct the physio-based attentional status of the car driver. To reconstruct the car driver physiological status, the author proposed the use of a bio-sensing probe consisting of a coupled LEDs at Near infrared (NiR) spectrum with a photodetector. This probe placed over the monitored subject allows to detect a physiological signal called PhotoPlethysmoGraphy (PPG). The PPG signal formation is regulated by the change in oxygenated and non-oxygenated hemoglobin concentration in the monitored subject bloodstream which will be directly connected to cardiac activity in turn regulated by the Autonomic Nervous System (
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36890;&#36807;&#20840;&#22871;&#30005;&#26497;&#25968;&#25454;&#35757;&#32451;&#30340;&#26816;&#27979;&#22120;&#20013;&#20256;&#36755;&#30693;&#35782;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#20026;&#20010;&#20307;&#24739;&#32773;&#23450;&#21046;&#23569;&#37327;&#36890;&#36947;&#65292;&#24182;&#25552;&#20379;&#20102;&#36731;&#37327;&#32423;&#12289;&#39640;&#20934;&#30830;&#29575;&#30340;&#23454;&#29616;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#20840;&#22871;&#30005;&#26497;&#30340;&#26041;&#26696;&#22823;&#20026;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2304.06038</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#24615;&#21270;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#30693;&#35782;&#33976;&#39311;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Distilled Graph Neural Networks for Personalized Epileptic Seizure Detection. (arXiv:2304.06038v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06038
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36890;&#36807;&#20840;&#22871;&#30005;&#26497;&#25968;&#25454;&#35757;&#32451;&#30340;&#26816;&#27979;&#22120;&#20013;&#20256;&#36755;&#30693;&#35782;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#20026;&#20010;&#20307;&#24739;&#32773;&#23450;&#21046;&#23569;&#37327;&#36890;&#36947;&#65292;&#24182;&#25552;&#20379;&#20102;&#36731;&#37327;&#32423;&#12289;&#39640;&#20934;&#30830;&#29575;&#30340;&#23454;&#29616;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#20840;&#22871;&#30005;&#26497;&#30340;&#26041;&#26696;&#22823;&#20026;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#35774;&#22791;&#23545;&#20110;&#30315;&#30187;&#24739;&#32773;&#30340;&#30417;&#27979;&#26816;&#27979;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22823;&#22810;&#20381;&#36182;&#20110;&#20840;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#27979;&#37327;&#30340;&#23436;&#25972;&#30005;&#26497;&#32452;&#65292;&#21487;&#33021;&#23545;&#20110;&#26085;&#24120;&#20351;&#29992;&#19981;&#22826;&#26041;&#20415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#32463;&#36807;&#20840;&#22871;&#30005;&#26497;&#25968;&#25454;&#35757;&#32451;&#30340;&#20808;&#36827;&#30315;&#30187;&#26816;&#27979;&#22120;&#65288;&#31216;&#20026;&#25945;&#24072;&#65289;&#20013;&#20256;&#36755;&#30693;&#35782;&#65292;&#20197;&#23398;&#20064;&#26032;&#30340;&#26816;&#27979;&#22120;&#65288;&#31216;&#20026;&#23398;&#29983;&#65289;&#12290;&#23427;&#20204;&#37117;&#25552;&#20379;&#36731;&#37327;&#32423;&#30340;&#23454;&#29616;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35760;&#24405;&#33041;&#30005;&#22270;&#25152;&#38656;&#30340;&#30005;&#26497;&#25968;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#30315;&#30187;&#26816;&#27979;&#22120;&#37117;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#31215;&#26497;&#20351;&#29992;&#36830;&#25509;&#20449;&#24687;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#65288;a&#65289;&#20351;&#29992;&#39044;&#36873;&#36890;&#36947;&#20026;&#25152;&#26377;&#24739;&#32773;&#23398;&#20064;&#21333;&#20010;&#23398;&#29983;&#65307;&#65288;b&#65289;&#20026;&#27599;&#20010;&#20010;&#20307;&#24739;&#32773;&#20351;&#29992;&#23569;&#37327;&#36890;&#36947;&#23398;&#20064;&#20010;&#24615;&#21270;&#23398;&#29983;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#30315;&#30187;&#26816;&#27979;&#26041;&#27861;&#20351;&#29992;&#26497;&#23569;&#37327;&#30005;&#26497;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable devices for seizure monitoring detection could significantly improve the quality of life of epileptic patients. However, existing solutions that mostly rely on full electrode set of electroencephalogram (EEG) measurements could be inconvenient for every day use. In this paper, we propose a novel knowledge distillation approach to transfer the knowledge from a sophisticated seizure detector (called the teacher) trained on data from the full set of electrodes to learn new detectors (called the student). They are both providing lightweight implementations and significantly reducing the number of electrodes needed for recording the EEG. We consider the case where the teacher and the student seizure detectors are graph neural networks (GNN), since these architectures actively use the connectivity information. We consider two cases (a) when a single student is learnt for all the patients using preselected channels; and (b) when personalized students are learnt for every individual p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#30340;&#36816;&#29992;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110; RL &#30340;&#20132;&#26131;&#31639;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#26377;&#28508;&#21147;&#20248;&#20110;&#20256;&#32479;&#30340;&#20132;&#26131;&#31639;&#27861;&#12290;&#36825;&#19968;&#30740;&#31350;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24341;&#23548;&#26356;&#20026;&#22797;&#26434;&#21644;&#26377;&#25928;&#30340;&#20132;&#26131;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2304.06037</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230; Q &#23398;&#20064;&#36827;&#34892;&#37327;&#21270;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative Trading using Deep Q Learning. (arXiv:2304.06037v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#30340;&#36816;&#29992;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110; RL &#30340;&#20132;&#26131;&#31639;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#26377;&#28508;&#21147;&#20248;&#20110;&#20256;&#32479;&#30340;&#20132;&#26131;&#31639;&#27861;&#12290;&#36825;&#19968;&#30740;&#31350;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24341;&#23548;&#26356;&#20026;&#22797;&#26434;&#21644;&#26377;&#25928;&#30340;&#20132;&#26131;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#24050;&#34987;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#28216;&#25103;&#29609;&#27861;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#20132;&#26131;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#24066;&#22330;&#19978;&#36827;&#34892;&#30408;&#21033;&#20132;&#26131;&#12290;&#26412;&#35770;&#25991;&#25506;&#35752;&#20102; RL &#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#30340;&#20351;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110; RL &#30340;&#20132;&#26131;&#31639;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;RL &#21487;&#20197;&#25104;&#20026;&#37327;&#21270;&#20132;&#26131;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#24182;&#19988;&#23427;&#26377;&#28508;&#21147;&#20248;&#20110;&#20256;&#32479;&#30340;&#20132;&#26131;&#31639;&#27861;&#12290;&#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#34920;&#30528;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24341;&#23548;&#26356;&#20026;&#22797;&#26434;&#21644;&#26377;&#25928;&#30340;&#20132;&#26131;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#20197;&#25506;&#32034;&#20351;&#29992;&#26367;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25972;&#21512;&#20854;&#20182;&#25968;&#25454;&#26469;&#28304;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#36164;&#20135;&#31867;&#21035;&#19978;&#27979;&#35797;&#35813;&#31995;&#32479;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#23427;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a branch of machine learning that has been used in a variety of applications such as robotics, game playing, and autonomous systems. In recent years, there has been growing interest in applying RL to quantitative trading, where the goal is to make profitable trades in financial markets. This paper explores the use of RL in quantitative trading and presents a case study of a RL-based trading algorithm. The results show that RL can be a powerful tool for quantitative trading, and that it has the potential to outperform traditional trading algorithms. The use of reinforcement learning in quantitative trading represents a promising area of research that can potentially lead to the development of more sophisticated and effective trading systems. Future work could explore the use of alternative reinforcement learning algorithms, incorporate additional data sources, and test the system on different asset classes. Overall, our research demonstrates the potential 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#29305;&#24449;&#23545;&#27169;&#22411;&#20256;&#36882;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24110;&#21161;&#36873;&#25321;&#36716;&#31227;&#23398;&#20064;&#28304;&#25968;&#25454;&#38598;&#24182;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06033</link><description>&lt;p&gt;
&#35780;&#20272;&#25968;&#25454;&#29305;&#24449;&#23545;&#30561;&#30496;&#20998;&#26399;&#27169;&#22411;&#20256;&#36882;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Impact of Data Characteristics on the Transferability of Sleep Stage Scoring Models. (arXiv:2304.06033v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#29305;&#24449;&#23545;&#27169;&#22411;&#20256;&#36882;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24110;&#21161;&#36873;&#25321;&#36716;&#31227;&#23398;&#20064;&#28304;&#25968;&#25454;&#38598;&#24182;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#36828;&#31243;&#30561;&#30496;&#30417;&#27979;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#26032;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#25968;&#25454;&#38598;&#65292;&#20250;&#24341;&#21457;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#33719;&#24471;&#27880;&#37322;&#26102;&#65292;&#21738;&#20123;&#19981;&#21516;&#30340;&#25968;&#25454;&#29305;&#24449;&#26368;&#24433;&#21709;&#30561;&#30496;&#20998;&#26399;&#30340;&#24471;&#20998;&#34920;&#29616;&#65292;&#24433;&#21709;&#26377;&#22810;&#22823;&#65311;&#20854;&#27425;&#65292;&#22312;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#25968;&#25454;&#38598;&#20316;&#20026;&#36716;&#31227;&#23398;&#20064;&#30340;&#26469;&#28304;&#65292;&#20197;&#20248;&#21270;&#24615;&#33021;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28304;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#35757;&#32451;&#21644;&#35780;&#20272;&#20004;&#20010;&#26377;&#26174;&#30528;&#26550;&#26500;&#24046;&#24322;&#30340;&#27169;&#22411;TinySleepNet&#21644;U-Time&#65292;&#26469;&#35745;&#31639;&#19981;&#21516;&#25968;&#25454;&#29305;&#24449;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20256;&#36882;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models for scoring sleep stages based on single-channel EEG have been proposed as a promising method for remote sleep monitoring. However, applying these models to new datasets, particularly from wearable devices, raises two questions. First, when annotations on a target dataset are unavailable, which different data characteristics affect the sleep stage scoring performance the most and by how much? Second, when annotations are available, which dataset should be used as the source of transfer learning to optimize performance? In this paper, we propose a novel method for computationally quantifying the impact of different data characteristics on the transferability of deep learning models. Quantification is accomplished by training and evaluating two models with significant architectural differences, TinySleepNet and U-Time, under various transfer configurations in which the source and target datasets have different recording channels, recording environments, and subject c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20262;&#29702;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#20013;&#30340;&#25345;&#32493;&#21327;&#35758;&#25361;&#25112;&#19981;&#23545;&#31216;&#30340;&#26435;&#21147;&#21160;&#24577;&#65292;&#24182;&#25903;&#25345;&#22242;&#38431;&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#30417;&#27979;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.06031</link><description>&lt;p&gt;
&#20844;&#27491;&#65306;&#20174;&#20262;&#29702;&#21407;&#21017;&#21040;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20013;&#19982;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#25345;&#32493;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Fairness: from the ethical principle to the practice of Machine Learning development as an ongoing agreement with stakeholders. (arXiv:2304.06031v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20262;&#29702;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#20013;&#30340;&#25345;&#32493;&#21327;&#35758;&#25361;&#25112;&#19981;&#23545;&#31216;&#30340;&#26435;&#21147;&#21160;&#24577;&#65292;&#24182;&#25903;&#25345;&#22242;&#38431;&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#30417;&#27979;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#26126;&#20102;&#20026;&#20160;&#20040;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#23558;&#27491;&#20041;&#19982;&#20844;&#27491;&#30340;&#20262;&#29702;&#21407;&#21017;&#36716;&#21270;&#20026;&#19982;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#25345;&#32493;&#21327;&#35758;&#65292;&#32435;&#20837;ML&#24320;&#21457;&#23454;&#36341;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#25903;&#25345;&#20262;&#29702;&#30340;&#36845;&#20195;&#36807;&#31243;&#26088;&#22312;&#25361;&#25112;ML&#35774;&#35745;&#20013;&#30340;&#19981;&#23545;&#31216;&#26435;&#21147;&#21160;&#24577;&#65292;&#24110;&#21161;ML&#24320;&#21457;&#22242;&#38431;&#22312;ML&#31995;&#32479;&#24320;&#21457;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#30417;&#27979;&#20559;&#35265;&#12290;&#35813;&#36807;&#31243;&#36824;&#25552;&#20379;&#20102;&#22914;&#20309;&#21521;&#29992;&#25143;&#35299;&#37322;&#20559;&#35265;&#22312;&#26435;&#34913;&#26041;&#38754;&#22987;&#32456;&#19981;&#23436;&#32654;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper clarifies why bias cannot be completely mitigated in Machine Learning (ML) and proposes an end-to-end methodology to translate the ethical principle of justice and fairness into the practice of ML development as an ongoing agreement with stakeholders. The pro-ethical iterative process presented in the paper aims to challenge asymmetric power dynamics in the fairness decision making within ML design and support ML development teams to identify, mitigate and monitor bias at each step of ML systems development. The process also provides guidance on how to explain the always imperfect trade-offs in terms of bias to users.
&lt;/p&gt;</description></item><item><title>ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.05977</link><description>&lt;p&gt;
ImageReward&#65306;&#23398;&#20064;&#21644;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;
&lt;/p&gt;
&lt;p&gt;
ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05977
&lt;/p&gt;
&lt;p&gt;
ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;ImageReward&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#21644;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#22522;&#20110;&#25105;&#20204;&#30340;&#31995;&#32479;&#27880;&#37322;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20998;&#21644;&#25490;&#21517;&#32452;&#20214;&#65292;&#36804;&#20170;&#24050;&#25910;&#38598;&#20102;137k&#30340;&#19987;&#23478;&#27604;&#36739;&#25968;&#25454;&#38598;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;ImageReward&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20998;&#26041;&#27861;&#65288;&#20363;&#22914;&#27604;CLIP&#39640;38.6\%&#65289;&#65292;&#22240;&#27492;&#23427;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#36890;&#36807;\texttt {image-reward}&#31243;&#24207;&#21253;&#20844;&#24320;&#25552;&#20379;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/THUDM/ImageReward}&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
&lt;/p&gt;</description></item><item><title>LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.05869</link><description>&lt;p&gt;
LMR: &#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36712;&#36857;&#39044;&#27979;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
LMR: Lane Distance-Based Metric for Trajectory Prediction. (arXiv:2304.05869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05869
&lt;/p&gt;
&lt;p&gt;
LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#30340;&#24320;&#21457;&#38656;&#35201;&#24230;&#37327;&#26469;&#39564;&#35777;&#21644;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#24050;&#32463;&#30830;&#23450;&#30340;&#24230;&#37327;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#37117;&#32473;&#20986;&#20102;&#30456;&#21516;&#30340;&#35823;&#24046;&#26435;&#37325;&#12290;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#23545;&#20110;&#20687;&#36947;&#36335;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#22949;&#21892;&#25429;&#25417;&#21040;&#19982;&#24213;&#23618;&#36710;&#36947;&#30456;&#20851;&#30340;&#25805;&#20316;&#21592;&#24847;&#22270;&#12290;&#20026;&#20102;&#38024;&#23545;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#21512;&#29702;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#65292;&#21363;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36710;&#36947;&#38169;&#36807;&#29575;&#65288;LMR&#65289;&#12290;&#23545;&#20110;LMR&#30340;&#35745;&#31639;&#65292;&#23558;&#22320;&#38754;&#23454;&#27979;&#21644;&#39044;&#27979;&#31471;&#28857;&#20998;&#37197;&#32473;&#36710;&#36947;&#32447;&#27573;&#65292;&#26356;&#30830;&#20999;&#22320;&#35828;&#26159;&#23427;&#20204;&#30340;&#20013;&#24515;&#32447;&#12290;&#36890;&#36807;&#27839;&#36710;&#36947;&#32447;&#27573;&#30340;&#36317;&#31163;&#27979;&#37327;&#65292;&#39044;&#27979;&#19982;&#23454;&#27979;&#20043;&#38388;&#30340;&#36317;&#31163;&#22312;&#19968;&#23450;&#38408;&#20540;&#33539;&#22260;&#20869;&#30340;&#39044;&#27979;&#34987;&#31216;&#20026;&#21629;&#20013;&#65292;&#21542;&#21017;&#31216;&#20026;&#38169;&#36807;&#12290;LMR&#21017;&#23450;&#20041;&#20026;&#20135;&#29983;&#38169;&#36807;&#30340;&#24207;&#21015;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;LMR&#26159;&#36866;&#29992;&#20110;&#31867;&#20284;&#36710;&#36947;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#30340;&#36712;&#36857;&#39044;&#27979;&#26356;&#20026;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of approaches for trajectory prediction requires metrics to validate and compare their performance. Currently established metrics are based on Euclidean distance, which means that errors are weighted equally in all directions. Euclidean metrics are insufficient for structured environments like roads, since they do not properly capture the agent's intent relative to the underlying lane. In order to provide a reasonable assessment of trajectory prediction approaches with regard to the downstream planning task, we propose a new metric that is lane distance-based: Lane Miss Rate (LMR). For the calculation of LMR, the ground-truth and predicted endpoints are assigned to lane segments, more precisely their centerlines. Measured by the distance along the lane segments, predictions that are within a certain threshold distance to the ground-truth count as hits, otherwise they count as misses. LMR is then defined as the ratio of sequences that yield a miss. Our results on three s
&lt;/p&gt;</description></item><item><title>Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05800</link><description>&lt;p&gt;
Proximity Forest 2.0&#65306;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series. (arXiv:2304.05800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05800
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#30001;&#20110;&#21487;&#33021;&#19982;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21253;&#25324;&#36235;&#21183;&#12289;&#26041;&#24046;&#12289;&#39057;&#29575;&#12289;&#24133;&#24230;&#21644;&#21508;&#31181;&#27169;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#31867;&#21035;&#65292;&#21253;&#25324;&#22522;&#20110;&#30456;&#20284;&#24615;&#12289;&#29305;&#24449;&#21644;&#38388;&#38548;&#12289;&#24418;&#29366;&#12289;&#23383;&#20856;&#12289;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;Proximity Forest&#29256;&#26412;2.0&#65288;PF 2.0&#65289;&#65292;&#23427;&#22312;UCR&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26368;&#36866;&#21512;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;PF 2.0 &#21512;&#24182;&#20102;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#26368;&#36817;&#30340;&#19977;&#20010;&#36827;&#23637;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed, including similarity-based, features and intervals, shapelets, dictionary, kernel, neural network, and hybrid approaches. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms state-of-the-art kernel, neural network, and hybrid methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series simi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05350</link><description>&lt;p&gt;
Astroformer&#65306;&#20998;&#31867;&#24182;&#19981;&#24635;&#26159;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#25110;&#37096;&#32626;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26143;&#31995;&#24418;&#24577;&#23545;&#20110;&#29702;&#35299;&#26143;&#31995;&#30340;&#24418;&#25104;&#21644;&#28436;&#21270;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#38656;&#35201;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#31867;&#26143;&#31995;&#24418;&#24577;&#65292;&#24182;&#20174;&#29616;&#20195;&#22825;&#25991;&#23398;&#35843;&#26597;&#20013;&#25552;&#21462;&#29289;&#29702;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#65292;&#20174;CoAtNet&#21644;MaxViT&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26032;&#22534;&#26632;&#35774;&#35745;&#21644;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#30340;Transformer - &#21367;&#31215;&#28151;&#21512;&#12290;&#24182;&#23558;&#20854;&#19982;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#37197;&#23545;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20180;&#32454;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.04807</link><description>&lt;p&gt;
&#22312;&#24658;&#26143;&#21464;&#24322;&#23384;&#22312;&#19979;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep-learning based measurement of planetary radial velocities in the presence of stellar variability. (arXiv:2304.04807v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24658;&#26143;&#21464;&#24322;&#23384;&#22312;&#19979;&#27979;&#37327;&#23567;&#34892;&#26143;&#30340;&#24452;&#21521;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#38477;&#32500;&#21644;&#25968;&#25454;&#20998;&#21106;&#26041;&#27861;&#65292;&#20197;&#21450;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#21333;&#32447;CNN&#12289;&#21333;&#32447;CNN&#38598;&#21512;&#21644;&#22810;&#32447;CNN&#12290;&#25105;&#20204;&#23558;&#31867;&#20284;&#20110;&#34892;&#26143;&#30340;&#24452;&#21521;&#36895;&#24230;&#27880;&#20837;&#20809;&#35889;&#20013;&#24182;&#20351;&#29992;&#32593;&#32476;&#24674;&#22797;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#32447;CNN&#33021;&#22815;&#24674;&#22797;0.2m/s&#21322;&#25391;&#24133;&#12289;50&#22825;&#21608;&#26399;&#30340;&#34892;&#26143;&#65292;&#20854;&#25391;&#24133;&#35823;&#24046;&#20026;8.8&#65285;&#65292;&#21608;&#26399;&#35823;&#24046;&#20026;0.7&#65285;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep-learning based approach for measuring small planetary radial velocities in the presence of stellar variability. We use neural networks to reduce stellar RV jitter in three years of HARPS-N sun-as-a-star spectra. We develop and compare dimensionality-reduction and data splitting methods, as well as various neural network architectures including single line CNNs, an ensemble of single line CNNs, and a multi-line CNN. We inject planet-like RVs into the spectra and use the network to recover them. We find that the multi-line CNN is able to recover planets with 0.2 m/s semi-amplitude, 50 day period, with 8.8% error in the amplitude and 0.7% in the period. This approach shows promise for mitigating stellar RV variability and enabling the detection of small planetary RVs with unprecedented precision.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21453;&#23556;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25200;&#21160;&#35780;&#20998;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#32422;&#26463;&#21407;&#21017;&#24615;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20197;&#21462;&#20195;&#20043;&#21069;&#37319;&#29992;&#30340;&#23548;&#33268;&#19981;&#33258;&#28982;&#26679;&#26412;&#30340;&#38408;&#20540;&#22788;&#29702;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.04740</link><description>&lt;p&gt;
&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reflected Diffusion Models. (arXiv:2304.04740v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21453;&#23556;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25200;&#21160;&#35780;&#20998;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#32422;&#26463;&#21407;&#21017;&#24615;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20197;&#21462;&#20195;&#20043;&#21069;&#37319;&#29992;&#30340;&#23548;&#33268;&#19981;&#33258;&#28982;&#26679;&#26412;&#30340;&#38408;&#20540;&#22788;&#29702;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22122;&#22768;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#65292;&#25968;&#20540;&#35823;&#24046;&#21487;&#20197;&#32047;&#31215;&#24182;&#23548;&#33268;&#39640;&#24230;&#19981;&#33258;&#28982;&#30340;&#26679;&#26412;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#38408;&#20540;&#22788;&#29702;&#26469;&#32531;&#35299;&#28418;&#31227;&#65292;&#27599;&#27425;&#25193;&#25955;&#27493;&#39588;&#21518;&#25237;&#24433;&#21040;&#33258;&#28982;&#25968;&#25454;&#22495;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#20687;&#32032;&#31354;&#38388;&#65289;&#65292;&#20294;&#36825;&#23548;&#33268;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#21512;&#24182;&#25968;&#25454;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21453;&#21521;&#28436;&#21270;&#22312;&#25968;&#25454;&#25903;&#25345;&#30340;&#21453;&#23556;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#33324;&#21270;&#30340;&#20998;&#25968;&#21305;&#37197;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25200;&#21160;&#35780;&#20998;&#20989;&#25968;&#65292;&#24182;&#25193;&#23637;&#20102;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#25193;&#25955;&#24341;&#23548;&#12289;&#22522;&#20110;&#20284;&#28982;&#30340;&#35757;&#32451;&#21644;ODE&#37319;&#26679;&#12290;&#25105;&#20204;&#36824;&#24357;&#21512;&#20102;&#38408;&#20540;&#22788;&#29702;&#30340;&#29702;&#35770;&#24046;&#36317;:&#36825;&#26679;&#30340;&#26041;&#26696;&#21482;&#26159;&#21453;&#23556;SDE&#30340;&#31163;&#25955;&#21270;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalize score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and ODE sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected SDEs. On standard image benchmarks, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04641</link><description>&lt;p&gt;
&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20854;&#20027;&#35201;&#25903;&#26609;&#20026;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#21516;&#26102;&#23454;&#29616;&#26080;&#31351;&#23567;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#22914;&#20309;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#35299;&#20915;&#26041;&#26696;&#26159;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26435;&#34913;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#32422;&#26463;&#38544;&#31169;&#27844;&#38706;&#19981;&#36229;&#36807;&#39044;&#23450;&#20540;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#38750;&#24120;&#32791;&#26102;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#23384;&#22312;&#24615;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26356;&#39640;&#25928;&#12289;&#26356;&#23481;&#26131;&#34987;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;FL&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;FL&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#37319;&#26679;&#27604;&#29575;&#65292;&#20197;&#24179;&#34913;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26368;&#21518;&#35777;&#26126;FedPAC&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FedPAC&#26694;&#26550;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>PriorCVAE &#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564; MCMC &#21442;&#25968;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#22788;&#29702;&#36229;&#21442;&#25968;&#25512;&#26029;&#19982;&#23398;&#20064;&#20808;&#39564;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26029;&#35010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04307</link><description>&lt;p&gt;
PriorCVAE: &#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637; MCMC &#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04307
&lt;/p&gt;
&lt;p&gt;
PriorCVAE &#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564; MCMC &#21442;&#25968;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#22788;&#29702;&#36229;&#21442;&#25968;&#25512;&#26029;&#19982;&#23398;&#20064;&#20808;&#39564;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26029;&#35010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25512;&#29702;&#36895;&#24230;&#21644;&#27169;&#22411;&#28789;&#27963;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#20855;&#26377;&#38543;&#26426;&#36807;&#31243;&#20808;&#39564;&#30340;&#27169;&#22411;&#20013;&#65288;&#22914;&#39640;&#26031;&#36807;&#31243;&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#30001; GP &#20808;&#39564;&#25110;&#20854;&#26377;&#38480;&#23454;&#29616;&#24341;&#36215;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#19988;&#25152;&#23398;&#29983;&#25104;&#22120;&#21487;&#20197;&#20195;&#26367; MCMC &#25512;&#26029;&#20013;&#30340;&#21407;&#22987;&#20808;&#39564;&#12290;&#34429;&#28982;&#27492;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;&#20294;&#23427;&#20002;&#22833;&#20102;&#20851;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#36229;&#21442;&#25968;&#25512;&#26029;&#19981;&#21487;&#33021;&#21644;&#23398;&#21040;&#30340;&#20808;&#39564;&#27169;&#31946;&#19981;&#28165;&#12290;&#25105;&#20204;&#24314;&#35758;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#65292;&#20197;&#20415;&#36229;&#21442;&#25968;&#19982; GP &#23454;&#29616;&#19968;&#36215;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applied fields where the speed of inference and model flexibility are crucial, the use of Bayesian inference for models with a stochastic process as their prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature has demonstrated that the computational bottleneck caused by GP priors or their finite realizations can be encoded using deep generative models such as variational autoencoders (VAEs), and the learned generators can then be used instead of the original priors during Markov chain Monte Carlo (MCMC) inference in a drop-in manner. While this approach enables fast and highly efficient inference, it loses information about the stochastic process hyperparameters, and, as a consequence, makes inference over hyperparameters impossible and the learned priors indistinct. We propose to resolve the aforementioned issue and disentangle the learned priors by conditioning the VAE on stochastic process hyperparameters. This way, the hyperparameters are encoded alongside GP real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#19978;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#39044;&#27979;&#32500;&#25252;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.04063</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21709;&#24212;&#26354;&#32447;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations of Neural Network-Generated Response Curves. (arXiv:2304.04063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#19978;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#39044;&#27979;&#32500;&#25252;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21709;&#24212;&#26354;&#32447;&#23637;&#31034;&#20102;&#19968;&#20010;&#25935;&#24863;&#31995;&#32479;&#23545;&#19981;&#21516;&#21050;&#28608;&#30340;&#21709;&#24212;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#31995;&#32479;&#30340;&#21453;&#24212;&#21487;&#33021;&#23545;&#22810;&#20010;&#19981;&#19968;&#23450;&#29420;&#31435;&#30340;&#21050;&#28608;&#65288;&#21363;&#36755;&#20837;&#29305;&#24449;&#65289;&#25935;&#24863;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#19968;&#20010;&#36873;&#23450;&#30340;&#36755;&#20837;&#29305;&#24449;&#65288;&#31216;&#20026;&#8220;&#27963;&#21160;&#29305;&#24449;&#8221;&#65289;&#25152;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#30340;&#24418;&#29366;&#21487;&#33021;&#20381;&#36182;&#20110;&#20854;&#20182;&#36755;&#20837;&#29305;&#24449;&#65288;&#31216;&#20026;&#8220;&#34987;&#21160;&#29305;&#24449;&#8221;&#65289;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21709;&#24212;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#26469;&#30830;&#23450;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#19978;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#12290; CFE&#26159;&#36890;&#36807;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20026;&#20027;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#65292;CFE&#25214;&#21040;&#38656;&#35201;&#20462;&#25913;&#30340;&#34987;&#21160;&#29305;&#24449;&#30340;&#26368;&#23567;&#32452;&#21512;&#65292;&#20197;&#25913;&#21464;&#26354;&#32447;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24037;&#19994;&#36164;&#20135;&#39044;&#27979;&#32500;&#25252;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#40657;&#30418;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CFEs&#26469;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Response curves exhibit the magnitude of the response of a sensitive system to a varying stimulus. However, response of such systems may be sensitive to multiple stimuli (i.e., input features) that are not necessarily independent. As a consequence, the shape of response curves generated for a selected input feature (referred to as "active feature") might depend on the values of the other input features (referred to as "passive features"). In this work we consider the case of systems whose response is approximated using regression neural networks. We propose to use counterfactual explanations (CFEs) for the identification of the features with the highest relevance on the shape of response curves generated by neural network black boxes. CFEs are generated by a genetic algorithm-based approach that solves a multi-objective optimization problem. In particular, given a response curve generated for an active feature, a CFE finds the minimum combination of passive features that need to be mod
&lt;/p&gt;</description></item><item><title>EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02012</link><description>&lt;p&gt;
EGC: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02012
&lt;/p&gt;
&lt;p&gt;
EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30456;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#22312;&#19968;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#39033;&#20219;&#21153;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGC&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#36755;&#20986;&#32473;&#23450;&#22270;&#20687;&#30340;&#26631;&#31614;&#65288;&#21363;&#26465;&#20214;&#20998;&#24067;$p(y|\mathbf{x})$&#65289;&#19981;&#21516;&#65292;EGC&#30340;&#21069;&#21521;&#20256;&#36882;&#22120;&#26159;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;$p(\mathbf{x},y)$&#65292;&#22312;&#21518;&#21521;&#20256;&#36882;&#22120;&#20013;&#36890;&#36807;&#36793;&#32536;&#21270;&#26631;&#31614;$y$&#23454;&#29616;&#29983;&#25104;&#22120;&#12290;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#20272;&#35745;&#32473;&#23450;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#37327;&#21644;&#20998;&#31867;&#27010;&#29575;&#65292;&#32780;&#22312;&#21518;&#21521;&#20256;&#36882;&#20013;&#65292;&#36890;&#36807;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;EGC&#22312;ImageNet-1k&#12289;CelebA-HQ&#21644;LSUN Church&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1k&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01432</link><description>&lt;p&gt;
&#38477;&#20302;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Reducing Discretization Error in the Frank-Wolfe Method. (arXiv:2304.01432v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frank-Wolfe&#31639;&#27861;&#26159;&#32467;&#26500;&#21463;&#38480;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#24555;&#36895;&#36845;&#20195;&#22797;&#26434;&#24230;&#32780;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#30001;&#20110;&#27493;&#38271;&#26041;&#21521;&#30340;&#19981;&#35268;&#21017;&#38663;&#33633;&#32780;&#38590;&#20197;&#21152;&#36895;&#65292;&#21363;&#20351;&#22312;&#25509;&#36817;&#35299;&#30340;&#28176;&#36817;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#31163;&#25955;&#21270;&#30340;&#20135;&#29289;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;Frank-Wolfe&#30340;&#27969;&#65288;&#21363;&#28176;&#36817;&#23567;&#27493;&#38271;&#24773;&#20917;&#19979;&#30340;&#36712;&#36857;&#65289;&#19981;&#20250;&#20986;&#29616;&#19981;&#35268;&#21017;&#38663;&#33633;&#65292;&#22240;&#27492;&#20943;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#23558;&#19982;&#20135;&#29983;&#26356;&#31283;&#23450;&#30340;&#26041;&#27861;&#21644;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#65306;&#19968;&#20010;&#22810;&#27493;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#21644;&#19968;&#20010;&#20855;&#26377;&#38477;&#20302;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#22312;&#19968;&#33324;&#20984;&#38598;&#19978;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#20174;$O(1/k)$&#21152;&#36895;&#21040;$O(1/k^{3/2})$ &#12290;
&lt;/p&gt;
&lt;p&gt;
The Frank-Wolfe algorithm is a popular method in structurally constrained machine learning applications, due to its fast per-iteration complexity. However, one major limitation of the method is a slow rate of convergence that is difficult to accelerate due to erratic, zig-zagging step directions, even asymptotically close to the solution. We view this as an artifact of discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its trajectory at asymptotically small step sizes, does not zig-zag, and reducing discretization error will go hand-in-hand in producing a more stabilized method, with better convergence properties. We propose two improvements: a multistep Frank-Wolfe method that directly applies optimized higher-order discretization schemes; and an LMO-averaging scheme with reduced discretization error, and whose local convergence rate over general convex sets accelerates from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>adapter-ALBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;NLP&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#29616;&#22810;&#20219;&#21153;&#25512;&#29702;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#25968;&#25454;&#37325;&#29992;&#65292;&#24182;&#25552;&#39640;&#23545;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16100</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#20869;&#23384;&#32467;&#26500;&#30340;&#36793;&#32536;NLP&#25512;&#29702;&#30340;&#33410;&#33021;&#20219;&#21153;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures. (arXiv:2303.16100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16100
&lt;/p&gt;
&lt;p&gt;
adapter-ALBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;NLP&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#29616;&#22810;&#20219;&#21153;&#25512;&#29702;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#25968;&#25454;&#37325;&#29992;&#65292;&#24182;&#25552;&#39640;&#23545;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#20219;&#21153;&#38656;&#35201;&#20180;&#32454;&#30340;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;ALBERT&#21487;&#20197;&#29992;&#20110;&#22312;&#31227;&#21160;&#31995;&#32479;&#32423;&#33455;&#29255;&#19978;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25512;&#29702;&#65292;&#24182;&#37197;&#22791;&#33258;&#23450;&#20041;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#34429;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#32531;&#35299;&#36816;&#34892;&#21333;&#20010;NLP&#20219;&#21153;&#30340;&#24310;&#36831;&#12289;&#33021;&#37327;&#21644;&#38754;&#31215;&#25104;&#26412;&#65292;&#20294;&#23545;&#20110;&#23454;&#29616;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#38656;&#35201;&#22312;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#20219;&#21153;&#30340;&#22810;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#22522;&#30784;&#19978;&#25191;&#34892;&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#33455;&#29255;&#20869;&#23384;&#38656;&#27714;&#25110;&#25903;&#20184;&#33455;&#29255;&#22806;&#37096;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;adapter-ALBERT&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26368;&#22823;&#21270;&#25968;&#25454;&#37325;&#29992;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23545;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executing machine learning inference tasks on resource-constrained edge devices requires careful hardware-software co-design optimizations. Recent examples have shown how transformer-based deep neural network models such as ALBERT can be used to enable the execution of natural language processing (NLP) inference on mobile systems-on-chip housing custom hardware accelerators. However, while these existing solutions are effective in alleviating the latency, energy, and area costs of running single NLP tasks, achieving multi-task inference requires running computations over multiple variants of the model parameters, which are tailored to each of the targeted tasks. This approach leads to either prohibitive on-chip memory requirements or paying the cost of off-chip memory access. This paper proposes adapter-ALBERT, an efficient model optimization for maximal data reuse across different tasks. The proposed model's performance and robustness to data compression methods are evaluated across s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HGNAS&#26694;&#26550;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#30828;&#20214;&#24863;&#30693;&#22270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#36890;&#36807;&#35299;&#32806;GNN&#33539;&#24335;&#65292;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#22312;GNN&#26550;&#26500;&#35774;&#35745;&#20013;&#23454;&#29616;&#20102;&#30828;&#20214;&#24863;&#30693;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HGNAS&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;GNN&#35774;&#35745;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24230;&#65292;&#30456;&#36739;&#20110;&#25163;&#21160;&#35774;&#35745;&#27169;&#22411;&#32780;&#35328;&#12290;</title><link>http://arxiv.org/abs/2303.10875</link><description>&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hardware-Aware Graph Neural Network Automated Design for Edge Computing Platforms. (arXiv:2303.10875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HGNAS&#26694;&#26550;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#30828;&#20214;&#24863;&#30693;&#22270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#36890;&#36807;&#35299;&#32806;GNN&#33539;&#24335;&#65292;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#22312;GNN&#26550;&#26500;&#35774;&#35745;&#20013;&#23454;&#29616;&#20102;&#30828;&#20214;&#24863;&#30693;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HGNAS&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;GNN&#35774;&#35745;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24230;&#65292;&#30456;&#36739;&#20110;&#25163;&#21160;&#35774;&#35745;&#27169;&#22411;&#32780;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#38750;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#30340;&#27969;&#34892;&#31574;&#30053;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22240;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#32780;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#20027;&#35201;&#20851;&#27880;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32570;&#20047;&#32771;&#34385;&#30828;&#20214;&#36164;&#28304;&#38480;&#21046;&#21644;&#36793;&#32536;&#24212;&#29992;&#22330;&#26223;&#30340;&#23454;&#26102;&#35201;&#27714;&#12290;&#23545;&#20856;&#22411;GNN&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#35745;&#31639;&#24179;&#21488;&#19978;&#65292;&#23427;&#20204;&#30340;&#25191;&#34892;&#29305;&#24615;&#21463;&#21040;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#65292;&#36825;&#38656;&#35201;&#39640;&#25928;&#30340;GNN&#35774;&#35745;&#19982;&#30828;&#20214;&#24847;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HGNAS&#20316;&#20026;&#31532;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#30828;&#20214;&#24863;&#30693;&#22270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#36890;&#36807;&#35299;&#32806;GNN&#33539;&#24335;&#65292;HGNAS&#26500;&#24314;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39640;&#25928;&#30340;&#22810;&#38454;&#27573;&#25628;&#32034;&#31574;&#30053;&#22312;&#25968;&#20010;GPU&#23567;&#26102;&#20869;&#25506;&#32034;&#26368;&#20339;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;HGNAS&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#22312;GNN&#26550;&#26500;&#35774;&#35745;&#20013;&#23454;&#29616;&#20102;&#30828;&#20214;&#24863;&#30693;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HGNAS&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;GNN&#35774;&#35745;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24230;&#65292;&#30456;&#36739;&#20110;&#25163;&#21160;&#35774;&#35745;&#27169;&#22411;&#32780;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have emerged as a popular strategy for handling non-Euclidean data due to their state-of-the-art performance. However, most of the current GNN model designs mainly focus on task accuracy, lacking in considering hardware resources limitation and real-time requirements of edge application scenarios. Comprehensive profiling of typical GNN models indicates that their execution characteristics are significantly affected across different computing platforms, which demands hardware awareness for efficient GNN designs. In this work, HGNAS is proposed as the first Hardware-aware Graph Neural Architecture Search framework targeting resource constraint edge devices. By decoupling the GNN paradigm, HGNAS constructs a fine-grained design space and leverages an efficient multi-stage search strategy to explore optimal architectures within a few GPU hours. Moreover, HGNAS achieves hardware awareness during the GNN architecture design by leveraging a hardware performance pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07068</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#26368;&#20248;n&#30340;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
n-Step Temporal Difference Learning with Optimal n. (arXiv:2303.07068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;n&#27493;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20013;&#25214;&#21040;&#26368;&#20248;n&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#30001;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA&#65289;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;n&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#27169;&#25311;SPSA&#31243;&#24207;&#65292;&#23558;&#20854;&#21407;&#22987;&#36830;&#32493;&#20248;&#21270;&#26694;&#26550;&#24341;&#20837;&#31163;&#25955;&#20248;&#21270;&#26694;&#26550;&#65292;&#20294;&#24182;&#32467;&#21512;&#20102;&#24490;&#29615;&#25200;&#21160;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;SDPSA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#20540;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;n&#27493;TD&#20013;&#30340;&#26368;&#20248;n&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SDPSA&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;n&#20540;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of finding the optimal value of n in the n-step temporal difference (TD) algorithm. We find the optimal n by resorting to the model-free optimization technique of simultaneous perturbation stochastic approximation (SPSA). We adopt a one-simulation SPSA procedure that is originally for continuous optimization to the discrete optimization framework but incorporates a cyclic perturbation sequence. We prove the convergence of our proposed algorithm, SDPSA, and show that it finds the optimal value of n in n-step TD. Through experiments, we show that the optimal value of n is achieved with SDPSA for any arbitrary initial value of the same.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2303.05735</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#24418;&#30340;&#30828;&#20214;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#28210;&#26579;&#21644;&#21453;&#28210;&#26579;&#31639;&#27861;&#24050;&#34987;&#31070;&#32463;&#34920;&#31034;&#65288;NR&#65289;&#25152;&#21462;&#20195;&#12290;NR&#26368;&#36817;&#34987;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#30340;&#20960;&#20309;&#21644;&#26448;&#36136;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#21512;&#25104;&#30495;&#23454;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#25215;&#35834;&#29992;&#21487;&#20280;&#32553;&#30340;&#36136;&#37327;&#21644;&#21487;&#39044;&#27979;&#30340;&#24615;&#33021;&#26367;&#25442;&#20256;&#32479;&#30340;&#28210;&#26579;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#38382;&#39064;&#65306;&#31070;&#32463;&#22270;&#24418;&#65288;NG&#65289;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#34920;&#24615;&#30340;NG&#24212;&#29992;&#31243;&#24207;&#65292;&#21457;&#29616;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#24403;&#21069;&#30340;GPU&#19978;&#20197;60FPS&#28210;&#26579;4K&#20998;&#36776;&#29575;&#65292;&#21017;&#25152;&#38656;&#24615;&#33021;&#19982;&#24403;&#21069;GPU&#30340;&#23454;&#38469;&#24615;&#33021;&#23384;&#22312;1.5&#20493;&#33267;55&#20493;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#25152;&#38656;&#24615;&#33021;&#19982;&#25152;&#38656;&#31995;&#32479;&#21151;&#29575;&#20043;&#38388;&#23384;&#22312;&#26356;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#65292;&#23545;&#20110;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#12289;&#22810;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#21644;&#20302;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#65292;&#23427;&#20204;&#21344;&#24212;&#29992;&#31243;&#24207;&#26102;&#38388;&#30340;72&#65285;&#12289;60&#65285;&#21644;59&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;&#20132;&#20114;&#24335;&#36827;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#20856;&#22411;&#30340;&#20154;&#31867;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#22797;&#26434;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#65293;&#24605;&#24819;&#30340;&#37325;&#32452;&#21644;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.02155</link><description>&lt;p&gt;
ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22312;&#32447;&#20114;&#21160;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#30340;&#36827;&#21270;&#24341;&#25806;&#65288;arXiv:2303.02155v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design. (arXiv:2303.02155v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;&#20132;&#20114;&#24335;&#36827;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#20856;&#22411;&#30340;&#20154;&#31867;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#22797;&#26434;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#65293;&#24605;&#24819;&#30340;&#37325;&#32452;&#21644;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#31185;&#23398;&#30028;&#25472;&#36215;&#39118;&#26292;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#29616;&#29366;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#24037;&#20855;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20986;&#22855;&#22320;&#23436;&#25104;&#26377;&#25361;&#25112;&#24615;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#20195;&#30721;&#21644;&#24212;&#29992;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20889;&#25925;&#20107;&#12289;&#38899;&#20048;&#29255;&#27573;&#31561;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20132;&#20114;&#24335;&#36827;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#26694;&#26550;&#65292;&#20197;&#27169;&#25311;&#20856;&#22411;&#30340;&#20154;&#31867;&#35774;&#35745;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32773;&#21033;&#29992;&#29992;&#25143;&#30340;&#21453;&#39304;&#36873;&#25321;&#26368;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#22797;&#26434;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#8212;&#8212;&#24605;&#24819;&#30340;&#37325;&#32452;&#21644;&#21464;&#24322;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#36807;&#31243;&#22987;&#20110;&#19968;&#20010;&#31616;&#35201;&#35828;&#26126;&#21644;&#19968;&#32452;&#20505;&#36873;&#35774;&#35745;&#65292;&#36825;&#20123;&#35774;&#35745;&#26159;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#25110;&#30001;&#29992;&#25143;&#25552;&#20986;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#29992;&#25143;&#36890;&#36807;&#21521;&#20132;&#20114;&#24335;&#36951;&#20256;&#31639;&#27861;&#25552;&#20379;&#21453;&#39304;&#26469;&#21327;&#20316;&#35774;&#35745;&#36807;&#31243;&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#12289;&#37325;&#32452;&#21644;&#31361;&#21464;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task - the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#35777;&#25454;&#25903;&#25345;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;AD&#30456;&#20851;&#30340;&#36741;&#21161;&#20219;&#21153;&#26469;&#23398;&#20064;MRI&#25195;&#25551;&#20013;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#35777;&#25454;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.01105</link><description>&lt;p&gt;
&#35777;&#25454;&#25903;&#25345;&#30340;&#36716;&#31227;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evidence-empowered Transfer Learning for Alzheimer's Disease. (arXiv:2303.01105v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#35777;&#25454;&#25903;&#25345;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;AD&#30456;&#20851;&#30340;&#36741;&#21161;&#20219;&#21153;&#26469;&#23398;&#20064;MRI&#25195;&#25551;&#20013;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#35777;&#25454;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20943;&#36731;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36716;&#31227;&#23398;&#20064;&#20381;&#36182;&#20110;&#37325;&#22797;&#20351;&#29992;&#22312;AD&#38750;&#30456;&#20851;&#20219;&#21153;&#65288;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#20998;&#31867;&#65289;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#21307;&#23398;&#26469;&#28304;&#21644;&#30446;&#26631;&#21307;&#23398;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23427;&#32463;&#24120;&#23548;&#33268;&#36127;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#25454;&#25903;&#25345;&#30340;AD&#35786;&#26029;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;AD&#30456;&#20851;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#21363;&#24418;&#24577;&#21464;&#21270;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;MRI&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#36741;&#21161;&#20219;&#21153;&#20013;&#65292;&#35786;&#26029;&#27169;&#22411;&#23398;&#20064;&#26469;&#33258;MRI&#25195;&#25551;&#20013;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#35777;&#25454;&#21644;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#26080;&#35770;&#27169;&#22411;&#23481;&#37327;&#22914;&#20309;&#65292;&#20063;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has been widely utilized to mitigate the data scarcity problem in the field of Alzheimer's disease (AD). Conventional transfer learning relies on re-using models trained on AD-irrelevant tasks such as natural image classification. However, it often leads to negative transfer due to the discrepancy between the non-medical source and target medical domains. To address this, we present evidence-empowered transfer learning for AD diagnosis. Unlike conventional approaches, we leverage an AD-relevant auxiliary task, namely morphological change prediction, without requiring additional MRI data. In this auxiliary task, the diagnosis model learns the evidential and transferable knowledge from morphological features in MRI scans. Experimental results demonstrate that our framework is not only effective in improving detection performance regardless of model capacity, but also more data-efficient and faithful.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BDR&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#31364;&#31934;&#24230;&#26684;&#24335;&#12290;&#36890;&#36807;BDR&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#30340;&#26032;&#26684;&#24335;&#65292;&#20854;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20197;&#21450;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08007</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#20139;&#24494;&#25351;&#25968;&#65292;&#24494;&#23567;&#30340;&#20301;&#31227;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
With Shared Microexponents, A Little Shifting Goes a Long Way. (arXiv:2302.08007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BDR&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#31364;&#31934;&#24230;&#26684;&#24335;&#12290;&#36890;&#36807;BDR&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#30340;&#26032;&#26684;&#24335;&#65292;&#20854;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20197;&#21450;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22359;&#25968;&#25454;&#34920;&#31034;&#65288;BDR&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#32034;&#21644;&#35780;&#20272;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31364;&#31934;&#24230;&#26684;&#24335;&#12290;&#23427;&#33021;&#22815;&#27604;&#36739;&#27969;&#34892;&#30340;&#37327;&#21270;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;BDR&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#65288;MX&#65289;&#30340;&#26032;&#26684;&#24335;&#65292;&#20854;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#31364;&#31934;&#24230;&#28014;&#28857;&#21644;&#22359;&#28014;&#28857;&#12290;MX &#22312;&#30828;&#20214;&#20013;&#21033;&#29992;&#22810;&#20010;&#37327;&#21270;&#32423;&#21035;&#65292;&#20351;&#29992;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#30340;&#36229;&#32454;&#32553;&#25918;&#22240;&#23376;&#12290;MX &#30340;&#26377;&#25928;&#24615;&#22312;&#21253;&#25324;&#22823;&#35268;&#27169;&#29983;&#25104;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20197;&#21450;&#29983;&#20135;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#22312;&#20869;&#30340;&#23454;&#38469;&#27169;&#22411;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#27169;&#22411;&#22312;&#20307;&#31995;&#26550;&#26500;&#21644;&#20248;&#21270;&#36873;&#25321;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20998;&#23376;&#21160;&#21147;&#23398;&#31283;&#23450;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#25439;&#22833;&#31354;&#38388;&#31561;&#36235;&#21183;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25439;&#22833;&#31354;&#38388;&#21487;&#35270;&#21270;&#21644;&#25439;&#22833;&#29109;&#30340;&#24230;&#37327;&#21487;&#20197;&#39044;&#27979;NNIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.05823</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#22806;&#25512;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Data efficiency and extrapolation trends in neural network interatomic potentials. (arXiv:2302.05823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#27169;&#22411;&#22312;&#20307;&#31995;&#26550;&#26500;&#21644;&#20248;&#21270;&#36873;&#25321;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20998;&#23376;&#21160;&#21147;&#23398;&#31283;&#23450;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#25439;&#22833;&#31354;&#38388;&#31561;&#36235;&#21183;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25439;&#22833;&#31354;&#38388;&#21487;&#35270;&#21270;&#21644;&#25439;&#22833;&#29109;&#30340;&#24230;&#37327;&#21487;&#20197;&#39044;&#27979;NNIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#65288;NNIP&#65289;&#30340;&#20851;&#38190;&#26550;&#26500;&#36827;&#27493;&#21253;&#25324;&#20102;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#12289;&#31561;&#21464;&#24615;&#25110;&#22810;&#20307;&#23637;&#24320;&#39033;&#12290;&#23613;&#31649;&#29616;&#20195;NNIP&#27169;&#22411;&#22312;&#33021;&#37327;/&#21147;&#35823;&#24046;&#26041;&#38754;&#23384;&#22312;&#24494;&#23567;&#24046;&#24322;&#65292;&#20294;&#22312;&#24320;&#21457;&#26032;&#30340;NNIP&#20307;&#31995;&#32467;&#26500;&#26102;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#20173;&#28982;&#34987;&#35270;&#20026;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20307;&#31995;&#32467;&#26500;&#21644;&#20248;&#21270;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;NNIP&#30340;&#26222;&#36941;&#21270;&#65292;&#25581;&#31034;&#20102;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#31283;&#23450;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#25439;&#22833;&#31354;&#38388;&#30340;&#36235;&#21183;&#12290;&#21033;&#29992;3BPA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#34920;&#26126;NNIP&#30340;&#27979;&#35797;&#35823;&#24046;&#36981;&#24490;&#27604;&#20363;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#32784;&#21463;&#22122;&#22768;&#65292;&#20294;&#19981;&#33021;&#39044;&#27979;&#39640;&#31934;&#24230;&#21306;&#22495;&#30340;MD&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25439;&#22833;&#31354;&#38388;&#21487;&#35270;&#21270;&#21644;&#25439;&#22833;&#29109;&#30340;&#24230;&#37327;&#26469;&#39044;&#27979;NNIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;NequIP&#21644;MACE&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25439;&#22833;&#29109;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, key architectural advances have been proposed for neural network interatomic potentials (NNIPs), such as incorporating message-passing networks, equivariance, or many-body expansion terms. Although modern NNIP models exhibit small differences in energy/forces errors, improvements in accuracy are still considered the main target when developing new NNIP architectures. In this work, we show how architectural and optimization choices influence the generalization of NNIPs, revealing trends in molecular dynamics (MD) stability, data efficiency, and loss landscapes. Using the 3BPA dataset, we show that test errors in NNIP follow a scaling relation and can be robust to noise, but cannot predict MD stability in the high-accuracy regime. To circumvent this problem, we propose the use of loss landscape visualizations and a metric of loss entropy for predicting the generalization power of NNIPs. With a large-scale study on NequIP and MACE, we show that the loss entropy pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00942</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#22120;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#22270;&#22330;&#31215;&#20998;&#65292;&#36866;&#29992;&#20110;&#28857;&#20113;&#32593;&#26684;&#22270;&#25110;&#949;-&#26368;&#36817;&#37051;&#22270;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#29992;&#20110;&#23545;&#32534;&#30721;&#28857;&#20113;&#30340;&#22270;&#24418;&#36827;&#34892;&#39640;&#25928;&#22330;&#31215;&#20998;&#12290;&#31532;&#19968;&#31867;&#31639;&#27861;&#20351;&#29992;&#28857;&#20113;&#32593;&#26684;&#22270;&#30340;&#26377;&#30028;&#20111;&#26684;&#65292;&#31532;&#20108;&#31867;&#31639;&#27861;&#21017;&#20351;&#29992;&#28857;&#20113;&#30340;&#27969;&#34892;&#30340;&#949;-&#26368;&#36817;&#37051;&#22270;&#34920;&#31034;&#26041;&#27861;&#12290;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#34987;&#30475;&#20316; Fast Multipole Methods(FMMs) &#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#20294;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25991;&#31456;&#37325;&#28857;&#30740;&#31350;&#22522;&#20110;&#28857;&#20043;&#38388;&#27493;&#38271;&#20998;&#24067;&#65288;&#22914;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#65289;&#25152;&#24341;&#21457;&#30340;&#20960;&#20309;&#23398;&#12290;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25991;&#31456;&#33719;&#24471;&#20102;&#32467;&#26500;&#22270;&#35770;&#30340;&#26032;&#32467;&#26524;&#12290;&#25991;&#31456;&#36824;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#21018;&#24615;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#34920;&#38754;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;&#29992;&#20110;&#32593;&#26684;&#21160;&#24577;&#24314;&#27169;&#65289;&#65292;&#28857;&#20113;&#30340;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20197;&#21450;Gromov-Wasserstein&#21464;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;ZiCo&#65292;&#36890;&#36807;&#30740;&#31350;&#29305;&#23450;&#30340;&#26799;&#24230;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;ZiCo&#25104;&#20026;&#20102;&#31532;&#19968;&#20010;&#22312;&#22810;&#20010;NAS&#22522;&#20934;&#27979;&#35797;&#19978;&#19968;&#33268;&#20248;&#20110;&#26420;&#32032;&#20195;&#29702;(#Params)&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23547;&#25214;&#26368;&#20339;&#31070;&#32463;&#32467;&#26500;&#30340;&#25628;&#32034;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2301.11300</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#31995;&#25968;&#21464;&#24322;&#30340;&#38646;&#26679;&#26412;NAS&#65306;ZiCo
&lt;/p&gt;
&lt;p&gt;
ZiCo: Zero-shot NAS via Inverse Coefficient of Variation on Gradients. (arXiv:2301.11300v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;ZiCo&#65292;&#36890;&#36807;&#30740;&#31350;&#29305;&#23450;&#30340;&#26799;&#24230;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;ZiCo&#25104;&#20026;&#20102;&#31532;&#19968;&#20010;&#22312;&#22810;&#20010;NAS&#22522;&#20934;&#27979;&#35797;&#19978;&#19968;&#33268;&#20248;&#20110;&#26420;&#32032;&#20195;&#29702;(#Params)&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23547;&#25214;&#26368;&#20339;&#31070;&#32463;&#32467;&#26500;&#30340;&#25628;&#32034;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#34987;&#24191;&#27867;&#29992;&#20110;&#33258;&#21160;&#33719;&#24471;&#22312;&#22823;&#37327;&#20505;&#36873;&#32467;&#26500;&#20013;&#24615;&#33021;&#26368;&#20339;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#32553;&#30701;&#25628;&#32034;&#26102;&#38388;&#65292;&#38646;&#26679;&#26412;NAS&#26088;&#22312;&#35774;&#35745;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#32467;&#26500;&#30340;&#27979;&#35797;&#24615;&#33021;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#25552;&#20986;&#30340;&#25152;&#26377;&#38646;&#26679;&#26412;&#20195;&#29702;&#37117;&#19981;&#33021;&#27604;&#26420;&#32032;&#20195;&#29702; (#Params) &#19968;&#33268;&#22320;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#29616;&#29366;&#65292;&#26412;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#26799;&#24230;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;ZiCo&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#27604;#Params&#19968;&#33268;&#26356;&#22909;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ZiCo&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;NAS&#22522;&#20934;&#27979;&#35797;(NASBench101&#12289;NATSBench-SSS/TSS&#21644;NASBench201)&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23547;&#25214;&#26368;&#20339;&#31070;&#32463;&#32467;&#26500;&#30340;&#25628;&#32034;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) is widely used to automatically obtain the neural network with the best performance among a large number of candidate architectures. To reduce the search time, zero-shot NAS aims at designing training-free proxies that can predict the test performance of a given architecture. However, as shown recently, none of the zero-shot proxies proposed to date can actually work consistently better than a naive proxy, namely, the number of network parameters (#Params). To improve this state of affairs, as the main theoretical contribution, we first reveal how some specific gradient properties across different samples impact the convergence rate and generalization capacity of neural networks. Based on this theoretical analysis, we propose a new zero-shot proxy, ZiCo, the first proxy that works consistently better than #Params. We demonstrate that ZiCo works better than State-Of-The-Art (SOTA) proxies on several popular NAS-Benchmarks (NASBench101, NATSBench-SSS/TSS,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.08243</link><description>&lt;p&gt;
&#20855;&#26377;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. (arXiv:2301.08243v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;I-JEPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#22270;&#20687;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#38750;&#29983;&#25104;&#26041;&#27861;&#12290;I-JEPA&#30340;&#26680;&#24515;&#35774;&#35745;&#36873;&#25321;&#26159;&#25513;&#27169;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;I-JEPA&#20135;&#29983;&#35821;&#20041;&#34920;&#31034;&#12290;&#24403;&#19982;Vision Transformers&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#35777;&#26126;I-JEPA&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.07285</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Sparse Regularizer. (arXiv:2301.07285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07285
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#22522;&#20110; $L_{p}$-norm &#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#21482;&#32771;&#34385;&#27169;&#22411;&#20013;&#21508;&#26435;&#37325;&#20540;&#30340;&#24230;&#37327;&#19981;&#21516;&#65292;&#36825;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#12290;&#35813;&#26041;&#27861;&#30340;&#21152;&#20837;&#39033;&#21487;&#20197;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21487;&#24494;&#20998;&#65292;&#31616;&#21333;&#24555;&#36895;&#35745;&#31639;&#65292;&#19982;&#23610;&#24230;&#26080;&#20851;&#65292;&#20165;&#38656;&#35201;&#24494;&#23567;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#23481;&#26131;&#24182;&#34892;&#21270;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#31934;&#24230;&#27700;&#24179;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
$L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and $L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniques such as weight decay and group LASSO compute a quantity which de pends on model weights considered in isolation from one another. This paper describes a novel regularizer which is not based on an $L_{p}$-norm. In contrast with $L_{p}$-norm-based regularization, this regularizer is concerned with the spatial arrangement of weights within a weight matrix. This regularizer is an additive term for the loss function and is differentiable, simple and fast to compute, scale-invariant, requires a trivial amount of additional memory, and can easily be parallelized. Empirically this method yields approximately a one order-of-magnitude improvement in the number of nonzero model parameters at a given level of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#34987;&#24046;&#20998;&#38544;&#31169;&#21644;&#23433;&#20840;&#32858;&#21512;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837; Sybil &#35774;&#22791;&#26469;&#37325;&#26500;&#29992;&#25143;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#26435;&#21147;&#19981;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.04017</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#23433;&#20840;&#32858;&#21512;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#20010;&#20307;&#25968;&#25454;&#28857;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation. (arXiv:2301.04017v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#34987;&#24046;&#20998;&#38544;&#31169;&#21644;&#23433;&#20840;&#32858;&#21512;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837; Sybil &#35774;&#22791;&#26469;&#37325;&#26500;&#29992;&#25143;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#26435;&#21147;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#35753;&#29992;&#25143;&#32852;&#21512;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#34987;&#23459;&#20256;&#20026;&#19968;&#31181;&#25552;&#20379;&#25968;&#25454;&#26368;&#23567;&#21270;&#30340;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#12290; &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#26222;&#36890;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#27169;&#22411;&#26356;&#26032;&#20013;&#25552;&#21462;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290; &#26412;&#25991;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#26377;&#38450;&#25252;&#25514;&#26045;&#30340;&#21327;&#35758;&#20013;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#29978;&#33267;&#21487;&#20197;&#37325;&#26500;&#29992;&#25143;&#25968;&#25454;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20351;&#29992;&#20998;&#24067;&#24335;&#24046;&#20998;&#38544;&#31169;&#65288;DDP&#65289;&#21644;&#23433;&#20840;&#32858;&#21512;&#65288;SA&#65289;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25915;&#20987;&#26041;&#27861;&#22522;&#20110;&#24341;&#20837;&#36829;&#21453;&#21327;&#35758;&#30340;Sybil&#35774;&#22791;&#65292;&#20197;&#25581;&#31034;&#20010;&#20154;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#20197;&#20379;&#26381;&#21153;&#22120;&#37325;&#26500;&#12290;&#23548;&#33268;&#27492;&#28431;&#27934;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#26435;&#21147;&#19981;&#24179;&#34913;&#65306;&#26381;&#21153;&#22120;&#21327;&#35843;&#25972;&#20010;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a framework for users to jointly train a machine learning model. FL is promoted as a privacy-enhancing technology (PET) that provides data minimization: data never "leaves" personal devices and users share only model updates with a server (e.g., a company) coordinating the distributed training. While prior work showed that in vanilla FL a malicious server can extract users' private data from the model updates, in this work we take it further and demonstrate that a malicious server can reconstruct user data even in hardened versions of the protocol. More precisely, we propose an attack against FL protected with distributed differential privacy (DDP) and secure aggregation (SA). Our attack method is based on the introduction of sybil devices that deviate from the protocol to expose individual users' data for reconstruction by the server. The underlying root cause for the vulnerability to our attack is a power imbalance: the server orchestrates the whole protoco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;NODE&#20248;&#28857;&#65292;&#21487;&#20197;&#21442;&#25968;&#21270;&#20122;&#32593;&#26684;&#23610;&#24230;&#12289;&#36817;&#20284;&#32806;&#21512;&#31639;&#23376;&#65292;&#24182;&#25552;&#39640;&#20302;&#38454;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.09967</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Subgrid-scale Models with Neural Ordinary Differential Equations. (arXiv:2212.09967v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;NODE&#20248;&#28857;&#65292;&#21487;&#20197;&#21442;&#25968;&#21270;&#20122;&#32593;&#26684;&#23610;&#24230;&#12289;&#36817;&#20284;&#32806;&#21512;&#31639;&#23376;&#65292;&#24182;&#25552;&#39640;&#20302;&#38454;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(NODE)&#26469;&#35299;&#20915;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#65292;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20174;&#31895;&#32593;&#26684;&#21040;&#32454;&#32593;&#26684;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20122;&#32593;&#26684;&#23610;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;NODE&#21644;&#37096;&#20998;&#30693;&#35782;&#26469;&#22312;&#36830;&#32493;&#32423;&#21035;&#19978;&#23398;&#20064;&#28304;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32487;&#25215;&#20102;NODE&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#21442;&#25968;&#21270;&#20122;&#32593;&#26684;&#23610;&#24230;&#12289;&#36817;&#20284;&#32806;&#21512;&#31639;&#23376;&#65292;&#24182;&#25552;&#39640;&#20302;&#38454;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to learning the subgrid-scale model when simulating partial differential equations (PDEs) solved by the method of lines and their representation in chaotic ordinary differential equations, based on neural ordinary differential equations (NODEs). Solving systems with fine temporal and spatial grid scales is an ongoing computational challenge, and closure models are generally difficult to tune. Machine learning approaches have increased the accuracy and efficiency of computational fluid dynamics solvers. In this approach neural networks are used to learn the coarse- to fine-grid map, which can be viewed as subgrid-scale parameterization. We propose a strategy that uses the NODE and partial knowledge to learn the source dynamics at a continuous level. Our method inherits the advantages of NODEs and can be used to parameterize subgrid scales, approximate coupling operators, and improve the efficiency of low-order solvers. Numerical results with the two-scale Loren
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;(FFL)&#65292;&#21487;&#20197;&#21327;&#21516;&#35757;&#32451;&#24322;&#36136;&#21270;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#24378;&#22823;&#32780;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#19981;&#21516;&#26426;&#26500;&#38388;&#30340;&#24191;&#27867;&#21512;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.13606</link><description>&lt;p&gt;
&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38750;&#22343;&#19968;&#26631;&#31614;&#30340;&#21327;&#21516;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Collaborative Training of Medical Artificial Intelligence Models with non-uniform Labels. (arXiv:2211.13606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;(FFL)&#65292;&#21487;&#20197;&#21327;&#21516;&#35757;&#32451;&#24322;&#36136;&#21270;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#24378;&#22823;&#32780;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#19981;&#21516;&#26426;&#26500;&#38388;&#30340;&#24191;&#27867;&#21512;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#20027;&#27969;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#24378;&#22823;&#32780;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#26041;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#24050;&#32463;&#25552;&#20379;&#20102;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#26159;&#36825;&#20123;&#25968;&#25454;&#30340;&#26631;&#31614;&#26041;&#24335;&#24046;&#24322;&#24456;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65306;&#28789;&#27963;&#32852;&#21512;&#23398;&#20064;(FFL)&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;&#20840;&#29699;&#20116;&#20010;&#26426;&#26500;&#30340;695,000&#20010;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#33016;&#37096;&#36879;&#35270;&#22270;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24322;&#36136;&#21270;&#26631;&#27880;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#20351;&#29992;FFL&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#24191;&#27867;&#21327;&#20316;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid advancements in recent years, medical image analysis is largely dominated by deep learning (DL). However, building powerful and robust DL models requires training with large multi-party datasets. While multiple stakeholders have provided publicly available datasets, the ways in which these data are labeled vary widely. For Instance, an institution might provide a dataset of chest radiographs containing labels denoting the presence of pneumonia, while another institution might have a focus on determining the presence of metastases in the lung. Training a single AI model utilizing all these data is not feasible with conventional federated learning (FL). This prompts us to propose an extension to the widespread FL process, namely flexible federated learning (FFL) for collaborative training on such data. Using 695,000 chest radiographs from five institutions from across the globe each with differing labels - we demonstrate that having heterogeneously labeled datasets, FF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#20132;&#20114;&#33539;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#30446;&#26631;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#37096;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#22810;&#36339;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;HopGNN&#26694;&#26550;&#23454;&#29616;&#36339;&#36291;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2211.11761</link><description>&lt;p&gt;
&#20174;&#33410;&#28857;&#20132;&#20114;&#21040;&#36339;&#36291;&#20132;&#20114;&#65306;&#26032;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm. (arXiv:2211.11761v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#20132;&#20114;&#33539;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#30446;&#26631;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#37096;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#22810;&#36339;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;HopGNN&#26694;&#26550;&#23454;&#29616;&#36339;&#36291;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36981;&#24490;&#36845;&#20195;&#22320;&#22312;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#20449;&#24687;&#20132;&#20114;&#30340;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33410;&#28857;&#20132;&#20114;&#33539;&#24335;&#20173;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#65292;&#21363;&#22312;&#24555;&#36895;&#25193;&#23637;&#30340;&#37051;&#23621;&#20043;&#38388;&#36827;&#34892;&#33410;&#28857;&#20132;&#20114;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#65307;&#20197;&#21450;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#33410;&#28857;&#30340;&#21028;&#21035;&#33021;&#21147;&#21463;&#38480;&#65292;&#37325;&#22797;&#33410;&#28857;&#20132;&#20114;&#21518;&#19981;&#21516;&#31867;&#21035;&#33410;&#28857;&#30340;&#34920;&#31034;&#23558;&#20250;&#25910;&#25947;&#21040;&#26080;&#27861;&#21306;&#20998;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#20132;&#20114;&#33539;&#24335;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#30446;&#26631;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#37096;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#22810;&#36339;&#29305;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340; HopGNN &#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#21033;&#29992;&#29616;&#26377;&#30340;GNN&#23454;&#29616;&#36339;&#36291;&#20132;&#24448;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Graph Neural Networks (GNNs) follow the message-passing mechanism that conducts information interaction among nodes iteratively. While considerable progress has been made, such node interaction paradigms still have the following limitation. First, the scalability limitation precludes the broad application of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors incurs high computation and memory costs. Second, the over-smoothing problem restricts the discrimination ability of nodes, i.e., node representations of different classes will converge to indistinguishable after repeated node interactions. In this work, we propose a novel hop interaction paradigm to address these limitations simultaneously. The core idea is to convert the interaction target among nodes to pre-processed multi-hop features inside each node. We design a simple yet effective HopGNN framework that can easily utilize existing GNNs to achieve hop interaction. Fur
&lt;/p&gt;</description></item><item><title>&#20196;&#29260;&#22270;&#28789;&#26426;&#26159;&#19968;&#31181;&#24207;&#21015;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#20855;&#26377;&#30340;&#22806;&#37096;&#23384;&#20648;&#22120;&#21487;&#20197;&#24635;&#32467;&#20808;&#21069;&#30340;&#21382;&#21490;&#24182;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#30456;&#27604;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#22312;&#24207;&#21015;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2211.09119</link><description>&lt;p&gt;
&#20196;&#29260;&#22270;&#28789;&#26426;
&lt;/p&gt;
&lt;p&gt;
Token Turing Machines. (arXiv:2211.09119v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09119
&lt;/p&gt;
&lt;p&gt;
&#20196;&#29260;&#22270;&#28789;&#26426;&#26159;&#19968;&#31181;&#24207;&#21015;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#20855;&#26377;&#30340;&#22806;&#37096;&#23384;&#20648;&#22120;&#21487;&#20197;&#24635;&#32467;&#20808;&#21069;&#30340;&#21382;&#21490;&#24182;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#30456;&#27604;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#22312;&#24207;&#21015;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20196;&#29260;&#22270;&#28789;&#26426;&#65288;TTM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#24102;&#26377;&#23384;&#20648;&#22120;&#30340;&#24207;&#21015;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#30340;&#24207;&#21015;&#35270;&#35273;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21463;&#21040; Neural Turing Machine &#30340;&#21551;&#21457;&#65292;&#24182;&#20855;&#26377;&#30001;&#19968;&#32452;&#20196;&#29260;&#32452;&#25104;&#30340;&#22806;&#37096;&#23384;&#20648;&#22120;&#65292;&#29992;&#20110;&#24635;&#32467;&#20808;&#21069;&#30340;&#21382;&#21490;&#65288;&#21363;&#24103;&#65289;&#12290;&#35813;&#23384;&#20648;&#22120;&#20351;&#29992;Transformer&#20316;&#20026;&#27599;&#20010;&#27493;&#39588;&#30340;&#22788;&#29702;&#21333;&#20803;/&#25511;&#21046;&#22120;&#36827;&#34892;&#39640;&#25928;&#22320;&#22336;&#23547;&#25214;&#12289;&#35835;&#21462;&#21644;&#20889;&#20837;&#12290;&#27169;&#22411;&#30340;&#23384;&#20648;&#22120;&#27169;&#22359;&#30830;&#20445;&#26032;&#30340;&#35266;&#23519;&#32467;&#26524;&#20165;&#20351;&#29992;&#23384;&#20648;&#22120;&#30340;&#20869;&#23481;&#65288;&#32780;&#19981;&#26159;&#25972;&#20010;&#21382;&#21490;&#35760;&#24405;&#65289;&#36827;&#34892;&#22788;&#29702;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#27599;&#27493;&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TTM&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#24207;&#21015;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#19978;&#65288;&#22312;&#32447;&#26102;&#38388;&#27963;&#21160;&#26816;&#27979;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#34892;&#21160;&#31574;&#30053;&#23398;&#20064;&#65289;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65292;&#20363;&#22914;&#20026;&#38271;&#24207;&#21015;&#35774;&#35745;&#30340;&#20854;&#20182;Transformer&#27169;&#22411;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Token Turing Machines (TTM), a sequential, autoregressive Transformer model with memory for real-world sequential visual understanding. Our model is inspired by the seminal Neural Turing Machine, and has an external memory consisting of a set of tokens which summarise the previous history (i.e., frames). This memory is efficiently addressed, read and written using a Transformer as the processing unit/controller at each step. The model's memory module ensures that a new observation will only be processed with the contents of the memory (and not the entire history), meaning that it can efficiently process long sequences with a bounded computational cost at each step. We show that TTM outperforms other alternatives, such as other Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential visual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning.  Code is publicly avail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#37327;&#23376;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#21487;&#20197;&#23545;&#38750;&#39640;&#26031;&#37327;&#23376;&#24577;&#36827;&#34892;&#30456;&#20284;&#24615;&#26816;&#27979;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.01668</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#23376;&#30456;&#20284;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum Similarity Testing with Convolutional Neural Networks. (arXiv:2211.01668v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#37327;&#23376;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#21487;&#20197;&#23545;&#38750;&#39640;&#26031;&#37327;&#23376;&#24577;&#36827;&#34892;&#30456;&#20284;&#24615;&#26816;&#27979;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20004;&#20010;&#26410;&#32463;&#29305;&#24449;&#21270;&#30340;&#37327;&#23376;&#35774;&#22791;&#26159;&#21542;&#20197;&#30456;&#21516;&#26041;&#24335;&#36816;&#20316;&#23545;&#20110;&#22522;&#20934;&#27979;&#35797;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#21644;&#37327;&#23376;&#27169;&#25311;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#33267;&#20170;&#20026;&#27490;&#65292;&#22312;&#36830;&#32493;&#21464;&#37327;&#37327;&#23376;&#31995;&#32479;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#26377;&#38480;&#19988;&#22024;&#26434;&#30340;&#25968;&#25454;&#27604;&#36739;&#26410;&#30693;&#30340;&#36830;&#32493;&#21464;&#37327;&#24577;&#12290;&#31639;&#27861;&#36866;&#29992;&#20110;&#38750;&#39640;&#26031;&#37327;&#23376;&#24577;&#65292;&#32780;&#27492;&#21069;&#30340;&#25216;&#26415;&#26080;&#27861;&#23454;&#29616;&#30456;&#20284;&#24615;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#20302;&#32500;&#24577;&#34920;&#31034;&#26469;&#35780;&#20272;&#37327;&#23376;&#24577;&#30340;&#30456;&#20284;&#24615;&#12290;&#32593;&#32476;&#21487;&#20197;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#20351;&#29992;&#20855;&#26377;&#19982;&#24453;&#27979;&#35797;&#24577;&#30456;&#20284;&#32467;&#26500;&#30340;&#19968;&#32452;&#22522;&#20934;&#24577;&#30340;&#32463;&#20856;&#27169;&#25311;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#23545;&#22522;&#20934;&#24577;&#30340;&#27979;&#37327;&#25152;&#29983;&#25104;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#27169;&#25311;&#21644;&#23454;&#39564;&#25968;&#25454;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#38750;&#39640;&#26031;&#21387;&#32553;&#30456;&#24178;&#24577;&#26469;&#27979;&#35797;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of testing whether two uncharacterized quantum devices behave in the same way is crucial for benchmarking near-term quantum computers and quantum simulators, but has so far remained open for continuous-variable quantum systems. In this Letter, we develop a machine learning algorithm for comparing unknown continuous variable states using limited and noisy data. The algorithm works on non-Gaussian quantum states for which similarity testing could not be achieved with previous techniques. Our approach is based on a convolutional neural network that assesses the similarity of quantum states based on a lower-dimensional state representation built from measurement data. The network can be trained offline with classically simulated data from a fiducial set of states sharing structural similarities with the states to be tested, or with experimental data generated by measurements on the fiducial states, or with a combination of simulated and experimental data. We test the performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36830;&#32493;&#32431;&#31574;&#30053;&#38598;&#21644;&#23545;&#25910;&#30410;&#20989;&#25968;&#30340;&#19968;&#38454;&#35775;&#38382;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#28385;&#36275;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20174;&#20219;&#20309;&#21021;&#22987;&#21270;&#25351;&#25968;&#25910;&#25947;&#20110;&#20934;&#30830;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.01280</link><description>&lt;p&gt;
&#19968;&#31181;&#36830;&#32493;&#21338;&#24328;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#30340;&#25351;&#25968;&#25910;&#25947;&#31890;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Exponentially Converging Particle Method for the Mixed Nash Equilibrium of Continuous Games. (arXiv:2211.01280v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36830;&#32493;&#32431;&#31574;&#30053;&#38598;&#21644;&#23545;&#25910;&#30410;&#20989;&#25968;&#30340;&#19968;&#38454;&#35775;&#38382;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#28385;&#36275;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20174;&#20219;&#20309;&#21021;&#22987;&#21270;&#25351;&#25968;&#25910;&#25947;&#20110;&#20934;&#30830;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#32431;&#31574;&#30053;&#38598;&#21644;&#23545;&#25910;&#30410;&#20989;&#25968;&#30340;&#19968;&#38454;&#35775;&#38382;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#35745;&#31639;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#22312;&#20197;&#21338;&#24328;&#29702;&#35770;&#20026;&#28789;&#24863;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#22914;&#20998;&#24067;&#24335;&#31283;&#20581;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#31574;&#30053;&#38598;&#26159;&#39640;&#32500;&#30340;&#65292;&#22240;&#27492;&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#19981;&#33021;&#36820;&#22238;&#39640;&#31934;&#24230;&#30340;&#35299;&#12290;&#26412;&#25991;&#24341;&#20837;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#27492;&#38382;&#39064;&#20855;&#26377;&#20445;&#35777;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#35813;&#26041;&#27861;&#23558;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#20026;&#21407;&#23376;&#27979;&#24230;&#65292;&#24182;&#23545;&#21407;&#23376;&#30340;&#26435;&#37325;&#21644;&#20301;&#32622;&#24212;&#29992;&#36817;&#31471;&#28857;&#26356;&#26032;&#12290;&#23427;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#8220;&#30456;&#20114;&#20316;&#29992;&#8221;Wasserstein-Fisher-Rao&#26799;&#24230;&#27969;&#30340;&#26102;&#38388;&#38544;&#24335;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38750;&#36864;&#21270;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20174;&#20219;&#20309;&#21021;&#22987;&#21270;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#20110;&#20934;&#30830;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#65292;&#24182;&#25552;&#20379;&#25968;&#20540;&#23454;&#39564;&#26469;&#35828;&#26126;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing mixed Nash equilibria of two-player zero-sum games with continuous sets of pure strategies and with first-order access to the payoff function. This problem arises for example in game-theory-inspired machine learning applications, such as distributionally-robust learning. In those applications, the strategy sets are high-dimensional and thus methods based on discretisation cannot tractably return high-accuracy solutions.  In this paper, we introduce and analyze a particle-based method that enjoys guaranteed local convergence for this problem. This method consists in parametrizing the mixed strategies as atomic measures and applying proximal point updates to both the atoms' weights and positions. It can be interpreted as a time-implicit discretization of the "interacting" Wasserstein-Fisher-Rao gradient flow.  We prove that, under non-degeneracy assumptions, this method converges at an exponential rate to the exact mixed Nash equilibrium from any init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#24187;&#35937;&#25945;&#23398;&#65288;DHT&#65289;&#30340;&#36845;&#20195;&#24335;&#25945;&#23398;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19979;&#25945;&#24072;&#25552;&#20379;&#31034;&#20363;&#30340;&#33021;&#21147;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#30340;&#25945;&#23398;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;DHT&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.17467</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#24187;&#35937;&#30340;&#36845;&#20195;&#24335;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Iterative Teaching by Data Hallucination. (arXiv:2210.17467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#24187;&#35937;&#25945;&#23398;&#65288;DHT&#65289;&#30340;&#36845;&#20195;&#24335;&#25945;&#23398;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19979;&#25945;&#24072;&#25552;&#20379;&#31034;&#20363;&#30340;&#33021;&#21147;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#30340;&#25945;&#23398;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;DHT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19979;&#36845;&#20195;&#25945;&#23398;&#30340;&#38382;&#39064;&#65292;&#21363;&#25945;&#24072;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#29366;&#24577;&#21644;&#30446;&#26631;&#27010;&#24565;&#25552;&#20379;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#24187;&#35937;&#25945;&#23398;&#65288;DHT&#65289;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#31354;&#38388;&#20869;&#65292;&#36890;&#36807;&#26234;&#33021;&#22320;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;&#25552;&#20379;&#31034;&#20363;&#30340;&#33021;&#21147;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35768;&#22810;&#25361;&#25112;&#24615;&#30340;&#25945;&#23398;&#35774;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;DHT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of iterative machine teaching, where a teacher sequentially provides examples based on the status of a learner under a discrete input space (i.e., a pool of finite samples), which greatly limits the teacher's capability. To address this issue, we study iterative teaching under a continuous input space where the input example (i.e., image) can be either generated by solving an optimization problem or drawn directly from a continuous distribution. Specifically, we propose data hallucination teaching (DHT) where the teacher can generate input data intelligently based on labels, the learner's status and the target concept. We study a number of challenging teaching setups (e.g., linear/neural learners in omniscient and black-box settings). Extensive empirical results verify the effectiveness of DHT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#27599;&#23567;&#26102;&#30149;&#20154;&#24694;&#21270;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31934;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36828;&#26399;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.16598</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#29992;&#20110;&#31934;&#32454;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#30149;&#20154;&#24694;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Predictive Coding with Multimodal Fusion for Patient Deterioration Prediction in Fine-grained Time Resolution. (arXiv:2210.16598v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#27599;&#23567;&#26102;&#30149;&#20154;&#24694;&#21270;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31934;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36828;&#26399;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#65292;&#31934;&#30830;&#22320;&#39044;&#27979;&#30149;&#20154;&#30340;&#20851;&#38190;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#35768;&#22810;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;(EHR)&#30340;&#33258;&#21160;&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#26102;&#38388;&#20998;&#36776;&#29575;&#36807;&#31895;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24613;&#35786;&#31185;&#21644;&#37325;&#30151;&#30417;&#25252;&#23460;&#31561;&#32039;&#24613;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#27599;&#23567;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#27515;&#20129;&#29575;&#21644;&#34880;&#31649;&#21319;&#21387;&#33647;&#38656;&#27714;&#39044;&#27979;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#33258;&#30417;&#30563;&#39044;&#27979;&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#36828;&#26399;&#39044;&#27979;&#26041;&#38754;&#65292;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#21333;&#27169;&#24577;/&#21452;&#27169;&#24577;/&#21452;&#27169;&#24577;&#33258;&#30417;&#30563;&#22312;&#27515;&#20129;&#29575;(&#36828;&#26399;&#27515;&#20129;&#29575;)&#26041;&#38754;&#24471;&#20998;&#20998;&#21035;&#20026;0.846/0.877/0.897 (0.824/0.855/0.886)&#65292;&#22312;&#34880;&#31649;&#21319;&#21387;&#33647;&#38656;&#27714;&#39044;&#27979;&#26041;&#38754;&#24471;&#20998;&#20998;&#21035;&#20026;0.817/0.820/0.858 (0.807/0.81/0.855)&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate time prediction of patients' critical events is crucial in urgent scenarios where timely decision-making is important. Though many studies have proposed automatic prediction methods using Electronic Health Records (EHR), their coarse-grained time resolutions limit their practical usage in urgent environments such as the emergency department (ED) and intensive care unit (ICU). Therefore, in this study, we propose an hourly prediction method based on self-supervised predictive coding and multi-modal fusion for two critical tasks: mortality and vasopressor need prediction. Through extensive experiments, we prove significant performance gains from both multi-modal fusion and self-supervised predictive regularization, most notably in far-future prediction, which becomes especially important in practice. Our uni-modal/bi-modal/bi-modal self-supervision scored 0.846/0.877/0.897 (0.824/0.855/0.886) and 0.817/0.820/0.858 (0.807/0.81/0.855) with mortality (far-future mortality) and with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22788;&#29702;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21644;&#20854;&#20182;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#26102;&#65292;&#23545;&#8220;&#30495;&#30456;&#8221;&#26469;&#28304;&#30340;&#21512;&#27861;&#24615;&#12289;&#26435;&#23041;&#24615;&#21644;&#23458;&#35266;&#24615;&#25152;&#37319;&#21462;&#30340;&#31435;&#22330;&#20197;&#21450;ML&#39537;&#21160;&#30340;&#23457;&#26597;&#31995;&#32479;&#21487;&#33021;&#22312;&#19981;&#21033;&#24433;&#21709;&#26041;&#38754;&#20135;&#29983;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#20598;&#28982;&#24615;&#21644;&#21487;&#33021;&#24341;&#36215;&#30340;&#35780;&#20272;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2210.09014</link><description>&lt;p&gt;
&#22788;&#29702;&#31639;&#27861;&#65288;&#35823;&#65289;&#20449;&#24687;&#20998;&#31867;&#20013;&#30340;&#20598;&#28982;&#24615;&#65306;&#36208;&#21521;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda. (arXiv:2210.09014v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22788;&#29702;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21644;&#20854;&#20182;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#26102;&#65292;&#23545;&#8220;&#30495;&#30456;&#8221;&#26469;&#28304;&#30340;&#21512;&#27861;&#24615;&#12289;&#26435;&#23041;&#24615;&#21644;&#23458;&#35266;&#24615;&#25152;&#37319;&#21462;&#30340;&#31435;&#22330;&#20197;&#21450;ML&#39537;&#21160;&#30340;&#23457;&#26597;&#31995;&#32479;&#21487;&#33021;&#22312;&#19981;&#21033;&#24433;&#21709;&#26041;&#38754;&#20135;&#29983;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#20598;&#28982;&#24615;&#21644;&#21487;&#33021;&#24341;&#36215;&#30340;&#35780;&#20272;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#35299;&#20915;&#24222;&#22823;&#19988;&#36895;&#24230;&#24555;&#30340;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21644;&#20854;&#20182;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#22312;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#23545;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#8220;&#30495;&#30456;&#8221;&#26469;&#28304;&#30340;&#21512;&#27861;&#24615;&#12289;&#26435;&#23041;&#24615;&#21644;&#23458;&#35266;&#24615;&#37319;&#21462;&#31435;&#22330;&#12290;&#36825;&#28041;&#21450;&#25919;&#27835;&#12289;&#20262;&#29702;&#21644;&#35748;&#35782;&#35770;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#25216;&#26415;&#35770;&#25991;&#20013;&#24456;&#23569;&#24471;&#21040;&#35299;&#20915;&#12290;&#23613;&#31649;&#65288;&#20063;&#23601;&#26159;&#30001;&#20110;&#65289;&#20854;&#25253;&#21578;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#65292;&#30001;ML&#39537;&#21160;&#30340;&#23457;&#26597;&#31995;&#32479;&#21487;&#33021;&#20250;&#22609;&#36896;&#22312;&#32447;&#20844;&#20849;&#36777;&#35770;&#65292;&#24182;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22914;&#19981;&#24403;&#23457;&#26597;&#21644;&#24378;&#21270;&#38169;&#35823;&#20449;&#24565;&#12290;&#25105;&#20204;&#37319;&#29992;&#21512;&#20316;&#30340;&#27665;&#26063;&#24535;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#23545;&#24314;&#31435;&#65288;&#35823;&#65289;&#20449;&#24687;&#20998;&#31867;&#30340;ML&#27169;&#22411;&#30340;&#36807;&#31243;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#19968;&#31995;&#21015;&#31639;&#27861;&#30340;&#20598;&#28982;&#24615;&#8212;&#8212;&#20851;&#38190;&#30340;&#27169;&#22411;&#20915;&#31574;&#28857;&#65292;&#36825;&#20123;&#20915;&#31574;&#28857;&#21487;&#33021;&#20250;&#22312;&#25216;&#26415;&#23454;&#29616;&#20013;&#24341;&#20837;&#33258;&#24049;&#30340;&#35808;&#37322;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#12290;&#22522;&#20110;650&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#20026;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#25512;&#29702;&#33021;&#21147;&#32771;&#23519;&#25552;&#20379;&#20102;&#24191;&#27867;&#19988;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.07474</link><description>&lt;p&gt;
SQA3D&#65306;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#12290;&#22522;&#20110;650&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#20026;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#25512;&#29702;&#33021;&#21147;&#32771;&#23519;&#25552;&#20379;&#20102;&#24191;&#27867;&#19988;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#65306;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#65288;SQA3D&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#22330;&#26223;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#19977;&#32500;&#25195;&#25551;&#65289;&#65292;SQA3D&#35201;&#27714;&#32463;&#36807;&#27979;&#35797;&#30340;&#20195;&#29702;&#20154;&#39318;&#20808;&#29702;&#35299;&#20854;&#22312;&#25991;&#26412;&#25551;&#36848;&#19979;&#30340;3D&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#65288;&#20301;&#32622;&#12289;&#26041;&#21521;&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#35813;&#24773;&#22659;&#19979;&#36827;&#34892;&#25512;&#29702;&#65292;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#26469;&#33258;ScanNet&#30340;650&#20010;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#24515;&#22260;&#32469;6.8k&#20010;&#21807;&#19968;&#24773;&#22659;&#65292;20.4k&#30340;&#25551;&#36848;&#21644;33.4k&#22810;&#26679;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#23545;&#26234;&#33021;&#20195;&#29702;&#20154;&#33539;&#22260;&#24191;&#27867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#32771;&#23519;&#65292;&#20174;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#21040;&#24120;&#35782;&#29702;&#35299;&#12289;&#23548;&#33322;&#21644;&#22810;&#36339;&#25512;&#29702;&#12290;SQA3D&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#23588;&#20854;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26368;&#20339;&#32467;&#26524;&#20165;&#36798;&#21040;&#20102;47.20%&#30340;&#24635;&#20307;&#24471;&#20998;&#65292;&#32780;&#19994;&#20313;&#27700;&#24179;&#30340;&#34920;&#29616;&#26356;&#20026;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#20026;&#26131;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#22312;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20687;&#32032;&#31354;&#38388;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2210.03142</link><description>&lt;p&gt;
&#20851;&#20110;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
On Distillation of Guided Diffusion Models. (arXiv:2210.03142v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#20026;&#26131;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#22312;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20687;&#32032;&#31354;&#38388;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#23427;&#20204;&#24050;&#24191;&#27867;&#29992;&#20110;&#21253;&#25324; DALLE-2&#12289;Stable Diffusion &#21644; Imagen &#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#25193;&#25955;&#26694;&#26550;&#20013;&#12290;&#28982;&#32780;&#65292;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#32570;&#28857;&#26159;&#65292;&#22312;&#25512;&#26029;&#26102;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#38656;&#35201;&#35780;&#20272;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#65288;&#19968;&#20010;&#31867;&#26377;&#26465;&#20214;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#26080;&#26465;&#20214;&#30340;&#27169;&#22411;&#65289;&#25968;&#21313;&#21040;&#25968;&#30334;&#27425;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#20026;&#26131;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;: &#32473;&#23450;&#19968;&#20010;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#20813;&#20998;&#31867;&#22120;&#24341;&#23548;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#20197;&#21305;&#37197;&#32852;&#21512;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36880;&#27493;&#23558;&#35813;&#27169;&#22411;&#33976;&#39311;&#21040;&#21482;&#38656;&#35201;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#23545;&#20110;&#22312;&#20687;&#32032;&#31354;&#38388;&#35757;&#32451;&#30340;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate im
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#26680;&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#25289;&#26222;&#25289;&#26031;&#26680;&#65292;RFF&#19981;&#33021;&#20351;&#29992;&#20302;&#32500;&#36817;&#20284;&#26680;&#36317;&#31163;&#24182;&#20445;&#25345;&#23567;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;&#24403;&#26680;&#26159;&#35299;&#26512;&#30340;&#26102;&#20505;&#65292;RFF&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#32500;&#24230;&#23454;&#29616;$\epsilon$-&#30456;&#23545;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#65292;&#32500;&#24230;&#36793;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.00244</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#30456;&#23545;&#35823;&#24046;&#20445;&#25345;&#26680;&#36317;&#31163;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Relative Error of Random Fourier Features for Preserving Kernel Distance. (arXiv:2210.00244v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00244
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#26680;&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#25289;&#26222;&#25289;&#26031;&#26680;&#65292;RFF&#19981;&#33021;&#20351;&#29992;&#20302;&#32500;&#36817;&#20284;&#26680;&#36317;&#31163;&#24182;&#20445;&#25345;&#23567;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;&#24403;&#26680;&#26159;&#35299;&#26512;&#30340;&#26102;&#20505;&#65292;RFF&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#32500;&#24230;&#23454;&#29616;$\epsilon$-&#30456;&#23545;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#65292;&#32500;&#24230;&#36793;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#38024;&#23545;&#24179;&#31227;&#19981;&#21464;&#26680;&#65292;&#22312;&#65288;&#39640;&#32500;&#65289;&#26680;&#31354;&#38388;&#20013;&#25214;&#21040;&#28857;&#30340;&#36817;&#20284;&#20302;&#32500;&#34920;&#31034;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#35823;&#24046;&#20445;&#35777;&#27010;&#24565;&#19979;&#20998;&#26512;&#20102;RFF&#65292;&#20294;&#20197;&#8220;&#30456;&#23545;&#35823;&#24046;&#8221;&#20445;&#25345;&#26680;&#36317;&#31163;&#30340;&#33021;&#21147;&#36739;&#23569;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21253;&#25324;&#33879;&#21517;&#30340;&#25289;&#26222;&#25289;&#26031;&#26680;&#22312;&#20869;&#30340;&#22823;&#22810;&#25968;&#26680;&#33539;&#22260;&#20869;&#65292;RFF&#19981;&#33021;&#20351;&#29992;&#20302;&#32500;&#36817;&#20284;&#26680;&#36317;&#31163;&#24182;&#20445;&#25345;&#23567;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;&#24403;&#24179;&#31227;&#19981;&#21464;&#26680;&#26159;&#8220;&#35299;&#26512;&#8221;&#30340;&#26102;&#20505;&#65292;RFF&#21487;&#20197;&#20351;&#29992;$\mathrm{poly}(\epsilon^{-1}\log n)$&#32500;&#24230;&#26469;&#23454;&#29616;&#23545;&#20110;$n$&#28857;&#30340;&#37197;&#23545;&#26680;&#36317;&#31163;&#30340;$\epsilon$-&#30456;&#23545;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#23545;&#20110;&#29305;&#23450;&#30340;&#8220;&#26680;$k$-means&#8221;&#30340;&#24212;&#29992;&#20013;&#65292;&#32500;&#24230;&#36793;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#21464;&#20026;$\mathrm{poly}(\epsilon^{-1}\log k)$&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;RFF&#65292;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The method of random Fourier features (RFF), proposed in a seminal paper by Rahimi and Recht (NIPS'07), is a powerful technique to find approximate low-dimensional representations of points in (high-dimensional) kernel space, for shift-invariant kernels. While RFF has been analyzed under various notions of error guarantee, the ability to preserve the kernel distance with \emph{relative} error is less understood. We show that for a significant range of kernels, including the well-known Laplacian kernels, RFF cannot approximate the kernel distance with small relative error using low dimensions. We complement this by showing as long as the shift-invariant kernel is analytic, RFF with $\mathrm{poly}(\epsilon^{-1} \log n)$ dimensions achieves $\epsilon$-relative error for pairwise kernel distance of $n$ points, and the dimension bound is improved to $\mathrm{poly}(\epsilon^{-1}\log k)$ for the specific application of kernel $k$-means. Finally, going beyond RFF, we make the first step toward
&lt;/p&gt;</description></item><item><title>PiFold &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#22522;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644; PiGNN &#23618;&#65292;&#33021;&#22815;&#19968;&#27425;&#24615;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#65292;&#22312; CATH 4.2 &#19978;&#36798;&#21040;&#20102; 51.66% &#30340;&#24674;&#22797;&#29575;&#65292;&#25512;&#29702;&#36895;&#24230;&#27604;&#33258;&#22238;&#24402;&#31454;&#20105;&#23545;&#25163;&#24555; 70 &#20493;&#65292;&#22312; TS50 &#21644; TS500 &#19978;&#20998;&#21035;&#36798;&#21040;&#20102; 58.72% &#21644; 60.42% &#30340;&#24674;&#22797;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.12643</link><description>&lt;p&gt;
PiFold: &#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#34507;&#30333;&#36136;&#36870;&#21521;&#25240;&#21472;
&lt;/p&gt;
&lt;p&gt;
PiFold: Toward effective and efficient protein inverse folding. (arXiv:2209.12643v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12643
&lt;/p&gt;
&lt;p&gt;
PiFold &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#22522;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644; PiGNN &#23618;&#65292;&#33021;&#22815;&#19968;&#27425;&#24615;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#65292;&#22312; CATH 4.2 &#19978;&#36798;&#21040;&#20102; 51.66% &#30340;&#24674;&#22797;&#29575;&#65292;&#25512;&#29702;&#36895;&#24230;&#27604;&#33258;&#22238;&#24402;&#31454;&#20105;&#23545;&#25163;&#24555; 70 &#20493;&#65292;&#22312; TS50 &#21644; TS500 &#19978;&#20998;&#21035;&#36798;&#21040;&#20102; 58.72% &#21644; 60.42% &#30340;&#24674;&#22797;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#39640;&#25928;&#32780;&#26377;&#25928;&#22320;&#35774;&#35745;&#33021;&#22815;&#25240;&#21472;&#25104;&#25152;&#38656;&#32467;&#26500;&#30340;&#34507;&#30333;&#24207;&#21015;&#65311;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;AI&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#34920;&#36798;&#19981;&#22815;&#20805;&#20998;&#21644;&#33258;&#22238;&#24402;&#24207;&#21015;&#35299;&#30721;&#22120;&#30340;&#32570;&#20047;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PiFold&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#22522;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;PiGNN&#23618;&#65292;&#20197;&#19968;&#27425;&#24615;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PiFold &#22312; CATH 4.2 &#19978;&#33021;&#22815;&#36798;&#21040; 51.66\% &#30340;&#24674;&#22797;&#29575;&#65292;&#32780;&#25512;&#29702;&#36895;&#24230;&#27604;&#33258;&#22238;&#24402;&#31454;&#20105;&#23545;&#25163;&#24555; 70 &#20493;&#12290;&#27492;&#22806;&#65292;PiFold &#22312; TS50 &#21644; TS500 &#19978;&#20998;&#21035;&#36798;&#21040;&#20102; 58.72\% &#21644; 60.42\% &#30340;&#24674;&#22797;&#20998;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;&#34507;&#30333;&#36136;&#29305;&#24449;&#21644;&#27169;&#22411;&#35774;&#35745;&#30340;&#20316;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#31616;&#21270;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#28789;&#24863;&#12290;PyTorch &#20195;&#30721;&#21487;&#22312; \href{https://github.com/idea-iitp/PiFold}{https://github.com/idea-iitp/PiFold} &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we design protein sequences folding into the desired structures effectively and efficiently? AI methods for structure-based protein design have attracted increasing attention in recent years; however, few methods can simultaneously improve the accuracy and efficiency due to the lack of expressive features and autoregressive sequence decoder. To address these issues, we propose PiFold, which contains a novel residue featurizer and PiGNN layers to generate protein sequences in a one-shot way with improved recovery. Experiments show that PiFold could achieve 51.66\% recovery on CATH 4.2, while the inference speed is 70 times faster than the autoregressive competitors. In addition, PiFold achieves 58.72\% and 60.42\% recovery scores on TS50 and TS500, respectively. We conduct comprehensive ablation studies to reveal the role of different types of protein features and model designs, inspiring further simplification and improvement. The PyTorch code is available at \href{https://gith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#31243;&#24207;&#65292;&#29992;&#20110;&#35299;&#20915;&#29983;&#23384;&#20998;&#26512;&#20013;&#30701;&#25968;&#25454;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#22806;&#37096;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#19982;&#26032;&#25910;&#38598;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#30456;&#32467;&#21512;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.05100</link><description>&lt;p&gt;
&#22522;&#20110;KL&#25955;&#24230;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
KL-divergence Based Deep Learning for Discrete Time Model. (arXiv:2208.05100v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#31243;&#24207;&#65292;&#29992;&#20110;&#35299;&#20915;&#29983;&#23384;&#20998;&#26512;&#20013;&#30701;&#25968;&#25454;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#22806;&#37096;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#19982;&#26032;&#25910;&#38598;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#30456;&#32467;&#21512;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29616;&#20195;&#27169;&#22411;&#65292;&#24182;&#24050;&#34987;&#21033;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#20013;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#19968;&#20123;&#25913;&#36827;&#65292;&#20294;&#35757;&#32451;&#20986;&#19968;&#20010;&#20248;&#31168;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Kullback-Leibler&#65288;KL&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#31243;&#24207;&#65292;&#23558;&#22806;&#37096;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#19982;&#26032;&#25910;&#38598;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;KL&#21306;&#20998;&#20449;&#24687;&#26469;&#34913;&#37327;&#22806;&#37096;&#25968;&#25454;&#21644;&#20869;&#37096;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#20351;&#29992;&#20808;&#21069;&#20449;&#24687;&#26469;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#30701;&#25968;&#25454;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network (Deep Learning) is a modern model in Artificial Intelligence and it has been exploited in Survival Analysis. Although several improvements have been shown by previous works, training an excellent deep learning model requires a huge amount of data, which may not hold in practice. To address this challenge, we develop a Kullback-Leibler-based (KL) deep learning procedure to integrate external survival prediction models with newly collected time-to-event data. Time-dependent KL discrimination information is utilized to measure the discrepancy between the external and internal data. This is the first work considering using prior information to deal with short data problem in Survival Analysis for deep learning. Simulation and real data results show that the proposed model achieves better performance and higher robustness compared with previous works.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#21521;&#37327;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#19979;&#30340;&#32593;&#32476;&#21160;&#24577;&#31995;&#32479;&#30340;&#22270;&#24418;&#24674;&#22797;&#65292;&#22240;&#26524;&#25512;&#26029;&#26426;&#21046;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21516;&#31867;&#26426;&#21046;&#65292;&#24182;&#19988;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.04405</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#19979;&#32593;&#32476;&#21160;&#24577;&#31995;&#32479;&#30340;&#22270;&#24418;&#24674;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recovering the Graph Underlying Networked Dynamical Systems under Partial Observability: A Deep Learning Approach. (arXiv:2208.04405v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#21521;&#37327;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#19979;&#30340;&#32593;&#32476;&#21160;&#24577;&#31995;&#32479;&#30340;&#22270;&#24418;&#24674;&#22797;&#65292;&#22240;&#26524;&#25512;&#26029;&#26426;&#21046;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21516;&#31867;&#26426;&#21046;&#65292;&#24182;&#19988;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22270;&#24418;&#32467;&#26500;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#24674;&#22797;&#26102;&#38388;&#24207;&#21015;&#30340;&#20381;&#23384;&#20851;&#31995;&#22270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20026;&#32447;&#24615;&#38543;&#26426;&#32593;&#32476;&#21160;&#24577;&#31995;&#32479;&#29366;&#24577;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#20551;&#35774;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20165;&#35266;&#27979;&#21040;&#21253;&#21547;&#32593;&#32476;&#30340;&#23376;&#38598;&#30340;&#29366;&#24577;&#28436;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#35745;&#31639;&#20986;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#29305;&#24449;&#26159;&#32447;&#24615;&#21487;&#20998;&#30340;&#65292;&#21363;&#23384;&#22312;&#19968;&#20010;&#36229;&#24179;&#38754;&#65292;&#21487;&#20197;&#23558;&#19982;&#36830;&#25509;&#33410;&#28857;&#37197;&#23545;&#30340;&#29305;&#24449;&#32676;&#19982;&#19982;&#26410;&#36830;&#25509;&#37197;&#23545;&#30340;&#29305;&#24449;&#32676;&#20998;&#24320;&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#29305;&#24449;&#36866;&#21512;&#35757;&#32451;&#21508;&#31181;&#20998;&#31867;&#22120;&#26469;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#22240;&#26524;&#25512;&#26029;&#26426;&#21046;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21516;&#31867;&#25512;&#26029;&#26426;&#21046;&#12290;&#35757;&#32451;&#30340;CNNs&#22312;&#32467;&#26500;&#19978;&#19981;&#21516;&#30340;&#32593;&#32476;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of graph structure identification, i.e., of recovering the graph of dependencies among time series. We model these time series data as components of the state of linear stochastic networked dynamical systems. We assume partial observability, where the state evolution of only a subset of nodes comprising the network is observed. We devise a new feature vector computed from the observed time series and prove that these features are linearly separable, i.e., there exists a hyperplane that separates the cluster of features associated with connected pairs of nodes from those associated with disconnected pairs. This renders the features amenable to train a variety of classifiers to perform causal inference. In particular, we use these features to train Convolutional Neural Networks (CNNs). The resulting causal inference mechanism outperforms state-of-the-art counterparts w.r.t. sample-complexity. The trained CNNs generalize well over structurally distinct networks (dense
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21327;&#21464;&#37327;&#24179;&#34913;&#32422;&#31616;&#26041;&#27861;&#65292;&#20351;&#29992;&#20849;&#36717;&#36873;&#21462;&#30340;&#24555;&#29031;&#26469;&#24179;&#34913;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#22522;&#20110;&#20849;&#36717;&#30340;&#26799;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20197;&#35299;&#20915;&#38477;&#38454;&#27169;&#22411;&#20013;&#23545;&#20302;&#26041;&#24046;&#22352;&#26631;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.14387</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#27169;&#22411;&#32553;&#20943;&#65306;&#36890;&#36807;&#24179;&#34913;&#29366;&#24577;&#21644;&#26799;&#24230;&#21327;&#26041;&#24046;&#30340;&#25130;&#26029;&#24179;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model Reduction for Nonlinear Systems by Balanced Truncation of State and Gradient Covariance. (arXiv:2207.14387v4 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21327;&#21464;&#37327;&#24179;&#34913;&#32422;&#31616;&#26041;&#27861;&#65292;&#20351;&#29992;&#20849;&#36717;&#36873;&#21462;&#30340;&#24555;&#29031;&#26469;&#24179;&#34913;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#22522;&#20110;&#20849;&#36717;&#30340;&#26799;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20197;&#35299;&#20915;&#38477;&#38454;&#27169;&#22411;&#20013;&#23545;&#20302;&#26041;&#24046;&#22352;&#26631;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#38477;&#38454;&#27169;&#22411;&#24120;&#24120;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#23545;&#20302;&#26041;&#24046;&#22352;&#26631;&#25935;&#24863;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#22240;&#20026;&#36825;&#20123;&#22352;&#26631;&#24448;&#24448;&#34987;&#25130;&#26029;&#65292;&#20363;&#22914;&#36890;&#36807;&#27491;&#20132;&#20998;&#35299;&#12289;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#33258;&#32534;&#30721;&#22120;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#22312;&#21463;&#21098;&#20999;&#30340;&#27969;&#20307;&#27969;&#20013;&#32463;&#24120;&#21457;&#29616;&#65292;&#38750;&#27491;&#24577;&#24615;&#22312;&#24178;&#25200;&#22686;&#38271;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20027;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#65292;&#23547;&#25214;&#24179;&#34913;&#30340;&#38477;&#32500;&#22352;&#26631;&#31995;&#32479;&#65292;&#20197;&#24179;&#34913;&#22522;&#20110;&#20849;&#36717;&#30340;&#20851;&#20110;&#31995;&#32479;&#25935;&#24863;&#24615;&#21644;&#29366;&#24577;&#27839;&#36712;&#36857;&#26041;&#24046;&#30340;&#20449;&#24687;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21327;&#21464;&#37327;&#24179;&#34913;&#32422;&#31616;&#26041;&#27861;&#65292;&#20351;&#29992;&#20849;&#36717;&#36873;&#21462;&#30340;&#24555;&#29031;&#26469;&#24179;&#34913;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#22522;&#20110;&#20849;&#36717;&#30340;&#26799;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20195;&#26367;&#31995;&#32479;&#26684;&#25289;&#31859;&#23433;&#30697;&#38453;&#65292;&#36981;&#24490;&#21516;&#26679;&#30340;&#20851;&#38190;&#36716;&#25442;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven reduced-order models often fail to make accurate forecasts of high-dimensional nonlinear dynamical systems that are sensitive along coordinates with low-variance because such coordinates are often truncated, e.g., by proper orthogonal decomposition, kernel principal component analysis, and autoencoders. Such systems are encountered frequently in shear-dominated fluid flows where non-normality plays a significant role in the growth of disturbances. In order to address these issues, we employ ideas from active subspaces to find low-dimensional systems of coordinates for model reduction that balance adjoint-based information about the system's sensitivity with the variance of states along trajectories. The resulting method, which we refer to as covariance balancing reduction using adjoint snapshots (CoBRAS), is analogous to balanced truncation with state and adjoint-based gradient covariance matrices replacing the system Gramians and obeying the same key transformation laws. H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Sobol&#25351;&#25968;&#23545;&#31181;&#23376;&#22312;IM&#20013;&#30340;&#24433;&#21709;&#21644;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIM&#30340;IM&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36807;&#24230;&#36873;&#25321;&#33410;&#28857;&#26469;&#25913;&#21892;&#24403;&#21069;IM&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.07833</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#20998;&#35299;&#29702;&#35299;&#24433;&#21709;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Influence Maximization via Higher-Order Decomposition. (arXiv:2207.07833v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Sobol&#25351;&#25968;&#23545;&#31181;&#23376;&#22312;IM&#20013;&#30340;&#24433;&#21709;&#21644;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIM&#30340;IM&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36807;&#24230;&#36873;&#25321;&#33410;&#28857;&#26469;&#25913;&#21892;&#24403;&#21069;IM&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#24191;&#27867;&#24212;&#29992;&#30340;&#24433;&#21709;&#26368;&#22823;&#21270;(IM)&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;IM&#30340;&#22797;&#26434;&#24615;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20272;&#35745;&#33410;&#28857;&#23545;&#31181;&#23376;&#38598;&#30340;&#19968;&#38454;&#36129;&#29486;&#65292;&#32780;&#24573;&#30053;&#19981;&#21516;&#31181;&#23376;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23454;&#38469;&#24433;&#21709;&#20256;&#25773;&#32463;&#24120;&#20559;&#31163;&#39044;&#26399;&#65292;&#24182;&#19988;&#31181;&#23376;&#38598;&#23545;&#36825;&#31181;&#20559;&#24046;&#30340;&#23450;&#37327;&#36129;&#29486;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#26412;&#25991;&#21033;&#29992;Sobol&#25351;&#25968;(&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;)&#23545;&#21508;&#20010;&#31181;&#23376;&#30340;&#24433;&#21709;&#21450;&#20854;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#36866;&#24212;IM&#29615;&#22659;&#65292;&#31181;&#23376;&#36873;&#25321;&#34987;&#34920;&#36848;&#20026;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#24182;&#20998;&#20026;&#19981;&#21516;&#38454;&#25968;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21508;&#31181;Sobol&#25351;&#25968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;IM&#31639;&#27861;SIM&#65292;&#36890;&#36807;&#36807;&#24230;&#36873;&#25321;&#33410;&#28857;&#25913;&#21892;&#20102;&#24403;&#21069;IM&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given its vast application on online social networks, Influence Maximization (IM) has garnered considerable attention over the last couple of decades. Due to the intricacy of IM, most current research concentrates on estimating the first-order contribution of the nodes to select a seed set, disregarding the higher-order interplay between different seeds. Consequently, the actual influence spread frequently deviates from expectations, and it remains unclear how the seed set quantitatively contributes to this deviation. To address this deficiency, this work dissects the influence exerted on individual seeds and their higher-order interactions utilizing the Sobol index, a variance-based sensitivity analysis. To adapt to IM contexts, seed selection is phrased as binary variables and split into distributions of varying orders. Based on our analysis with various Sobol indices, an IM algorithm dubbed SIM is proposed to improve the performance of current IM algorithms by over-selecting nodes f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaserMix&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#28857;&#20113;&#30340;&#24378;&#31354;&#38388;&#32447;&#32034;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#32479;&#35745;&#22522;&#30784;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SemanticKITTI&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.00026</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;LiDAR&#35821;&#20041;&#20998;&#21106;&#30340;LaserMix&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaserMix for Semi-Supervised LiDAR Semantic Segmentation. (arXiv:2207.00026v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaserMix&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#28857;&#20113;&#30340;&#24378;&#31354;&#38388;&#32447;&#32034;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#32479;&#35745;&#22522;&#30784;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SemanticKITTI&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26631;&#27880;LiDAR&#28857;&#20113;&#32791;&#36153;&#24040;&#22823;&#65292;&#38480;&#21046;&#20102;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LiDAR&#20998;&#21106;&#20013;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24819;&#27861;&#26159;&#21033;&#29992;LiDAR&#28857;&#20113;&#30340;&#24378;&#31354;&#38388;&#32447;&#32034;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LaserMix&#26041;&#27861;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;LiDAR&#25195;&#25551;&#30340;&#28608;&#20809;&#26463;&#28151;&#21512;&#65292;&#28982;&#21518;&#20419;&#20351;&#27169;&#22411;&#22312;&#28151;&#21512;&#21069;&#21518;&#20570;&#20986;&#19968;&#33268;&#19988;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#19977;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65306;1&#65289;&#36890;&#29992;&#24615;&#65306;LaserMix&#19982;LiDAR&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#35270;&#35282;&#21644;&#20307;&#32032;&#65289;&#26080;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#21487;&#20197;&#26222;&#36941;&#24212;&#29992;&#12290;2&#65289;&#32479;&#35745;&#22522;&#30784;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;3&#65289;&#26377;&#25928;&#24615;&#65306;&#23545;&#27969;&#34892;&#30340;LiDAR&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;nuScenes&#65292;SemanticKITTI&#21644;ScribbleKITTI&#65289;&#30340;&#20840;&#38754;&#23454;&#39564;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;SemanticKITTI&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Densely annotating LiDAR point clouds is costly, which restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans, and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g., range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#20284;&#25193;&#25955;&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#21453;&#36716;&#28909;&#26041;&#31243;&#22312;2D&#24179;&#38754;&#19978;&#36816;&#34892;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#30340;&#26032;&#39062;&#23450;&#24615;&#24615;&#36136;&#65292;&#21253;&#25324;&#22270;&#20687;&#20013;&#25972;&#20307;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#35299;&#32544;&#32469;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2206.13397</link><description>&lt;p&gt;
&#24102;&#26377;&#36870;&#28909;&#20256;&#23548;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modelling With Inverse Heat Dissipation. (arXiv:2206.13397v7 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#20284;&#25193;&#25955;&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#21453;&#36716;&#28909;&#26041;&#31243;&#22312;2D&#24179;&#38754;&#19978;&#36816;&#34892;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#30340;&#26032;&#39062;&#23450;&#24615;&#24615;&#36136;&#65292;&#21253;&#25324;&#22270;&#20687;&#20013;&#25972;&#20307;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#35299;&#32544;&#32469;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#22122;&#22768;&#21453;&#28436;&#29983;&#25104;&#36807;&#31243;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#22270;&#20687;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#20854;&#22266;&#26377;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#12290;&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#21644;&#31895;&#21040;&#32454;&#24314;&#27169;&#30340;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#20284;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#38543;&#26426;&#22320;&#21453;&#36716;&#28909;&#26041;&#31243;&#22312;2D&#24179;&#38754;&#19978;&#36816;&#34892;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#24403;&#20854;&#36816;&#34892;&#26102;&#22320;&#23616;&#37096;&#25273;&#21435;&#20102;&#22270;&#20687;&#30340;&#32454;&#23610;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#24658;&#23450;&#21152;&#24615;&#22122;&#22768;&#30340;&#27491;&#21521;&#28909;&#26041;&#31243;&#30340;&#35299;&#37322;&#20026;&#25193;&#25955;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#21464;&#20998;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#26174;&#31034;&#20986;&#24182;&#19981;&#22312;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#20013;&#30475;&#21040;&#30340;&#26032;&#39062;&#30340;&#23450;&#24615;&#24615;&#36136;&#65292;&#20363;&#22914;&#22270;&#20687;&#20013;&#25972;&#20307;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#35299;&#32544;&#32469;&#29616;&#35937;&#12290;&#33258;&#28982;&#22270;&#20687;&#30340;&#35889;&#20998;&#26512;&#31361;&#20986;&#20102;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#32852;&#31995;&#24182;&#25581;&#31034;&#20102;&#20854;&#20013;&#19968;&#20010;&#38544;&#21547;&#30340;&#31895;&#21040;&#32454;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#20013;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#27969;&#34892;&#26041;&#27861;&#19981;&#21487;&#20449;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#19981;&#21487;&#33021;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Cohort Shapley&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#27982;&#21338;&#24328;&#29702;&#35770;&#65292;&#20165;&#20351;&#29992;&#23454;&#38469;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#26469;&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.15750</link><description>&lt;p&gt;
&#26080;&#38656;&#19981;&#21487;&#33021;&#25968;&#25454;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variable importance without impossible data. (arXiv:2205.15750v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15750
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#20013;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#27969;&#34892;&#26041;&#27861;&#19981;&#21487;&#20449;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#19981;&#21487;&#33021;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Cohort Shapley&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#27982;&#21338;&#24328;&#29702;&#35770;&#65292;&#20165;&#20351;&#29992;&#23454;&#38469;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#26469;&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35780;&#20272;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#20013;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#26159;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#32467;&#21512;&#20102;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#20123;&#36755;&#20837;&#25968;&#25454;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12289;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#65292;&#29978;&#33267;&#26159;&#36923;&#36753;&#19978;&#19981;&#21487;&#33021;&#30340;&#65292;&#30001;&#27492;&#24471;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#19982;&#40657;&#31665;&#35757;&#32451;&#25968;&#25454;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#24403;&#35299;&#37322;&#20915;&#31574;&#26102;&#20351;&#29992;&#36825;&#20123;&#20540;&#26102;&#65292;&#29992;&#25143;&#19981;&#33021;&#20449;&#20219;&#39044;&#27979;&#31639;&#27861;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#21517;&#20026;Cohort Shapley&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#27982;&#21338;&#24328;&#29702;&#35770;&#65292;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#21338;&#24328;&#35770;&#26041;&#27861;&#19981;&#21516;&#65292;&#20165;&#20351;&#29992;&#23454;&#38469;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#26469;&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#12290;Cohort Shapley&#36890;&#36807;&#32553;&#23567;&#19982;&#30446;&#26631;&#23545;&#35937;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#29305;&#24449;&#19978;&#30456;&#20284;&#30340;&#23545;&#35937;&#32452;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#24517;&#39035;&#23558;&#37325;&#35201;&#24615;&#24402;&#22240;&#20110;&#27169;&#22411;&#26410;&#32463;&#35757;&#32451;&#30340;&#21463;&#20445;&#25252;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#65292;&#23427;&#19987;&#27880;&#20110;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#20154;-&#29289;&#20132;&#20114;&#30340;&#32452;&#21512;&#24615;&#23398;&#20064;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#40723;&#21169;&#24320;&#21457;&#26356;&#22909;&#30340;&#31639;&#27861;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#27867;&#21270;&#21040;&#26032;&#30340;HOI&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2205.13803</link><description>&lt;p&gt;
Bongard-HOI&#65306;&#22522;&#20110;&#20154;-&#29289;&#20132;&#20114;&#30340;&#20960;&#31181;&#24773;&#20917;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#65292;&#23427;&#19987;&#27880;&#20110;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#20154;-&#29289;&#20132;&#20114;&#30340;&#32452;&#21512;&#24615;&#23398;&#20064;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#40723;&#21169;&#24320;&#21457;&#26356;&#22909;&#30340;&#31639;&#27861;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#27867;&#21270;&#21040;&#26032;&#30340;HOI&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24403;&#28041;&#21450;&#21040;&#26032;&#27010;&#24565;&#30340;&#20960;&#31181;&#24773;&#20917;&#23398;&#20064;&#21644;&#32452;&#21512;&#25512;&#29702;&#26102;&#65292;&#20170;&#22825;&#30340;&#35270;&#35273;&#22270;&#26696;&#35782;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#35270;&#35273;&#35748;&#30693;&#20043;&#38388;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;Bongard-HOI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#20391;&#37325;&#20110;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#20154;-&#29289;&#20132;&#20114;&#30340;&#32452;&#21512;&#24615;&#23398;&#20064;&#12290;&#23427;&#21463;&#21040;&#21476;&#20856;&#30340;Bongard&#38382;&#39064;&#65288;BPs&#65289;&#20013;&#30340;&#20004;&#20010;&#21487;&#21462;&#29305;&#24449;&#30340;&#21551;&#21457;&#65306;1&#65289;&#20960;&#31181;&#24773;&#20917;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;2&#65289;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#23569;&#26679;&#26412;&#23454;&#20363;&#21253;&#21547;&#22256;&#38590;&#30340;&#36127;&#20363;&#65292;&#20854;&#20013;&#27491;&#36127;&#22270;&#20687;&#20165;&#22312;&#21160;&#20316;&#26631;&#31614;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20165;&#20165;&#23545;&#23545;&#35937;&#31867;&#21035;&#30340;&#35782;&#21035;&#19981;&#36275;&#20197;&#23436;&#25104;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#31995;&#32479;&#30740;&#31350;&#35270;&#35273;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#21464;&#20102;&#20960;&#31181;&#24773;&#20917;&#23454;&#20363;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;HOI&#27010;&#24565;&#37325;&#21472;&#31243;&#24230;&#65292;&#20174;&#37096;&#20998;&#21040;&#27809;&#26377;&#37325;&#21472;&#12290;Bongard-HOI&#23545;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#20986;&#20102;&#23454;&#36136;&#24615;&#25361;&#25112;&#65292;&#24182;&#20026;&#24320;&#21457;&#21487;&#20197;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;HOI&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant gap remains between today's visual pattern recognition models and human-level visual cognition especially when it comes to few-shot learning and compositional reasoning of novel concepts. We introduce Bongard-HOI, a new visual reasoning benchmark that focuses on compositional learning of human-object interactions (HOIs) from natural images. It is inspired by two desirable characteristics from the classical Bongard problems (BPs): 1) few-shot concept learning, and 2) context-dependent reasoning. We carefully curate the few-shot instances with hard negatives, where positive and negative images only disagree on action labels, making mere recognition of object categories insufficient to complete our benchmarks. We also design multiple test sets to systematically study the generalization of visual learning models, where we vary the overlap of the HOI concepts between the training and test sets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a substanti
&lt;/p&gt;</description></item><item><title>&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#65292;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#37325;&#35201;&#39044;&#27979;&#31867;&#21035;&#65292;&#22914;&#39640;&#26031;&#26680;&#12289;&#25289;&#26222;&#25289;&#26031;&#26680;&#21644;&#26607;&#35199;&#26680;&#65292;&#22312;&#20219;&#20309;&#38750;&#38646;&#22238;&#24402;&#20989;&#25968;&#21644;&#20219;&#20309;&#24102;&#23485;&#36873;&#25321;&#19979;&#37117;&#19981;&#20855;&#22791;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13525</link><description>&lt;p&gt;
&#20851;&#20110;&#22266;&#23450;&#32500;&#24230;&#19979;&#26680;&#23725;&#22238;&#24402;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Inconsistency of Kernel Ridgeless Regression in Fixed Dimensions. (arXiv:2205.13525v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13525
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#65292;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#37325;&#35201;&#39044;&#27979;&#31867;&#21035;&#65292;&#22914;&#39640;&#26031;&#26680;&#12289;&#25289;&#26222;&#25289;&#26031;&#26680;&#21644;&#26607;&#35199;&#26680;&#65292;&#22312;&#20219;&#20309;&#38750;&#38646;&#22238;&#24402;&#20989;&#25968;&#21644;&#20219;&#20309;&#24102;&#23485;&#36873;&#25321;&#19979;&#37117;&#19981;&#20855;&#22791;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#26159;&#25351;&#26576;&#20123;&#31639;&#27861;&#33021;&#22815;&#25554;&#20540;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#26679;&#26412;&#22806;&#34920;&#29616;&#33391;&#22909;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#26368;&#36817;&#30340;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#35774;&#35745;&#35774;&#32622;&#34920;&#26126;&#65292;&#20855;&#26377;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#37325;&#35201;&#39044;&#27979;&#31867;&#21035;&#65292;&#20363;&#22914;&#39640;&#26031;&#26680;&#65292;&#25289;&#26222;&#25289;&#26031;&#26680;&#21644;&#26607;&#35199;&#26680;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#24182;&#19981;&#34920;&#29616;&#20986;&#33391;&#24615;&#36807;&#25311;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;&#20219;&#20309;&#38750;&#38646;&#22238;&#24402;&#20989;&#25968;&#21644;&#20219;&#20309;&#65288;&#29978;&#33267;&#26159;&#33258;&#36866;&#24212;&#30340;&#65289;&#24102;&#23485;&#36873;&#25321;&#37117;&#19981;&#20250;&#20351;&#24471;&#20272;&#35745;&#30340;&#39044;&#27979;&#22120;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#25910;&#25947;&#20110;&#22522;&#26412;&#30495;&#30456;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#27867;&#21270;&#35823;&#24046;&#21450;&#20854;&#22312;&#23545;kernel&#24102;&#23485;&#30340;&#36873;&#25321;&#19978;&#20135;&#29983;&#26435;&#34913;&#20043;&#19979;&#30340;&#36817;&#20284;&#35823;&#24046;&#21644;&#20272;&#35745;&#35823;&#24046;&#30340;&#30830;&#20999;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
``Benign overfitting'', the ability of certain algorithms to interpolate noisy training data and yet perform well out-of-sample, has been a topic of considerable recent interest. We show, using a fixed design setup, that an important class of predictors, kernel machines with translation-invariant kernels, does not exhibit benign overfitting in fixed dimensions. In particular, the estimated predictor does not converge to the ground truth with increasing sample size, for any non-zero regression function and any (even adaptive) bandwidth selection. To prove these results, we give exact expressions for the generalization error, and its decomposition in terms of an approximation error and an estimation error that elicits a trade-off based on the selection of the kernel bandwidth. Our results apply to commonly used translation-invariant kernels such as Gaussian, Laplace, and Cauchy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.01815</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#19968;&#33268;&#24615;&#21644;&#20844;&#24179;&#20445;&#35777;&#30340;&#25512;&#33616;&#31995;&#32479;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#38750;&#36127;/&#27491;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#19981;&#26159;&#20154;&#20026;&#22320;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20219;&#24847;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#32467;&#26500;&#37327;&#65292;&#22914;&#31209;&#25110;&#33539;&#25968;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23646;&#24615;/&#32422;&#26463;&#65306;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#19968;&#33268;&#24615;&#65292;&#20445;&#35777;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#30456;&#23545;&#36739;&#24369;&#30340;&#25903;&#25345;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#35299;&#30340;&#21807;&#19968;&#24615;&#12290;&#35813;&#26694;&#26550;&#21644;&#35299;&#31639;&#27861;&#20063;&#30452;&#25509;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#30340;&#24352;&#37327;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22266;&#23450;&#32500;&#24230; d &#30340;&#38382;&#39064;&#35268;&#27169;&#30340;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#21512;&#29702;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#24212;&#35813;&#36866;&#29992;&#20110;&#20219;&#20309; RS &#38382;&#39064;&#30340;&#35299;&#65292;&#36275;&#20197;&#20801;&#35768;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#24314;&#31435;&#21807;&#19968;&#24615;&#20445;&#35777;&#12290;&#20851;&#38190;&#29702;&#35770;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#19979;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#31034;&#25945;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#35760;&#20303;&#38271;&#24207;&#21015;&#36712;&#36857;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#23384;&#20648;&#26469;&#33258;&#36807;&#21435;&#31034;&#25945;&#30340;&#20219;&#20309;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2202.06843</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#31034;&#25945;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continual Learning from Demonstration of Robotics Skills. (arXiv:2202.06843v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#31034;&#25945;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#35760;&#20303;&#38271;&#24207;&#21015;&#36712;&#36857;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#23384;&#20648;&#26469;&#33258;&#36807;&#21435;&#31034;&#25945;&#30340;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26426;&#22120;&#20154;&#23398;&#20064;&#36816;&#21160;&#25216;&#33021;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#19968;&#27425;&#35757;&#32451;&#19968;&#20010;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#20174;&#31034;&#25945;&#20013;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#26032;&#22686;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#26469;&#23398;&#20064;&#26032;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#36807;&#21435;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#36827;&#34892;&#25345;&#32493;&#30340;&#31034;&#25945;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#35760;&#20303;&#38271;&#24207;&#21015;&#36712;&#36857;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23384;&#20648;&#26469;&#33258;&#36807;&#21435;&#31034;&#25945;&#30340;&#20219;&#20309;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#32593;&#32476;&#22312;&#23398;&#20064;&#31034;&#25945;&#30340;&#26368;&#26032;&#25216;&#26415;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;LASA&#22522;&#20934;&#27979;&#35797;&#12289;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#30340;&#20004;&#20010;&#26032;&#30340;&#30495;&#23454;&#26426;&#22120;&#20154;&#22522;&#24231;&#28436;&#31034;&#25968;&#25454;&#38598;HelloWorld&#21644;RoboTasks&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for teaching motion skills to robots focus on training for a single skill at a time. Robots capable of learning from demonstration can considerably benefit from the added ability to learn new movement skills without forgetting what was learned in the past. To this end, we propose an approach for continual learning from demonstration using hypernetworks and neural ordinary differential equation solvers. We empirically demonstrate the effectiveness of this approach in remembering long sequences of trajectory learning tasks without the need to store any data from past demonstrations. Our results show that hypernetworks outperform other state-of-the-art continual learning approaches for learning from demonstration. In our experiments, we use the popular LASA benchmark, and two new datasets of kinesthetic demonstrations collected with a real robot that we introduce in this paper called the HelloWorld and RoboTasks datasets. We evaluate our approach on a physical robot and demonstrat
&lt;/p&gt;</description></item><item><title>GoSafeOpt&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#25506;&#32034;&#39640;&#32500;&#31995;&#32479;&#24182;&#25552;&#20379;&#20840;&#23616;&#26368;&#20248;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09562</link><description>&lt;p&gt;
GoSafeOpt: &#21487;&#25193;&#23637;&#30340;&#23433;&#20840;&#20840;&#23616;&#20248;&#21270;&#21160;&#24577;&#31995;&#32479;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GoSafeOpt: Scalable Safe Exploration for Global Optimization of Dynamical Systems. (arXiv:2201.09562v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09562
&lt;/p&gt;
&lt;p&gt;
GoSafeOpt&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#25506;&#32034;&#39640;&#32500;&#31995;&#32479;&#24182;&#25552;&#20379;&#20840;&#23616;&#26368;&#20248;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#22312;&#29289;&#29702;&#31995;&#32479;&#19978;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#19968;&#27425;&#22833;&#36133;&#20063;&#21487;&#33021;&#23548;&#33268;&#26114;&#36149;&#30340;&#30828;&#20214;&#25439;&#22351;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20445;&#35777;&#23433;&#20840;&#65288;&#21363;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#27809;&#26377;&#22833;&#36133;&#65289;&#30340;&#26080;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#20165;&#23616;&#38480;&#20110;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;GoSafe&#31639;&#27861;&#26159;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#26080;&#27861;&#22788;&#29702;&#39640;&#32500;&#31995;&#32479;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;GoSafeOpt&#20316;&#20026;&#31532;&#19968;&#20010;&#33021;&#22815;&#23433;&#20840;&#22320;&#21457;&#29616;&#39640;&#32500;&#31995;&#32479;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#24182;&#20855;&#26377;&#23433;&#20840;&#21644;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26426;&#26800;&#33218;&#19978;&#23637;&#31034;&#20102;GoSafeOpt&#27604;&#20854;&#20182;&#27169;&#22411;&#26080;&#20851;&#30340;&#23433;&#20840;&#23398;&#20064;&#26041;&#27861;&#26356;&#20248;&#31168;&#65292;&#32780;GoSafe&#22312;&#35813;&#26426;&#26800;&#33218;&#19978;&#23558;&#26159;&#31105;&#27490;&#20351;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning optimal control policies directly on physical systems is challenging since even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. A notable exception is the GoSafe algorithm, which, unfortunately, cannot handle high-dimensional systems and hence cannot be applied to most real-world dynamical systems. This work proposes GoSafeOpt as the first algorithm that can safely discover globally optimal policies for high-dimensional systems while giving safety and optimality guarantees. We demonstrate the superiority of GoSafeOpt over competing model-free safe learning methods on a robot arm that would be prohibitive for GoSafe.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;&#65288;ADIs&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20856;&#22411;VFL&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#20026;&#38450;&#27490;ADIs&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.02775</link><description>&lt;p&gt;
ADI: &#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems. (arXiv:2201.02775v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;&#65288;ADIs&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20856;&#22411;VFL&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#20026;&#38450;&#27490;ADIs&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#31995;&#32479;&#20316;&#20026;&#22788;&#29702;&#20998;&#25955;&#22312;&#35768;&#22810;&#20010;&#20307;&#26469;&#28304;&#20013;&#30340;&#25968;&#25454;&#30340;&#27010;&#24565;&#32780;&#21464;&#24471;&#31361;&#20986;&#65292;&#26080;&#38656;&#23558;&#20854;&#38598;&#20013;&#21270;&#12290;&#22810;&#20010;&#21442;&#19982;&#32773;&#20197;&#38544;&#31169;&#24847;&#35782;&#30340;&#26041;&#24335;&#21327;&#20316;&#35757;&#32451;&#22522;&#20110;&#20854;&#26412;&#22320;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;VFL&#24050;&#25104;&#20026;&#22312;&#32452;&#32455;&#20043;&#38388;&#23433;&#20840;&#23398;&#20064;&#27169;&#22411;&#30340;&#20107;&#23454;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#20849;&#20139;&#30693;&#35782;&#32780;&#19981;&#24433;&#21709;&#20219;&#20309;&#20010;&#20154;&#30340;&#38544;&#31169;&#12290;&#23613;&#31649;VFL&#31995;&#32479;&#30340;&#21457;&#23637;&#26124;&#30427;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21442;&#19982;&#32773;&#30340;&#36755;&#20837;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;&#65288;ADIs&#65289;&#65292;&#21487;&#20197;&#25903;&#37197;&#20849;&#21516;&#25512;&#26029;&#26397;&#30528;&#23545;&#25163;&#30340;&#24847;&#24895;&#26041;&#21521;&#24182;&#36843;&#20351;&#20854;&#20182;&#65288;&#21463;&#23475;&#32773;&#65289;&#21442;&#19982;&#32773;&#20570;&#20986;&#24494;&#19981;&#36275;&#36947;&#30340;&#36129;&#29486;&#65292;&#22833;&#21435;&#36890;&#24120;&#22312;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#25552;&#20379;&#30340;&#23545;&#20854;&#36129;&#29486;&#37325;&#35201;&#24615;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23545;ADIs&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#39318;&#20808;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20856;&#22411;&#30340;VFL&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#28982;&#32780;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#34987;&#21160;&#30340;&#12289;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#25915;&#20987;&#32773;&#35266;&#23519;&#21040;&#26799;&#24230;&#65292;&#20063;&#21487;&#20197;&#37325;&#26500;&#21442;&#19982;&#21327;&#35758;&#30340;&#20010;&#20154;&#29992;&#25143;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#35753;&#19968;&#20010;&#20027;&#21160;&#30340;&#12289;&#19981;&#35802;&#23454;&#30340;&#20013;&#22830;&#26041;&#20174;&#25509;&#25910;&#21040;&#30340;&#26799;&#24230;&#20013;&#39640;&#25928;&#22320;&#25552;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;FL&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.02918</link><description>&lt;p&gt;
&#24403;&#22909;&#22855;&#30340;&#20154;&#25918;&#24323;&#35802;&#23454;&#65306;&#32852;&#37030;&#23398;&#20064;&#19981;&#26159;&#38544;&#31169;&#30340;&#20445;&#25252;&#32773;
&lt;/p&gt;
&lt;p&gt;
When the Curious Abandon Honesty: Federated Learning Is Not Private. (arXiv:2112.02918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02918
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#28982;&#32780;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#34987;&#21160;&#30340;&#12289;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#25915;&#20987;&#32773;&#35266;&#23519;&#21040;&#26799;&#24230;&#65292;&#20063;&#21487;&#20197;&#37325;&#26500;&#21442;&#19982;&#21327;&#35758;&#30340;&#20010;&#20154;&#29992;&#25143;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#35753;&#19968;&#20010;&#20027;&#21160;&#30340;&#12289;&#19981;&#35802;&#23454;&#30340;&#20013;&#22830;&#26041;&#20174;&#25509;&#25910;&#21040;&#30340;&#26799;&#24230;&#20013;&#39640;&#25928;&#22320;&#25552;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;FL&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#19981;&#20250;&#22312;&#32852;&#21512;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#31163;&#24320;&#20010;&#20154;&#35774;&#22791;&#12290;&#30456;&#21453;&#65292;&#36825;&#20123;&#35774;&#22791;&#20250;&#20849;&#20139;&#26799;&#24230;&#12289;&#21442;&#25968;&#25110;&#20854;&#20182;&#27169;&#22411;&#26356;&#26032;&#65292;&#24182;&#30001;&#19968;&#20010;&#20013;&#22830;&#26041;&#65288;&#20363;&#22914;&#20844;&#21496;&#65289;&#21327;&#35843;&#22521;&#35757;&#12290;&#22240;&#20026;&#25968;&#25454;&#20174;&#26410;&#8220;&#31163;&#24320;&#8221;&#20010;&#20154;&#35774;&#22791;&#65292;&#25152;&#20197;FL&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20445;&#25252;&#38544;&#31169;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#20445;&#25252;&#21482;&#26159;&#19968;&#20010;&#34180;&#34180;&#30340;&#22806;&#34915;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#19968;&#20010;&#34987;&#21160;&#30340;&#65292;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#25915;&#20987;&#32773;&#35266;&#23519;&#21040;&#26799;&#24230;&#65292;&#20063;&#21487;&#20197;&#37325;&#26500;&#21442;&#19982;&#21327;&#35758;&#30340;&#20010;&#20154;&#29992;&#25143;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#35753;&#19968;&#20010;&#20027;&#21160;&#30340;&#65292;&#19981;&#35802;&#23454;&#30340;&#20013;&#22830;&#26041;&#20174;&#25509;&#25910;&#21040;&#30340;&#26799;&#24230;&#20013;&#39640;&#25928;&#22320;&#25552;&#21462;&#29992;&#25143;&#25968;&#25454;&#12290;&#34429;&#28982;FL&#30340;&#25968;&#25454;&#37325;&#26500;&#20808;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#35299;&#20915;&#35745;&#31639;&#26114;&#36149;&#30340;&#20248;&#21270;&#38382;&#39064;&#25110;&#23545;&#20849;&#20139;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#25110;&#21442;&#25968;&#36827;&#34892;&#23481;&#26131;&#26816;&#27979;&#30340;&#20462;&#25913;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#25915;&#20987;&#20013;&#65292;&#20013;&#22830;&#26041;&#23545;FL&#21327;&#35758;&#26412;&#36523;&#36827;&#34892;&#20102;&#19981;&#24341;&#20154;&#27880;&#30446;&#30340;&#20462;&#25913;&#20197;&#23454;&#29616;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;FL&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), data does not leave personal devices when they are jointly training a machine learning model. Instead, these devices share gradients, parameters, or other model updates, with a central party (e.g., a company) coordinating the training. Because data never "leaves" personal devices, FL is often presented as privacy-preserving. Yet, recently it was shown that this protection is but a thin facade, as even a passive, honest-but-curious attacker observing gradients can reconstruct data of individual users contributing to the protocol. In this work, we show a novel data reconstruction attack which allows an active and dishonest central party to efficiently extract user data from the received gradients. While prior work on data reconstruction in FL relies on solving computationally expensive optimization problems or on making easily detectable modifications to the shared model's architecture or parameters, in our attack the central party makes inconspicuous changes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21518;&#24724;&#65292;&#36890;&#36807;&#22312;&#25152;&#26377;&#26377;&#30028;&#24178;&#25200;&#19979;&#25214;&#21040;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#21518;&#24724;&#30340;&#22240;&#26524;&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#21518;&#24724;&#26368;&#23567;LQR&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2105.01244</link><description>&lt;p&gt;
&#21518;&#24724;&#26368;&#23567;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Regret-Optimal LQR Control. (arXiv:2105.01244v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21518;&#24724;&#65292;&#36890;&#36807;&#22312;&#25152;&#26377;&#26377;&#30028;&#24178;&#25200;&#19979;&#25214;&#21040;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#21518;&#24724;&#30340;&#22240;&#26524;&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#21518;&#24724;&#26368;&#23567;LQR&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26080;&#38480;&#26102;&#38388;LQR&#25511;&#21046;&#38382;&#39064;&#12290;&#21463;&#22312;&#32447;&#23398;&#20064;&#31454;&#20105;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#21160;&#24577;&#21518;&#24724;&#20316;&#20026;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#26631;&#20934;&#12290;&#21160;&#24577;&#21518;&#24724;&#26159;&#25351;&#22240;&#26524;&#25511;&#21046;&#22120;&#65288;&#20165;&#20855;&#26377;&#36807;&#21435;&#24178;&#25200;&#30340;&#35775;&#38382;&#26435;&#38480;&#65289;&#30340;LQR&#25104;&#26412;&#19982;&#24050;&#30693;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#25511;&#21046;&#22120;&#30340;&#21807;&#19968;&#39044;&#30693;&#25511;&#21046;&#22120;&#65288;&#20063;&#20855;&#26377;&#26410;&#26469;&#24178;&#25200;&#30340;&#35775;&#38382;&#26435;&#38480;&#65289;&#30340;LQR&#25104;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#21518;&#24724;&#26412;&#36523;&#26159;&#24178;&#25200;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#35758;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#26368;&#23567;&#21270;&#25152;&#26377;&#26377;&#30028;&#33021;&#37327;&#24178;&#25200;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#21518;&#24724;&#30340;&#22240;&#26524;&#25511;&#21046;&#22120;&#12290;&#24471;&#21040;&#30340;&#25511;&#21046;&#22120;&#30340;&#35299;&#37322;&#26159;&#65292;&#19982;&#33021;&#22815;&#30475;&#21040;&#26410;&#26469;&#30340;&#26368;&#20339;&#38750;&#22240;&#26524;&#25511;&#21046;&#22120;&#30456;&#27604;&#65292;&#20445;&#35777;&#20102;&#26368;&#23567;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29366;&#24577;&#31354;&#38388;&#35774;&#32622;&#30340;&#26368;&#20248;&#21518;&#24724;&#21644;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#26174;&#24335;&#20844;&#24335;&#12290;&#36890;&#36807;&#35777;&#26126;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#24402;&#32422;&#20026;Nehari&#25351;&#25968;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#36825;&#20123;&#26174;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the infinite-horizon LQR control problem. Motivated by competitive analysis in online learning, as a criterion for controller design we introduce the dynamic regret, defined as the difference between the LQR cost of a causal controller (that has only access to past disturbances) and the LQR cost of the \emph{unique} clairvoyant one (that has also access to future disturbances) that is known to dominate all other controllers. The regret itself is a function of the disturbances, and we propose to find a causal controller that minimizes the worst-case regret over all bounded energy disturbances. The resulting controller has the interpretation of guaranteeing the smallest regret compared to the best non-causal controller that can see the future. We derive explicit formulas for the optimal regret and for the regret-optimal controller for the state-space setting. These explicit solutions are obtained by showing that the regret-optimal control problem can be reduced to a Nehari ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#24179;&#22343;&#31934;&#24230;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;AUPRC&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#25910;&#25947;&#30340;SOAP&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2104.08736</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#31934;&#30830;&#29575;-&#21484;&#22238;&#26354;&#32447;&#19979;&#38754;&#31215;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence. (arXiv:2104.08736v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#24179;&#22343;&#31934;&#24230;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;AUPRC&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#25910;&#25947;&#30340;SOAP&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ROC&#19979;&#38754;&#31215;&#65288;AUROC&#65289;&#21644;&#31934;&#30830;&#29575;-&#21484;&#22238;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUPRC&#65289;&#26159;&#29992;&#20110;&#35780;&#20272;&#19981;&#24179;&#34913;&#38382;&#39064;&#20998;&#31867;&#24615;&#33021;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#19982;AUROC&#30456;&#27604;&#65292;AUPRC&#26159;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#26356;&#21512;&#36866;&#30340;&#25351;&#26631;&#12290;&#34429;&#28982;&#20851;&#20110;AUROC&#30340;&#38543;&#26426;&#20248;&#21270;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22522;&#20110;&#21407;&#21017;&#30340;AUPRC&#30340;&#38543;&#26426;&#20248;&#21270;&#21364;&#24456;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#21270;&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#30340;&#25216;&#26415;&#26041;&#27861;&#26469;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#30340;AUPRC&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#34920;&#31034;&#20026;&#20381;&#36182;&#32452;&#21512;&#20989;&#25968;&#20043;&#21644;&#65292;&#20854;&#20013;&#20869;&#37096;&#20989;&#25968;&#20381;&#36182;&#20110;&#22806;&#23618;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#38543;&#26426;&#31639;&#27861;SOAP&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#20855;&#26377;&#21487;&#35777;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#22312;&#22270;&#20687;&#21644;&#22270;&#24418;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of {\it dependent compositional functions} with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with {\it provable convergence guarantee under mild conditions} by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2012.14331</link><description>&lt;p&gt;
&#19968;&#31181;&#25972;&#21512;&#21644;&#20998;&#31867;&#27491;&#24577;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.14331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#27491;&#24577;&#27010;&#29575;&#20998;&#24067;&#22312;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#38656;&#35201;&#22312;&#29305;&#23450;&#21306;&#22495;&#20869;&#23545;&#36825;&#20123;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#65292;&#36825;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#36890;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12289;&#26631;&#20934;&#25968;&#20540;&#26041;&#27861;&#25110;&#36719;&#20214;&#26469;&#35745;&#31639;&#36825;&#20123;&#31215;&#20998;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25968;&#23398;&#32467;&#26524;&#21644;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#20197;&#19979;&#20869;&#23481;&#65306;&#65288;i&#65289;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#20219;&#24847;&#22495;&#20869;&#27861;&#21521;&#30340;&#27010;&#29575;&#65292;&#65288;ii&#65289;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#12289;&#32047;&#31215;&#20998;&#24067;&#21644;&#36870;&#32047;&#31215;&#20998;&#24067;&#65292;&#65288;iii&#65289;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#31867;&#35823;&#24046;&#12289;&#36125;&#21494;&#26031;&#26368;&#20248;&#36776;&#21035;&#25351;&#25968;&#20197;&#21450;&#20854;&#19982;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#30340;&#20851;&#31995;&#65292;&#65288;iv&#65289;&#27492;&#31867;&#38382;&#39064;&#30340;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#65288;v&#65289;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#29983;&#29289;&#21644;&#24515;&#29702;&#23398;&#26469;&#28436;&#31034;&#36825;&#20123;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#32534;&#30721;&#30340;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#24471;&#20986;&#20102;&#27867;&#21270;&#35823;&#24046;&#21463;&#36755;&#20837;&#29305;&#24449;&#21644;&#28508;&#31354;&#38388;&#34920;&#31034;&#20043;&#38388;&#20114;&#20449;&#24687;&#38480;&#21046;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2010.11642</link><description>&lt;p&gt;
&#20114;&#20449;&#24687;&#22312;&#21464;&#20998;&#20998;&#31867;&#22120;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Mutual Information in Variational Classifiers. (arXiv:2010.11642v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#32534;&#30721;&#30340;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#24471;&#20986;&#20102;&#27867;&#21270;&#35823;&#24046;&#21463;&#36755;&#20837;&#29305;&#24449;&#21644;&#28508;&#31354;&#38388;&#34920;&#31034;&#20043;&#38388;&#20114;&#20449;&#24687;&#38480;&#21046;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#29616;&#35937;&#65292;&#19982;&#29983;&#25104;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25968;&#25454;&#23454;&#20363;&#30340;&#27169;&#22411;&#26377;&#20851;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#35266;&#23519;&#32467;&#26524;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#21508;&#31181;&#65292;&#26377;&#26102;&#26159;&#21551;&#21457;&#24335;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#25511;&#21046;&#27492;&#34892;&#20026;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#21160;&#26426;&#26159;&#20197;&#24320;&#21457;&#19978;&#38480;&#26469;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;&#38543;&#26426;&#32534;&#30721;&#30340;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#32463;&#24120;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#36793;&#30028;&#65292;&#34920;&#26126;&#23384;&#22312;&#19968;&#31181;&#21306;&#22495;&#65292;&#20854;&#20013;&#27867;&#21270;&#35823;&#24046;&#30001;&#36755;&#20837;&#29305;&#24449;&#21644;&#19982;&#32534;&#30721;&#20998;&#24067;&#38543;&#26426;&#29983;&#25104;&#30340;&#28508;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#25152;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#36793;&#30028;&#25552;&#20379;&#20102;&#23545;&#25152;&#35859;&#30340;&#21464;&#24322;&#32423;&#21035;&#30340;&#27867;&#21270;&#30340;&#20449;&#24687;&#35770;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting data is a well-known phenomenon related with the generation of a model that mimics too closely (or exactly) a particular instance of data, and may therefore fail to predict future observations reliably. In practice, this behaviour is controlled by various--sometimes heuristics--regularization techniques, which are motivated by developing upper bounds to the generalization error. In this work, we study the generalization error of classifiers relying on stochastic encodings trained on the cross-entropy loss, which is often used in deep learning for classification problems. We derive bounds to the generalization error showing that there exists a regime where the generalization error is bounded by the mutual information between input features and the corresponding representations in the latent space, which are randomly generated according to the encoding distribution. Our bounds provide an information-theoretic understanding of generalization in the so-called class of variation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#8220;&#32447;&#25193;&#23637;(LE)&#8221;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39030;&#28857;-&#36229;&#36793;&#23545;&#20316;&#20026;&#8220;&#32447;&#33410;&#28857;&#8221;&#65292;&#22312;&#36229;&#22270;&#20013;&#24341;&#20986;&#21516;&#26500;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#36229;&#22270;&#25193;&#23637;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#20248;&#20110;SOTA&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2005.04843</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#32447;&#25193;&#23637;&#30340;&#21322;&#30417;&#30563;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Hypergraph Node Classification on Hypergraph Line Expansion. (arXiv:2005.04843v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.04843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#8220;&#32447;&#25193;&#23637;(LE)&#8221;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39030;&#28857;-&#36229;&#36793;&#23545;&#20316;&#20026;&#8220;&#32447;&#33410;&#28857;&#8221;&#65292;&#22312;&#36229;&#22270;&#20013;&#24341;&#20986;&#21516;&#26500;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#36229;&#22270;&#25193;&#23637;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#20248;&#20110;SOTA&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#36229;&#22270;&#25193;&#23637;&#20165;&#22312;&#39030;&#28857;&#32423;&#21035;&#25110;&#36229;&#36793;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#25454;&#20849;&#29616;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20449;&#24687;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24179;&#31561;&#23545;&#24453;&#39030;&#28857;&#21644;&#36229;&#36793;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\emph{&#32447;&#25193;&#23637;(LE)}&#30340;&#26032;&#22411;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39030;&#28857;-&#36229;&#36793;&#23545;&#20316;&#20026;&#8220;&#32447;&#33410;&#28857;&#8221;&#65292;&#22312;&#36229;&#22270;&#20013;&#21452;&#23556;&#24341;&#20986;&#19968;&#31181;&#21516;&#26500;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#36229;&#22270;&#20943;&#23569;&#20026;&#31616;&#21333;&#22270;&#65292;&#25552;&#20986;&#30340;\emph{&#32447;&#25193;&#23637;}&#20351;&#24471;&#24050;&#26377;&#30340;&#22270;&#23398;&#20064;&#31639;&#27861;&#36866;&#29992;&#20110;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#24050;&#34987;&#35777;&#26126;&#26159;&#21508;&#31181;&#36229;&#22270;&#25193;&#23637;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#36229;&#22270;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32447;&#25193;&#23637;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous hypergraph expansions are solely carried out on either vertex level or hyperedge level, thereby missing the symmetric nature of data co-occurrence, and resulting in information loss. To address the problem, this paper treats vertices and hyperedges equally and proposes a new hypergraph formulation named the \emph{line expansion (LE)} for hypergraphs learning. The new expansion bijectively induces a homogeneous structure from the hypergraph by treating vertex-hyperedge pairs as "line nodes". By reducing the hypergraph to a simple graph, the proposed \emph{line expansion} makes existing graph learning algorithms compatible with the higher-order structure and has been proven as a unifying framework for various hypergraph expansions. We evaluate the proposed line expansion on five hypergraph datasets, the results show that our method beats SOTA baselines by a significant margin.
&lt;/p&gt;</description></item></channel></rss>