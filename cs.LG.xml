<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ARB&#26159;&#19968;&#20010;&#26032;&#22411;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#65292;&#20026;&#20102;&#25552;&#39640;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13692</link><description>&lt;p&gt;
ARB&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#32423;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13692
&lt;/p&gt;
&lt;p&gt;
ARB&#26159;&#19968;&#20010;&#26032;&#22411;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#65292;&#20026;&#20102;&#25552;&#39640;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#23450;&#37327;&#25512;&#29702;&#21644;&#30693;&#35782;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#36824;&#27809;&#26377;&#36798;&#21040;&#19987;&#23478;&#27700;&#24179;&#65292;&#20294;&#35768;&#22810;&#36825;&#20123;&#22522;&#20934;&#38543;&#30528;LLMs&#33719;&#24471;&#36234;&#26469;&#36234;&#39640;&#30340;&#20998;&#25968;&#32780;&#22833;&#21435;&#20102;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ARB&#65292;&#19968;&#20010;&#30001;&#22810;&#20010;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#32452;&#25104;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;ARB&#25552;&#20379;&#27604;&#20197;&#21069;&#30340;&#22522;&#20934;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;ARB&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#39640;&#32423;&#31526;&#21495;&#25512;&#29702;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-4&#21644;Claude&#22312;ARB&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#24403;&#21069;&#27169;&#22411;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#12290;&#20026;&#20102;&#25913;&#36827;&#33258;&#21160;&#21644;&#36741;&#21161;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20801;&#35768;GPT-4&#23545;&#20854;&#33258;&#36523;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#20855;&#26377;&#20462;&#21098;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#27969;&#34892;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#30028;&#38480;&#21644;&#27867;&#21270;&#30028;&#38480;&#65292;&#20026;&#22788;&#29702;&#37325;&#23614;&#34892;&#20026;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.13680</link><description>&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#20462;&#21098;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High Probability Analysis for Non-Convex Stochastic Optimization with Clipping. (arXiv:2307.13680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#20855;&#26377;&#20462;&#21098;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#27969;&#34892;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#30028;&#38480;&#21644;&#27867;&#21270;&#30028;&#38480;&#65292;&#20026;&#22788;&#29702;&#37325;&#23614;&#34892;&#20026;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20462;&#21098;&#26159;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#22788;&#29702;&#38543;&#26426;&#20248;&#21270;&#20013;&#20986;&#29616;&#30340;&#37325;&#23614;&#34892;&#20026;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#26799;&#24230;&#20462;&#21098;&#24456;&#37325;&#35201;&#65292;&#20294;&#20854;&#29702;&#35770;&#20445;&#35777;&#24456;&#23569;&#12290;&#22823;&#22810;&#25968;&#29702;&#35770;&#20445;&#35777;&#21482;&#25552;&#20379;&#26399;&#26395;&#20540;&#20998;&#26512;&#65292;&#24182;&#19988;&#20165;&#20851;&#27880;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;&#65292;&#24182;&#21516;&#26102;&#25512;&#23548;&#20102;&#24102;&#26377;&#26799;&#24230;&#20462;&#21098;&#30340;&#27969;&#34892;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#30028;&#38480;&#21644;&#27867;&#21270;&#30028;&#38480;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21450;&#20854;&#21160;&#37327;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#21464;&#20307;&#12290;&#22312;&#26799;&#24230;&#20462;&#21098;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#23614;&#30340;&#20551;&#35774;&#65292;&#21363;&#26799;&#24230;&#21482;&#26377;&#23545;&#20110;&#26576;&#20123;$\alpha \in (1, 2]$&#26377;&#30028;&#30340;$\alpha$-&#38454;&#30697;&#65292;&#36825;&#27604;&#26631;&#20934;&#30340;&#26377;&#30028;&#20108;&#38454;&#30697;&#20551;&#35774;&#35201;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\alpha$-th moments for some $\alpha \in (1, 2]$, which is much weaker than the standard bounded second-moment ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RED CoMETS&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23427;&#22312;&#22810;&#21464;&#37327;&#35774;&#32622;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;'HandMovementDirection'&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#25253;&#21578;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13679</link><description>&lt;p&gt;
RED CoMETS: &#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
RED CoMETS: An ensemble classifier for symbolically represented multivariate time series. (arXiv:2307.13679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RED CoMETS&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23427;&#22312;&#22810;&#21464;&#37327;&#35774;&#32622;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;'HandMovementDirection'&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#25253;&#21578;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#12289;&#24037;&#31243;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#22797;&#26434;&#24615;&#26469;&#33258;&#20110;&#20854;&#39640;&#32500;&#24230;&#12289;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#38271;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RED CoMETS&#65288;Random Enhanced Co-eye for Multivariate Time Series&#65289;&#30340;&#26032;&#22411;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;RED CoMETS&#22522;&#20110;Co-eye&#30340;&#25104;&#21151;&#65292;&#24182;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#22788;&#29702;&#22810;&#21464;&#37327;&#25968;&#25454;&#12290;&#20351;&#29992;UCR&#26723;&#26696;&#20013;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;RED CoMETS&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#22810;&#21464;&#37327;&#35774;&#32622;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;'HandMovementDirection'&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#25253;&#21578;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#22320;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;Co-eye&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method signific
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13658</link><description>&lt;p&gt;
&#20851;&#20110;AI&#38382;&#36131;&#25919;&#31574;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#20316;&#20986;&#30340;&#22238;&#24212;&#12290;&#22312;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#20851;&#38190;&#21477;&#23376;&#26411;&#23614;&#65292;&#25552;&#20379;&#20102;&#35201;&#27714;&#35780;&#35770;&#30340;&#38382;&#39064;&#32534;&#21495;&#30340;&#19978;&#26631;&#12290;&#35813;&#30333;&#30382;&#20070;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.13642</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#33258;&#20027;&#25511;&#21046;&#22120;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#37117;&#21487;&#33021;&#19981;&#23433;&#20840;&#12290;&#33021;&#22815;&#23450;&#37327;&#22320;&#30830;&#23450;&#20309;&#26102;&#20250;&#21457;&#29983;&#36825;&#20123;&#19981;&#23433;&#20840;&#24773;&#20917;&#23545;&#20110;&#21450;&#26102;&#24341;&#20837;&#20154;&#31867;&#30417;&#30563;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#36135;&#36816;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20195;&#29702;&#30340;&#24773;&#20917;&#30340;&#30495;&#27491;&#20851;&#38190;&#24615;&#21487;&#20197;&#34987;&#31283;&#20581;&#22320;&#23450;&#20041;&#20026;&#22312;&#19968;&#20123;&#38543;&#26426;&#21160;&#20316;&#19979;&#22870;&#21169;&#30340;&#24179;&#22343;&#20943;&#23569;&#12290;&#21487;&#20197;&#23558;&#23454;&#26102;&#21487;&#35745;&#31639;&#30340;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#65288;&#21363;&#65292;&#26080;&#38656;&#23454;&#38469;&#27169;&#25311;&#38543;&#26426;&#21160;&#20316;&#30340;&#24433;&#21709;&#65289;&#19982;&#30495;&#27491;&#30340;&#20851;&#38190;&#24615;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20195;&#29702;&#25351;&#26631;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#65292;&#23558;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#30452;&#25509;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;Atari&#29615;&#22659;&#20013;&#36890;&#36807;APE-X&#21644;A3C&#30340;&#23398;&#20064;&#31574;&#30053;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#30340;&#20943;&#23567;&#12290;&#23558;&#23433;&#20840;&#36793;&#30028;&#25972;&#21512;&#21040;&#30417;&#25511;&#31243;&#24207;&#20013;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#25918;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#24037;&#21378;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#36739;&#22823;&#24037;&#21378;&#26102;&#20986;&#29616;&#30340;&#24490;&#29615;&#27714;&#35299;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13621</link><description>&lt;p&gt;
&#32553;&#25918;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#24037;&#21378;&#27169;&#25311;&#65306;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#20197;&#35825;&#23548;&#31283;&#23450;&#22266;&#23450;&#28857;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points. (arXiv:2307.13621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#25918;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#24037;&#21378;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#36739;&#22823;&#24037;&#21378;&#26102;&#20986;&#29616;&#30340;&#24490;&#29615;&#27714;&#35299;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#21270;&#30340;&#21270;&#24037;&#21378;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#30452;&#25509;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25311;&#21512;&#21040;&#24037;&#21378;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#65306;&#23558;&#24037;&#21378;&#20869;&#30340;&#27599;&#20010;&#21333;&#20803;&#34920;&#31034;&#20026;&#19968;&#20010;ML&#27169;&#22411;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#25968;&#25454;&#21518;&#65292;&#23558;&#27169;&#22411;&#36830;&#25509;&#25104;&#31867;&#20284;&#27969;&#31243;&#22270;&#30340;&#26377;&#21521;&#22270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#36739;&#23567;&#30340;&#24037;&#21378;&#65292;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#23545;&#20110;&#36739;&#22823;&#30340;&#24037;&#21378;&#65292;&#30001;&#20110;&#27969;&#31243;&#22270;&#20013;&#23384;&#22312;&#22823;&#22411;&#21644;&#23884;&#22871;&#24490;&#29615;&#25152;&#23548;&#33268;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#20250;&#23548;&#33268;&#24490;&#29615;&#27714;&#35299;&#22120;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#36825;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#29305;&#27530;&#20851;&#27880;&#28857;&#65292;&#32780;&#26159;&#19968;&#20010;&#26356;&#26222;&#36941;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22312;&#24212;&#29992;&#20110;&#36739;&#22823;&#30340;&#24037;&#21378;&#26102;&#20250;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;ML&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20351;&#24471;&#24120;&#35268;&#26041;&#27861;&#19979;&#30340;&#24490;&#29615;&#27714;&#35299;&#21464;&#24471;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#21464;&#37327;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#65292;&#20943;&#23569;&#39118;&#38505;&#24314;&#27169;&#20013;&#30340;&#25311;&#35758;&#24046;&#21035;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20445;&#38505;&#23450;&#20215;&#21644;&#39118;&#38505;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.13616</link><description>&lt;p&gt;
AI&#19982;&#20445;&#38505;&#20262;&#29702;&#65306;&#22312;&#39118;&#38505;&#24314;&#27169;&#20013;&#20943;&#23569;&#25311;&#35758;&#24046;&#21035;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling. (arXiv:2307.13616v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#21464;&#37327;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#65292;&#20943;&#23569;&#39118;&#38505;&#24314;&#27169;&#20013;&#30340;&#25311;&#35758;&#24046;&#21035;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20445;&#38505;&#23450;&#20215;&#21644;&#39118;&#38505;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#24191;&#22823;&#20844;&#20247;&#30340;&#20851;&#27880;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#26377;&#35768;&#22810;&#26032;&#38395;&#25991;&#31456;&#36136;&#30097;&#20854;&#23458;&#35266;&#24615;&#65306;&#31181;&#26063;&#20027;&#20041;&#65292;&#24615;&#21035;&#27495;&#35270;&#31561;&#12290;&#21463;&#30417;&#31649;&#26426;&#26500;&#23545;&#20445;&#38505;&#20013;&#25968;&#25454;&#36947;&#24503;&#20351;&#29992;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#38271;&#65292;&#20445;&#38505;&#31934;&#31639;&#24072;&#31038;&#21306;&#24517;&#39035;&#37325;&#26032;&#24605;&#32771;&#23450;&#20215;&#21644;&#39118;&#38505;&#36873;&#25321;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20445;&#38505;&#12290;&#20844;&#24179;&#26159;&#19968;&#20010;&#21746;&#23398;&#27010;&#24565;&#65292;&#22312;&#27599;&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#37117;&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#23450;&#20041;&#65292;&#36825;&#20123;&#23450;&#20041;&#30456;&#20114;&#24433;&#21709;&#65292;&#30446;&#21069;&#23578;&#26410;&#36798;&#25104;&#19968;&#33268;&#24847;&#35265;&#12290;&#22312;&#27431;&#27954;&#65292;&#22522;&#26412;&#26435;&#21033;&#23466;&#31456;&#35268;&#23450;&#20102;&#26377;&#20851;&#27495;&#35270;&#21644;&#31639;&#27861;&#20013;&#20351;&#29992;&#25935;&#24863;&#20010;&#20154;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22914;&#26524;&#31616;&#21333;&#22320;&#21024;&#38500;&#21463;&#20445;&#25252;&#21464;&#37327;&#21487;&#20197;&#38450;&#27490;&#20219;&#20309;&#25152;&#35859;&#30340;&#8220;&#30452;&#25509;&#8221;&#27495;&#35270;&#65292;&#37027;&#20040;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#21464;&#37327;&#20043;&#38388;&#28508;&#22312;&#30340;&#30456;&#20114;&#20316;&#29992;&#8220;&#38388;&#25509;&#8221;&#27495;&#35270;&#20010;&#20154;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65288;&#22240;&#27492;&#26356;&#22909;&#22320;&#37327;&#21270;&#39118;&#38505;&#65292;&#36827;&#34892;&#32454;&#20998;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Machine Learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots Driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. Equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. In Europe, the Charter of Fundamental Rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. If the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;(DIQNN)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#24341;&#20837;&#36793;&#30028;&#26469;&#21051;&#30011;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23558;&#36793;&#30028;&#25972;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21152;&#36895;&#20102;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.13609</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dendritic Integration Based Quadratic Neural Networks Outperform Traditional Aritificial Ones. (arXiv:2307.13609v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13609
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;(DIQNN)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#24341;&#20837;&#36793;&#30028;&#26469;&#21051;&#30011;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23558;&#36793;&#30028;&#25972;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21152;&#36895;&#20102;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#29305;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20197;&#22686;&#24378;&#35745;&#31639;&#33021;&#21147;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26368;&#36817;&#21457;&#29616;&#30340;&#26641;&#31361;&#36981;&#24490;&#20108;&#27425;&#32508;&#21512;&#35268;&#21017;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#26641;&#31361;&#32508;&#21512;&#30340;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;(DIQNN)&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#38477;&#20302;DIQNN&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20302;&#31209;DIQNN&#65292;&#21457;&#29616;&#20854;&#21487;&#20197;&#20445;&#25345;&#21407;&#22987;DIQNN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#30028;&#26469;&#21051;&#30011;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#36825;&#20010;&#36793;&#30028;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#21333;&#35843;&#22686;&#21152;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27867;&#21270;&#35823;&#24046;&#19982;&#36793;&#30028;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20010;&#36793;&#30028;&#25972;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#21518;&#65292;&#27979;&#35797;&#20934;&#30830;&#29575;&#30340;&#25913;&#21464;&#30830;&#23454;&#21152;&#36895;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities poses a formidable challenge in the field of machine learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rules for synaptic inputs, we propose a novel ANN model, Dendritic Integration-Based Quadratic Neural Network (DIQNN). This model shows superior performance over traditional ANNs in a variety of classification tasks. To reduce the computational cost of DIQNN, we introduce the Low-Rank DIQNN, while we find it can retain the performance of the original DIQNN. We further propose a margin to characterize the generalization error and theoretically prove this margin will increase monotonically during training. And we show the consistency between generalization and our margin using numerical experiments. Finally, by integrating this margin into the loss function, the change of test accuracy is indeed accelerated. Our work cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#25239;&#20307;-&#25239;&#21407;&#32467;&#21512;&#20301;&#28857;&#30340;&#26368;&#20339;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#20960;&#20309;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#34920;&#38754;&#30340;&#27169;&#22411;&#26356;&#39640;&#25928;&#65292;O-GEP&#23454;&#39564;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.13608</link><description>&lt;p&gt;
&#20960;&#20309;&#34920;&#20301;&#21644;&#37197;&#20301;&#20301;&#28857;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geometric Epitope and Paratope Prediction. (arXiv:2307.13608v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#25239;&#20307;-&#25239;&#21407;&#32467;&#21512;&#20301;&#28857;&#30340;&#26368;&#20339;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#20960;&#20309;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#34920;&#38754;&#30340;&#27169;&#22411;&#26356;&#39640;&#25928;&#65292;O-GEP&#23454;&#39564;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;-&#25239;&#21407;&#30456;&#20114;&#20316;&#29992;&#22312;&#35782;&#21035;&#21644;&#20013;&#21644;&#26377;&#23475;&#22806;&#26469;&#20998;&#23376;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#27979;&#36825;&#20004;&#31181;&#20998;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#26368;&#20339;&#34920;&#31034;&#65292;&#24182;&#24378;&#35843;&#20102;&#20960;&#20309;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20869;&#37096;&#65288;I-GEP&#65289;&#21644;&#22806;&#37096;&#65288;O-GEP&#65289;&#32467;&#26500;&#30340;&#19981;&#21516;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#19977;&#32500;&#22352;&#26631;&#21644;&#35889;&#20960;&#20309;&#25551;&#36848;&#31526;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#20805;&#20998;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#38754;&#30340;&#27169;&#22411;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#25105;&#20204;&#30340;O-GEP&#23454;&#39564;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody-antigen interactions play a crucial role in identifying and neutralizing harmful foreign molecules. In this paper, we investigate the optimal representation for predicting the binding sites in the two molecules and emphasize the importance of geometric information. Specifically, we compare different geometric deep learning methods applied to proteins' inner (I-GEP) and outer (O-GEP) structures. We incorporate 3D coordinates and spectral geometric descriptors as input features to fully leverage the geometric information. Our research suggests that surface-based models are more efficient than other methods, and our O-GEP experiments have achieved state-of-the-art results with significant performance improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#25193;&#23637;&#21040;&#24037;&#19994;&#30456;&#20851;&#32593;&#26684;&#23610;&#23544;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22810;GPU&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#30340;&#22823;&#35268;&#27169;&#32593;&#26684;&#19978;&#36827;&#34892;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#25968;&#20540;&#27969;&#21160;&#27169;&#25311;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.13592</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;GPU&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#22823;&#35268;&#27169;CFD&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes. (arXiv:2307.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#25193;&#23637;&#21040;&#24037;&#19994;&#30456;&#20851;&#32593;&#26684;&#23610;&#23544;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22810;GPU&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#30340;&#22823;&#35268;&#27169;&#32593;&#26684;&#19978;&#36827;&#34892;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#25968;&#20540;&#27969;&#21160;&#27169;&#25311;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#22312;&#35768;&#22810;&#35774;&#35745;&#24037;&#20855;&#38142;&#20013;&#26159;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#30340;&#27169;&#25311;&#65292;&#22914;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65292;&#26159;&#26102;&#32791;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#65292;&#22240;&#27492;&#24120;&#24120;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#26469;&#21152;&#36895;&#35299;&#20915;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#22312;&#39044;&#27979;&#36817;&#20284;&#35299;&#26102;&#36895;&#24230;&#36739;&#24555;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#31934;&#24230;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#24320;&#21457;&#19968;&#20010;&#39044;&#27979;&#22120;-&#26657;&#27491;&#22120;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#27969;&#22330;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#36827;&#34892;&#26657;&#27491;&#12290;&#26412;&#25991;&#23558;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;&#25968;&#20540;&#27969;&#21160;&#27169;&#25311;&#20013;&#30340;&#24037;&#19994;&#30456;&#20851;&#32593;&#26684;&#23610;&#23544;&#12290;&#35813;&#26041;&#27861;&#23558;&#27969;&#22330;&#21010;&#20998;&#24182;&#20998;&#37197;&#21040;&#22810;&#20010;GPU&#19978;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#20998;&#21306;&#20043;&#38388;&#30340;&#36793;&#30028;&#20132;&#25442;&#12290;&#25152;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20316;&#29992;&#20110;&#25968;&#20540;&#32593;&#26684;&#65292;&#33021;&#22815;&#20445;&#23384;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20197;&#21450;&#25152;&#26377;&#20854;&#20182;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mesh-based numerical solvers are an important part in many design tool chains. However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process. Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy. Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it. This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all oth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13586</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling the Sample Complexity of Online Reinforcement Learning. (arXiv:2307.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#23567;&#30340;&#21518;&#24724;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#30340;&#26368;&#20248;&#24615;&#20165;&#22312;&#8220;&#22823;&#26679;&#26412;&#8221;&#24773;&#20917;&#19979;&#24471;&#21040;&#20445;&#35777;&#65292;&#20026;&#20102;&#20351;&#20854;&#31639;&#27861;&#36816;&#34892;&#26368;&#20339;&#65292;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#39044;&#29123;&#25104;&#26412;&#12290;&#22914;&#20309;&#22312;&#19981;&#20135;&#29983;&#20219;&#20309;&#39044;&#29123;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#21333;&#35843;&#20540;&#20256;&#25773;(MVP)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#30001;\cite{zhang2020reinforcement}&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20351;&#24471;&#21518;&#24724;&#30340;&#37327;&#32423;&#20026;(&#27169;&#38500;&#23545;&#25968;&#22240;&#23376;)\begin{equation *} \min\biggr\{ \sqrt{SAH^3K}&#65292;\&#65292;HK \biggr\}&#65292;\end{equation *}&#20854;&#20013;$S$&#26159;&#29366;&#24577;&#25968;&#65292;$A$&#26159;&#21160;&#20316;&#25968;&#65292;$H$&#26159;&#35268;&#21010;&#26102;&#22495;&#65292;$K$&#26159;&#24635;&#30340;&#22238;&#21512;&#25968;&#12290;&#36825;&#20010;&#21518;&#24724;&#30340;&#37327;&#32423;&#19982;&#26497;&#23567;&#21270;&#21518;&#24724;&#37327;&#32423;&#26159;&#30456;&#21305;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}  \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#35774;&#35745;&#33539;&#24335;&#22312;&#32784;&#28779;&#39640;&#29109;&#21512;&#37329;&#35774;&#35745;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23450;&#37327;&#27604;&#36739;&#20102;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#19982;&#20854;&#20182;&#27491;&#21521;&#26041;&#26696;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.13581</link><description>&lt;p&gt;
&#27604;&#36739;&#27491;&#21521;&#21644;&#36870;&#21521;&#35774;&#35745;&#33539;&#24335;&#65306;&#32784;&#28779;&#39640;&#29109;&#21512;&#37329;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys. (arXiv:2307.13581v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#35774;&#35745;&#33539;&#24335;&#22312;&#32784;&#28779;&#39640;&#29109;&#21512;&#37329;&#35774;&#35745;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23450;&#37327;&#27604;&#36739;&#20102;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#19982;&#20854;&#20182;&#27491;&#21521;&#26041;&#26696;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#35774;&#35745;&#20808;&#36827;&#26448;&#26009;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#8220;&#27491;&#21521;&#8221;&#26448;&#26009;&#35774;&#35745;&#33539;&#24335;&#28041;&#21450;&#35780;&#20272;&#22810;&#20010;&#20505;&#36873;&#26448;&#26009;&#20197;&#30830;&#23450;&#19982;&#30446;&#26631;&#29305;&#24615;&#26368;&#21305;&#37197;&#30340;&#20505;&#36873;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20808;&#36827;&#26448;&#26009;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#8220;&#36870;&#21521;&#8221;&#35774;&#35745;&#33539;&#24335;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#36890;&#36807;&#25552;&#20379;&#30446;&#26631;&#29305;&#24615;&#65292;&#27169;&#22411;&#33021;&#22815;&#25214;&#21040;&#26368;&#20248;&#30340;&#20505;&#36873;&#26448;&#26009;&#12290;&#30001;&#20110;&#36825;&#20010;&#27010;&#24565;&#30456;&#23545;&#36739;&#26032;&#65292;&#26377;&#24517;&#35201;&#31995;&#32479;&#22320;&#35780;&#20272;&#36825;&#20004;&#31181;&#33539;&#24335;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#30452;&#25509;&#12289;&#23450;&#37327;&#22320;&#27604;&#36739;&#27491;&#21521;&#21644;&#36870;&#21521;&#35774;&#35745;&#24314;&#27169;&#33539;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#20004;&#20010;&#32784;&#28779;&#39640;&#29109;&#21512;&#37329;&#35774;&#35745;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23558;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#19982;&#20854;&#20182;&#27491;&#21521;&#26041;&#26696;&#65288;&#22914;&#23616;&#37096;&#27491;&#21521;&#25628;&#32034;&#12289;&#39640;&#36890;&#37327;&#31579;&#36873;&#31561;&#65289;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid design of advanced materials is a topic of great scientific interest. The conventional, ``forward'' paradigm of materials design involves evaluating multiple candidates to determine the best candidate that matches the target properties. However, recent advances in the field of deep learning have given rise to the possibility of an ``inverse'' design paradigm for advanced materials, wherein a model provided with the target properties is able to find the best candidate. Being a relatively new concept, there remains a need to systematically evaluate how these two paradigms perform in practical applications. Therefore, the objective of this study is to directly, quantitatively compare the forward and inverse design modeling paradigms. We do so by considering two case studies of refractory high-entropy alloy design with different objectives and constraints and comparing the inverse design method to other forward schemes like localized forward search, high throughput screening, and
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#23384;&#20998;&#26512;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#36830;&#25509;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#26080;&#38656;&#25968;&#20540;&#31215;&#20998;&#30340;&#36890;&#29992;&#25311;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13579</link><description>&lt;p&gt;
&#22312;&#36890;&#29992;&#25311;&#21512;&#22120;&#26102;&#20195;&#37325;&#26032;&#35299;&#35835;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reinterpreting survival analysis in the universal approximator age. (arXiv:2307.13579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#23384;&#20998;&#26512;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#36830;&#25509;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#26080;&#38656;&#25968;&#20540;&#31215;&#20998;&#30340;&#36890;&#29992;&#25311;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#32479;&#35745;&#23398;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#32463;&#20856;&#32479;&#35745;&#39046;&#22495;&#24050;&#32463;&#25509;&#21463;&#20102;&#28145;&#24230;&#23398;&#20064;&#65292;&#20294;&#26159;&#29983;&#23384;&#20998;&#26512;&#30452;&#21040;&#26368;&#36817;&#25165;&#24341;&#36215;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#20123;&#27880;&#24847;&#12290;&#36825;&#19968;&#26368;&#36817;&#30340;&#21457;&#23637;&#21487;&#33021;&#37096;&#20998;&#21463;&#21040;COVID-19&#22823;&#27969;&#34892;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#29983;&#23384;&#20998;&#26512;&#28508;&#21147;&#25152;&#38656;&#30340;&#24037;&#20855;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#23384;&#20998;&#26512;&#19982;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25216;&#26415;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#38656;&#25968;&#20540;&#31215;&#20998;&#20135;&#29983;&#29983;&#23384;&#26354;&#32447;&#30340;&#36890;&#29992;&#25311;&#21512;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#37096;&#20998;&#20256;&#36755;$\mathrm{L}^{p}$&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#36890;&#29992;&#20449;&#21495;&#65292;&#24182;&#19988;&#21033;&#29992;&#37096;&#20998;&#20256;&#36755;&#36317;&#31163;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#20171;&#32461;&#20102;&#35813;&#36317;&#31163;&#30340;&#20999;&#29255;&#21464;&#20307;&#65292;&#21487;&#20197;&#24555;&#36895;&#27604;&#36739;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13571</link><description>&lt;p&gt;
PT$\mathrm{L}^{p}$&#65306;&#37096;&#20998;&#20256;&#36755;$\mathrm{L}^{p}$&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances. (arXiv:2307.13571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37096;&#20998;&#20256;&#36755;$\mathrm{L}^{p}$&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#36890;&#29992;&#20449;&#21495;&#65292;&#24182;&#19988;&#21033;&#29992;&#37096;&#20998;&#20256;&#36755;&#36317;&#31163;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#20171;&#32461;&#20102;&#35813;&#36317;&#31163;&#30340;&#20999;&#29255;&#21464;&#20307;&#65292;&#21487;&#20197;&#24555;&#36895;&#27604;&#36739;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#21450;&#20854;&#30456;&#20851;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35745;&#31639;&#27010;&#29575;&#25110;&#27491;&#27979;&#24230;&#20043;&#38388;&#26377;&#24847;&#20041;&#30340;&#36317;&#31163;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#21253;&#25324;&#26368;&#20248;&#37096;&#20998;&#20256;&#36755;&#12290;&#36825;&#31181;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#20256;&#36755;&#22522;&#20110;&#36317;&#31163;&#30340;&#23450;&#20041;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#65292;&#20351;&#24471;&#21487;&#20197;&#27604;&#36739;&#24102;&#31526;&#21495;&#30340;&#27979;&#24230;&#21644;&#26356;&#19968;&#33324;&#30340;&#22810;&#36890;&#36947;&#20449;&#21495;&#12290;&#20256;&#36755;$\mathrm{L}^{p}$&#36317;&#31163;&#26159;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#21040;&#24102;&#31526;&#21495;&#21644;&#21487;&#33021;&#26159;&#22810;&#36890;&#36947;&#20449;&#21495;&#30340;&#26174;&#33879;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#20256;&#36755;$\mathrm{L}^{p}$&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#36890;&#29992;&#20449;&#21495;&#30340;&#26032;&#22411;&#24230;&#37327;&#65292;&#20174;&#37096;&#20998;&#20256;&#36755;&#36317;&#31163;&#30340;&#40065;&#26834;&#24615;&#20013;&#33719;&#30410;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#32972;&#26223;&#65292;&#20363;&#22914;&#26368;&#20248;&#26041;&#26696;&#30340;&#23384;&#22312;&#21644;&#36317;&#31163;&#22312;&#21508;&#31181;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36825;&#20123;&#36317;&#31163;&#30340;&#20999;&#29255;&#21464;&#20307;&#65292;&#21487;&#20197;&#24555;&#36895;&#27604;&#36739;&#36890;&#29992;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures. This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals. Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals. In this paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances. We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits. Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals. Finally, we demonstrate the applicatio
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31192;&#19988;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#31169;&#26377;&#38142;&#25509;&#26469;&#26292;&#38706;GNNs&#30340;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#25345;&#27169;&#22411;&#25928;&#33021;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#24212;&#29992;&#20110;&#20943;&#36731;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#25928;&#33021;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.13548</link><description>&lt;p&gt;
&#33410;&#28857;&#27880;&#20837;&#38142;&#25509;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Node Injection Link Stealing Attack. (arXiv:2307.13548v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31192;&#19988;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#31169;&#26377;&#38142;&#25509;&#26469;&#26292;&#38706;GNNs&#30340;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#25345;&#27169;&#22411;&#25928;&#33021;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#24212;&#29992;&#20110;&#20943;&#36731;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#25928;&#33021;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31192;&#19988;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#31169;&#26377;&#38142;&#25509;&#26469;&#26292;&#38706;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#38544;&#31169;&#28431;&#27934;&#12290;&#25105;&#20204;&#20851;&#27880;&#26032;&#33410;&#28857;&#21152;&#20837;&#22270;&#24182;&#20351;&#29992;API&#26597;&#35810;&#39044;&#27979;&#30340;&#24402;&#32435;&#35774;&#32622;&#65292;&#30740;&#31350;&#31169;&#26377;&#36793;&#32536;&#20449;&#24687;&#30340;&#28508;&#22312;&#27844;&#38706;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#27169;&#22411;&#25928;&#33021;&#30340;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#25512;&#26029;&#38142;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#24212;&#29992;&#20110;&#20943;&#36731;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#19982;&#27169;&#22411;&#25928;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;GNNs&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#28431;&#27934;&#65292;&#24378;&#35843;&#20102;&#20026;&#20854;&#24212;&#29992;&#24320;&#21457;&#24378;&#22823;&#30340;&#20445;&#25252;&#38544;&#31169;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#37329;&#34701;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;"&#36801;&#31227;&#39118;&#38505;"&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36801;&#31227;&#39118;&#38505;&#19982;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25972;&#20307;&#24615;&#33021;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#36801;&#31227;&#39118;&#38505;&#35782;&#21035;&#36866;&#24403;&#30340;&#28304;&#20219;&#21153;&#26469;&#25552;&#39640;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#36824;&#20026;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13546</link><description>&lt;p&gt;
&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Portfolio Optimization. (arXiv:2307.13546v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#37329;&#34701;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;"&#36801;&#31227;&#39118;&#38505;"&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36801;&#31227;&#39118;&#38505;&#19982;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25972;&#20307;&#24615;&#33021;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#36801;&#31227;&#39118;&#38505;&#35782;&#21035;&#36866;&#24403;&#30340;&#28304;&#20219;&#21153;&#26469;&#25552;&#39640;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#36824;&#20026;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#37329;&#34701;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#8220;&#36801;&#31227;&#39118;&#38505;&#8221;&#65292;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#20174;&#36328;&#27954;&#21035;&#36801;&#31227;&#12289;&#36328;&#34892;&#19994;&#36801;&#31227;&#21644;&#36328;&#39057;&#29575;&#36801;&#31227;&#19977;&#20010;&#31867;&#21035;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65306;1. &#24314;&#31435;&#20102;&#36801;&#31227;&#39118;&#38505;&#19982;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25972;&#20307;&#24615;&#33021;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#24378;&#35843;&#20102;&#36801;&#31227;&#39118;&#38505;&#20316;&#20026;&#21487;&#34892;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#65307;2. &#21457;&#29616;&#36801;&#31227;&#39118;&#38505;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#35782;&#21035;&#36866;&#24403;&#30340;&#28304;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65307;3. &#27492;&#22806;&#65292;&#25968;&#20540;&#23454;&#39564;&#20026;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called "transfer risk", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of "transferability"; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23450;&#21521;&#26102;&#24577;&#32593;&#32476;&#20013;&#25512;&#26029;&#33410;&#28857;&#30340;&#21160;&#24577;&#25490;&#24207;&#65292;&#36890;&#36807;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23454;&#29616;&#65292;&#20165;&#38656;&#35843;&#25972;&#19968;&#20010;&#21442;&#25968;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#21160;&#24577;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.13544</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#39640;&#25928;&#21160;&#24577;&#25490;&#24207;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A model for efficient dynamical ranking in networks. (arXiv:2307.13544v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23450;&#21521;&#26102;&#24577;&#32593;&#32476;&#20013;&#25512;&#26029;&#33410;&#28857;&#30340;&#21160;&#24577;&#25490;&#24207;&#65292;&#36890;&#36807;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23454;&#29616;&#65292;&#20165;&#38656;&#35843;&#25972;&#19968;&#20010;&#21442;&#25968;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#21160;&#24577;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23450;&#21521;&#26102;&#24577;&#32593;&#32476;&#20013;&#30340;&#21160;&#24577;&#25490;&#24207; - &#27599;&#20010;&#23450;&#21521;&#19988;&#24102;&#26102;&#38388;&#25139;&#30340;&#36793;&#21453;&#26144;&#20102;&#19968;&#23545;&#20132;&#20114;&#30340;&#32467;&#26524;&#21644;&#26102;&#38388;&#12290;&#27599;&#20010;&#33410;&#28857;&#30340;&#25512;&#26029;&#25490;&#24207;&#26159;&#23454;&#20540;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#65292;&#27599;&#27425;&#26032;&#30340;&#36793;&#32536;&#37117;&#20250;&#25552;&#39640;&#25110;&#38477;&#20302;&#33410;&#28857;&#30340;&#20272;&#35745;&#24378;&#24230;&#25110;&#22768;&#26395;&#65292;&#36825;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#65292;&#21253;&#25324;&#28216;&#25103;&#24207;&#21015;&#65292;&#38182;&#26631;&#36187;&#25110;&#21160;&#29289;&#31561;&#32423;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#19968;&#32452;&#32447;&#24615;&#26041;&#31243;&#26469;&#24037;&#20316;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35843;&#25972;&#19968;&#20010;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#23545;&#24212;&#30340;&#31639;&#27861;&#26159;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#26041;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#39044;&#27979;&#20132;&#20114;&#65288;&#36793;&#32536;&#23384;&#22312;&#65289;&#21450;&#20854;&#32467;&#26524;&#65288;&#36793;&#32536;&#26041;&#21521;&#65289;&#30340;&#33021;&#21147;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#21160;&#24577;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dyna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26631;&#31614;&#25200;&#21160;&#30340;&#27169;&#22411;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#26657;&#20934;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#32479;&#19968;&#19981;&#21516;&#24418;&#24335;&#30340;&#26631;&#31614;&#25200;&#21160;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#39044;&#27979;&#29109;&#26469;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#32416;&#27491;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13539</link><description>&lt;p&gt;
&#22312;&#23494;&#38598;&#20998;&#31867;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#26631;&#31614;&#25200;&#21160;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26631;&#31614;&#25200;&#21160;&#30340;&#27169;&#22411;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#26657;&#20934;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#32479;&#19968;&#19981;&#21516;&#24418;&#24335;&#30340;&#26631;&#31614;&#25200;&#21160;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#39044;&#27979;&#29109;&#26469;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#32416;&#27491;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23433;&#20840;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#65292;&#20135;&#29983;&#21487;&#20449;&#36182;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20854;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#30456;&#20851;&#65292;&#21487;&#20197;&#20195;&#34920;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#20379;&#21518;&#32493;&#20915;&#31574;&#20351;&#29992;&#12290;&#29616;&#26377;&#30340;&#23494;&#38598;&#20108;&#20998;&#31867;&#27169;&#22411;&#23481;&#26131;&#36807;&#20110;&#33258;&#20449;&#12290;&#20026;&#20102;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38543;&#26426;&#26631;&#31614;&#25200;&#21160;&#65288;ASLP&#65289;&#65292;&#23427;&#20026;&#27599;&#20010;&#35757;&#32451;&#22270;&#20687;&#23398;&#20064;&#19968;&#20010;&#29420;&#29305;&#30340;&#26631;&#31614;&#25200;&#21160;&#32423;&#21035;&#12290;ASLP&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#26657;&#20934;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;&#65288;SC-BCE&#65289;&#25439;&#22833;&#65292;&#23558;&#21253;&#25324;&#38543;&#26426;&#26041;&#27861;&#65288;&#22914;&#25200;&#21160;&#26631;&#31614;&#65289;&#21644;&#26631;&#31614;&#24179;&#28369;&#22312;&#20869;&#30340;&#26631;&#31614;&#25200;&#21160;&#36807;&#31243;&#32479;&#19968;&#36215;&#26469;&#65292;&#20197;&#22312;&#20445;&#25345;&#20998;&#31867;&#29575;&#30340;&#21516;&#26102;&#32416;&#27491;&#26657;&#20934;&#38382;&#39064;&#12290;ASLP&#37319;&#29992;&#32463;&#20856;&#32479;&#35745;&#21147;&#23398;&#30340;&#26368;&#22823;&#29109;&#25512;&#26029;&#65292;&#20197;&#26368;&#22823;&#21270;&#30456;&#23545;&#20110;&#32570;&#22833;&#20449;&#24687;&#30340;&#39044;&#27979;&#29109;&#12290;&#23427;&#21487;&#20197;&#22312;&#20197;&#19979;&#24773;&#20917;&#19979;&#25191;&#34892;&#65306;&#65288;1&#65289;&#22312;&#24050;&#30693;&#25968;&#25454;&#19978;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#20316;&#20026;&#20445;&#23432;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#65288;2&#65289;&#19987;&#38376;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model cali
&lt;/p&gt;</description></item><item><title>INFINITY&#26159;&#19968;&#20010;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35774;&#35745;&#25506;&#32034;&#21644;&#24418;&#29366;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13538</link><description>&lt;p&gt;
INFINITY: &#38024;&#23545;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#30340;&#31070;&#32463;&#22330;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations. (arXiv:2307.13538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13538
&lt;/p&gt;
&lt;p&gt;
INFINITY&#26159;&#19968;&#20010;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35774;&#35745;&#25506;&#32034;&#21644;&#24418;&#29366;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25968;&#20540;&#35774;&#35745;&#26469;&#35828;&#65292;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#21487;&#20197;&#36817;&#20284;&#22797;&#26434;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#20174;&#32780;&#20943;&#23569;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;INFINITY&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20960;&#20309;&#20449;&#24687;&#21644;&#29289;&#29702;&#22330;&#32534;&#30721;&#25104;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#26144;&#23556;&#20197;&#25512;&#26029;&#29289;&#29702;&#22330;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#32764;&#22411;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#20316;&#20026;&#31034;&#20363;&#20219;&#21153;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;AirfRANS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;&#30495;&#23454;&#30340;&#24037;&#19994;&#29992;&#20363;&#38750;&#24120;&#25509;&#36817;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20934;&#30830;&#25512;&#26029;&#20307;&#31215;&#21644;&#26354;&#38754;&#19978;&#30340;&#29289;&#29702;&#22330;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#35774;&#35745;&#25506;&#32034;&#21644;&#24418;&#29366;&#20248;&#21270;&#31561;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#27491;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
For numerical design, the development of efficient and accurate surrogate models is paramount. They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations. We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge. Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields. We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases. The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface. Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly pre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23574;&#23792;Wishart&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#19968;&#31867;&#23376;&#31354;&#38388;&#24182;&#38598;&#27169;&#22411;&#25429;&#25417;&#20449;&#21495;&#32467;&#26500;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#32479;&#35745;&#21644;&#35745;&#31639;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#28982;&#30340;&#25237;&#24433;&#21151;&#29575;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#32479;&#35745;&#36817;&#20284;&#26368;&#20248;&#37051;&#22495;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20855;&#20307;&#26696;&#20363;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;&#35745;&#31639;&#38590;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22522;&#26412;&#31232;&#30095;PCA&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#22312;&#20854;&#32467;&#26500;&#21270;&#23545;&#24212;&#29289;&#20013;&#20063;&#21516;&#26679;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.13535</link><description>&lt;p&gt;
&#31639;&#27861;&#21644;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#38556;&#30861;&#26159;&#21542;&#36866;&#29992;&#20110;&#20854;&#20182;&#32467;&#26500;&#35774;&#32622;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do algorithms and barriers for sparse principal component analysis extend to other structured settings?. (arXiv:2307.13535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23574;&#23792;Wishart&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#19968;&#31867;&#23376;&#31354;&#38388;&#24182;&#38598;&#27169;&#22411;&#25429;&#25417;&#20449;&#21495;&#32467;&#26500;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#32479;&#35745;&#21644;&#35745;&#31639;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#28982;&#30340;&#25237;&#24433;&#21151;&#29575;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#32479;&#35745;&#36817;&#20284;&#26368;&#20248;&#37051;&#22495;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20855;&#20307;&#26696;&#20363;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;&#35745;&#31639;&#38590;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22522;&#26412;&#31232;&#30095;PCA&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#22312;&#20854;&#32467;&#26500;&#21270;&#23545;&#24212;&#29289;&#20013;&#20063;&#21516;&#26679;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23574;&#23792;Wishart&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#20854;&#20013;&#20449;&#21495;&#20013;&#30340;&#32467;&#26500;&#36890;&#36807;&#19968;&#31867;&#23376;&#31354;&#38388;&#24182;&#38598;&#27169;&#22411;&#26469;&#25429;&#25417;&#12290;&#36825;&#20010;&#36890;&#29992;&#31867;&#21035;&#21253;&#25324;&#22522;&#26412;&#31232;&#30095;PCA&#20197;&#21450;&#24102;&#26377;&#22270;&#31232;&#30095;&#24615;&#30340;&#21464;&#20307;&#12290;&#20026;&#20102;&#22312;&#32479;&#35745;&#21644;&#35745;&#31639;&#30340;&#32479;&#19968;&#35270;&#35282;&#19979;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#38382;&#39064;&#23454;&#20363;&#30340;&#20960;&#20309;&#26377;&#20851;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#28982;&#30340;&#25237;&#24433;&#21151;&#29575;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#32479;&#35745;&#36817;&#20284;&#26368;&#20248;&#37051;&#22495;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26222;&#36866;&#22522;&#30784;&#20013;&#36335;&#24452;&#31232;&#30095;&#24615;&#21644;&#26641;&#31232;&#30095;&#24615;&#30340;&#20004;&#31181;&#37325;&#35201;&#29305;&#27530;&#24773;&#20917;&#36827;&#34892;&#31471;&#21040;&#31471;&#20998;&#26512;&#65292;&#34917;&#20805;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#30456;&#21305;&#37197;&#30340;&#35745;&#31639;&#38590;&#24230;&#35777;&#25454;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22522;&#26412;&#31232;&#30095;PCA&#35266;&#23519;&#21040;&#30340;&#20960;&#20010;&#29616;&#35937;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#20854;&#32467;&#26500;&#21270;&#23545;&#24212;&#29289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23884;&#20837;&#21040;&#36890;&#29992;&#26377;&#38480;&#20803;&#25968;&#20540;&#26041;&#26696;&#20013;&#65292;&#29992;&#20110;&#35299;Navier-Stokes&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#12290;&#22312;&#23454;&#29616;&#22810;&#31181;&#27969;&#21160;&#24773;&#20917;&#26102;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#23398;&#21040;&#30340;&#38381;&#21512;&#27169;&#22411;&#22312;&#36895;&#24230;&#21152;&#24555;10&#20493;&#30340;&#26356;&#32454;&#32593;&#26684;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#22823;&#28065;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.13533</link><description>&lt;p&gt;
&#21487;&#24494;&#28237;&#27969; II
&lt;/p&gt;
&lt;p&gt;
Differentiable Turbulence II. (arXiv:2307.13533v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23884;&#20837;&#21040;&#36890;&#29992;&#26377;&#38480;&#20803;&#25968;&#20540;&#26041;&#26696;&#20013;&#65292;&#29992;&#20110;&#35299;Navier-Stokes&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#12290;&#22312;&#23454;&#29616;&#22810;&#31181;&#27969;&#21160;&#24773;&#20917;&#26102;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#23398;&#21040;&#30340;&#38381;&#21512;&#27169;&#22411;&#22312;&#36895;&#24230;&#21152;&#24555;10&#20493;&#30340;&#26356;&#32454;&#32593;&#26684;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#22823;&#28065;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#27969;&#20307;&#27169;&#25311;&#22120;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#20013;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#21487;&#24494;&#20998;&#28237;&#27969;&#25110;&#32773;&#35828;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23884;&#20837;CFD&#35299;&#31639;&#31639;&#27861;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#26082;&#20855;&#22791;&#20102;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#38480;&#30340;&#21069;&#26399;&#25104;&#26412;&#65292;&#21448;&#20855;&#22791;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#33258;&#21160;&#21270;&#35757;&#32451;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#26377;&#38480;&#20803;&#25968;&#20540;&#26041;&#26696;&#20013;&#65292;&#29992;&#20110;&#35299;Navier-Stokes&#26041;&#31243;&#65292;&#24212;&#29992;&#35813;&#25216;&#26415;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#12290;&#25105;&#20204;&#22312;&#20960;&#31181;&#21453;&#21521;&#38454;&#26799;&#27969;&#30340;&#23454;&#29616;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#65292;&#27979;&#35797;&#20102;&#19981;&#21516;&#38647;&#35834;&#25968;&#21644;&#26032;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#38381;&#21512;&#27169;&#22411;&#22312;&#30456;&#24403;&#20110;&#36895;&#24230;&#21152;&#24555;10&#20493;&#30340;&#26356;&#32454;&#32593;&#26684;&#19978;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#22823;&#28065;&#27169;&#25311;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#39044;&#27979;&#28237;&#27969;&#27969;&#21160;&#65292;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#27169;&#22411;&#37197;&#32622;&#21644;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25351;&#26631;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#22797;&#26434;&#27969;&#21160;&#21644;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.13517</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#28237;&#27969;&#30340;&#38271;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Long-Term predictions of Turbulence using Neural Operators. (arXiv:2307.13517v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#39044;&#27979;&#28237;&#27969;&#27969;&#21160;&#65292;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#27169;&#22411;&#37197;&#32622;&#21644;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25351;&#26631;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#22797;&#26434;&#27969;&#21160;&#21644;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#39044;&#27979;&#28237;&#27969;&#27969;&#21160;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;(FNO)&#27169;&#22411;&#12290;&#23427;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#28237;&#27969;&#27969;&#21160;&#20223;&#30495;&#30340;&#38477;&#38454;/&#20195;&#29702;&#27169;&#22411;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#37197;&#32622;&#65292;U-NET&#32467;&#26500;(UNO&#21644;U-FNET)&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;FNO&#12290;U-FNET&#22312;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#27491;&#21017;&#21270;&#39033;&#65292;&#22914;&#26799;&#24230;&#21644;&#31283;&#23450;&#24615;&#25439;&#22833;&#65292;&#23545;&#20110;&#31283;&#23450;&#20934;&#30830;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27969;&#20307;&#27969;&#21160;&#39044;&#27979;&#20013;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22788;&#29702;&#22797;&#26434;&#27969;&#21160;&#21644;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#25351;&#26631;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores Neural Operators to predict turbulent flows, focusing on the Fourier Neural Operator (FNO) model. It aims to develop reduced-order/surrogate models for turbulent flow simulations using Machine Learning. Different model configurations are analyzed, with U-NET structures (UNO and U-FNET) performing better than the standard FNO in accuracy and stability. U-FNET excels in predicting turbulence at higher Reynolds numbers. Regularization terms, like gradient and stability losses, are essential for stable and accurate predictions. The study emphasizes the need for improved metrics for deep learning models in fluid flow prediction. Further research should focus on models handling complex flows and practical benchmarking metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#20013;&#23398;&#20064;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#35777;&#25454;&#20998;&#24067;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#26102;&#38388;&#19978;&#23545;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#21644;&#28789;&#27963;&#30340;&#25512;&#26029;&#65292;&#24182;&#19988;&#22312;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#35266;&#27979;&#30340;&#26102;&#38388;&#19978;&#25193;&#23637;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#22312;&#36935;&#21040;&#22122;&#38899;&#25968;&#25454;&#26102;&#23454;&#29616;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2307.13503</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#26102;&#38388;&#35777;&#25454;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Continuous Time Evidential Distributions for Irregular Time Series. (arXiv:2307.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#20013;&#23398;&#20064;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#35777;&#25454;&#20998;&#24067;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#26102;&#38388;&#19978;&#23545;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#21644;&#28789;&#27963;&#30340;&#25512;&#26029;&#65292;&#24182;&#19988;&#22312;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#35266;&#27979;&#30340;&#26102;&#38388;&#19978;&#25193;&#23637;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#22312;&#36935;&#21040;&#22122;&#38899;&#25968;&#25454;&#26102;&#23454;&#29616;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#24456;&#38590;&#36827;&#34892;&#39044;&#27979;&#12290;&#24403;&#35266;&#27979;&#19981;&#36830;&#32493;&#26102;&#65292;&#22312;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#25512;&#26029;&#29305;&#24449;&#30340;&#20540;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#21462;&#20915;&#20110;&#26368;&#21518;&#19968;&#27425;&#35266;&#23519;&#30340;&#26102;&#38388;&#32780;&#20855;&#26377;&#19968;&#31995;&#21015;&#30340;&#20540;&#12290;&#20026;&#20102;&#25551;&#36848;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDICT&#65292;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#20013;&#23398;&#20064;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#35777;&#25454;&#20998;&#24067;&#30340;&#31574;&#30053;&#12290;&#36825;&#20010;&#20998;&#24067;&#21487;&#20197;&#22312;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#26102;&#38388;&#19978;&#23545;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#21644;&#28789;&#27963;&#30340;&#25512;&#26029;&#65292;&#21516;&#26102;&#22312;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#35266;&#27979;&#30340;&#26102;&#38388;&#19978;&#25193;&#23637;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;EDICT&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36935;&#21040;&#22122;&#38899;&#25968;&#25454;&#26102;&#23454;&#29616;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24378;&#20581;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#21382;&#21490;&#24066;&#22330;&#25968;&#25454;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.13501</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#24378;&#20581;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Robust Goal-Based Wealth Management. (arXiv:2307.13501v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24378;&#20581;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#21382;&#21490;&#24066;&#22330;&#25968;&#25454;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#25237;&#36164;&#26159;&#19968;&#31181;&#20248;&#20808;&#23454;&#29616;&#29305;&#23450;&#36130;&#21153;&#30446;&#26631;&#30340;&#36130;&#23500;&#31649;&#29702;&#26041;&#27861;&#12290;&#30001;&#20110;&#38656;&#35201;&#36873;&#25321;&#36866;&#24403;&#30340;&#25237;&#36164;&#30452;&#21040;&#36798;&#21040;&#30446;&#26631;&#65292;&#22240;&#27492;&#23427;&#33258;&#28982;&#22320;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#20248;&#21270;&#36825;&#20123;&#25237;&#36164;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24378;&#20581;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#21382;&#21490;&#24066;&#22330;&#25968;&#25454;&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22810;&#20010;&#30446;&#26631;&#23548;&#21521;&#36130;&#23500;&#31649;&#29702;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-based investing is an approach to wealth management that prioritizes achieving specific financial goals. It is naturally formulated as a sequential decision-making problem as it requires choosing the appropriate investment until a goal is achieved. Consequently, reinforcement learning, a machine learning technique appropriate for sequential decision-making, offers a promising path for optimizing these investment strategies. In this paper, a novel approach for robust goal-based wealth management based on deep reinforcement learning is proposed. The experimental results indicate its superiority over several goal-based wealth management benchmarks on both simulated and historical market data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23547;&#25214;&#27927;&#38065;&#32773;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#38134;&#34892;&#20132;&#26131;&#21644;&#21830;&#19994;&#35282;&#33394;&#25968;&#25454;&#26500;&#24314;&#30340;&#22823;&#22411;&#24322;&#26500;&#32593;&#32476;&#20013;&#35782;&#21035;&#27927;&#38065;&#27963;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#27927;&#38065;&#27963;&#21160;&#20013;&#29359;&#32618;&#20998;&#23376;&#30340;&#21512;&#20316;&#38382;&#39064;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#21516;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28040;&#24687;&#32858;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13499</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#23547;&#25214;&#27927;&#38065;&#32773;
&lt;/p&gt;
&lt;p&gt;
Finding Money Launderers Using Heterogeneous Graph Neural Networks. (arXiv:2307.13499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23547;&#25214;&#27927;&#38065;&#32773;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#38134;&#34892;&#20132;&#26131;&#21644;&#21830;&#19994;&#35282;&#33394;&#25968;&#25454;&#26500;&#24314;&#30340;&#22823;&#22411;&#24322;&#26500;&#32593;&#32476;&#20013;&#35782;&#21035;&#27927;&#38065;&#27963;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#27927;&#38065;&#27963;&#21160;&#20013;&#29359;&#32618;&#20998;&#23376;&#30340;&#21512;&#20316;&#38382;&#39064;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#21516;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28040;&#24687;&#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#21453;&#27927;&#38065;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#35268;&#21017;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#38590;&#20197;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#26816;&#27979;&#27927;&#38065;&#27963;&#21160;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#23545;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#25506;&#32034;&#28909;&#28526;&#12290;&#30001;&#20110;&#29359;&#32618;&#20998;&#23376;&#36890;&#24120;&#22312;&#27927;&#38065;&#27963;&#21160;&#20013;&#21512;&#20316;&#65292;&#22240;&#27492;&#32771;&#34385;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#23458;&#25143;&#20851;&#31995;&#21644;&#38142;&#25509;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#25386;&#23041;&#26368;&#22823;&#30340;&#38134;&#34892;DNB&#30340;&#30495;&#23454;&#38134;&#34892;&#20132;&#26131;&#21644;&#21830;&#19994;&#35282;&#33394;&#25968;&#25454;&#26500;&#24314;&#30340;&#22823;&#22411;&#24322;&#26500;&#32593;&#32476;&#20013;&#35782;&#21035;&#27927;&#38065;&#27963;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#31216;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#21516;&#36136;GNN&#26041;&#27861;&#65292;&#20197;&#22312;&#24322;&#26500;&#22270;&#19978;&#26377;&#25928;&#36816;&#34892;&#12290;&#20316;&#20026;&#35813;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22270;&#30340;&#19981;&#21516;&#36793;&#20043;&#38388;&#32858;&#21512;&#28040;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our finding
&lt;/p&gt;</description></item><item><title>Zshot&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26368;&#26032;ZSL&#26041;&#27861;&#65292;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.13497</link><description>&lt;p&gt;
Zshot&#65306;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction. (arXiv:2307.13497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13497
&lt;/p&gt;
&lt;p&gt;
Zshot&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26368;&#26032;ZSL&#26041;&#27861;&#65292;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#20219;&#21153;&#28041;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#25110;&#20851;&#31995;&#12290;&#30001;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;ZSL&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#36817;&#24180;&#26469;&#24212;&#29992;&#33539;&#22260;&#24050;&#22823;&#24133;&#22686;&#38271;&#12290;&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#26032;&#30340;&#26041;&#27861;&#65292;ZSL&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#12290;&#30740;&#31350;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#19968;&#20010;&#20840;&#38754;&#25903;&#25345;&#26368;&#26032;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#21457;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;ZSL&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Zshot&#30340;&#21019;&#26032;ZSL&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#27604;&#36739;&#19981;&#21516;&#30340;&#26368;&#26032;ZSL&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25903;&#25345;&#24037;&#19994;&#30028;&#30340;&#26694;&#26550;&#65292;&#20855;&#22791;&#26131;&#29992;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readi
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22797;&#20540;&#20989;&#25968;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25311;&#21512;&#36807;&#31243;&#20013;&#26631;&#20934;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13484</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rational kernel-based interpolation for complex-valued frequency response functions. (arXiv:2307.13484v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22797;&#20540;&#20989;&#25968;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25311;&#21512;&#36807;&#31243;&#20013;&#26631;&#20934;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#22312;&#22797;&#20540;&#20989;&#25968;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#39057;&#22495;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#26680;&#26041;&#27861;&#36234;&#26469;&#36234;&#24120;&#29992;&#65292;&#28982;&#32780;&#26631;&#20934;&#30340;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#20540;&#24773;&#20917;&#19979;&#65292;&#24213;&#23618;&#26680;&#23545;&#30340;&#25968;&#23398;&#21547;&#20041;&#21644;&#25968;&#23398;&#25512;&#23548;&#23578;&#24453;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22797;&#20540;&#20989;&#25968;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#23558;&#24102;&#26377;&#26680;&#23545;&#30340;&#22797;&#20540;&#25554;&#20540;&#38382;&#39064;&#36716;&#21270;&#20026;&#36825;&#20123;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25554;&#20540;&#22120;&#19982;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#32467;&#21512;&#65292;&#20854;&#20013;&#38454;&#25968;&#26681;&#25454;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#33258;&#36866;&#24212;&#36873;&#25321;&#12290;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#30005;&#30913;&#23398;&#21644;&#22768;&#23398;&#65289;&#30340;&#20363;&#23376;&#30340;&#25968;&#20540;&#32467;&#26524;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#30495;&#23454;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#20248;&#21270;MLOps&#24037;&#20316;&#27969;&#31243;&#24182;&#25552;&#20379;&#23454;&#29992;&#30340;&#25552;&#31034;&#21644;&#24314;&#35758;&#65292;&#24378;&#35843;&#31215;&#26497;&#35268;&#21010;&#21644;&#25345;&#32493;&#25913;&#36827;&#65292;&#20197;&#22686;&#24378;MLOps&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13473</link><description>&lt;p&gt;
&#25506;&#32034;MLOps&#21160;&#24577;: &#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#20013;&#30340;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project. (arXiv:2307.13473v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#30495;&#23454;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#20248;&#21270;MLOps&#24037;&#20316;&#27969;&#31243;&#24182;&#25552;&#20379;&#23454;&#29992;&#30340;&#25552;&#31034;&#21644;&#24314;&#35758;&#65292;&#24378;&#35843;&#31215;&#26497;&#35268;&#21010;&#21644;&#25345;&#32493;&#25913;&#36827;&#65292;&#20197;&#22686;&#24378;MLOps&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#65292;&#37325;&#28857;&#26159;&#20248;&#21270;MLOps&#65288;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#65289;&#36807;&#31243;&#65292;&#36825;&#26159;&#39640;&#25928;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#27169;&#24335;&#21644;&#35265;&#35299;&#65292;&#20197;&#25552;&#21319;MLOps&#24037;&#20316;&#27969;&#31243;&#65292;&#32771;&#34385;&#21040;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#24320;&#21457;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#21644;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#23454;&#39564;&#28085;&#30422;&#20102;&#23436;&#25972;&#30340;MLOps&#24037;&#20316;&#27969;&#31243;&#65292;&#28085;&#30422;&#20102;&#38382;&#39064;&#23450;&#20041;&#12289;&#25968;&#25454;&#33719;&#21462;&#12289;&#25968;&#25454;&#20934;&#22791;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#27169;&#22411;&#37096;&#32626;&#12289;&#30417;&#25511;&#12289;&#31649;&#29702;&#12289;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#27835;&#29702;&#21644;&#21512;&#35268;&#24615;&#31561;&#20851;&#38190;&#38454;&#27573;&#12290;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#20102;&#23454;&#29992;&#30340;&#25552;&#31034;&#21644;&#24314;&#35758;&#65292;&#24378;&#35843;&#20102;&#23545;MLOps&#24037;&#20316;&#27969;&#31243;&#30340;&#31215;&#26497;&#35268;&#21010;&#21644;&#25345;&#32493;&#25913;&#36827;&#12290;&#23454;&#39564;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#30340;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#26377;&#26426;&#22320;&#38598;&#25104;&#65292;&#35813;&#39033;&#30446;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#25353;&#29031;MLOps&#36807;&#31243;&#30340;&#20851;&#38190;&#38454;&#27573;&#36827;&#34892;&#25805;&#20316;&#65292;&#22788;&#29702;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#37319;&#29992;&#20102;&#31995;&#32479;&#21270;&#30340;&#36319;&#36394;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.  The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.  The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#25293;&#21334;&#26694;&#26550;&#29992;&#20110;&#26412;&#22320;&#33021;&#37327;&#28789;&#27963;&#24066;&#22330;&#65292;&#36890;&#36807;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35299;&#20915;&#20102;NP&#23436;&#20840;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#26412;&#22320;&#24066;&#22330;&#20013;&#20998;&#37197;&#33021;&#37327;&#28789;&#27963;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13470</link><description>&lt;p&gt;
&#32452;&#21512;&#25293;&#21334;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26412;&#22320;&#33021;&#37327;&#28789;&#27963;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets. (arXiv:2307.13470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#25293;&#21334;&#26694;&#26550;&#29992;&#20110;&#26412;&#22320;&#33021;&#37327;&#28789;&#27963;&#24066;&#22330;&#65292;&#36890;&#36807;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35299;&#20915;&#20102;NP&#23436;&#20840;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#26412;&#22320;&#24066;&#22330;&#20013;&#20998;&#37197;&#33021;&#37327;&#28789;&#27963;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#25293;&#21334;&#26694;&#26550;&#29992;&#20110;&#26412;&#22320;&#33021;&#37327;&#28789;&#27963;&#24066;&#22330;&#65292;&#35299;&#20915;&#20102;&#29983;&#20135;&#32773;&#26080;&#27861;&#25171;&#21253;&#22810;&#20010;&#28789;&#27963;&#26102;&#38388;&#38388;&#38548;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#24213;&#23618;&#30340;NP&#23436;&#20840;&#33719;&#32988;&#32773;&#20915;&#23450;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#24322;&#26500;&#19977;&#37096;&#22270;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#29616;&#25104;&#30340;&#20248;&#21270;&#24037;&#20855;&#30456;&#27604;&#65292;&#24179;&#22343;&#26368;&#20248;&#20540;&#20559;&#24046;&#23567;&#20110;5&#65285;&#65292;&#24182;&#19988;&#19982;&#21830;&#19994;&#27714;&#35299;&#22120;&#30340;&#25351;&#25968;&#22797;&#26434;&#24230;&#30456;&#27604;&#65292;&#23637;&#29616;&#20986;&#32447;&#24615;&#25512;&#29702;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#36129;&#29486;&#21644;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#26412;&#22320;&#24066;&#22330;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39640;&#25928;&#20998;&#37197;&#33021;&#37327;&#28789;&#27963;&#36164;&#28304;&#20197;&#21450;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39640;&#26031;&#22270;&#21644;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;GPCL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#22871;&#39184;&#25512;&#33616;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#37319;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13468</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#22871;&#39184;&#25512;&#33616;&#20013;&#30340;&#39640;&#26031;&#22270;&#19982;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation. (arXiv:2307.13468v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39640;&#26031;&#22270;&#21644;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;GPCL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#22871;&#39184;&#25512;&#33616;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#37319;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22871;&#39184;&#25512;&#33616;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20559;&#22909;&#12290;&#26082;&#26377;&#30340;&#25104;&#21151;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#23545;&#27604;&#22270;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20174;&#29992;&#25143;&#32423;&#21035;&#21644;&#22871;&#39184;&#32423;&#21035;&#30340;&#22270;&#35270;&#22270;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#22686;&#24378;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#21512;&#20316;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#31232;&#30095;&#24615;&#25110;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#32570;&#20047;&#26377;&#21306;&#21035;&#24615;&#20449;&#24687;&#65292;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#22312;&#23454;&#38469;&#22871;&#39184;&#25512;&#33616;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24573;&#35270;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#23427;&#20204;&#30340;&#36880;&#23454;&#20363;&#23545;&#27604;&#23398;&#20064;&#26080;&#27861;&#21306;&#20998;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#65288;&#21363;&#37319;&#26679;&#20559;&#24046;&#38382;&#39064;&#65289;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39640;&#26031;&#22270;&#21644;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;GPCL&#65289;&#26694;&#26550;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22522;&#20110;&#36807;&#31243;&#30340;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20803;&#27169;&#24314;&#26041;&#27861;&#29992;&#20110;&#39532;&#38083;&#34223;&#20135;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27169;&#25311;&#24212;&#29992;&#21644;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20013;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13466</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#36827;&#34892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integrating processed-based models and machine learning for crop yield prediction. (arXiv:2307.13466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22522;&#20110;&#36807;&#31243;&#30340;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20803;&#27169;&#24314;&#26041;&#27861;&#29992;&#20110;&#39532;&#38083;&#34223;&#20135;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27169;&#25311;&#24212;&#29992;&#21644;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20013;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#29702;&#35770;&#39537;&#21160;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26657;&#20934;&#26412;&#22320;&#29615;&#22659;&#26041;&#38754;&#24448;&#24448;&#36739;&#20026;&#22256;&#38590;&#65292;&#25110;&#32773;&#20351;&#29992;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#28151;&#21512;&#20803;&#27169;&#24314;&#26041;&#27861;&#30740;&#31350;&#20102;&#39532;&#38083;&#34223;&#20135;&#37327;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#65288;&#39044;&#65289;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#28982;&#21518;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#27169;&#25311;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#20803;&#27169;&#24314;&#26041;&#27861;&#27604;&#32431;&#31929;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#26469;&#33258;&#30000;&#38388;&#35797;&#39564;&#65288;n=303&#65289;&#21644;&#21830;&#19994;&#30000;&#22320;&#65288;n=77&#65289;&#30340;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#20013;&#65292;&#20803;&#27169;&#24314;&#26041;&#27861;&#30456;&#23545;&#20110;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#21518;&#32773;&#20013;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#34920;&#29616;&#37117;&#19981;&#22914;&#30001;&#39046;&#22495;&#19987;&#23478;&#35774;&#35745;&#30340;&#25163;&#21160;&#36873;&#25321;&#29305;&#24449;&#38598;&#21644;&#19987;&#38376;&#39044;&#22788;&#29702;&#30340;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#30456;&#23545;&#35770;&#37327;&#23376;&#22330;&#35770;&#21644;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;Lieb-Robinson&#30028;&#38480;&#65292;&#25209;&#21028;&#24615;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#24555;&#36895;&#37327;&#23376;&#23384;&#20648;&#22120;&#30340;&#20869;&#22312;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#22312;&#28151;&#21512;&#37327;&#23376;&#22768;&#23398;&#31995;&#32479;&#20013;&#65292;QRAM&#21487;&#20197;&#23481;&#32435;&#26368;&#22810;O(10^7)&#20010;&#36923;&#36753;&#27604;&#29305;&#30340;&#19968;&#32500;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.13460</link><description>&lt;p&gt;
&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#20869;&#23384;&#30340;&#22522;&#26412;&#22240;&#26524;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Fundamental causal bounds of quantum random access memories. (arXiv:2307.13460v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#30456;&#23545;&#35770;&#37327;&#23376;&#22330;&#35770;&#21644;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;Lieb-Robinson&#30028;&#38480;&#65292;&#25209;&#21028;&#24615;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#24555;&#36895;&#37327;&#23376;&#23384;&#20648;&#22120;&#30340;&#20869;&#22312;&#30028;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#22312;&#28151;&#21512;&#37327;&#23376;&#22768;&#23398;&#31995;&#32479;&#20013;&#65292;QRAM&#21487;&#20197;&#23481;&#32435;&#26368;&#22810;O(10^7)&#20010;&#36923;&#36753;&#27604;&#29305;&#30340;&#19968;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35774;&#22791;&#24212;&#36981;&#23432;&#37327;&#23376;&#29289;&#29702;&#21407;&#21017;&#12290;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#20869;&#23384;&#65288;QRAM&#65289;&#26159;&#35768;&#22810;&#37325;&#35201;&#37327;&#23376;&#31639;&#27861;&#65288;&#22914;&#32447;&#24615;&#20195;&#25968;&#12289;&#25968;&#25454;&#25628;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#65289;&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#32473;&#23450;N&#20010;&#37327;&#23376;&#27604;&#29305;&#26102;&#65292;&#21487;&#20197;&#20197;O(log N)&#30340;&#30005;&#36335;&#28145;&#24230;&#22788;&#29702;O(N)&#30340;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22823;&#37327;&#37327;&#23376;&#27604;&#29305;&#30340;&#30456;&#20114;&#20316;&#29992;&#23616;&#37096;&#30340;&#37327;&#23376;&#26448;&#26009;&#26102;&#65292;&#36825;&#19968;&#20027;&#24352;&#20284;&#20046;&#36829;&#21453;&#20102;&#30456;&#23545;&#35770;&#21407;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#24555;&#36895;&#37327;&#23376;&#23384;&#20648;&#22120;&#30340;&#20869;&#22312;&#30028;&#38480;&#65292;&#21033;&#29992;&#30456;&#23545;&#35770;&#37327;&#23376;&#22330;&#35770;&#21644;Lieb-Robinson&#30028;&#38480;&#22312;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#28151;&#21512;&#37327;&#23376;&#22768;&#23398;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#30828;&#20214;&#35774;&#35745;&#30340;QRAM&#12290;&#20551;&#35774;&#26102;&#38047;&#21608;&#26399;&#32422;&#20026;10^{-3}&#31186;&#65292;&#26684;&#23376;&#38388;&#36317;&#32422;&#20026;1&#24494;&#31859;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;QRAM&#21487;&#20197;&#23481;&#32435;&#26368;&#22810;O(10^7)&#20010;&#36923;&#36753;&#27604;&#29305;&#30340;&#19968;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimens
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20026;&#21464;&#25442;&#22120;(BeTrans)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#30340;&#38750;&#38745;&#24577;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39034;&#24207;&#25968;&#25454;&#26469;&#36866;&#24212;&#26032;&#30340;&#38750;&#38745;&#24577;&#30340;&#20154;&#31867;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;BeTrans&#22312;&#21327;&#20316;&#29615;&#22659;&#20013;&#25928;&#26524;&#26174;&#33879;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#24555;&#22320;&#36866;&#24212;&#20102;&#38750;&#38745;&#24577;&#30340;&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.13447</link><description>&lt;p&gt;
&#19968;&#20010;&#34892;&#20026;&#21464;&#25442;&#22120;&#29992;&#20110;&#26426;&#22120;&#20154;&#19982;&#38750;&#38745;&#27490;&#20154;&#31867;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
A behavioural transformer for effective collaboration between a robot and a non-stationary human. (arXiv:2307.13447v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20026;&#21464;&#25442;&#22120;(BeTrans)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#30340;&#38750;&#38745;&#24577;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39034;&#24207;&#25968;&#25454;&#26469;&#36866;&#24212;&#26032;&#30340;&#38750;&#38745;&#24577;&#30340;&#20154;&#31867;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;BeTrans&#22312;&#21327;&#20316;&#29615;&#22659;&#20013;&#25928;&#26524;&#26174;&#33879;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#24555;&#22320;&#36866;&#24212;&#20102;&#38750;&#38745;&#24577;&#30340;&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20154;&#31867;&#34892;&#20026;&#30340;&#38750;&#38745;&#27490;&#24615;&#65292;&#30001;&#20110;&#20854;&#34892;&#20026;&#30340;&#21464;&#21270;&#25152;&#20135;&#29983;&#30340;&#38750;&#38745;&#27490;&#24615;&#20250;&#25913;&#21464;&#29615;&#22659;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#25506;&#32034;&#26426;&#22120;&#20154;&#22914;&#20309;&#26356;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#24182;&#22240;&#27492;&#35299;&#20915;&#38750;&#38745;&#27490;&#24615;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#34892;&#20026;&#21464;&#25442;&#22120;(BeTrans)&#12290;BeTrans&#26159;&#19968;&#20010;&#26465;&#20214;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#38750;&#38745;&#24577;&#34892;&#20026;&#30340;&#26032;&#30340;&#20154;&#31867;&#20195;&#29702;&#65292;&#22240;&#20026;&#23427;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;BeTrans&#22312;&#27169;&#25311;&#30340;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#20559;&#24046;&#30340;&#20154;&#31867;&#20195;&#29702;&#20013;&#65292;&#22312;&#21327;&#20316;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#21407;&#22987;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#30340;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;BeTrans&#19982;&#27169;&#25311;&#30340;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21327;&#20316;&#65292;&#24182;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#24555;&#22320;&#36866;&#24212;&#20102;&#38750;&#38745;&#24577;&#30340;&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27969;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;69&#20010;&#36890;&#29992;&#29305;&#24449;&#65292;&#21487;&#22312;&#21508;&#31181;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#19982;&#30456;&#20851;&#24037;&#20316;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13434</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#27969;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Network Traffic Classification based on Single Flow Time Series Analysis. (arXiv:2307.13434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27969;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;69&#20010;&#36890;&#29992;&#29305;&#24449;&#65292;&#21487;&#22312;&#21508;&#31181;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#19982;&#30456;&#20851;&#24037;&#20316;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;IP&#27969;&#36827;&#34892;&#32593;&#32476;&#27969;&#37327;&#30417;&#27979;&#26159;&#22788;&#29702;&#20998;&#26512;&#21152;&#23494;&#32593;&#32476;&#36890;&#20449;&#30340;&#24403;&#21069;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#21253;&#32858;&#21512;&#20026;&#27969;&#35760;&#24405;&#33258;&#28982;&#20250;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#65307;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27969;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#27969;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27599;&#20010;&#25968;&#25454;&#21253;&#30340;&#23383;&#33410;&#25968;&#21644;&#26102;&#38388;&#25139;&#21019;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#22522;&#20110;&#25968;&#25454;&#28857;&#30340;&#32479;&#35745;&#20998;&#26512;&#12289;&#26102;&#22495;&#20998;&#26512;&#12289;&#27969;&#26102;&#38388;&#36328;&#24230;&#20869;&#30340;&#25968;&#25454;&#21253;&#20998;&#24067;&#12289;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#21644;&#39057;&#22495;&#20998;&#26512;&#25552;&#20986;&#20102;69&#20010;&#36890;&#29992;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;15&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30693;&#21517;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#29305;&#24449;&#21521;&#37327;&#22312;&#21508;&#31181;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#29305;&#24449;&#21521;&#37327;&#22312;&#20108;&#20998;&#31867;&#21644;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#19982;&#30456;&#20851;&#24037;&#20316;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13430</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#32452;&#21512;&#26497;&#23567;&#21270;&#20248;&#21270;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization. (arXiv:2307.13430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#26426;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#28085;&#30422;&#20102;&#35768;&#22810;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#22312;&#20998;&#25955;&#35774;&#32622;&#19979;&#20248;&#21270;&#36825;&#31181;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#32473;&#35774;&#35745;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#23545;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#36739;&#22823;&#65292;&#26631;&#20934;&#30340;&#20256;&#36882;&#31574;&#30053;&#26080;&#27861;&#22312;&#20998;&#25955;&#24335;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#21160;&#37327;&#30340;&#20998;&#25955;&#24335;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20197;&#20943;&#23567;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#19982;&#24037;&#20316;&#32773;&#25968;&#37327;&#25104;&#32447;&#24615;&#21152;&#36895;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20026;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#25552;&#20379;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#20449;&#21495;&#22788;&#29702;&#30340;&#22522;&#26412;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30456;&#36830;&#25509;&#65292;&#20197;&#35299;&#37322;&#19981;&#21516;&#30340;&#22122;&#22768;&#25233;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#26032;&#22411;CNN&#26550;&#26500;&#30340;&#37325;&#35201;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2307.13425</link><description>&lt;p&gt;
&#19968;&#31181;&#22122;&#22768;&#25233;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#21495;&#22788;&#29702;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A signal processing interpretation of noise-reduction convolutional neural networks. (arXiv:2307.13425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#20449;&#21495;&#22788;&#29702;&#30340;&#22522;&#26412;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30456;&#36830;&#25509;&#65292;&#20197;&#35299;&#37322;&#19981;&#21516;&#30340;&#22122;&#22768;&#25233;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#26032;&#22411;CNN&#26550;&#26500;&#30340;&#37325;&#35201;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;-&#35299;&#30721;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#22122;&#22768;&#25233;&#21046;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;CNN&#26550;&#26500;&#30340;&#24320;&#21457;&#36890;&#24120;&#26159;&#20197;&#20020;&#26102;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#23545;&#20110;&#37325;&#35201;&#30340;&#35774;&#35745;&#36873;&#25321;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#26377;&#19981;&#21516;&#30340;&#30456;&#20851;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#37322;&#36825;&#20123;CNN&#30340;&#20869;&#37096;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24819;&#27861;&#35201;&#20040;&#38646;&#25955;&#20998;&#24067;&#65292;&#35201;&#20040;&#21487;&#33021;&#38656;&#35201;&#30456;&#24403;&#30340;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#20026;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#25152;&#29702;&#35299;&#12290;&#20026;&#20102;&#25171;&#24320;&#36825;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#39046;&#22495;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#20449;&#21495;&#22788;&#29702;&#30340;&#22522;&#26412;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30456;&#36830;&#25509;&#65292;&#20197;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#35299;&#37322;&#20102;&#19981;&#21516;&#30340;ED CNN&#26550;&#26500;&#65292;&#24182;&#22312;&#36825;&#19968;&#33258;&#21253;&#21547;&#30340;&#26448;&#26009;&#20013;&#25552;&#20379;&#20102;&#35774;&#35745;&#24378;&#22823;&#39640;&#25928;&#26032;&#22411;CNN&#26550;&#26500;&#30340;&#37325;&#35201;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#24212;&#29992;&#20110;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#20010;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.13423</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#23545;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#28165;&#26224;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#24212;&#29992;&#20110;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034; (SSSRs) &#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20316;&#20026;&#35821;&#38899;&#36136;&#37327; (SQ) &#39044;&#27979;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#27491;&#24120;&#25110;&#26377;&#21548;&#21147;&#38556;&#30861;&#30340;&#29992;&#25143;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#21644;&#22914;&#20309;&#23558;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#23884;&#20837;&#21040;&#36825;&#26679;&#30340;&#34920;&#31034;&#20013;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38750;&#20405;&#20837;&#24335; SQ &#35780;&#32423;&#39044;&#27979;&#25216;&#26415;&#34987;&#25193;&#23637;&#21040;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#29992;&#25143;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#21457;&#29616;&#33258;&#23398;&#20064;&#34920;&#31034;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#24615;&#33021;&#31454;&#20105;&#21147;&#24378;&#20110;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#12290;&#38024;&#23545; Clarity Prediction Challenge 1 &#21463;&#35797;&#32773;&#21644;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#65288;&#21548;&#21147;&#21463;&#25439;&#30340;&#65289;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#20027;&#32039;&#24613;&#21046;&#21160;&#31995;&#32479;&#20013;&#23398;&#20064;&#33021;&#21147;&#32452;&#20214;&#21644;&#36234;&#30028;&#26816;&#27979;&#22120;&#20043;&#38388;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#24102;&#26469;&#30340;&#38169;&#35823;&#20915;&#31574;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#21151;&#33021;&#24615;&#33021;&#12289;&#38750;&#21151;&#33021;&#24615;&#33021;&#21644;&#31995;&#32479;&#23433;&#20840;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.13419</link><description>&lt;p&gt;
&#33258;&#20027;&#32039;&#24613;&#21046;&#21160;&#31995;&#32479;&#30340;&#36234;&#30028;&#26816;&#27979;&#22120;&#30340;&#32852;&#21512;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems. (arXiv:2307.13419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#20027;&#32039;&#24613;&#21046;&#21160;&#31995;&#32479;&#20013;&#23398;&#20064;&#33021;&#21147;&#32452;&#20214;&#21644;&#36234;&#30028;&#26816;&#27979;&#22120;&#20043;&#38388;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#24102;&#26469;&#30340;&#38169;&#35823;&#20915;&#31574;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#21151;&#33021;&#24615;&#33021;&#12289;&#38750;&#21151;&#33021;&#24615;&#33021;&#21644;&#31995;&#32479;&#23433;&#20840;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#32452;&#20214;&#65288;LEC&#65289;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#24403;&#38754;&#20020;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#24456;&#21487;&#33021;&#20570;&#20986;&#38169;&#35823;&#30340;&#20915;&#31574;&#12290;&#36234;&#30028;&#26816;&#27979;&#22120;&#24050;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31867;&#26679;&#26412;&#65292;&#20174;&#32780;&#20805;&#24403;&#23433;&#20840;&#30417;&#25511;&#65292;&#28982;&#32780;&#65292;&#36234;&#30028;&#26816;&#27979;&#22120;&#21644;LEC&#37117;&#38656;&#35201;&#22823;&#37327;&#20351;&#29992;&#36890;&#24120;&#22312;AV&#20013;&#25214;&#21040;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#32452;&#20214;&#65292;&#23384;&#22312;&#38750;&#21151;&#33021;&#21644;&#21151;&#33021;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20004;&#32773;&#37117;&#20250;&#24433;&#21709;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#12290;&#20363;&#22914;&#65292;&#32473;&#36234;&#30028;&#26816;&#27979;&#22120;&#26356;&#38271;&#30340;&#21709;&#24212;&#26102;&#38388;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#65292;&#20294;&#20250;&#20197;LEC&#30340;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20855;&#26377;&#20108;&#36827;&#21046;&#36755;&#20986;&#30340;LEC&#65292;&#23601;&#20687;&#19968;&#20010;&#33258;&#20027;&#32039;&#24613;&#21046;&#21160;&#31995;&#32479;&#65288;AEBS&#65289;&#65292;&#24182;&#20351;&#29992;&#39118;&#38505;&#26469;&#27169;&#25311;&#20004;&#20010;&#32452;&#20214;&#35774;&#35745;&#21442;&#25968;&#23545;&#24444;&#27492;&#30340;&#21151;&#33021;&#24615;&#33021;&#21644;&#38750;&#21151;&#33021;&#24615;&#33021;&#20197;&#21450;&#23545;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#21516;&#25511;&#21046;&#24490;&#29615;&#26102;&#38388;&#23610;&#24230;&#30340;&#22810;&#32423;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;URLLC&#26381;&#21153;&#32534;&#25490;&#65292;&#20943;&#23569;&#20102;&#25511;&#21046;&#24490;&#29615;&#30340;&#24310;&#36831;&#65292;&#20449;&#20196;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2307.13415</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;URLLC&#26381;&#21153;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning. (arXiv:2307.13415v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#21516;&#25511;&#21046;&#24490;&#29615;&#26102;&#38388;&#23610;&#24230;&#30340;&#22810;&#32423;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;URLLC&#26381;&#21153;&#32534;&#25490;&#65292;&#20943;&#23569;&#20102;&#25511;&#21046;&#24490;&#29615;&#30340;&#24310;&#36831;&#65292;&#20449;&#20196;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#65288;URLLC&#65289;&#26381;&#21153;&#34987;&#35270;&#20026;&#23454;&#29616;&#23545;5G&#20013;&#20855;&#26377;&#20005;&#26684;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#35201;&#27714;&#30340;&#24212;&#29992;&#22330;&#26223;&#30340;&#20851;&#38190;&#12290;&#19968;&#31181;&#23454;&#29616;URLLC&#26381;&#21153;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#39640;&#25928;&#20998;&#37197;&#26080;&#32447;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#65292;&#20915;&#31574;&#21464;&#37327;&#65288;&#23613;&#31649;&#37096;&#32626;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#23618;&#65289;&#36890;&#24120;&#22312;&#21516;&#19968;&#20010;&#25511;&#21046;&#24490;&#29615;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#25511;&#21046;&#24490;&#29615;&#30340;&#24310;&#36831;&#23384;&#22312;&#26174;&#33879;&#30340;&#23454;&#38469;&#38480;&#21046;&#65292;&#20197;&#21450;&#36807;&#22810;&#30340;&#20449;&#20196;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#25511;&#21046;&#24490;&#29615;&#26102;&#38388;&#23610;&#24230;&#30340;&#22810;&#32423;&#31574;&#30053;&#12290;&#20855;&#26377;&#26356;&#24555;&#25511;&#21046;&#24490;&#29615;&#30340;&#26234;&#33021;&#20307;&#37096;&#32626;&#22312;&#38752;&#36817;&#22522;&#31449;&#30340;&#20301;&#32622;&#65292;&#32780;&#20855;&#26377;&#36739;&#24930;&#25511;&#21046;&#24490;&#29615;&#30340;&#26234;&#33021;&#20307;&#21017;&#37096;&#32626;&#22312;&#36793;&#32536;&#25110;&#38752;&#36817;&#26680;&#24515;&#32593;&#32476;&#30340;&#20301;&#32622;&#65292;&#20026;&#20302;&#32423;&#21160;&#20316;&#25552;&#20379;&#39640;&#32423;&#25351;&#23548;&#12290;&#22312;&#19968;&#20010;&#26469;&#33258;&#29616;&#26377;&#25216;&#26415;&#30340;&#29992;&#20363;&#20013;&#65292;&#20511;&#21161;&#25105;&#20204;&#30340;HRL&#26694;&#26550;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#25511;&#21046;&#24490;&#29615;&#30340;&#24310;&#36831;&#65292;&#20449;&#20196;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#21363;&#26102;&#29983;&#25104;&#26435;&#37325;&#30340;&#26041;&#27861;&#20943;&#36731;CNN&#24341;&#25806;&#20013;&#30340;&#20869;&#23384;&#29942;&#39048;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;unzipFPGA&#30340;CNN&#25512;&#29702;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.13412</link><description>&lt;p&gt;
&#29992;&#21363;&#26102;&#29983;&#25104;&#26435;&#37325;&#30340;&#26041;&#27861;&#20943;&#36731;CNN&#24341;&#25806;&#20013;&#30340;&#20869;&#23384;&#29942;&#39048;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation. (arXiv:2307.13412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#21363;&#26102;&#29983;&#25104;&#26435;&#37325;&#30340;&#26041;&#27861;&#20943;&#36731;CNN&#24341;&#25806;&#20013;&#30340;&#20869;&#23384;&#29942;&#39048;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;unzipFPGA&#30340;CNN&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23548;&#33268;&#23427;&#20204;&#22312;&#31227;&#21160;&#21644;&#23884;&#20837;&#24335;&#29615;&#22659;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#20302;&#33021;&#32791;&#30340;&#25512;&#29702;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#22522;&#20110;FPGA&#30340;CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#38754;&#25237;&#20837;&#20102;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#35745;&#31639;&#24341;&#25806;&#24120;&#24120;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#23618;&#19978;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#30001;&#20110;&#26576;&#20123;&#23618;&#22312;&#24341;&#25806;&#30340;&#22266;&#23450;&#37197;&#32622;&#20013;&#30340;&#27425;&#20248;&#26144;&#23556;&#32780;&#23548;&#33268;&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24341;&#20837;&#39044;&#21367;&#31215;&#38454;&#27573;&#22312;&#36816;&#34892;&#26102;&#35299;&#21387;&#32553;&#26435;&#37325;&#30340;&#27169;&#22411;&#23545;CNN&#24341;&#25806;&#35774;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#31216;&#20026;&#21363;&#26102;&#29983;&#25104;&#26435;&#37325;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN&#25512;&#29702;&#31995;&#32479;unzipFPGA&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#38134;&#34892;&#20316;&#20026;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#25216;&#26415;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#65289;&#25152;&#24102;&#26469;&#30340;&#20844;&#24179;&#24615;&#38544;&#24739;&#12290;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#33030;&#24369;&#24615;&#30340;&#32500;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#33268;&#20132;&#26131;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#37266;&#23545;&#20854;&#24212;&#24403;&#35880;&#24910;&#20351;&#29992;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13408</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#21644;&#20449;&#24687;&#25216;&#26415;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#21452;&#20995;&#21073;&#65306;&#24320;&#25918;&#38134;&#34892;&#30340;&#19968;&#20010;&#35686;&#31034;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking. (arXiv:2307.13408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#38134;&#34892;&#20316;&#20026;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#25216;&#26415;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#65289;&#25152;&#24102;&#26469;&#30340;&#20844;&#24179;&#24615;&#38544;&#24739;&#12290;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#33030;&#24369;&#24615;&#30340;&#32500;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#33268;&#20132;&#26131;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#37266;&#23545;&#20854;&#24212;&#24403;&#35880;&#24910;&#20351;&#29992;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#21644;&#23637;&#31034;&#20102;&#30475;&#20284;&#20013;&#31435;&#30340;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#25216;&#26415;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#65289;&#22312;&#24320;&#25918;&#38134;&#34892;&#31561;&#39046;&#22495;&#23545;&#20844;&#24179;&#24615;&#30340;&#38544;&#34255;&#24433;&#21709;&#12290;&#24320;&#25918;&#38134;&#34892;&#22312;&#37329;&#34701;&#26381;&#21153;&#39046;&#22495;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#20026;&#23458;&#25143;&#33719;&#21462;&#12289;&#31649;&#29702;&#12289;&#20445;&#30041;&#21644;&#39118;&#38505;&#35780;&#20272;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20132;&#26131;&#25968;&#25454;&#30340;&#32454;&#33268;&#31243;&#24230;&#21487;&#33021;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#26410;&#34987;&#27880;&#24847;&#30340;&#25935;&#24863;&#21644;&#31105;&#27490;&#24615;&#29305;&#24449;&#30340;&#20195;&#29702;&#21487;&#33021;&#23548;&#33268;&#38388;&#25509;&#27495;&#35270;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20844;&#24179;&#35299;&#37322;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#37329;&#34701;&#33030;&#24369;&#24615;&#65288;FV&#65289;&#30340;&#32500;&#24230;&#65292;&#36825;&#26159;COVID-19&#21644;&#36890;&#32960;&#19978;&#21319;&#24102;&#26469;&#30340;&#20840;&#29699;&#20851;&#27880;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#23548;&#33268;FV&#30340;&#34892;&#20026;&#22240;&#32032;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#24433;&#21709;&#22788;&#20110;&#39118;&#38505;&#32676;&#20307;&#20013;&#30340;&#24369;&#21183;&#32676;&#20307;&#12290;&#20351;&#29992;&#26469;&#33258;&#33521;&#22269;&#19968;&#23478;&#37329;&#34701;&#31185;&#25216;&#20511;&#36151;&#20844;&#21496;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#31890;&#24230;&#20132;&#26131;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#23545;&#20854;&#25552;&#20986;&#20102;&#35686;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#25628;&#32034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13390</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#28508;&#31354;&#38388;&#30340;&#25628;&#32034;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#25628;&#32034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#26159;&#29992;&#20110;&#35299;&#20915;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24037;&#20855;&#65306;1. &#26159;&#20160;&#20040;&#20851;&#38190;&#22240;&#32032;&#23548;&#33268;&#20102;&#33258;&#21160;&#39044;&#27979;/&#20915;&#31574;&#65311;2. &#22914;&#20309;&#25913;&#21464;&#36825;&#20123;&#22240;&#32032;&#20197;&#20174;&#29992;&#25143;&#35282;&#24230;&#33719;&#24471;&#26356;&#26377;&#21033;&#30340;&#32467;&#26524;&#65311;&#22240;&#27492;&#65292;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#21644;&#26131;&#20110;&#23454;&#29616;&#30340;&#21487;&#34892;&#21464;&#21270;&#26469;&#24341;&#23548;&#29992;&#25143;&#19982;AI&#31995;&#32479;&#30340;&#20132;&#20114;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#37319;&#29992;&#21644;&#38271;&#26399;&#25509;&#21463;AI&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;CEs&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#19981;&#21516;&#30340;&#36136;&#37327;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CEs&#30340;&#29983;&#25104;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#24314;&#35758;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#27492;&#19981;&#21487;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23558;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#24418;&#25104;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#65292;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#20108;&#20998;&#31867;&#22120;&#29983;&#25104;CEs&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#26426;&#22120;&#20154;&#36134;&#21495;&#30340;&#34892;&#20026;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#36134;&#21495;&#65292;&#24182;&#21019;&#24314;&#20102;&#39640;&#25928;&#30340;BotHawk&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2307.13386</link><description>&lt;p&gt;
BotHawk&#65306;&#19968;&#20010;&#29992;&#20110;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#26816;&#27979;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BotHawk: An Approach for Bots Detection in Open Source Software Projects. (arXiv:2307.13386v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#26426;&#22120;&#20154;&#36134;&#21495;&#30340;&#34892;&#20026;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#36134;&#21495;&#65292;&#24182;&#21019;&#24314;&#20102;&#39640;&#25928;&#30340;BotHawk&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32534;&#30721;&#24179;&#21488;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#24443;&#24213;&#25913;&#21464;&#20102;&#21512;&#20316;&#26041;&#24335;&#65292;&#23548;&#33268;&#20351;&#29992;&#36719;&#20214;&#26426;&#22120;&#20154;&#26469;&#31616;&#21270;&#36816;&#33829;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#36719;&#20214;&#65288;OSS&#65289;&#26426;&#22120;&#20154;&#30340;&#23384;&#22312;&#24102;&#26469;&#20102;&#20266;&#35013;&#12289;&#22403;&#22334;&#37038;&#20214;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#39118;&#38505;&#31561;&#38382;&#39064;&#12290;&#22312;OSS&#39033;&#30446;&#20013;&#35782;&#21035;&#26426;&#22120;&#20154;&#36134;&#21495;&#21644;&#34892;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#24182;&#20197;&#23613;&#21487;&#33021;&#39640;&#30340;&#20934;&#30830;&#24615;&#35782;&#21035;&#26426;&#22120;&#20154;&#36134;&#21495;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#25910;&#38598;&#20102;&#19968;&#32452;&#28385;&#36275;&#26631;&#20934;&#21270;&#26631;&#20934;&#30340;19,779&#20010;&#36134;&#21495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#26410;&#26469;&#23545;&#24320;&#28304;&#39033;&#30446;&#20013;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#36981;&#24490;&#20005;&#26684;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#30830;&#20445;&#25105;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#20934;&#30830;&#12289;&#20855;&#26377;&#26222;&#36866;&#24615;&#12289;&#21487;&#25193;&#23637;&#24182;&#20445;&#25345;&#26368;&#26032;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#26426;&#22120;&#20154;&#36134;&#21495;&#22312;5&#20010;&#32500;&#24230;&#30340;17&#20010;&#29305;&#24449;&#19978;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#24050;&#32463;&#30830;&#23450;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#36134;&#21495;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#21019;&#24314;&#20102;BotHawk&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#39640;&#25928;&#26816;&#27979;&#26426;&#22120;&#20154;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations. However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks. Identifying bot accounts and behavior is a challenging task in the OSS project. This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy. Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects. We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date. We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions. Our team created BotHawk, a highly effective model for detecting bots in open-source software proj
&lt;/p&gt;</description></item><item><title>Scaff-PD&#26159;&#19968;&#20010;&#39640;&#25928;&#36890;&#20449;&#12289;&#20844;&#24179;&#21450;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#19968;&#31995;&#21015;&#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#40065;&#26834;&#30446;&#26631;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#21033;&#29992;&#29305;&#27530;&#32467;&#26500;&#21644;&#21152;&#36895;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Scaff-PD&#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25928;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;Scaff-PD&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13381</link><description>&lt;p&gt;
Scaff-PD:&#39640;&#25928;&#29575;&#36890;&#20449;&#12289;&#20844;&#24179;&#21450;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scaff-PD: Communication Efficient Fair and Robust Federated Learning. (arXiv:2307.13381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13381
&lt;/p&gt;
&lt;p&gt;
Scaff-PD&#26159;&#19968;&#20010;&#39640;&#25928;&#36890;&#20449;&#12289;&#20844;&#24179;&#21450;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#19968;&#31995;&#21015;&#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#40065;&#26834;&#30446;&#26631;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#21033;&#29992;&#29305;&#27530;&#32467;&#26500;&#21644;&#21152;&#36895;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Scaff-PD&#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25928;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;Scaff-PD&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Scaff-PD&#30340;&#24555;&#36895;&#21644;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#19968;&#31995;&#21015;&#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#40065;&#26834;&#30446;&#26631;&#26469;&#25913;&#21892;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30446;&#26631;&#30340;&#29305;&#27530;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21152;&#36895;&#30340;&#21407;&#22987;-&#23545;&#20598;&#65288;APD&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20462;&#27491;&#20559;&#24046;&#30340;&#23616;&#37096;&#27493;&#39588;&#65288;&#22914;Scaffold&#65289;&#20197;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Scaff-PD&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Scaff-PD&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
&lt;/p&gt;</description></item><item><title>&#23376;&#27169;&#22359;&#24378;&#21270;&#23398;&#20064;(SubRL)&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#38750;&#21487;&#21152;&#22870;&#21169;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#23376;&#27169;&#22359;&#38598;&#21512;&#20989;&#25968;&#26469;&#24314;&#27169;&#36882;&#20943;&#22238;&#25253;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SubRL&#30340;&#31616;&#21333;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;SubPO&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#31867;&#22411;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2307.13372</link><description>&lt;p&gt;
&#23376;&#27169;&#22359;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Submodular Reinforcement Learning. (arXiv:2307.13372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13372
&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#22359;&#24378;&#21270;&#23398;&#20064;(SubRL)&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#38750;&#21487;&#21152;&#22870;&#21169;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#23376;&#27169;&#22359;&#38598;&#21512;&#20989;&#25968;&#26469;&#24314;&#27169;&#36882;&#20943;&#22238;&#25253;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SubRL&#30340;&#31616;&#21333;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;SubPO&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#31867;&#22411;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#29366;&#24577;&#30340;&#22870;&#21169;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#21152;&#30340;&#65292;&#24182;&#19988;&#26681;&#25454;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#23427;&#20204;&#19982;&#20043;&#21069;&#35775;&#38382;&#30340;&#29366;&#24577;$\textit{&#29420;&#31435;}$&#12290;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#65292;&#22914;&#35206;&#30422;&#25511;&#21046;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#22870;&#21169;&#33258;&#28982;&#20855;&#26377;&#36882;&#20943;&#22238;&#25253;&#65292;&#21363;&#20854;&#20215;&#20540;&#38543;&#20043;&#21069;&#35775;&#38382;&#36807;&#30340;&#30456;&#20284;&#29366;&#24577;&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{&#23376;&#27169;&#22359;&#24378;&#21270;&#23398;&#20064;}$ (SubRL) &#65292;&#36825;&#19968;&#33539;&#24335;&#26088;&#22312;&#36890;&#36807;&#23376;&#27169;&#22359;&#38598;&#21512;&#20989;&#25968;&#26469;&#24314;&#27169;&#36882;&#20943;&#22238;&#25253;&#65292;&#20174;&#32780;&#20248;&#21270;&#26356;&#19968;&#33324;&#30340;&#38750;&#21487;&#21152;&#22870;&#21169;&#65288;&#21382;&#21490;&#30456;&#20851;&#65289;&#12290;&#28982;&#32780;&#65292;&#19981;&#24184;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20248;&#21270;&#38382;&#39064;&#24456;&#38590;&#36817;&#20284;&#35299;&#20915;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21463;&#32463;&#20856;&#23376;&#27169;&#22359;&#20248;&#21270;&#20013;&#36138;&#23146;&#31639;&#27861;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SubPO&#65292;&#19968;&#31181;&#29992;&#20110;SubRL&#30340;&#31616;&#21333;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#22320;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#22788;&#29702;&#38750;&#21487;&#21152;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BALLET&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#21644;&#38750;&#24179;&#31283;&#22330;&#26223;&#19979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#23427;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#19968;&#20010;&#31895;&#31961;&#30340;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65292;&#19968;&#20010;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#20248;&#21270;&#35813;&#21306;&#22495;&#12290;BALLET&#33021;&#22815;&#26377;&#25928;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#30340;&#26080;&#24863;&#20852;&#36259;&#21306;&#22495;&#36807;&#28388;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20855;&#26377;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.13371</link><description>&lt;p&gt;
&#23398;&#20064;&#36866;&#24212;&#24615;&#27700;&#24179;&#38598;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#24863;&#20852;&#36259;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation. (arXiv:2307.13371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13371
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BALLET&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#21644;&#38750;&#24179;&#31283;&#22330;&#26223;&#19979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#23427;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#19968;&#20010;&#31895;&#31961;&#30340;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65292;&#19968;&#20010;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#20248;&#21270;&#35813;&#21306;&#22495;&#12290;BALLET&#33021;&#22815;&#26377;&#25928;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#30340;&#26080;&#24863;&#20852;&#36259;&#21306;&#22495;&#36807;&#28388;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20855;&#26377;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#21644;&#38750;&#24179;&#31283;&#22330;&#26223;&#19979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#29616;&#26377;&#30340;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;BALLET&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#38750;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#65288;&#22914;&#39640;&#26031;&#36807;&#31243;&#65289;&#30340;&#36229;&#32423;&#32423;&#38598;&#30340;&#39640;&#32622;&#20449;&#24230;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#20026;&#33258;&#36866;&#24212;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#35843;&#25972;&#65292;&#24182;&#19988;&#33021;&#22815;&#19987;&#27880;&#20110;&#21487;&#20197;&#36890;&#36807;&#29616;&#26377;&#30340;BO&#26041;&#27861;&#35299;&#20915;&#30340;&#20248;&#21270;&#31354;&#38388;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#27169;&#22411;&#65306;&#19968;&#20010;&#31895;&#31961;&#30340;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#35782;&#21035;ROI&#65292;&#19968;&#20010;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;ROI&#20869;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;BALLET&#21487;&#20197;&#39640;&#25928;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#30340;&#26080;ROI&#36807;&#28388;&#30340;BO&#20855;&#26377;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;BALLET&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#21452;&#35268;&#21017;&#21270;Wasserstein&#37325;&#24515;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#38459;&#23612;Sinkhorn&#36845;&#20195;&#21644;&#31934;&#30830;&#30340;&#26368;&#22823;&#21270;/&#26368;&#23567;&#21270;&#27493;&#39588;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;&#27492;&#31639;&#27861;&#30340;&#38750;&#31934;&#30830;&#21464;&#20307;&#20351;&#29992;&#36817;&#20284;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#23454;&#29616;&#65292;&#22312;&#33258;&#30001;&#25903;&#25745;/&#32593;&#26684;&#33258;&#30001;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38750;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.13370</link><description>&lt;p&gt;
&#36890;&#36807;&#38459;&#23612;Sinkhorn&#36845;&#20195;&#23454;&#29616;&#21452;&#29109;Wasserstein&#37325;&#24515;&#30340;&#35745;&#31639;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations. (arXiv:2307.13370v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#21452;&#35268;&#21017;&#21270;Wasserstein&#37325;&#24515;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#38459;&#23612;Sinkhorn&#36845;&#20195;&#21644;&#31934;&#30830;&#30340;&#26368;&#22823;&#21270;/&#26368;&#23567;&#21270;&#27493;&#39588;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;&#27492;&#31639;&#27861;&#30340;&#38750;&#31934;&#30830;&#21464;&#20307;&#20351;&#29992;&#36817;&#20284;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#23454;&#29616;&#65292;&#22312;&#33258;&#30001;&#25903;&#25745;/&#32593;&#26684;&#33258;&#30001;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38750;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21452;&#35268;&#21017;&#21270;Wasserstein&#37325;&#24515;&#30340;&#35745;&#31639;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#30001;&#20869;&#37096;&#21644;&#22806;&#37096;&#35268;&#21017;&#21270;&#24378;&#24230;&#25511;&#21046;&#30340;&#29109;&#37325;&#24515;&#26063;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21508;&#31181;&#35268;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#32479;&#19968;&#20102;&#20960;&#20010;&#29109;&#24809;&#32602;&#37325;&#24515;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26032;&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#20559;&#24046;&#37325;&#24515;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#35745;&#31639;&#21452;&#35268;&#21017;&#21270;Wasserstein&#37325;&#24515;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#22522;&#20110;&#38459;&#23612;Sinkhorn&#36845;&#20195;&#65292;&#28982;&#21518;&#26159;&#31934;&#30830;&#30340;&#26368;&#22823;&#21270;/&#26368;&#23567;&#21270;&#27493;&#39588;&#65292;&#23545;&#20219;&#20309;&#35268;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#37117;&#20445;&#35777;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#38750;&#31934;&#30830;&#21464;&#20307;&#65292;&#21487;&#20197;&#20351;&#29992;&#36817;&#20284;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#26469;&#23454;&#29616;&#65292;&#22312;&#33258;&#30001;&#25903;&#25745;/&#32593;&#26684;&#33258;&#30001;&#35774;&#32622;&#20013;&#20026;&#36817;&#20284;Wasserstein&#37325;&#24515;&#20043;&#38388;&#30340;&#31163;&#25955;&#28857;&#20113;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38750;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#12289;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26680;&#24515;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#39640;&#32500;&#21322;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#20855;&#26377;&#26497;&#23567;&#26497;&#20540;&#32479;&#35745;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13352</link><description>&lt;p&gt;
&#39640;&#32500;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#12289;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26680;&#24515;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#39640;&#32500;&#21322;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#20855;&#26377;&#26497;&#23567;&#26497;&#20540;&#32479;&#35745;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#25308;&#21344;&#24237;&#25925;&#38556;&#30340;&#24378;&#40065;&#26834;&#20998;&#24067;&#24335;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#38480;&#21046;&#65292;&#38543;&#30528;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#20005;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#12289;&#22312;&#20219;&#24847;&#25968;&#37327;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#19979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#39640;&#32500;&#21322;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#39318;&#20808;&#35782;&#21035;&#19968;&#20010;&#23376;&#31354;&#38388;&#65292;&#36890;&#36807;&#24037;&#20316;&#26426;&#19978;&#20256;&#30340;&#26799;&#24230;&#21521;&#37327;&#20272;&#35745;&#19982;&#35813;&#23376;&#31354;&#38388;&#22402;&#30452;&#30340;&#22343;&#20540;&#20998;&#37327;&#65292;&#32780;&#36890;&#36807;&#36741;&#21161;&#25968;&#25454;&#38598;&#20272;&#35745;&#35813;&#23376;&#31354;&#38388;&#20869;&#30340;&#22343;&#20540;&#20998;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#29992;&#20316;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#32858;&#21512;&#22120;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#26032;&#26041;&#27861;&#20855;&#26377;&#26497;&#23567;&#26497;&#20540;&#32479;&#35745;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compar
&lt;/p&gt;</description></item><item><title>DT-Sampler&#26159;&#19968;&#31181;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13333</link><description>&lt;p&gt;
&#22522;&#20110;&#20915;&#31574;&#26641;&#25277;&#26679;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Feature Importance Measurement based on Decision Tree Sampling. (arXiv:2307.13333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13333
&lt;/p&gt;
&lt;p&gt;
DT-Sampler&#26159;&#19968;&#31181;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#26641;&#30340;&#38543;&#26426;&#29983;&#25104;&#20351;&#24471;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#21463;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DT-Sampler&#65292;&#19968;&#31181;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#29992;&#20110;&#22312;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20013;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#38543;&#26426;&#26862;&#26519;&#30340;&#21442;&#25968;&#26356;&#23569;&#65292;&#24182;&#20026;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#20998;&#26512;&#25552;&#20379;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;DT-Sampler&#30340;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/tsudalab/DT-sampler&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.13332</link><description>&lt;p&gt;
&#22312;&#38169;&#35823;&#25351;&#23450;&#30340;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30693;&#36947;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#38169;&#35823;&#25351;&#23450;&#20013;&#20250;&#20986;&#29616;&#20056;&#27861;&#25918;&#22823;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;\emph{&#36924;&#36817;&#22240;&#23376;}&#30340;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#26368;&#20339;&#24418;&#24335;&#65292;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#24191;&#27867;&#35774;&#32622;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20854;&#20013;&#20173;&#26377;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20363;&#22914;&#21152;&#26435;$L_2$&#33539;&#25968;&#65288;&#20854;&#20013;&#21152;&#26435;&#26159;&#31163;&#32447;&#29366;&#24577;&#20998;&#24067;&#65289;&#65292;$L_\infty$&#33539;&#25968;&#65292;&#29366;&#24577;&#21035;&#21517;&#30340;&#23384;&#22312;&#19982;&#21542;&#20197;&#21450;&#23545;&#29366;&#24577;&#31354;&#38388;&#30340;&#20840;&#38754;&#19982;&#37096;&#20998;&#35206;&#30422;&#12290;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65288;&#33267;&#22810;&#24120;&#25968;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#30830;&#23450;&#20102;$L_2(\mu)$&#33539;&#25968;&#30340;&#20004;&#20010;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22240;&#23376;&#21644;$L_\infty$&#33539;&#25968;&#30340;&#19968;&#20010;&#22240;&#23376;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;QuIP&#65292;&#36890;&#36807;&#20351;&#26435;&#37325;&#21644;Hessian&#30697;&#38453;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#32463;&#36807;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#22312;&#20165;&#20351;&#29992;&#20004;&#27604;&#29305;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#21487;&#34892;&#30340;LLM&#37327;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13304</link><description>&lt;p&gt;
QuIP&#65306;&#20855;&#26377;&#20445;&#35777;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;2&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;QuIP&#65292;&#36890;&#36807;&#20351;&#26435;&#37325;&#21644;Hessian&#30697;&#38453;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#32463;&#36807;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#22312;&#20165;&#20351;&#29992;&#20004;&#27604;&#29305;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#21487;&#34892;&#30340;LLM&#37327;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#21518;&#21442;&#25968;&#37327;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#65288;QuIP&#65289;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35265;&#35299;&#65306;&#37327;&#21270;&#20174;&#19981;&#30456;&#20851;&#30340;&#26435;&#37325;&#21644; Hessian &#30697;&#38453;&#20013;&#25910;&#30410;&#65292;&#21363;&#36890;&#36807;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#33293;&#20837;&#20026;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#33719;&#21462;&#37325;&#35201;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;QuIP &#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#26368;&#23567;&#21270;&#20108;&#27425;&#36817;&#20284;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#33293;&#20837;&#36807;&#31243;&#65307;&#65288;2&#65289;&#36890;&#36807;&#19982;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#30456;&#20056;&#26469;&#30830;&#20445;&#26435;&#37325;&#21644; Hessian &#26080;&#20851;&#30340;&#39640;&#25928;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#31532;&#19968;&#27425;&#38024;&#23545; LLM &#35268;&#27169;&#30340;&#37327;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20063;&#36866;&#29992;&#20110;&#29616;&#26377;&#26041;&#27861; OPTQ&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26080;&#20851;&#39044;&#22788;&#29702;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#22810;&#20010;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#20010;&#26435;&#37325;2&#27604;&#29305;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#20462;&#25913;&#35757;&#32451;&#26041;&#21521;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#21644;&#32479;&#35745;&#29702;&#35770;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#24635;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.13290</link><description>&lt;p&gt;
&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#20462;&#25913;&#35757;&#32451;&#26041;&#21521;&#20197;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Modify Training Directions in Function Space to Reduce Generalization Error. (arXiv:2307.13290v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#20462;&#25913;&#35757;&#32451;&#26041;&#21521;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#21644;&#32479;&#35745;&#29702;&#35770;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#24635;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#31354;&#38388;&#20013;&#22522;&#20110;&#31070;&#32463;&#20999;&#25442;&#26680;&#21644;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#35299;&#30340;&#20462;&#25913;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#22312;&#39640;&#26031;&#20998;&#24067;&#21644;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#30340;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#29702;&#35770;&#26041;&#27861;&#20174;&#29305;&#24449;&#20998;&#35299;&#21644;&#32479;&#35745;&#29702;&#35770;&#20013;&#26174;&#24335;&#25512;&#23548;&#20986;&#35813;&#20462;&#25913;&#33258;&#28982;&#26799;&#24230;&#25152;&#23398;&#20064;&#30340;&#20989;&#25968;&#30340;&#34920;&#36798;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#24635;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#35299;&#20026;&#20989;&#25968;&#31354;&#38388;&#20013;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#30340;&#35823;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#35757;&#32451;&#38598;&#35823;&#24046;&#21644;&#35757;&#32451;&#38598;&#19982;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#24046;&#24322;&#30340;&#20934;&#21017;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#26041;&#21521;&#20250;&#23548;&#33268;&#24635;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13275</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;DNN&#27169;&#22411;&#30340;&#26368;&#24120;&#29992;&#29305;&#24449;&#34920;&#31034;&#22522;&#20110;SMILES&#21644;&#20998;&#23376;&#22270;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#31616;&#27905;&#39640;&#25928;&#65292;&#20294;&#20063;&#38480;&#21046;&#20102;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837; Ricci &#26354;&#29575;&#31163;&#25955;&#21270;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23558;&#26354;&#29575;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#26399;&#38388;&#65292;&#25105;&#20204;&#23558;&#22270;&#30340;&#26354;&#29575;&#20449;&#24687;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#28155;&#21152;&#21040;&#33410;&#28857;&#29305;&#24449;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#32593;&#32476;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26354;&#29575;&#20449;&#24687;&#24341;&#20837;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#26367;&#25442;&#21333;&#20803;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#39640;&#25928;&#30340;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.13270</link><description>&lt;p&gt;
&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unbiased Weight Maximization. (arXiv:2307.13270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#26367;&#25442;&#21333;&#20803;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#39640;&#25928;&#30340;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#29983;&#29289;&#23398;&#21512;&#29702;&#30340;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;&#32593;&#32476;&#35270;&#20026;&#20195;&#29702;&#22242;&#38431;&#12290;&#22240;&#27492;&#65292;&#25152;&#26377;&#21333;&#20803;&#37117;&#21487;&#20197;&#36890;&#36807;REINFORCE&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#36827;&#34892;&#35843;&#33410;&#65292;&#26356;&#21152;&#31526;&#21512;&#29983;&#29289;&#35266;&#23519;&#21040;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36895;&#24230;&#36739;&#24930;&#65292;&#19988;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#22823;&#32780;&#25193;&#23637;&#24615;&#36739;&#24046;&#65292;&#22240;&#20026;&#21333;&#20010;&#22870;&#21169;&#20449;&#21495;&#34987;&#24191;&#25773;&#32473;&#25152;&#26377;&#21333;&#20803;&#32780;&#19981;&#32771;&#34385;&#20010;&#20307;&#36129;&#29486;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#37327;&#26368;&#22823;&#21270;&#65292;&#29992;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#26367;&#25442;&#21333;&#20803;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#20801;&#35768;&#27599;&#20010;&#38544;&#34255;&#21333;&#20803;&#26368;&#22823;&#21270;&#20986;&#31449;&#26435;&#37325;&#30340;&#33539;&#25968;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#37325;&#37327;&#26368;&#22823;&#21270;&#30340;&#29702;&#35770;&#23646;&#24615;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#65292;&#26080;&#20559;&#37325;&#37327;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximizati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#21452;&#20998;&#35299;&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;K&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25110;&#35745;&#31639;&#25928;&#29575;&#12290;&#35770;&#25991;&#36890;&#36807;&#23545;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#35268;&#21010;&#26041;&#27861;&#30340;K&#22343;&#20540;&#32858;&#31867;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.13267</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#20998;&#35299;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#32852;&#37030;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization. (arXiv:2307.13267v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#21452;&#20998;&#35299;&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;K&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25110;&#35745;&#31639;&#25928;&#29575;&#12290;&#35770;&#25991;&#36890;&#36807;&#23545;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#35268;&#21010;&#26041;&#27861;&#30340;K&#22343;&#20540;&#32858;&#31867;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#20351;&#29992;&#21487;&#20197;&#36890;&#36807;&#20445;&#25252;&#38544;&#31169;&#25110;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#26469;&#36827;&#34892;&#21160;&#26426;&#12290;&#19968;&#26041;&#38754;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#23384;&#20648;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#21482;&#33021;&#35775;&#38382;&#20854;&#20445;&#23494;&#25968;&#25454;&#30340;&#32593;&#32476;&#20013;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#21363;&#20351;&#25968;&#25454;&#19981;&#26159;&#26426;&#23494;&#30340;&#65292;&#30001;&#20110;&#24102;&#23485;&#38480;&#21046;&#65292;&#20849;&#20139;&#25968;&#25454;&#21487;&#33021;&#26159;&#31105;&#27490;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#26029;&#22686;&#21152;&#30340;&#21487;&#29992;&#25968;&#25454;&#37327;&#23548;&#33268;&#20102;&#22823;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#21106;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;&#26412;&#25991;&#26088;&#22312;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;&#21452;&#20998;&#35299;&#26469;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;K&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#12290;&#22312;&#23545;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#21518;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#35268;&#21010;&#30340;K&#22343;&#20540;&#32858;&#31867;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering traini
&lt;/p&gt;</description></item><item><title>&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#27491;&#26631;&#31614;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SFPL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#26469;&#25913;&#21892;&#22810;&#31867;&#21035;&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13266</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#27491;&#26631;&#31614;&#30340;&#36164;&#28304;&#21463;&#38480;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Split Learning with Only Positive Labels for resource-constrained IoT environment. (arXiv:2307.13266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13266
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#27491;&#26631;&#31614;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SFPL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#26469;&#25913;&#21892;&#22810;&#31867;&#21035;&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;DCML&#65289;&#26159;&#29289;&#32852;&#32593;&#39046;&#22495;&#20013;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#28857;&#26159;&#36890;&#36807;&#28040;&#38500;&#21407;&#22987;&#25968;&#25454;&#30340;&#38598;&#20013;&#32858;&#21512;&#26469;&#25913;&#21892;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20063;&#20026;&#20855;&#26377;&#20302;&#35745;&#31639;&#33021;&#21147;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#25552;&#20379;&#21160;&#21147;&#12290;&#22312;DCML&#26694;&#26550;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#20013;&#65292;&#31216;&#20026;splitfed&#23398;&#20064;&#65288;SFL&#65289;&#30340;&#32852;&#37030;&#20998;&#21106;&#23398;&#20064;&#26159;&#22312;&#35774;&#22791;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#26102;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21482;&#26377;&#27491;&#26631;&#35760;&#25968;&#25454;&#26102;&#65292;SFL&#20013;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#25910;&#25947;&#25110;&#25552;&#20379;&#27425;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#27491;&#26631;&#31614;&#30340;splitfed&#23398;&#20064;&#65288;SFPL&#65289;&#12290;SFPL&#22312;&#23558;&#23458;&#25143;&#31471;&#25509;&#25910;&#21040;&#30340;&#30772;&#30862;&#25968;&#25454;&#25552;&#20379;&#32473;&#26381;&#21153;&#22120;&#20043;&#21069;&#65292;&#23545;&#20854;&#24212;&#29992;&#38543;&#26426;&#27927;&#29260;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#19982;&#21327;&#35843;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#35843;&#33410;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#36890;&#36807;&#25552;&#39640;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#30340;&#25928;&#29575;&#26469;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.13256</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#19982;&#21327;&#35843;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Structural Credit Assignment with Coordinated Exploration. (arXiv:2307.13256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#19982;&#21327;&#35843;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#35843;&#33410;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#36890;&#36807;&#25552;&#39640;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#30340;&#25928;&#29575;&#26469;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21512;&#29702;&#30340;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#30340;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#21333;&#20803;&#35270;&#20026;&#19968;&#20010;&#38543;&#26426;&#24378;&#21270;&#23398;&#20064;(RL)&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;&#32593;&#32476;&#35270;&#20026;&#20195;&#29702;&#22242;&#38431;&#12290;&#22240;&#27492;&#65292;&#25152;&#26377;&#21333;&#20803;&#37117;&#21487;&#20197;&#36890;&#36807;REINFORCE&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20840;&#23616;&#22870;&#21169;&#20449;&#21495;&#35843;&#33410;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#26356;&#25509;&#36817;&#29983;&#29289;&#35266;&#23519;&#21040;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#36739;&#24930;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#36866;&#24212;&#32593;&#32476;&#30340;&#35268;&#27169;&#12290;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#20027;&#35201;&#26159;&#30001;&#20004;&#20010;&#22240;&#32032;&#36896;&#25104;&#30340;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#65306;(i)&#25152;&#26377;&#21333;&#20803;&#29420;&#31435;&#25506;&#32034;&#32593;&#32476;&#65292;(ii)&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;&#26469;&#35780;&#20272;&#25152;&#26377;&#21333;&#20803;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#26088;&#22312;&#25913;&#21892;&#32467;&#26500;&#21270;&#20449;&#29992;&#20998;&#37197;&#30340;&#26041;&#27861;&#36890;&#24120;&#21487;&#20998;&#20026;&#20004;&#31867;&#12290;&#31532;&#19968;&#31867;&#21253;&#25324;&#20801;&#35768;&#21333;&#20803;&#20043;&#38388;&#36827;&#34892;&#21327;&#35843;&#25506;&#32034;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;MAP&#20256;&#25773;&#12290;&#31532;&#20108;&#31867;&#28085;&#30422;&#20102;&#35745;&#31639;&#32467;&#26500;&#24615;&#20449;&#29992;&#20998;&#37197;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that comput
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36136;&#37327;&#25554;&#20540;&#26041;&#27861;&#21019;&#36896;&#20102;&#36830;&#32493;&#24322;&#24120;&#24230;&#26631;&#31614;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#24322;&#24120;&#27745;&#26579;&#21644;&#31163;&#25955;&#30417;&#30563;&#20449;&#24687;&#21033;&#29992;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13239</link><description>&lt;p&gt;
RoSAS:&#20855;&#26377;&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision. (arXiv:2307.13239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36136;&#37327;&#25554;&#20540;&#26041;&#27861;&#21019;&#36896;&#20102;&#36830;&#32493;&#24322;&#24120;&#24230;&#26631;&#31614;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#24322;&#24120;&#27745;&#26579;&#21644;&#31163;&#25955;&#30417;&#30563;&#20449;&#24687;&#21033;&#29992;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21033;&#29992;&#19968;&#20123;&#24322;&#24120;&#26679;&#26412;&#65292;&#19982;&#26080;&#30417;&#30563;&#27169;&#22411;&#30456;&#27604;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1) &#26410;&#26631;&#35760;&#30340;&#24322;&#24120;&#65288;&#21363;&#24322;&#24120;&#27745;&#26579;&#65289;&#21487;&#33021;&#22312;&#23558;&#25152;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#29992;&#20316;&#20869;&#28857;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26102;&#35823;&#23548;&#23398;&#20064;&#36807;&#31243;; 2) &#21482;&#21033;&#29992;&#31163;&#25955;&#30340;&#30417;&#30563;&#20449;&#24687;&#65288;&#22914;&#20108;&#36827;&#21046;&#25110;&#39034;&#24207;&#25968;&#25454;&#26631;&#31614;&#65289;&#65292;&#36825;&#23548;&#33268;&#24322;&#24120;&#20998;&#25968;&#30340;&#23376;&#20248;&#23398;&#20064;&#65292;&#23454;&#36136;&#19978;&#37319;&#29992;&#36830;&#32493;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;"&#25239;&#27745;&#26579;&#36830;&#32493;&#30417;&#30563;&#20449;&#21495;"&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#25554;&#20540;&#26041;&#27861;&#26469;&#25193;&#25955;&#26631;&#35760;&#24322;&#24120;&#30340;&#24322;&#24120;&#31243;&#24230;&#65292;&#20174;&#32780;&#21019;&#24314;&#24102;&#26377;&#36830;&#32493;&#24322;&#24120;&#24230;&#26631;&#31614;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#32452;&#21512;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#24322;&#24120;&#31243;&#24230;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#35206;&#30422;&#21463;&#27745;&#26579;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#30340;&#21464;&#24418;&#22120; (AuTR) &#26041;&#27861;&#26469;&#35299;&#20915;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#24863;&#21463;&#37326;&#23567;&#21644;&#29305;&#24449;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#21464;&#24418;&#22120;&#26550;&#26500;&#20197;&#21450;&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#30340;&#21464;&#24418;&#22120;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#34701;&#21512;&#21644;&#32858;&#21512;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#22768;&#38899;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13236</link><description>&lt;p&gt;
&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#21464;&#24418;&#22120;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation. (arXiv:2307.13236v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#30340;&#21464;&#24418;&#22120; (AuTR) &#26041;&#27861;&#26469;&#35299;&#20915;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#24863;&#21463;&#37326;&#23567;&#21644;&#29305;&#24449;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#21464;&#24418;&#22120;&#26550;&#26500;&#20197;&#21450;&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#30340;&#21464;&#24418;&#22120;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#34701;&#21512;&#21644;&#32858;&#21512;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#22768;&#38899;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#38899;&#39057;&#32447;&#32034;&#22312;&#35270;&#39057;&#24103;&#20013;&#23545;&#21457;&#22768;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#34701;&#21512;&#30340;&#26041;&#27861;&#30001;&#20110;&#21367;&#31215;&#30340;&#23567;&#24863;&#21463;&#37326;&#21644;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#19981;&#20805;&#20998;&#30340;&#34701;&#21512;&#32780;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#30340;&#21464;&#24418;&#22120;&#65288;AuTR&#65289;&#26469;&#24212;&#23545;&#36825;&#20010;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21464;&#24418;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#28145;&#24230;&#34701;&#21512;&#21644;&#32858;&#21512;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22768;&#38899;&#24863;&#30693;&#30340;&#26597;&#35810;&#22686;&#24378;&#30340;&#21464;&#24418;&#22120;&#35299;&#30721;&#22120;&#65292;&#26126;&#30830;&#22320;&#24110;&#21161;&#27169;&#22411;&#22312;&#22522;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#23450;&#20301;&#21457;&#22768;&#23545;&#35937;&#30340;&#20998;&#21106;&#26102;&#32858;&#28966;&#65292;&#21516;&#26102;&#24573;&#30053;&#23490;&#38745;&#20294;&#26174;&#33879;&#30340;&#23545;&#35937;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#22768;&#38899;&#21644;&#24320;&#25918;&#24615;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
&lt;/p&gt;</description></item><item><title>Spectral-DP&#26159;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39057;&#22495;&#20013;&#30340;&#26799;&#24230;&#25200;&#21160;&#19982;&#39057;&#35889;&#28388;&#27874;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26356;&#20302;&#22122;&#22768;&#27604;&#20363;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13231</link><description>&lt;p&gt;
Spectral-DP: &#36890;&#36807;&#39057;&#35889;&#25200;&#21160;&#21644;&#28388;&#27874;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering. (arXiv:2307.13231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13231
&lt;/p&gt;
&lt;p&gt;
Spectral-DP&#26159;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39057;&#22495;&#20013;&#30340;&#26799;&#24230;&#25200;&#21160;&#19982;&#39057;&#35889;&#28388;&#27874;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26356;&#20302;&#22122;&#22768;&#27604;&#20363;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#38544;&#31169;&#24230;&#37327;&#65292;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20381;&#36182;&#20110;&#19968;&#31181;&#31216;&#20026;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#30340;&#22122;&#22768;&#35757;&#32451;&#26041;&#27861;&#12290;DP-SGD&#38656;&#35201;&#22312;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#32473;&#27599;&#20010;&#26799;&#24230;&#30452;&#25509;&#28155;&#21152;&#22122;&#22768;&#65292;&#20294;&#36825;&#31181;&#38544;&#31169;&#26159;&#20197;&#26174;&#33879;&#30340;&#25928;&#29992;&#25439;&#22833;&#20026;&#20195;&#20215;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Spectral-DP&#65292;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#39057;&#22495;&#20013;&#30340;&#26799;&#24230;&#25200;&#21160;&#19982;&#39057;&#35889;&#28388;&#27874;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#20302;&#30340;&#22122;&#22768;&#27604;&#20363;&#23454;&#29616;&#25152;&#38656;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;Spectral-DP&#24320;&#21457;&#20102;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#21367;&#31215;&#23618;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20840;&#36830;&#25509;&#23618;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#22359;&#24490;&#29615;&#30340;&#31354;&#38388;&#37325;&#26500;&#19982;Spectral-DP&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#29992;&#12290;&#36890;&#36807;&#32508;&#21512;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#24182;&#25552;&#20379;&#20102;&#23454;&#26045;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#30340;&#27010;&#24565;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#24072;&#22312;&#28165;&#27905;&#25968;&#25454;&#19978;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12289;&#39044;&#27979;&#20998;&#26512;&#25110;&#32479;&#35745;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.13219</link><description>&lt;p&gt;
&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#30340;&#20837;&#38376;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Primer on the Data Cleaning Pipeline. (arXiv:2307.13219v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#30340;&#27010;&#24565;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#24072;&#22312;&#28165;&#27905;&#25968;&#25454;&#19978;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12289;&#39044;&#27979;&#20998;&#26512;&#25110;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#30340;&#21487;&#29992;&#24615;&#22823;&#24133;&#22686;&#38271;&#65292;&#20854;&#20013;&#21253;&#25324;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#12289;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#12289;&#19987;&#21033;&#25968;&#25454;&#21644;&#23454;&#26102;&#26356;&#26032;&#30340;&#35843;&#26597;&#25968;&#25454;&#31561;&#12290;&#38543;&#30528;&#36825;&#31181;&#25193;&#23637;&#65292;&#20851;&#20110;&#25968;&#25454;&#38598;&#25104;&#25110;&#32773;&#35828;&#21512;&#24182;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#32479;&#35745;&#21644;&#26041;&#27861;&#23398;&#38382;&#39064;&#20063;&#22312;&#19981;&#26029;&#22686;&#38271;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;"&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;"&#30340;&#31185;&#23398;&#21253;&#21547;&#22235;&#20010;&#38454;&#27573;&#65292;&#20351;&#20998;&#26512;&#24072;&#33021;&#22815;&#22312;&#8220;&#28165;&#27905;&#25968;&#25454;&#8221;&#19978;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#12289;&#39044;&#27979;&#20998;&#26512;&#25110;&#32479;&#35745;&#20998;&#26512;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#20171;&#32461;&#20102;&#25216;&#26415;&#26415;&#35821;&#21644;&#24120;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade. With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown. Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.'' This article provides a review of this emerging field, introducing technical terminology and commonly used methods.
&lt;/p&gt;</description></item><item><title>FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2307.13214</link><description>&lt;p&gt;
FedMEKT: &#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13214
&lt;/p&gt;
&lt;p&gt;
FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#24191;&#20041;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#21482;&#26159;&#38024;&#23545;&#21333;&#27169;&#24577;&#25968;&#25454;&#25552;&#20986;&#20102;&#20856;&#22411;&#30340;FL&#31995;&#32479;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#23545;&#20110;&#21033;&#29992;&#23453;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26410;&#26469;&#20010;&#24615;&#21270;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#30001;&#20110;&#29992;&#25143;&#26080;&#27861;&#36827;&#34892;&#33258;&#27880;&#37322;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#26377;&#38480;&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;FL&#26694;&#26550;&#65292;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#23558;&#36825;&#20010;&#27010;&#24565;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;&#26426;&#21046;&#65292;&#31216;&#20026;FedMEKT&#65292;&#23427;&#20801;&#35768;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20132;&#25442;&#20174;&#23567;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#32852;&#21512;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#35889;&#21644;&#37319;&#26679;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13206</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#35770;&#21644;&#37319;&#26679;&#29702;&#35770;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferability of Graph Neural Networks using Graphon and Sampling Theories. (arXiv:2307.13206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#35889;&#21644;&#37319;&#26679;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#22312;&#21508;&#20010;&#39046;&#22495;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#20449;&#24687;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;GNN&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#21487;&#36801;&#31227;&#24615;&#65292;&#21363;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20132;&#25442;&#26469;&#33258;&#19981;&#21516;&#22270;&#30340;&#20449;&#24687;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#19968;&#31181;&#25429;&#25417;GNN&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22270;&#35889;&#65292;&#23427;&#26159;&#23545;&#22823;&#22411;&#31264;&#23494;&#22270;&#30340;&#26497;&#38480;&#30340;&#23545;&#31216;&#21487;&#27979;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#65288;WNN&#65289;&#26550;&#26500;&#65292;&#23545;&#22270;&#35889;&#24212;&#29992;&#20110;GNN&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20197;&#25351;&#23450;&#35823;&#24046;&#23481;&#38480;&#22312;&#26368;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#19979;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#22312;&#19968;&#20010;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20004;&#23618;GNN&#22312;&#25152;&#26377;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#21152;&#26435;&#22270;&#21644;&#31616;&#21333;&#38543;&#26426;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLO-v4&#36827;&#34892;&#32958;&#23567;&#29699;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20840;&#20999;&#29255;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#32958;&#33039;H&amp;E&#21644;PAS&#22270;&#20687;&#20013;&#23454;&#29616;&#33258;&#21160;&#30340;&#32452;&#32455;&#32467;&#26500;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2307.13199</link><description>&lt;p&gt;
&#20351;&#29992;YOLO&#22312;&#32958;&#33039;H&amp;E&#21644;PAS&#22270;&#20687;&#20013;&#36827;&#34892;&#32958;&#23567;&#29699;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO. (arXiv:2307.13199v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLO-v4&#36827;&#34892;&#32958;&#23567;&#29699;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20840;&#20999;&#29255;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#32958;&#33039;H&amp;E&#21644;PAS&#22270;&#20687;&#20013;&#23454;&#29616;&#33258;&#21160;&#30340;&#32452;&#32455;&#32467;&#26500;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#36890;&#36807;&#20998;&#26512;&#32452;&#32455;&#27169;&#24335;&#21644;&#32454;&#32990;&#24418;&#24577;&#65292;&#23545;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#26159;&#20026;&#20102;&#24471;&#20986;&#35786;&#26029;&#32467;&#35770;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35780;&#20272;&#36153;&#26102;&#12289;&#26114;&#36149;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#35266;&#23519;&#32773;&#20043;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#37096;&#30340;&#21464;&#24322;&#24615;&#12290;&#30446;&#26631;&#65306;&#20026;&#20102;&#24110;&#21161;&#30149;&#29702;&#23398;&#23478;&#20351;&#29992;&#35745;&#31639;&#26426;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#24517;&#39035;&#25552;&#20986;&#33258;&#21160;&#32452;&#32455;&#32467;&#26500;&#26816;&#27979;&#21644;&#20998;&#21106;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#29983;&#25104;&#20687;&#32032;&#32423;&#23545;&#35937;&#27880;&#37322;&#20063;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#36793;&#30028;&#26694;&#26631;&#31614;&#30340;&#26816;&#27979;&#27169;&#22411;&#21487;&#33021;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35774;&#35745;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;&#26174;&#24494;&#22270;&#20687;&#30340;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#22120;YOLO-v4&#65288;You Only Look Once&#65289;&#12290;YOLO&#20351;&#29992;&#21333;&#19968;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#24863;&#20852;&#36259;&#30446;&#26631;&#30340;&#22810;&#20010;&#36793;&#30028;&#26694;&#21644;&#31867;&#21035;&#27010;&#29575;&#12290;YOLO&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20840;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#21319;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#20351;&#29992;YOLO-v4&#26469;&#36827;&#34892;&#20154;&#31867;&#32958;&#33039;&#22270;&#20687;&#20013;&#30340;&#32958;&#23567;&#29699;&#26816;&#27979;&#12290;&#22810;&#20010;&#23454;&#39564;&#34920;&#26126;YOLO-v4&#22312;&#32958;&#23567;&#29699;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;COUNTERPOL&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23545;&#31574;&#30053;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#26469;&#20998;&#26512;RL&#31574;&#30053;&#65292;&#24182;&#36798;&#21040;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;RL&#20013;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.13192</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation Policies in RL. (arXiv:2307.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;COUNTERPOL&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23545;&#31574;&#30053;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#26469;&#20998;&#26512;RL&#31574;&#30053;&#65292;&#24182;&#36798;&#21040;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;RL&#20013;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#20351;&#29992;&#22870;&#21169;&#20559;&#22909;&#30340;&#22810;&#26679;&#21270;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#30830;&#20445;&#36825;&#20123;&#26694;&#26550;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#33021;&#22815;&#35299;&#37322;&#21464;&#24471;&#24456;&#37325;&#35201;&#65292;&#21363;&#23558;&#35266;&#23519;&#26144;&#23556;&#21040;&#21487;&#33021;&#34892;&#21160;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#31574;&#30053;&#22914;&#20309;&#20197;&#23545;&#27604;&#30340;&#26041;&#24335;&#31995;&#32479;&#22320;&#29702;&#35299;&#65292;&#21363;&#65292;&#20351;&#20854;&#24615;&#33021;&#36798;&#21040;&#25152;&#38656;&#27700;&#24179;&#30340;&#31574;&#30053;&#26368;&#23567;&#25913;&#21464;&#26159;&#20160;&#20040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COUNTERPOL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#20998;&#26512;RL&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#23545;&#31574;&#30053;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#65292;&#36798;&#21040;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21453;&#20107;&#23454;&#34701;&#20837;RL&#20013;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26399;&#26395;&#25910;&#30410;&#35843;&#25511;&#30446;&#26631;&#32467;&#26524;&#65292;&#24314;&#31435;&#20102;Counterpol&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#22823;&#37327;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;EEG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#31070;&#32463;&#35760;&#24518;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#20174;EEG&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#34987;&#21484;&#22238;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13181</link><description>&lt;p&gt;
&#20351;&#29992;EEG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#31070;&#32463;&#35760;&#24518;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Neural Memory Decoding with EEG Data and Representation Learning. (arXiv:2307.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;EEG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#31070;&#32463;&#35760;&#24518;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#20174;EEG&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#34987;&#21484;&#22238;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20174;EEG&#25968;&#25454;&#20013;&#35299;&#30721;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;EEG&#27874;&#24418;&#20013;&#35782;&#21035;&#20986;&#34987;&#21484;&#22238;&#30340;&#27010;&#24565;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;78.4&#65285;&#65288;&#26426;&#20250;4&#65285;&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#19982;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#26469;&#23558;&#33041;&#27963;&#21160;&#30340;EEG&#35760;&#24405;&#26144;&#23556;&#21040;&#19968;&#20010;&#20302;&#32500;&#31354;&#38388;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#34920;&#31034;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#27010;&#24565;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#20986;&#29616;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27010;&#24565;&#37117;&#24517;&#39035;&#23384;&#22312;&#30456;&#24212;&#30340;&#21442;&#32771;EEG&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#24403;&#29992;&#25143;&#22238;&#24518;&#25991;&#26723;&#20869;&#23481;&#26102;&#25429;&#33719;EEG&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#39044;&#27979;&#25991;&#26723;&#30340;&#38142;&#25509;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#34892;&#20154;&#21644;&#33258;&#34892;&#36710;&#20107;&#25925;&#26367;&#20195;&#21697;&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#25552;&#39640;&#33030;&#24369;&#36947;&#36335;&#29992;&#25143;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13178</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#34892;&#20154;&#21644;&#33258;&#34892;&#36710;&#20107;&#25925;&#26367;&#20195;&#21697;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates. (arXiv:2307.13178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#34892;&#20154;&#21644;&#33258;&#34892;&#36710;&#20107;&#25925;&#26367;&#20195;&#21697;&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#25552;&#39640;&#33030;&#24369;&#36947;&#36335;&#29992;&#25143;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33030;&#24369;&#30340;&#36947;&#36335;&#20351;&#29992;&#32773;&#65292;&#22914;&#34892;&#20154;&#21644;&#33258;&#34892;&#36710;&#25163;&#65292;&#26356;&#23481;&#26131;&#21367;&#20837;&#19982;&#26426;&#21160;&#36710;&#36742;&#30340;&#20107;&#25925;&#20013;&#65292;&#24182;&#19988;&#19982;&#33030;&#24369;&#36947;&#36335;&#20351;&#29992;&#32773;&#26377;&#20851;&#30340;&#20107;&#25925;&#26356;&#23481;&#26131;&#23548;&#33268;&#20005;&#37325;&#20260;&#23475;&#25110;&#27515;&#20129;&#12290;&#20449;&#21495;&#28783;&#36335;&#21475;&#26159;&#33030;&#24369;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#20027;&#35201;&#23433;&#20840;&#38544;&#24739;&#65292;&#22240;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#36825;&#20123;&#36947;&#36335;&#20351;&#29992;&#32773;&#22914;&#20309;&#19982;&#26426;&#21160;&#36710;&#36742;&#20114;&#21160;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#35777;&#25454;&#30340;&#23545;&#31574;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#33021;&#30340;&#24517;&#35201;&#24615;&#12290;&#30001;&#20110;&#28041;&#21450;&#33030;&#24369;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#20107;&#25925;&#30456;&#23545;&#36739;&#23569;&#65292;&#24456;&#38590;&#29702;&#35299;&#20854;&#20013;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26159;&#35782;&#21035;&#21644;&#20351;&#29992;&#33030;&#24369;&#36947;&#36335;&#20351;&#29992;&#32773;&#19982;&#26426;&#21160;&#36710;&#36742;&#20043;&#38388;&#30340;&#20914;&#31361;&#20316;&#20026;&#23433;&#20840;&#24615;&#33021;&#30340;&#26367;&#20195;&#21697;&#12290;&#21033;&#29992;&#22522;&#20110;&#35270;&#39057;&#30340;&#31995;&#32479;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#20914;&#31361;&#26159;&#21457;&#23637;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#20197;&#25552;&#39640;&#33030;&#24369;&#36947;&#36335;&#20351;&#29992;&#32773;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23486;&#22805;&#27861;&#23612;&#20122;&#20132;&#36890;&#37096;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#22522;&#20110;&#35270;&#39057;&#30340;&#20107;&#20214;&#30417;&#25511;&#31995;&#32479;&#26469;&#35780;&#20272;&#33030;&#24369;&#36947;&#36335;&#20351;&#29992;&#32773;&#21644;&#26426;&#21160;&#36710;&#36742;&#30340;&#20914;&#31361;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#26080;&#20154;&#26426;&#22312;&#19977;&#32500;&#31354;&#20013;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#23567;&#21306;&#20851;&#32852;&#20915;&#31574;&#21644;&#31227;&#21160;&#36895;&#24230;&#65292;&#20197;&#25552;&#21319;&#20132;&#36890;&#21644;&#36890;&#20449;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;18.32%&#12290;</title><link>http://arxiv.org/abs/2307.13158</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#36895;&#24230;&#25511;&#21046;&#20860;&#39038;&#36991;&#38556;&#21644;&#20132;&#25509;&#24863;&#30693;&#30340;&#23567;&#21306;&#20851;&#32852;&#65306;&#24102;&#26377;&#21160;&#20316;&#20998;&#25903;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#26080;&#20154;&#26426;&#22312;&#19977;&#32500;&#31354;&#20013;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#23567;&#21306;&#20851;&#32852;&#20915;&#31574;&#21644;&#31227;&#21160;&#36895;&#24230;&#65292;&#20197;&#25552;&#21319;&#20132;&#36890;&#21644;&#36890;&#20449;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;18.32%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#19977;&#32500;&#31354;&#20013;&#39640;&#36895;&#20844;&#36335;&#19978;&#22810;&#26080;&#20154;&#26426;&#30340;&#23567;&#21306;&#20851;&#32852;&#20915;&#31574;&#21644;&#31227;&#21160;&#36895;&#24230;&#65292;&#20197;&#25552;&#21319;&#20132;&#36890;&#21644;&#36890;&#20449;&#24615;&#33021;&#65292;&#21253;&#25324;&#36991;&#38556;&#12289;&#36830;&#25509;&#24615;&#21644;&#20132;&#25509;&#25928;&#26524;&#12290;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#20013;&#26080;&#20154;&#26426;&#30340;&#29366;&#24577;&#30001;&#36895;&#24230;&#21644;&#36890;&#20449;&#25968;&#25454;&#36895;&#29575;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#20915;&#31574;&#27169;&#22359;&#21644;&#22810;&#20010;&#32593;&#32476;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#19987;&#38376;&#22788;&#29702;&#20108;&#32500;&#20132;&#36890;-&#36890;&#20449;&#31354;&#38388;&#20013;&#30340;&#29305;&#23450;&#21160;&#20316;&#32500;&#24230;&#12290;&#36825;&#31181;&#35774;&#35745;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#22810;&#32500;&#21160;&#20316;&#31354;&#38388;&#65292;&#20351;&#24471;&#21508;&#20010;&#21160;&#20316;&#32500;&#24230;&#21487;&#20197;&#29420;&#31435;&#20915;&#31574;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#20998;&#25903;&#22411;&#23545;&#25239;&#24615;Q&#32593;&#32476;&#65288;BDQ&#65289;&#21644;&#20998;&#25903;&#22411;&#23545;&#25239;&#24615;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;Dueling DDQN&#65289;&#65292;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;18.32%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13149</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#36890;&#24120;&#34987;&#35748;&#20026;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#27493;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19987;&#23478;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#65292;&#20854;&#20013;&#23624;&#26381;&#26354;&#38754;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#19968;&#32452;&#21333;&#21464;&#37327;&#29305;&#24449;&#26144;&#23556;&#26469;&#34920;&#31034;&#30340;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#36825;&#32452;&#21333;&#21464;&#37327;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#20989;&#25968;&#37325;&#26032;&#35299;&#37322;&#20026;&#25968;&#23398;&#24418;&#24335;&#12290;&#36825;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#25552;&#39640;&#20102;&#29992;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26448;&#26009;&#30340;&#23646;&#24615;&#65288;&#22914;&#20984;&#24615;&#21644;&#23545;&#31216;&#24615;&#65289;&#26377;&#19968;&#20010;&#20855;&#20307;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#22122;&#22768;&#35266;&#27979;&#21644;&#30456;&#20851;&#35266;&#27979;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13147</link><description>&lt;p&gt;
&#23558;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#25193;&#23637;&#21040;&#26377;&#22122;&#22768;&#30340;&#35266;&#27979;&#21644;&#30456;&#20851;&#35266;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework. (arXiv:2307.13147v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#22122;&#22768;&#35266;&#27979;&#21644;&#30456;&#20851;&#35266;&#27979;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE (PD-NJ-ODE) &#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20855;&#26377;&#19981;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#32473;&#23450;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#19981;&#23436;&#25972;&#36807;&#21435;&#35266;&#27979;&#30340;&#26368;&#20248;&#39044;&#27979;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20551;&#35774;&#36807;&#31243;&#26412;&#36523;&#21644;&#22352;&#26631;&#20998;&#21035;&#35266;&#27979;&#26102;&#38388;&#26159;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#20551;&#35774;&#35266;&#27979;&#26159;&#26080;&#22122;&#22768;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#25193;&#23637;&#26469;&#35299;&#38500;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#20197;&#21450;&#23427;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#28085;&#30422;&#20840;&#29699;&#21508;&#22320;&#23478;&#24237;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#22312;&#30446;&#26631;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#30340;&#36827;&#23637;&#24182;&#27809;&#26377;&#25913;&#21892;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13136</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#30340;&#36827;&#23637;&#26159;&#21542;&#25913;&#21892;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#27867;&#21270;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?. (arXiv:2307.13136v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13136
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#28085;&#30422;&#20840;&#29699;&#21508;&#22320;&#23478;&#24237;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#22312;&#30446;&#26631;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#30340;&#36827;&#23637;&#24182;&#27809;&#26377;&#25913;&#21892;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21313;&#22810;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#29992;&#22522;&#20110;ImageNet&#30340;&#27867;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;ImageNet-A&#12289;-C&#21644;-R&#65289;&#26469;&#34913;&#37327;&#30446;&#26631;&#35782;&#21035;&#30340;&#36827;&#23637;&#12290;&#26368;&#36817;&#22522;&#20110;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#28982;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;&#36825;&#34920;&#26126;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#19981;&#36275;&#20197;&#34913;&#37327;&#29616;&#23454;&#19990;&#30028;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#38598;&#20013;&#22312;&#39044;&#23450;&#20041;&#25110;&#21512;&#25104;&#30340;&#21464;&#21270;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#28085;&#30422;&#20840;&#29699;&#21508;&#22320;&#23478;&#24237;&#29289;&#20307;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#27867;&#21270;&#33021;&#21147;&#20316;&#20026;&#26356;&#29616;&#23454;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23545;&#36817;100&#20010;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#21644;&#30495;&#23454;&#19990;&#30028;&#22320;&#29702;&#21464;&#21270;&#20043;&#38388;&#30340;&#36827;&#23637;&#24046;&#36317;&#65306;&#22312;ImageNet&#19978;&#30340;&#36827;&#23637;&#22312;&#26631;&#20934;&#27867;&#21270;&#22522;&#20934;&#27979;&#35797;&#19978;&#20135;&#29983;&#30340;&#36827;&#23637;&#27604;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#21464;&#21270;&#39640;&#20986;2.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;simPLE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#12289;&#23450;&#20301;&#12289;&#37325;&#26032;&#25235;&#21462;&#21644;&#25918;&#32622;&#29289;&#20307;&#12290;simPLE&#21253;&#25324;&#20219;&#21153;&#24863;&#30693;&#25235;&#21462;&#12289;&#35270;&#35273;&#35302;&#35273;&#24863;&#30693;&#21644;&#37325;&#26032;&#25235;&#21462;&#35268;&#21010;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20808;&#21069;&#30340;&#32463;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.13133</link><description>&lt;p&gt;
simPLE:&#19968;&#31181;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#30340;&#35270;&#35302;&#35273;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#25235;&#21462;&#12289;&#23450;&#20301;&#12289;&#37325;&#26032;&#25235;&#21462;&#21644;&#25918;&#32622;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects. (arXiv:2307.13133v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;simPLE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#12289;&#23450;&#20301;&#12289;&#37325;&#26032;&#25235;&#21462;&#21644;&#25918;&#32622;&#29289;&#20307;&#12290;simPLE&#21253;&#25324;&#20219;&#21153;&#24863;&#30693;&#25235;&#21462;&#12289;&#35270;&#35273;&#35302;&#35273;&#24863;&#30693;&#21644;&#37325;&#26032;&#25235;&#21462;&#35268;&#21010;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20808;&#21069;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#36890;&#29992;&#24615;&#21644;&#31934;&#24230;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#37096;&#32626;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#23646;&#20110;&#19968;&#20010;&#26426;&#22120;&#20154;&#35299;&#20915;&#21333;&#19968;&#20219;&#21153;&#30340;&#33539;&#24335;&#65292;&#32570;&#20047;&#31934;&#30830;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31934;&#30830;&#21644;&#36890;&#29992;&#30340;&#25235;&#21462;&#21644;&#25918;&#32622;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#31934;&#30830;&#30340;&#25235;&#21462;&#21644;&#25918;&#32622;&#20013;&#65292;&#21363;&#22871;&#20214;&#21270;&#20013;&#65292;&#26426;&#22120;&#20154;&#23558;&#19968;&#20010;&#26080;&#32452;&#32455;&#30340;&#29289;&#20307;&#25490;&#21015;&#36716;&#21270;&#20026;&#26377;&#32452;&#32455;&#30340;&#25490;&#21015;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;simPLE&#65288;&#27169;&#25311;&#25235;&#21462;&#23450;&#20301;&#21644;&#25918;&#32622;&#65289;&#20316;&#20026;&#31934;&#30830;&#25235;&#21462;&#21644;&#25918;&#32622;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;simPLE&#23398;&#20250;&#20102;&#20934;&#30830;&#22320;&#25235;&#21462;&#12289;&#37325;&#26032;&#25235;&#21462;&#21644;&#25918;&#32622;&#29289;&#20307;&#65292;&#21482;&#38656;&#29289;&#20307;CAD&#27169;&#22411;&#32780;&#26080;&#38656;&#20808;&#21069;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#20219;&#21153;&#24863;&#30693;&#25235;&#21462;&#65292;&#35270;&#35273;&#35302;&#35273;&#24863;&#30693;&#21644;&#37325;&#26032;&#25235;&#21462;&#35268;&#21010;&#12290;&#20219;&#21153;&#24863;&#30693;&#25235;&#21462;&#35745;&#31639;&#31283;&#23450;&#12289;&#21487;&#35266;&#27979;&#19988;&#26377;&#21033;&#20110;&#25918;&#32622;&#30340;&#25235;&#21462;&#30340;&#36866;&#24212;&#24230;&#12290;&#35270;&#35273;&#35302;&#35273;&#24863;&#30693;&#27169;&#22411; r
&lt;/p&gt;
&lt;p&gt;
Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26435;&#37325;ERM&#20013;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;DP&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.13127</link><description>&lt;p&gt;
&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#21450;&#20854;&#22312;&#32467;&#26524;&#21152;&#26435;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning. (arXiv:2307.13127v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26435;&#37325;ERM&#20013;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;DP&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#19978;&#21487;&#20197;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#24046;&#20998;&#38544;&#31169;(DP)&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#25968;&#23398;&#19978;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#25439;&#22833;&#30028;&#38480;&#26469;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;DP&#24212;&#29992;&#20110;&#26080;&#26435;&#37325;&#30340;ERM&#20013;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#20102;&#26435;&#37325;ERM(wERM)&#30340;&#37325;&#35201;&#25512;&#24191;&#12290;&#22312;wERM&#20013;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#20010;&#20307;&#30340;&#30446;&#26631;&#20989;&#25968;&#36129;&#29486;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;wERM&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#23558;&#29616;&#26377;&#30340;DP-ERM&#31243;&#24207;&#25193;&#23637;&#21040;wERM&#20026;&#32467;&#26524;&#21152;&#26435;&#23398;&#20064;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to derivin
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.13124</link><description>&lt;p&gt;
&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#23558;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#25193;&#23637;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#22522;&#30784;&#20005;&#37325;&#24615;&#27169;&#22411;&#26159;&#38543;&#26426;&#26862;&#26519;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20004;&#38454;&#27573;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
&lt;/p&gt;</description></item><item><title>Pathway&#26159;&#19968;&#20010;&#24555;&#36895;&#28789;&#27963;&#30340;&#32479;&#19968;&#27969;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23427;&#33021;&#22815;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#30340;&#25968;&#25454;&#27969;&#19978;&#36816;&#34892;&#65292;&#36890;&#36807;Table API&#21644;&#20998;&#24067;&#24335;&#22686;&#37327;&#25968;&#25454;&#27969;&#39537;&#21160;&#65292;&#22312;&#25209;&#22788;&#29702;&#21644;&#27969;&#22788;&#29702;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13116</link><description>&lt;p&gt;
Pathway:&#19968;&#31181;&#24555;&#36895;&#28789;&#27963;&#30340;&#32479;&#19968;&#27969;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pathway: a fast and flexible unified stream data processing framework for analytical and Machine Learning applications. (arXiv:2307.13116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13116
&lt;/p&gt;
&lt;p&gt;
Pathway&#26159;&#19968;&#20010;&#24555;&#36895;&#28789;&#27963;&#30340;&#32479;&#19968;&#27969;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23427;&#33021;&#22815;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#30340;&#25968;&#25454;&#27969;&#19978;&#36816;&#34892;&#65292;&#36890;&#36807;Table API&#21644;&#20998;&#24067;&#24335;&#22686;&#37327;&#25968;&#25454;&#27969;&#39537;&#21160;&#65292;&#22312;&#25209;&#22788;&#29702;&#21644;&#27969;&#22788;&#29702;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pathway&#65292;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26377;&#30028;&#21644;&#26080;&#30028;&#25968;&#25454;&#27969;&#19978;&#36816;&#34892;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#24314;&#26368;&#21021;&#26159;&#20026;&#20102;&#35299;&#20915;&#22312;&#20998;&#26512;&#21644;&#22788;&#29702;&#29289;&#29702;&#32463;&#27982;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#29289;&#32852;&#32593;&#21644;&#20225;&#19994;&#31995;&#32479;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#12290;&#36825;&#20123;&#37117;&#38656;&#35201;&#24555;&#36895;&#21453;&#24212;&#65292;&#24182;&#38656;&#35201;&#24212;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#33539; paradigms&#65288;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#26512;&#65292;&#19978;&#19979;&#25991;&#20998;&#26512;&#21644;&#22797;&#26434;&#20107;&#20214;&#22788;&#29702;&#30340;&#20854;&#20182;&#20803;&#32032;&#65289;&#12290;Pathway&#37197;&#22791;&#20102;&#38024;&#23545;Python&#21644;Python/SQL&#24037;&#20316;&#27969;&#31243;&#37327;&#36523;&#23450;&#21046;&#30340;Table API&#65292;&#24182;&#30001;Rust&#20013;&#30340;&#20998;&#24067;&#24335;&#22686;&#37327;&#25968;&#25454;&#27969;&#39537;&#21160;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#31995;&#32479;&#65292;&#24182;&#21576;&#29616;&#20102;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#22312;&#25209;&#22788;&#29702;&#21644;&#27969;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#34892;&#19994;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#30001;Pathway&#22788;&#29702;&#30340;&#27969;&#22788;&#29702;&#29992;&#20363;&#65292;&#36825;&#20123;&#29992;&#20363;&#26080;&#27861;&#36731;&#26494;&#35299;&#20915;&#20197;&#29366;&#24577;&#20026;&#22522;&#30784;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Pathway, a new unified data processing framework that can run workloads on both bounded and unbounded data streams. The framework was created with the original motivation of resolving challenges faced when analyzing and processing data from the physical economy, including streams of data generated by IoT and enterprise systems. These required rapid reaction while calling for the application of advanced computation paradigms (machinelearning-powered analytics, contextual analysis, and other elements of complex event processing). Pathway is equipped with a Table API tailored for Python and Python/SQL workflows, and is powered by a distributed incremental dataflow in Rust. We describe the system and present benchmarking results which demonstrate its capabilities in both batch and streaming contexts, where it is able to surpass state-of-the-art industry frameworks in both scenarios. We also discuss streaming use cases handled by Pathway which cannot be easily resolved with state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;xGW-GAT&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#21151;&#33021;&#32593;&#32476;&#65292;&#20197;&#25512;&#21160;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.13108</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#21151;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment. (arXiv:2307.13108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;xGW-GAT&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#21151;&#33021;&#32593;&#32476;&#65292;&#20197;&#25512;&#21160;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#30340;&#19968;&#20010;&#26174;&#33879;&#30151;&#29366;&#26159;&#23039;&#21183;&#21453;&#23556;&#30340;&#36880;&#28176;&#20007;&#22833;&#65292;&#26368;&#32456;&#23548;&#33268;&#27493;&#24577;&#22256;&#38590;&#21644;&#24179;&#34913;&#38382;&#39064;&#12290;&#35782;&#21035;&#19982;&#27493;&#24577;&#38556;&#30861;&#30456;&#20851;&#30340;&#33041;&#21151;&#33021;&#32010;&#20081;&#23545;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24085;&#37329;&#26862;&#30149;&#30340;&#36816;&#21160;&#36827;&#23637;&#20197;&#21450;&#25512;&#21160;&#26356;&#26377;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#30340;&#21457;&#23637;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#12289;&#20960;&#20309;&#30340;&#12289;&#21152;&#26435;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;xGW-GAT&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#22256;&#38590;&#36827;&#23637;&#30340;&#21151;&#33021;&#32593;&#32476;&#12290;xGW-GAT&#22312;MDS&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#26631;&#20934;&#65288;MDS-UPDRS&#65289;&#19978;&#39044;&#27979;&#22810;&#31867;&#21035;&#30340;&#27493;&#24577;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#23558;&#21151;&#33021;&#36830;&#25509;&#32452;&#34920;&#31034;&#20026;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#65292;&#20197;&#26126;&#30830;&#32534;&#30721;&#25972;&#20010;&#36830;&#25509;&#32452;&#30340;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#65292;&#26681;&#25454;&#27492;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#20986;&#19968;&#20010;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#20197;&#20135;&#29983;&#20010;&#20307;&#21644;&#32676;&#20307;&#32423;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#27169;&#22411;&#30340;&#22810;&#27493;&#36716;&#31227;&#26469;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#27491;&#35268;&#21270;&#21644;&#26102;&#24046;&#26356;&#26032;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13101</link><description>&lt;p&gt;
&#23545;&#27604;&#31034;&#20363;&#39537;&#21160;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Contrastive Example-Based Control. (arXiv:2307.13101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#27169;&#22411;&#30340;&#22810;&#27493;&#36716;&#31227;&#26469;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#27491;&#35268;&#21270;&#21644;&#26102;&#24046;&#26356;&#26032;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#21463;&#30410;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#24456;&#23569;&#31526;&#21512;MDP&#27169;&#22411;&#65292;&#19982;&#29615;&#22659;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#20063;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20174;&#36716;&#31227;&#21160;&#24577;&#30340;&#26679;&#26412;&#21644;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20174;&#39640;&#22238;&#25253;&#29366;&#24577;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#29992;&#35813;&#22870;&#21169;&#20989;&#25968;&#26631;&#35760;&#36716;&#31227;&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#36825;&#20123;&#36716;&#31227;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#36890;&#24120;&#38656;&#35201;&#27491;&#35268;&#21270;&#21644;&#26102;&#24046;&#26356;&#26032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#12289;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#38544;&#24335;&#27169;&#22411;&#30340;&#22810;&#27493;&#36716;&#31227;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38544;&#24335;&#27169;&#22411;&#21487;&#20197;&#34920;&#31034;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;Q&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a ran
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20184;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#30340;&#30452;&#25509;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#26102;&#22122;&#22768;&#24191;&#20041;&#39118;&#38505;&#30340;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#32463;&#39564;&#39118;&#38505;&#26045;&#21152;&#19979;&#30028;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26126;&#30830;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#26368;&#23567;&#21487;&#23454;&#29616;&#22122;&#22768;&#39118;&#38505;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.13100</link><description>&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#65306;&#23545;&#20462;&#27491;&#30340;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Label Noise: Correcting a Correction. (arXiv:2307.13100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20184;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#30340;&#30452;&#25509;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#26102;&#22122;&#22768;&#24191;&#20041;&#39118;&#38505;&#30340;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#32463;&#39564;&#39118;&#38505;&#26045;&#21152;&#19979;&#30028;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26126;&#30830;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#26368;&#23567;&#21487;&#23454;&#29616;&#22122;&#22768;&#39118;&#38505;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26631;&#31614;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#26356;&#40065;&#26834;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26367;&#20195;&#26041;&#26696;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#24182;&#19988;&#20173;&#28982;&#23481;&#26131;&#36807;&#25311;&#21512;&#25110;&#27424;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#30452;&#25509;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30001;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#22122;&#22768;&#24191;&#20041;&#39118;&#38505;&#30340;&#19979;&#30028;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#32463;&#39564;&#39118;&#38505;&#26045;&#21152;&#19968;&#20010;&#19979;&#30028;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#20026;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#26368;&#23567;&#21487;&#23454;&#29616;&#22122;&#22768;&#39118;&#38505;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20351;&#29992;&#36825;&#20123;&#30028;&#38480;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#65292;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.13081</link><description>&lt;p&gt;
&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#19979;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20551;&#35774;&#27169;&#22411;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#20154;&#21475;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#26399;&#38388;&#26410;&#20445;&#30041;&#35760;&#24405;&#25110;&#20986;&#20110;&#38544;&#31169;&#21407;&#22240;&#65292;&#23384;&#22312;&#20154;&#21475;&#20449;&#24687;&#37096;&#20998;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#19968;&#20010;&#23646;&#24615;&#20998;&#31867;&#22120;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#25935;&#24863;&#23646;&#24615;&#65288;&#20195;&#29702;&#65289;&#20173;&#28982;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#30495;&#23454;&#25935;&#24863;&#23646;&#24615;&#30456;&#27604;&#65292;&#20351;&#29992;&#20195;&#29702;&#25935;&#24863;&#23646;&#24615;&#20250;&#21152;&#21095;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26500;&#24314;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#20855;&#26377;&#25512;&#26029;&#20986;&#30340;&#26368;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#20154;&#21475;&#20449;&#24687;&#30340;&#26679;&#26412;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#26679;&#26412;&#19978;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#32422;&#26463;&#20250;&#25439;&#23475;&#31639;&#27861;&#30340;&#24635;&#20307;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26102;&#20351;&#29992;&#33258;&#36866;&#24212;&#35748;&#35777;&#21322;&#24452;&#30340;&#20851;&#38190;&#35266;&#28857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25512;&#36827;&#20102;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.13078</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#35748;&#35777;&#35757;&#32451;: &#36808;&#21521;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs. (arXiv:2307.13078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26102;&#20351;&#29992;&#33258;&#36866;&#24212;&#35748;&#35777;&#21322;&#24452;&#30340;&#20851;&#38190;&#35266;&#28857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25512;&#36827;&#20102;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#26029;&#36827;&#27493;&#21644;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#26085;&#30410;&#24191;&#27867;&#24212;&#29992;&#65292;&#40065;&#26834;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#22312;&#26576;&#20123;&#25200;&#21160;&#27700;&#24179;&#19978;&#33719;&#24471;&#39640;&#24230;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#22312;&#24178;&#20928;&#30340;&#26410;&#25200;&#21160;&#25968;&#25454;&#19978;&#20934;&#30830;&#24615;&#20005;&#37325;&#38477;&#20302;&#65292;&#20351;&#20854;&#19981;&#23454;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#29305;&#23450;&#30340;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#19978;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#26356;&#29616;&#23454;&#30340;&#35282;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#35266;&#28857;&#30340;&#26032;&#22411;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#33258;&#36866;&#24212;&#35748;&#35777;&#21322;&#24452;&#36827;&#34892;&#35757;&#32451;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25512;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;MNIST&#12289;CIFAR-10&#21644;TinyImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#21517;&#20026;WOOD&#65292;&#22312;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#24694;&#21155;&#22825;&#27668;&#21644;&#29615;&#22659;&#21464;&#21270;&#31561;&#22810;&#31181;&#22240;&#32032;&#24341;&#36215;&#30340;&#24322;&#24120;&#24773;&#20917;&#19979;&#32454;&#31890;&#24230;&#21516;&#26102;&#26816;&#27979;&#22810;&#20010;OOD&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.13069</link><description>&lt;p&gt;
&#36890;&#29992;&#22810;&#27169;&#24577;OOD&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Multi-Modal OOD Detection Framework. (arXiv:2307.13069v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#21517;&#20026;WOOD&#65292;&#22312;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#24694;&#21155;&#22825;&#27668;&#21644;&#29615;&#22659;&#21464;&#21270;&#31561;&#22810;&#31181;&#22240;&#32032;&#24341;&#36215;&#30340;&#24322;&#24120;&#24773;&#20917;&#19979;&#32454;&#31890;&#24230;&#21516;&#26102;&#26816;&#27979;&#22810;&#20010;OOD&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#26679;&#26412;&#26816;&#27979;&#65288;OOD&#65289;&#35782;&#21035;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#36825;&#23545;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#22823;&#37327;&#26041;&#27861;&#26469;&#26816;&#27979;&#21333;&#27169;&#24577;OOD&#26679;&#26412;&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#26041;&#27861;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;OOD&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#30740;&#31350;&#22810;&#27169;&#24577;OOD&#26816;&#27979;&#30340;&#22330;&#26223;&#26159;&#32473;&#23450;&#30340;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#33258;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;ML&#31995;&#32479;&#37096;&#32626;&#21487;&#33021;&#38754;&#20020;&#26356;&#22810;&#30001;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#24694;&#21155;&#22825;&#27668;&#21644;&#29615;&#22659;&#21464;&#21270;&#31561;&#22810;&#31181;&#22240;&#32032;&#24341;&#36215;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#21516;&#26102;&#26816;&#27979;&#22810;&#20010;&#19981;&#21516;&#30340;OOD&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24369;&#30417;&#30563;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;WOOD&#65292;&#23427;&#32467;&#21512;&#20102;&#20108;&#20803;&#20998;&#31867;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#32452;&#20214;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#26799;&#24230;&#27969;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#19978;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#27979;&#37327;&#21487;&#35299;&#37322;&#29305;&#24449;&#19982;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#29305;&#23450;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13061</link><description>&lt;p&gt;
&#29305;&#24449;&#26799;&#24230;&#27969;&#29992;&#20110;&#35299;&#37322;&#22836;&#39048;&#30284;&#39044;&#27979;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction. (arXiv:2307.13061v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#26799;&#24230;&#27969;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#19978;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#27979;&#37327;&#21487;&#35299;&#37322;&#29305;&#24449;&#19982;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#29305;&#23450;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21363;&#29305;&#24449;&#26799;&#24230;&#27969;&#65292;&#29992;&#20110;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#20026;&#23545;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#23616;&#37096;&#23450;&#20041;&#20102;&#38750;&#32447;&#24615;&#22352;&#26631;&#65292;&#34920;&#31034;&#27169;&#22411;&#29992;&#20110;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#36890;&#36807;&#27979;&#37327;&#21487;&#35299;&#37322;&#29305;&#24449;&#19982;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#29305;&#23450;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#27604;&#36739;&#35813;&#29305;&#24449;&#30340;&#26799;&#24230;&#27969;&#24230;&#37327;&#19982;&#22522;&#32447;&#22122;&#22768;&#29305;&#24449;&#30340;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#30340;&#26041;&#24335;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20351;&#27169;&#22411;&#26799;&#24230;&#19982;&#25152;&#36873;&#35299;&#37322;&#29305;&#24449;&#30340;&#26799;&#24230;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;&#20174;Cancer Imaging Archive&#33719;&#21462;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#20013;&#30340;&#22836;&#39048;&#30284;&#36828;&#22788;&#36716;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces feature gradient flow, a new technique for interpreting deep learning models in terms of features that are understandable to humans. The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions. Our idea is to measure the agreement of interpretable features with the gradient flow of a model. To then evaluate the importance of a particular feature to the model, we compare that feature's gradient flow measure versus that of a baseline noise feature. We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features. We test our method in a convolutional neural network prediction of distant metastasis of head and neck cancer from a computed tomography dataset from the Cancer Imaging Archive.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;MARIO&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;OOD&#27867;&#21270;&#24615;&#33021;&#12290;MARIO&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#21644;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#26088;&#22312;&#33719;&#24471;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13055</link><description>&lt;p&gt;
MARIO: &#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;&#65292;&#25552;&#39640;OOD&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;MARIO&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;OOD&#27867;&#21270;&#24615;&#33021;&#12290;MARIO&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#21644;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#26088;&#22312;&#33719;&#24471;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#22495;&#22806;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#24773;&#20917;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#26377;&#26631;&#31614;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20063;&#26174;&#31034;&#20986;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARIO&#30340;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;&#65292;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#30340;&#22270;&#23545;&#27604;&#26041;&#27861;&#65292;&#20811;&#26381;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65306;(i)&#20449;&#24687;&#29942;&#39048;(IB)&#21407;&#21017;&#29992;&#20110;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#65292;(ii)&#19981;&#21464;&#24615;&#21407;&#21017;&#37319;&#29992;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26469;&#33719;&#24471;&#19981;&#21464;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;OOD&#27867;&#21270;&#38382;&#39064;&#30340;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13014</link><description>&lt;p&gt;
&#29992;&#20110;&#31243;&#24207;&#20043;&#38388;&#21464;&#37327;&#26144;&#23556;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;&#25193;&#23637;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20998;&#26512;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24418;&#24335;&#26041;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#20110;&#31243;&#24207;&#31561;&#20215;&#38382;&#39064;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#65292;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#65292;&#38656;&#35201;&#23545;&#20004;&#20010;&#31243;&#24207;&#30340;&#21464;&#37327;&#38598;&#20043;&#38388;&#24314;&#31435;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#22312;&#35832;&#22914;&#31243;&#24207;&#31561;&#20215;&#24615;&#12289;&#31243;&#24207;&#20998;&#26512;&#12289;&#31243;&#24207;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#65292;&#26144;&#23556;&#20004;&#20010;&#31243;&#24207;&#20043;&#38388;&#30340;&#21464;&#37327;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#20004;&#20010;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#12290;&#20026;&#20102;&#23637;&#31034;&#21464;&#37327;&#26144;&#23556;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312;&#31243;&#24207;&#20462;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#36825;&#20123;&#26144;&#23556;&#30340;&#19977;&#20010;&#29992;&#20363;&#65292;&#20197;&#20462;&#22797;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#24120;&#35265;&#30340;&#21644;&#32463;&#24120;&#21457;&#29983;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19968;&#20010;&#21253;&#21547;4166&#23545;&#38169;&#35823;/&#20462;&#27491;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#27010;&#24565;&#30340;&#22270;&#24418;&#27744;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#32422;&#26463;&#22312;&#22270;&#24418;&#27744;&#21270;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13011</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#29992;&#20110;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
Maximal Independent Sets for Pooling in Graph Neural Networks. (arXiv:2307.13011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#27010;&#24565;&#30340;&#22270;&#24418;&#27744;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#32422;&#26463;&#22312;&#22270;&#24418;&#27744;&#21270;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#22270;&#20687;&#20998;&#31867;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#27744;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22270;&#24418;&#32780;&#35328;&#65292;&#24182;&#19981;&#23384;&#22312;&#28385;&#36275;&#36825;&#20123;&#24615;&#36136;&#30340;&#27744;&#21270;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#22270;&#24418;&#27744;&#21270;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#20043;&#19968;&#65306;&#22270;&#24418;&#26029;&#24320;&#25110;&#36830;&#25509;&#36807;&#24230;&#12289;&#38477;&#37319;&#26679;&#27604;&#36739;&#20302;&#12289;&#20197;&#21450;&#21024;&#38500;&#22823;&#37096;&#20998;&#22270;&#24418;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#27010;&#24565;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#22312;&#22270;&#24418;&#27744;&#21270;&#20013;&#26368;&#22823;&#29420;&#31435;&#38598;&#21512;&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36229;&#36234;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#32479;&#35745;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#20083;&#33146;&#30284;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23558;&#30149;&#20363;&#20154;&#32676;&#20998;&#25104;&#39640;&#36951;&#20256;&#39118;&#38505;&#20122;&#20154;&#32676;&#12290;</title><link>http://arxiv.org/abs/2307.13010</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#20102;&#20083;&#33146;&#30284;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep neural network improves the estimation of polygenic risk scores for breast cancer. (arXiv:2307.13010v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13010
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36229;&#36234;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#32479;&#35745;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#20083;&#33146;&#30284;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23558;&#30149;&#20363;&#20154;&#32676;&#20998;&#25104;&#39640;&#36951;&#20256;&#39118;&#38505;&#20122;&#20154;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#65288;PRS&#65289;&#26159;&#26681;&#25454;&#25972;&#20010;&#22522;&#22240;&#32452;&#20013;&#30340;&#35768;&#22810;&#36951;&#20256;&#21464;&#24322;&#26469;&#20272;&#35745;&#20010;&#20307;&#23545;&#22797;&#26434;&#30142;&#30149;&#30340;&#36951;&#20256;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#20272;&#35745;&#20083;&#33146;&#30284;PRS&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#24050;&#24314;&#31435;&#30340;&#32479;&#35745;&#31639;&#27861;&#65292;&#21253;&#25324;BLUP&#12289;BayesA&#21644;LDpred&#12290;&#22312;&#20855;&#26377;50%&#24739;&#30149;&#29575;&#30340;&#27979;&#35797;&#38431;&#21015;&#20013;&#65292;DNN&#30340;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;67.4&#65285;&#65292;BLUP&#20026;64.2&#65285;&#65292;BayesA&#20026;64.5&#65285;&#65292;LDpred&#20026;62.4&#65285;&#12290;BLUP&#65292;BayesA&#21644;LDpred&#22312;&#30149;&#20363;&#20154;&#32676;&#20013;&#29983;&#25104;&#30340;PRS&#31526;&#21512;&#27491;&#24577;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;DNN&#22312;&#30149;&#20363;&#20154;&#32676;&#20013;&#29983;&#25104;&#30340;PRS&#36981;&#24490;&#30001;&#20004;&#20010;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#22343;&#20540;&#30340;&#27491;&#24577;&#20998;&#24067;&#32452;&#25104;&#30340;&#21452;&#23792;&#20998;&#24067;&#12290;&#36825;&#34920;&#26126;&#65292;DNN&#33021;&#22815;&#23558;&#30149;&#20363;&#20154;&#32676;&#20998;&#25104;&#19968;&#20010;&#39640;&#36951;&#20256;&#39118;&#38505;&#30149;&#20363;&#20122;&#20154;&#32676;&#65292;&#20854;&#24179;&#22343;PRS&#26126;&#26174;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polygenic risk scores (PRS) estimate the genetic risk of an individual for a complex disease based on many genetic variants across the whole genome. In this study, we compared a series of computational models for estimation of breast cancer PRS. A deep neural network (DNN) was found to outperform alternative machine learning techniques and established statistical algorithms, including BLUP, BayesA and LDpred. In the test cohort with 50% prevalence, the Area Under the receiver operating characteristic Curve (AUC) were 67.4% for DNN, 64.2% for BLUP, 64.5% for BayesA, and 62.4% for LDpred. BLUP, BayesA, and LPpred all generated PRS that followed a normal distribution in the case population. However, the PRS generated by DNN in the case population followed a bi-modal distribution composed of two normal distributions with distinctly different means. This suggests that DNN was able to separate the case population into a high-genetic-risk case sub-population with an average PRS significantly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31232;&#30095;&#28608;&#21457;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26102;&#38388;&#21040;&#31532;&#19968;&#20010;&#23574;&#23792;&#32534;&#30721;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#20449;&#24687;&#22788;&#29702;&#30340;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.13007</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#28608;&#21457;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#24102;&#26377;&#26102;&#38388;&#21040;&#31532;&#19968;&#20010;&#23574;&#23792;&#32534;&#30721;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding. (arXiv:2307.13007v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31232;&#30095;&#28608;&#21457;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26102;&#38388;&#21040;&#31532;&#19968;&#20010;&#23574;&#23792;&#32534;&#30721;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#20449;&#24687;&#22788;&#29702;&#30340;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#23545;&#22810;&#23618;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#20013;&#65292;&#30452;&#25509;&#20351;&#29992;&#31070;&#32463;&#20803;&#30340;&#21457;&#25918;&#26102;&#38388;&#30340;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#26102;&#38388;&#32534;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#21040;&#31532;&#19968;&#20010;&#23574;&#23792;&#65288;TTFS&#65289;&#32534;&#30721;&#65292;&#22312;&#36825;&#31181;&#32534;&#30721;&#20013;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#26368;&#22810;&#21482;&#33021;&#21457;&#25918;&#19968;&#27425;&#65292;&#32780;&#36825;&#31181;&#21457;&#25918;&#27425;&#25968;&#38480;&#21046;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#21457;&#25918;&#39057;&#29575;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;&#12290;&#36825;&#31181;&#20302;&#21457;&#25918;&#39057;&#29575;&#22686;&#21152;&#20102;SNNs&#20013;&#30340;&#20449;&#24687;&#22788;&#29702;&#33021;&#25928;&#65292;&#36825;&#19981;&#20165;&#22240;&#20026;&#20854;&#19982;&#22823;&#33041;&#20449;&#24687;&#22788;&#29702;&#30340;&#30456;&#20284;&#24615;&#65292;&#32780;&#19988;&#36824;&#20174;&#24037;&#31243;&#35282;&#24230;&#26469;&#30475;&#20063;&#26159;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;TTFS&#32534;&#30721;&#30340;SNNs&#65292;&#30446;&#21069;&#21482;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#36824;&#27809;&#26377;&#23436;&#20840;&#30740;&#31350;&#22312;&#26356;&#20302;&#30340;&#21457;&#25918;&#39057;&#29575;&#19979;&#30340;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31232;&#30095;&#28608;&#21457;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike 
&lt;/p&gt;</description></item><item><title>DeepGATGO&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#39044;&#35757;&#32451;&#30340;&#22270;&#27880;&#24847;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#34507;&#30333;&#21151;&#33021;&#39044;&#27979;&#12290;&#23427;&#36890;&#36807;&#21482;&#20351;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#32780;&#19981;&#38656;&#35201;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#25110;&#32593;&#32476;&#25299;&#25169;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#19988;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#30340;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.13004</link><description>&lt;p&gt;
DeepGATGO:&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#39044;&#35757;&#32451;&#30340;&#22270;&#27880;&#24847;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#34507;&#30333;&#21151;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction. (arXiv:2307.13004v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13004
&lt;/p&gt;
&lt;p&gt;
DeepGATGO&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#39044;&#35757;&#32451;&#30340;&#22270;&#27880;&#24847;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#34507;&#30333;&#21151;&#33021;&#39044;&#27979;&#12290;&#23427;&#36890;&#36807;&#21482;&#20351;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#32780;&#19981;&#38656;&#35201;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#25110;&#32593;&#32476;&#25299;&#25169;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#19988;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#30340;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#34507;&#30333;&#21151;&#33021;&#39044;&#27979;(AFP)&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#34507;&#30333;&#36136;&#23500;&#38598;&#20998;&#26512;&#65292;&#20197;&#28040;&#38500;&#24403;&#21069;&#23545;&#21171;&#21160;&#23494;&#38598;&#22411;&#28287;&#23454;&#39564;&#26041;&#27861;&#30340;&#20381;&#36182;&#12290;&#30446;&#21069;&#65292;&#20027;&#27969;&#26041;&#27861;&#20027;&#35201;&#32467;&#21512;&#34507;&#30333;&#36136;&#30456;&#20851;&#20449;&#24687;&#21644;&#22522;&#22240;&#26412;&#20307;(GO)&#26415;&#35821;&#65292;&#29983;&#25104;&#26368;&#32456;&#30340;&#21151;&#33021;&#39044;&#27979;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#20449;&#24687;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#34987;&#25972;&#21512;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#19982;GO&#26415;&#35821;&#23884;&#20837;&#34701;&#21512;&#65292;&#24182;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#33719;&#21462;&#32467;&#26500;&#20449;&#24687;&#25110;&#32593;&#32476;&#25299;&#25169;&#20449;&#24687;&#30340;&#22256;&#38590;&#20197;&#21450;&#36825;&#20123;&#25968;&#25454;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#21482;&#20351;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#34892;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#21487;&#38752;&#19988;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#25552;&#21462;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic protein function prediction (AFP) is classified as a large-scale multi-label classification problem aimed at automating protein enrichment analysis to eliminate the current reliance on labor-intensive wet-lab methods. Currently, popular methods primarily combine protein-related information and Gene Ontology (GO) terms to generate final functional predictions. For example, protein sequences, structural information, and protein-protein interaction networks are integrated as prior knowledge to fuse with GO term embeddings and generate the ultimate prediction results. However, these methods are limited by the difficulty in obtaining structural information or network topology information, as well as the accuracy of such data. Therefore, more and more methods that only use protein sequences for protein function prediction have been proposed, which is a more reliable and computationally cheaper approach. However, the existing methods fail to fully extract feature information from pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#65292;&#36890;&#36807;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#21644;&#24341;&#20837;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#21644;+1.54%&#30340;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.12996</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. (arXiv:2307.12996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#65292;&#36890;&#36807;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#21644;&#24341;&#20837;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#21644;+1.54%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#29983;&#29289;&#21270;&#23398;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20256;&#32479;&#19978;&#19987;&#27880;&#20110;&#20998;&#23376;&#22270;&#31070;&#32463;&#34920;&#24449;&#65307;&#28982;&#32780;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#31361;&#26174;&#20102;&#25991;&#26412;&#20013;&#25152;&#32534;&#30721;&#30340;&#31185;&#23398;&#30693;&#35782;&#37327;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20004;&#31181;&#27169;&#24577;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#20174;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#22270;&#34920;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#31070;&#32463;&#22270;&#34920;&#24449;&#19982;&#20854;&#29305;&#24449;&#30340;&#25991;&#26412;&#25551;&#36848;&#34920;&#24449;&#23545;&#40784;&#21518;&#65292;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#31070;&#32463;&#30456;&#20851;&#24615;&#35780;&#20998;&#31574;&#30053;&#20197;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#26377;&#26426;&#21453;&#24212;&#21551;&#21457;&#30340;&#26032;&#39062;&#21512;&#27861;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#65292;&#24182;&#22312;&#19979;&#28216;&#30340;MoleculeNet&#23646;&#24615;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#65292;&#24182;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23376;&#22270;/&#25991;&#26412;&#23545;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;+1.54%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#24863;&#30693;&#26816;&#27979;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26816;&#27979;&#22270;&#38598;&#20869;&#24322;&#24120;&#22270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.12994</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-representations Space Separation based Graph-level Anomaly-aware Detection. (arXiv:2307.12994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#24863;&#30693;&#26816;&#27979;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26816;&#27979;&#22270;&#38598;&#20869;&#24322;&#24120;&#22270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#32467;&#26500;&#27169;&#24335;&#34987;&#24191;&#27867;&#29992;&#20110;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#24314;&#27169;&#12290;&#22914;&#20309;&#26816;&#27979;&#36825;&#20123;&#22270;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#22270;&#20449;&#24687;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#38598;&#20013;&#22312;&#22914;&#20309;&#26816;&#27979;&#22270;&#38598;&#20869;&#30340;&#24322;&#24120;&#22270;&#36825;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24322;&#24120;&#22270;&#20027;&#35201;&#34920;&#29616;&#20026;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#24322;&#24120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#35780;&#20272;&#24322;&#24120;&#22270;&#26102;&#21516;&#31561;&#23545;&#24453;&#19978;&#36848;&#20004;&#31181;&#24322;&#24120;&#24418;&#24335;&#65292;&#32780;&#20107;&#23454;&#19978;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#22270;&#25968;&#25454;&#22312;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#24322;&#24120;&#26041;&#38754;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#27491;&#24120;&#22270;&#20855;&#26377;&#24494;&#22937;&#24046;&#24322;&#30340;&#24322;&#24120;&#22270;&#24456;&#23481;&#26131;&#36867;&#36991;&#29616;&#26377;&#26041;&#27861;&#30340;&#26816;&#27979;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#34920;&#31034;&#31354;&#38388;&#20998;&#31163;&#30340;&#22270;&#32423;&#24322;&#24120;&#24863;&#30693;&#26816;&#27979;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anoma
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#21644;&#33298;&#23572;&#22810;&#39033;&#24335;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#19979;&#23398;&#20064;$k$&#20010;ReLU&#28608;&#27963;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;&#26679;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19978;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#33021;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#36739;&#23567;&#30340;&#39640;&#38454;&#30697;&#35823;&#24046;&#24352;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.12840</link><description>&lt;p&gt;
&#36890;&#36807;&#33298;&#23572;&#22810;&#39033;&#24335;&#39640;&#25928;&#23398;&#20064;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials. (arXiv:2307.12840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12840
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#21644;&#33298;&#23572;&#22810;&#39033;&#24335;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#19979;&#23398;&#20064;$k$&#20010;ReLU&#28608;&#27963;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;&#26679;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19978;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#33021;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#36739;&#23567;&#30340;&#39640;&#38454;&#30697;&#35823;&#24046;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#19979;&#65292;&#20851;&#20110;&#24179;&#26041;&#25439;&#22833;&#30340;PAC&#23398;&#20064;$k$&#20010;ReLU&#28608;&#27963;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#36825;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20026;$(dk/\epsilon)^{O(k)}$&#65292;&#20854;&#20013;$\epsilon&gt;0$&#26159;&#30446;&#26631;&#31934;&#24230;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#32473;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#20026;$(dk/\epsilon)^{h(k)}$&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#20989;&#25968;$h(k)$&#22312;$k$&#19978;&#30340;&#35268;&#27169;&#26159;&#36229;&#22810;&#39033;&#24335;&#30340;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#30456;&#20851;&#32479;&#35745;&#26597;&#35810;&#31639;&#27861;&#31867;&#20013;&#25509;&#36817;&#26368;&#20248;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26469;&#35782;&#21035;&#19968;&#20010;&#23376;&#31354;&#38388;&#65292;&#20351;&#24471;&#25152;&#26377;$O(k)$&#38454;&#30697;&#22312;&#27491;&#20132;&#26041;&#21521;&#19978;&#37117;&#24456;&#23567;&#12290;&#20854;&#20998;&#26512;&#22522;&#20110;&#33298;&#23572;&#22810;&#39033;&#24335;&#29702;&#35770;&#65292;&#20197;&#26174;&#31034;&#36739;&#20302;&#38454;&#35823;&#24046;&#24352;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#39640;&#38454;&#30340;&#35823;&#24046;&#24352;&#37327;&#20063;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon&gt;0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#27714;&#35299;&#30340;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26500;&#36896;&#28385;&#36275;&#27491;&#23450;&#24615;&#21644;&#21487;&#23454;&#38469;&#35745;&#31639;&#23646;&#24615;&#30340;&#39532;&#27663;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#35813;&#20272;&#35745;&#22120;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#33021;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.12438</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#65306;&#36890;&#36807;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices. (arXiv:2307.12438v2 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#27714;&#35299;&#30340;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26500;&#36896;&#28385;&#36275;&#27491;&#23450;&#24615;&#21644;&#21487;&#23454;&#38469;&#35745;&#31639;&#23646;&#24615;&#30340;&#39532;&#27663;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#35813;&#20272;&#35745;&#22120;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#33021;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#65292;&#20854;&#26500;&#24314;&#20026;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#26500;&#36896;&#26159;&#27491;&#23450;&#30340;&#65292;&#24182;&#19988;&#20854;&#26368;&#23567;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#20855;&#26377;&#21487;&#23454;&#38469;&#35745;&#31639;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#24418;&#22238;&#24402;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#26159;&#22312;&#29305;&#23450;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#40654;&#26364;&#22238;&#24402;&#26694;&#26550;&#21253;&#21547;&#20102;&#20174;&#25511;&#21046;&#21464;&#37327;&#26500;&#24314;&#30340;&#29616;&#26377;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#31034;&#20363;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#21333;&#20445;&#30495;&#24230;&#21644;&#20854;&#20182;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#30340;&#24179;&#26041;&#65292;&#20943;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;&#27491;&#23450;&#24615;&#30340;&#20445;&#25345;&#30830;&#20445;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#19982;&#19979;&#28216;&#20219;&#21153;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#36827;&#34892;&#26080;&#22768;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#37325;&#22823;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#20102;&#22810;&#31181;&#25915;&#20987;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#26080;&#38656;&#28155;&#21152;&#30828;&#20214;&#25110;&#22686;&#21152;&#35774;&#22791;&#25216;&#33021;&#30340;&#29289;&#29702;&#35775;&#38382;&#26469;&#21457;&#29616;&#21644;&#25317;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#23569;&#25968;&#27493;&#39588;&#20013;&#24555;&#36895;&#25317;&#26377;&#20102;&#25152;&#26377;&#33410;&#28857;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#38750;&#20256;&#32479;&#32593;&#32476;&#21644;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12204</link><description>&lt;p&gt;
&#23545;&#26080;&#22768;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Adversarial Agents For Attacking Inaudible Voice Activated Devices. (arXiv:2307.12204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#36827;&#34892;&#26080;&#22768;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#37325;&#22823;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#20102;&#22810;&#31181;&#25915;&#20987;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#26080;&#38656;&#28155;&#21152;&#30828;&#20214;&#25110;&#22686;&#21152;&#35774;&#22791;&#25216;&#33021;&#30340;&#29289;&#29702;&#35775;&#38382;&#26469;&#21457;&#29616;&#21644;&#25317;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#23569;&#25968;&#27493;&#39588;&#20013;&#24555;&#36895;&#25317;&#26377;&#20102;&#25152;&#26377;&#33410;&#28857;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#38750;&#20256;&#32479;&#32593;&#32476;&#21644;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26080;&#22768;&#25915;&#20987;&#23545;&#35821;&#38899;&#28608;&#27963;&#35774;&#22791;&#30340;&#20998;&#26512;&#30830;&#35748;&#20102;7.6&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#30001;NIST&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#65288;NVD&#65289;&#29420;&#31435;&#35780;&#20998;&#30340;&#37325;&#22823;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#25915;&#20987;&#32773;&#20351;&#29992;&#26080;&#22768;&#35821;&#38899;&#21629;&#20196;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#21463;&#20445;&#25252;&#31508;&#35760;&#26412;&#19978;&#26426;&#23494;&#20449;&#24687;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#35813;&#22522;&#32447;&#32593;&#32476;&#27169;&#22411;&#19978;&#27169;&#25311;&#20102;&#35768;&#22810;&#25915;&#20987;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#29289;&#29702;&#35775;&#38382;&#32780;&#26080;&#38656;&#28155;&#21152;&#26032;&#30828;&#20214;&#25110;&#22686;&#24378;&#35774;&#22791;&#25216;&#33021;&#30340;&#20114;&#32852;&#35774;&#22791;&#30340;&#22823;&#35268;&#27169;&#21033;&#29992;&#28508;&#21147;&#26469;&#21457;&#29616;&#21644;&#25317;&#26377;&#29305;&#26435;&#20449;&#24687;&#12290;&#20351;&#29992;&#24494;&#36719;&#30340;CyberBattleSim&#26694;&#26550;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21457;&#29616;&#28145;&#24230;Q&#23398;&#20064;&#19982;&#24320;&#21457;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#65292;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#36805;&#36895;&#25317;&#26377;&#25152;&#26377;&#33410;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#38750;&#20256;&#32479;&#32593;&#32476;&#21644;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measure
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11768</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39564;&#35777;&#20854;&#34892;&#20026;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#35201;&#27714;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20197;&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#24335;&#22806;&#21270;&#20854;&#25512;&#29702;&#36807;&#31243;&#65288;&#24605;&#32500;&#38142;&#65307;CoT&#65289;&#12290;&#25512;&#29702;&#36807;&#31243;&#21487;&#20197;&#35753;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#25152;&#38472;&#36848;&#30340;&#25512;&#29702;&#33021;&#22815;&#24544;&#23454;&#22320;&#21453;&#26144;&#27169;&#22411;&#30340;&#23454;&#38469;&#25512;&#29702;&#65292;&#32780;&#36825;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#25552;&#39640;CoT&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#26469;&#29983;&#25104;&#25512;&#29702;&#12290;&#22522;&#20110;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#25509;&#36817;CoT&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#25152;&#38472;&#36848;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#22312;&#21333;&#29420;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#31572;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#22823;&#22823;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.11465</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25972;&#20307;&#29983;&#23384;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11465
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24212;&#29992;&#20110;&#32954;&#30284;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#30149;&#20154;&#29366;&#24577;&#30340;&#25972;&#20307;&#29983;&#23384;&#65288;OS&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#29983;&#23384;&#27010;&#29575;&#19981;&#21516;&#30340;&#20122;&#32452;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#21644;&#25913;&#21892;&#25972;&#20307;&#29983;&#23384;&#29575;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27599;&#20010;&#30149;&#20154;&#30340;&#21487;&#29992;&#20449;&#24687;&#65292;&#21033;&#29992;&#26410;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#27515;&#20129;&#65289;&#21644;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#24184;&#23384;&#32773;&#65289;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#20063;&#35201;&#32771;&#34385;&#21040;&#27515;&#20129;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#19981;&#23436;&#25972;&#25968;&#25454;&#22788;&#29702;&#26159;&#21307;&#23398;&#39046;&#22495;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#25554;&#34917;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30149;&#20154;&#21450;&#20854;&#21487;&#29992;&#29305;&#24449;&#20013;&#26377;&#25928;&#23398;&#20064;&#65292;&#39044;&#27979;NSCLC&#30149;&#20154;&#30340;OS&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2307.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#23454;&#29616;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#19979;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65288;SCMs&#65289;&#24050;&#25104;&#20026;&#27604;&#36739;&#26696;&#20363;&#30740;&#31350;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;SCMs&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#35266;&#27979;&#32467;&#26524;&#30340;&#21152;&#26435;&#21644;&#26469;&#20272;&#35745;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#21512;&#25104;&#23545;&#29031;&#65288;SC&#65289;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;SC&#26435;&#37325;&#30340;&#20272;&#35745;&#25104;&#20026;&#20102;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#29616;&#26377;&#30340;SCMs&#23384;&#22312;&#19968;&#20010;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#65292;&#21363;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#19982;&#21453;&#20107;&#23454;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20250;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#21305;&#37197;&#30340;&#26032;&#22411;SCM&#65292;&#20551;&#35774;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#21487;&#20197;&#29992;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#36817;&#20284;&#65288;&#21363;&#28151;&#21512;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#31526;&#21512;Feldman&#30340;&#38271;&#23614;&#29702;&#35770;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38271;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#65292;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#32447;&#24615;&#20998;&#31867;&#22120;&#19981;&#33021;&#12290;&#35813;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#20110;&#38271;&#23614;&#20998;&#24067;&#65292;&#38656;&#35201;&#32771;&#34385;&#32597;&#35265;&#30340;&#35757;&#32451;&#26679;&#26412;&#20197;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10736</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#19979;&#30340;&#38271;&#23614;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#31526;&#21512;Feldman&#30340;&#38271;&#23614;&#29702;&#35770;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38271;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#65292;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#32447;&#24615;&#20998;&#31867;&#22120;&#19981;&#33021;&#12290;&#35813;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#20110;&#38271;&#23614;&#20998;&#24067;&#65292;&#38656;&#35201;&#32771;&#34385;&#32597;&#35265;&#30340;&#35757;&#32451;&#26679;&#26412;&#20197;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#29983;&#25104;&#36981;&#24490;Feldman&#30340;&#38271;&#23614;&#29702;&#35770;&#65288;2020&#65289;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#32447;&#24615;&#20998;&#31867;&#22120;&#26080;&#27861;&#23558;&#27867;&#21270;&#35823;&#24046;&#38477;&#20302;&#21040;&#19968;&#23450;&#27700;&#24179;&#20197;&#19979;&#65292;&#32780;&#20855;&#26377;&#35760;&#24518;&#33021;&#21147;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21487;&#20197;&#12290;&#36825;&#35777;&#23454;&#20102;&#23545;&#20110;&#38271;&#23614;&#20998;&#24067;&#65292;&#24517;&#39035;&#32771;&#34385;&#32597;&#35265;&#30340;&#35757;&#32451;&#26679;&#26412;&#20197;&#23454;&#29616;&#23545;&#26032;&#25968;&#25454;&#30340;&#26368;&#20339;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23376;&#32676;&#20307;&#39057;&#29575;&#20998;&#24067;&#30340;&#23614;&#37096;&#21464;&#30701;&#26102;&#65292;&#32447;&#24615;&#27169;&#22411;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23384;&#20648;&#36890;&#36947;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#26469;&#22686;&#21152;&#36890;&#36947;&#23481;&#37327;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#40657;&#30418;&#35775;&#38382;&#23454;&#29616;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2307.08811</link><description>&lt;p&gt;
DeepMem: &#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20316;&#23384;&#20648;&#36890;&#36947;&#21450;&#20854;&#65288;&#35823;&#29992;&#65289;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23384;&#20648;&#36890;&#36947;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#26469;&#22686;&#21152;&#36890;&#36947;&#23481;&#37327;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#40657;&#30418;&#35775;&#38382;&#23454;&#29616;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20026;&#20102;&#25903;&#25345;&#36890;&#29992;&#24615;&#21644;&#36991;&#20813;&#36807;&#25311;&#21512;&#32780;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#26082;&#21487;&#20197;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#38544;&#34255;&#19968;&#20010;&#27169;&#22411;&#65289;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#26377;&#30410;&#30446;&#30340;&#65288;&#20363;&#22914;&#65292;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#24687;&#35770;&#35270;&#35282;&#65307;&#25105;&#20204;&#23558;ML&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#23384;&#20648;&#36890;&#36947;&#65292;&#20854;&#23481;&#37327;&#38543;&#30528;&#36807;&#24230;&#21442;&#25968;&#21270;&#32780;&#22686;&#21152;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21457;&#36865;&#26041;&#65292;&#22312;&#35757;&#32451;&#26102;&#23558;&#20219;&#24847;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#25509;&#25910;&#26041;&#21487;&#20197;&#36890;&#36807;&#23545;&#37096;&#32626;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#26681;&#25454;&#21487;&#29992;&#21442;&#25968;&#30340;&#25968;&#37327;&#25512;&#23548;&#20986;&#36890;&#36947;&#23481;&#37327;&#30340;&#19978;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#40657;&#30418;&#20889;&#20837;&#21644;&#35835;&#21462;&#21407;&#35821;&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#65306;&#65288;i&#65289;&#36890;&#36807;&#22312;&#21457;&#23556;&#26426;&#31471;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#24335;&#20197;&#20248;&#21270;&#22320;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#27169;&#22411;&#20013;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35835;&#21462;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte
&lt;/p&gt;</description></item><item><title>Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.08621</link><description>&lt;p&gt;
Retentive Network: &#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#30340;&#32487;&#20219;&#32773;
&lt;/p&gt;
&lt;p&gt;
Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08621
&lt;/p&gt;
&lt;p&gt;
Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retentive Network (RetNet)&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;&#24490;&#29615;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#20445;&#30041;&#26426;&#21046;&#65292;&#25903;&#25345;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;&#21363;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24182;&#34892;&#34920;&#31034;&#20801;&#35768;&#36827;&#34892;&#35757;&#32451;&#24182;&#34892;&#21270;&#12290;&#24490;&#29615;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#20302;&#25104;&#26412;&#30340;$O(1)$&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#35299;&#30721;&#21534;&#21520;&#37327;&#12289;&#24310;&#36831;&#21644;GPU&#20869;&#23384;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#20998;&#22359;&#24490;&#29615;&#34920;&#31034;&#20415;&#20110;&#20351;&#29992;&#32447;&#24615;&#22797;&#26434;&#24230;&#36827;&#34892;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#21487;&#20197;&#24182;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#36827;&#34892;&#24490;&#29615;&#25688;&#35201;&#12290;&#35821;&#35328;&#24314;&#27169;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RetNet&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#32467;&#26524;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#20302;&#25104;&#26412;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#22312;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#26412;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#29992;&#26368;&#23567;&#38169;&#35823;&#29109;&#20195;&#26367;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.08572</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#30340;&#40065;&#26834;&#24615;&#65306;&#36716;&#31227;&#23398;&#20064;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#38169;&#35823;&#29109;&#20934;&#21017;&#22312;&#22788;&#29702;&#38750;&#39640;&#26031;&#22122;&#22768;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#26412;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#29992;&#26368;&#23567;&#38169;&#35823;&#29109;&#20195;&#26367;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#26159;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#20415;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#25968;&#25454;&#19981;&#21253;&#21547;&#22122;&#22768;&#65292;&#35201;&#20040;&#37319;&#29992;&#22797;&#26434;&#30340;&#35757;&#32451;&#33539;&#24335;&#25110;&#27169;&#22411;&#35774;&#35745;&#26469;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32479;&#35745;&#20449;&#21495;&#22788;&#29702;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#23567;&#38169;&#35823;&#29109;&#65288;MEE&#65289;&#20934;&#21017;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#23454;&#38469;&#36716;&#31227;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20854;&#20013;&#20998;&#24067;&#36716;&#31227;&#26159;&#24120;&#35265;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;MEE&#23545;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#26367;&#25442;&#20026;MEE&#65292;&#22312;&#22522;&#26412;&#30340;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#24494;&#35843;&#21644;&#32447;&#24615;&#25506;&#27979;&#65289;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#19982;&#29616;&#26377;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.07679</link><description>&lt;p&gt;
&#21305;&#37197;&#36861;&#36394;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21363;&#36890;&#36807;&#23383;&#20856;&#20013;&#30340;&#20803;&#32032;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#36817;&#20284;&#30446;&#26631;&#20989;&#25968;&#30340;&#32431;&#36138;&#23146;&#31639;&#27861;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21253;&#21547;&#22312;&#23545;&#24212;&#20110;&#23383;&#20856;&#30340;&#21464;&#21270;&#31354;&#38388;&#20013;&#26102;&#65292;&#35768;&#22810;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#30740;&#31350;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#33719;&#24471;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#33719;&#24471;&#21305;&#37197;&#36861;&#36394;&#24615;&#33021;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#19979;&#30028;&#20197;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19978;&#30028;&#19981;&#33021;&#25913;&#36827;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#36138;&#23146;&#31639;&#27861;&#21464;&#20307;&#19981;&#21516;&#65292;&#25910;&#25947;&#36895;&#24230;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#30001;&#35299;&#26576;&#20010;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#20915;&#23450;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20219;&#24847;&#31243;&#24230;&#30340;&#25910;&#32553;&#37117;&#20250;&#25913;&#21892;&#21305;&#37197;&#36861;&#36394;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07343</link><description>&lt;p&gt;
MaxMin-L2-SVC-NCH:&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21442;&#25968;&#30340;&#36873;&#25321;&#22312;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#65288;SVC&#65289;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24120;&#29992;&#30340;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#26159;k&#25240;&#20132;&#21449;&#39564;&#35777;&#19982;&#26684;&#28857;&#25628;&#32034;&#65288;CV&#65289;&#12290;&#30001;&#20110;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#30340;SVC&#27169;&#22411;&#65292;&#36825;&#20010;&#26041;&#27861;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;SVC&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#39318;&#20808;&#65292;&#23558;&#20855;&#26377;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#30340;SVC&#35757;&#32451;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65288;MaxMin-L2-SVC-NCH&#65289;&#65292;&#20854;&#20013;&#26497;&#23567;&#21270;&#38382;&#39064;&#26159;&#23547;&#25214;&#20004;&#20010;&#27491;&#24120;&#20984;&#22771;&#20043;&#38388;&#26368;&#25509;&#36817;&#28857;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;L2-SVC-NCH&#65289;&#65292;&#32780;&#26497;&#22823;&#21270;&#38382;&#39064;&#26159;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;MaxMin-L2-SVC-NCH&#20855;&#26377;&#36739;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#25918;&#24323;&#20102;CV&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26469;&#27714;&#35299;MaxMin-L2-SVC-NCH&#65292;&#20854;&#20013;L2-SVC-NCH&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#20174;&#19994;&#32773;&#19982;&#24037;&#20855;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#36825;&#20123;&#20114;&#21160;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#21644;&#31995;&#32479;&#24320;&#21457;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#22312;&#23398;&#20064;&#21644;&#21327;&#35843;&#23454;&#36341;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06518</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#21644;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
Machine Learning practices and infrastructures. (arXiv:2307.06518v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#20174;&#19994;&#32773;&#19982;&#24037;&#20855;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#36825;&#20123;&#20114;&#21160;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#21644;&#31995;&#32479;&#24320;&#21457;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#22312;&#23398;&#20064;&#21644;&#21327;&#35843;&#23454;&#36341;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#22312;&#37325;&#22823;&#39046;&#22495;&#37096;&#32626;&#26102;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#33021;&#21152;&#21095;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#65292;&#21019;&#36896;&#26032;&#30340;&#27495;&#35270;&#27169;&#24335;&#65292;&#24182;&#22266;&#21270;&#36807;&#26102;&#30340;&#31038;&#20250;&#26500;&#36896;&#12290;&#22240;&#27492;&#65292;ML&#31995;&#32479;&#24320;&#21457;&#30340;&#31038;&#20250;&#32972;&#26223;&#65288;&#21363;&#32452;&#32455;&#12289;&#22242;&#38431;&#12289;&#25991;&#21270;&#65289;&#26159;AI&#20262;&#29702;&#39046;&#22495;&#21644;&#20915;&#31574;&#32773;&#36827;&#34892;&#31215;&#26497;&#30740;&#31350;&#21644;&#24178;&#39044;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#24120;&#34987;&#24573;&#35270;&#30340;&#31038;&#20250;&#32972;&#26223;&#26041;&#38754;&#65306;&#20174;&#19994;&#32773;&#19982;&#20182;&#20204;&#25152;&#20381;&#36182;&#30340;&#24037;&#20855;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#36825;&#20123;&#20114;&#21160;&#22312;&#22609;&#36896;ML&#23454;&#36341;&#21644;ML&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;Stack Exchange&#35770;&#22363;&#19978;&#25552;&#20986;&#30340;&#38382;&#39064;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;ML&#23454;&#36341;&#20013;&#20351;&#29992;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#65288;&#22914;Jupyter Notebook&#21644;Google Colab&#65289;&#12290;&#25105;&#21457;&#29616;&#20132;&#20114;&#24335;&#35745;&#31639;&#24179;&#21488;&#22312;&#23398;&#20064;&#21644;&#21327;&#35843;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#26500;&#25104;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05946</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25968;&#25454;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22810;&#23618;&#26550;&#26500;&#23545;&#22797;&#26434;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#24314;&#27169;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#19981;&#25552;&#20379;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#36825;&#23545;&#20110;&#20132;&#36890;&#36816;&#33829;&#21644;&#25511;&#21046;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#24402;&#19968;&#21270;&#21040;&#20854;&#38544;&#34255;&#23618;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#36890;&#36807;&#25511;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#24182;&#20943;&#23569;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
&lt;/p&gt;</description></item><item><title>&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05747</link><description>&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#30456;&#32467;&#21512;&#65306;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05747
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#33719;&#21462;&#26032;&#25216;&#33021;&#25110;&#30693;&#35782;&#26102;&#65292;&#36890;&#36807;&#35838;&#31243;&#36827;&#34892;&#23398;&#20064;&#21644;&#22797;&#20064;&#12290;&#36825;&#31181;&#20154;&#31867;&#23398;&#20064;&#34892;&#20026;&#21551;&#21457;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#30693;&#35782;&#20445;&#30041;&#21644;&#20419;&#36827;&#23398;&#20064;&#36716;&#31227;&#12290;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#22238;&#25918;&#26041;&#27861;&#28041;&#21450;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#38543;&#26426;&#36873;&#25321;&#21644;&#25490;&#24207;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#19977;&#20010;&#20855;&#20307;&#26041;&#38754;&#65306;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#65292;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#65292;&#20197;&#21450;&#36873;&#25321;&#23454;&#20363;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#35838;&#31243;&#35774;&#35745;&#30340;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20043;&#38388;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#21327;&#35843;&#29305;&#24449;&#24402;&#22240;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#29305;&#24449;&#24402;&#22240;&#30340;&#21327;&#35843;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02150</link><description>&lt;p&gt;
&#36328;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23454;&#29616;&#29305;&#24449;&#24402;&#22240;&#30340;&#21327;&#35843;: &#25552;&#21319;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02150
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20043;&#38388;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#21327;&#35843;&#29305;&#24449;&#24402;&#22240;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#29305;&#24449;&#24402;&#22240;&#30340;&#21327;&#35843;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#35768;&#22810;&#20851;&#27880;&#65292;&#23427;&#20204;&#36890;&#36807;&#23558;&#37325;&#35201;&#24615;&#24402;&#22240;&#32473;&#20010;&#21035;&#36755;&#20837;&#29305;&#24449;&#26469;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29305;&#24449;&#24402;&#22240;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65289;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#23558;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20316;&#20026;&#26410;&#26469;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#22312;&#37319;&#29992;&#19981;&#21516;&#26550;&#26500;&#20294;&#20197;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#20043;&#38388;&#21327;&#35843;&#36825;&#20123;&#29305;&#24449;&#12290;&#36890;&#36807;&#25506;&#32034;&#36825;&#31181;&#21327;&#35843;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#20986;&#26356;&#19968;&#33268;&#21644;&#20048;&#35266;&#30340;&#29305;&#24449;&#24402;&#22240;&#29702;&#35299;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#23616;&#37096;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#23454;&#29616;&#29305;&#24449;&#24402;&#22240;&#21327;&#35843;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature att
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#22686;&#22823;&#27493;&#38271;&#21644;&#31639;&#23376;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#22823;&#27493;&#38271;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#25910;&#25947;&#24615;&#30340;&#21516;&#26102;&#31616;&#21333;&#26377;&#25928;&#22320;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#65307;&#32780;&#31639;&#23376;&#23398;&#20064;&#21017;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#21152;&#36895;&#27714;&#35299;&#28041;&#21450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.00296</link><description>&lt;p&gt;
&#25193;&#22823;&#27493;&#38271;&#21644;&#31639;&#23376;&#23398;&#20064;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems. (arXiv:2307.00296v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#22686;&#22823;&#27493;&#38271;&#21644;&#31639;&#23376;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#22823;&#27493;&#38271;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#25910;&#25947;&#24615;&#30340;&#21516;&#26102;&#31616;&#21333;&#26377;&#25928;&#22320;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#65307;&#32780;&#31639;&#23376;&#23398;&#20064;&#21017;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#21152;&#36895;&#27714;&#35299;&#28041;&#21450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31867;&#20855;&#26377;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#38750;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#21644;&#31163;&#25955;&#21270;&#21518;&#30340;&#39640;&#32500;&#21644;&#30149;&#24577;&#31995;&#32479;&#65292;&#36825;&#31867;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20998;&#21035;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#37327;&#65292;&#22240;&#27492;&#27599;&#27425;&#36845;&#20195;&#30340;&#20027;&#35201;&#35745;&#31639;&#21482;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;PDE&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#36739;&#22823;&#30340;&#27493;&#38271;&#25110;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;&#26469;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#23545;&#20110;&#20855;&#26377;&#36739;&#22823;&#27493;&#38271;&#30340;&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#20854;&#25910;&#25947;&#24615;&#20173;&#28982;&#21487;&#20197;&#24471;&#21040;&#20005;&#26684;&#35777;&#26126;&#65292;&#21516;&#26102;&#23427;&#20197;&#19968;&#31181;&#31616;&#21333;&#19988;&#26222;&#36941;&#30340;&#26041;&#24335;&#25968;&#20540;&#19978;&#21152;&#36895;&#20102;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#23545;&#20110;&#31639;&#23376;&#23398;&#20064;&#21152;&#36895;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#34920;&#31034;&#28041;&#21450;&#30340;PDE&#12290;&#19968;&#26086;&#23398;&#20064;&#21040;&#19968;&#20010;&#31070;&#32463;&#31639;&#23376;&#65292;&#35299;&#20915;&#19968;&#20010;PDE&#21482;&#38656;&#35201;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a general class of nonsmooth optimal control problems with partial differential equation (PDE) constraints, which are very challenging due to its nonsmooth objective functionals and the resulting high-dimensional and ill-conditioned systems after discretization. We focus on the application of a primal-dual method, with which different types of variables can be treated individually and thus its main computation at each iteration only requires solving two PDEs. Our target is to accelerate the primal-dual method with either larger step sizes or operator learning techniques. For the accelerated primal-dual method with larger step sizes, its convergence can be still proved rigorously while it numerically accelerates the original primal-dual method in a simple and universal way. For the operator learning acceleration, we construct deep neural network surrogate models for the involved PDEs. Once a neural operator is learned, solving a PDE requires only a forward pass of the neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15865</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20132;&#25442;&#20449;&#24687;&#26469;&#20272;&#35745;&#20174;&#20854;&#31169;&#19979;&#35266;&#23519;&#30340;&#26679;&#26412;&#20013;&#26410;&#30693;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#20294;&#20182;&#20204;&#20063;&#38754;&#20020;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#26696;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#21644;&#32593;&#32476;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#65292;&#21516;&#26102;&#28385;&#36275;&#20195;&#29702;&#30340;&#38544;&#31169;&#38656;&#27714;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36229;&#36234;&#20182;&#20204;&#26412;&#22320;&#38468;&#36817;&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#21442;&#19982;&#30340;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25110;&#38543;&#26102;&#38388;&#22312;&#32447;&#33719;&#21462;&#30340;&#31169;&#26377;&#20449;&#21495;&#20013;&#20272;&#35745;&#23436;&#25972;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#24182;&#20445;&#25252;&#20854;&#20449;&#21495;&#21644;&#32593;&#32476;&#38468;&#36817;&#30340;&#38544;&#31169;&#12290;&#36825;&#26159;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#23454;&#29616;&#30340;&#65292;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#20132;&#25442;&#30340;&#20272;&#35745;&#25968;&#25454;&#20013;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
&lt;/p&gt;</description></item><item><title>&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10891</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10891
&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#20934;&#30830;&#30340;&#36127;&#36733;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#22312;&#26412;&#22320;&#24179;&#34913;&#20379;&#38656;&#65292;&#24182;&#38450;&#27490;&#30005;&#32593;&#25925;&#38556;&#12290;&#23613;&#31649;&#34987;&#30417;&#27979;&#30340;&#23458;&#25143;&#25968;&#37327;&#23558;&#38543;&#30528;&#19981;&#26029;&#25512;&#36827;&#30340;&#26234;&#33021;&#30005;&#34920;&#23433;&#35013;&#32780;&#22686;&#21152;&#65292;&#20294;&#27599;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#37327;&#22987;&#32456;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#25442;&#22120;&#36127;&#36733;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#21463;&#30410;&#20110;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#21363;&#22312;&#22810;&#20010;&#23458;&#25143;&#30340;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#20840;&#23616;&#30340;&#21333;&#21464;&#37327;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#20004;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#20248;&#20110;&#30456;&#20851;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;&#20854;&#20182;&#20004;&#31181;&#31574;&#30053;&#30456;&#27604;&#65292;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#22312;&#20174;&#26410;&#26469;&#19968;&#22825;&#21040;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;LSTM&#27169;&#22411;&#30340;&#27604;&#36739;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#36755;&#20837;&#21644;&#37096;&#20998;&#25110;&#23436;&#20840;&#29366;&#24577;&#36755;&#20986;&#30340;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Lorenz&#31995;&#32479;&#21644;Chua&#25391;&#33633;&#22120;&#65292;&#23637;&#31034;&#20102;ESN&#30340;&#30701;&#26399;&#39044;&#27979;&#33021;&#21147;&#21644;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;&#65292;&#20197;&#21450;ESN&#22312;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10797</link><description>&lt;p&gt;
&#37096;&#20998;&#35266;&#27979;&#21160;&#21147;&#31995;&#32479;&#20013;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Variability of echo state network prediction horizon for partially observed dynamical systems. (arXiv:2306.10797v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#36755;&#20837;&#21644;&#37096;&#20998;&#25110;&#23436;&#20840;&#29366;&#24577;&#36755;&#20986;&#30340;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Lorenz&#31995;&#32479;&#21644;Chua&#25391;&#33633;&#22120;&#65292;&#23637;&#31034;&#20102;ESN&#30340;&#30701;&#26399;&#39044;&#27979;&#33021;&#21147;&#21644;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;&#65292;&#20197;&#21450;ESN&#22312;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30740;&#31350;&#21160;&#21147;&#31995;&#32479;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#36755;&#20837;&#21644;&#37096;&#20998;&#25110;&#23436;&#20840;&#29366;&#24577;&#36755;&#20986;&#30340;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;Lorenz&#31995;&#32479;&#21644;Chua&#25391;&#33633;&#22120;&#65288;&#21253;&#25324;&#25968;&#20540;&#27169;&#25311;&#21644;&#23454;&#39564;&#31995;&#32479;&#65289;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#26174;&#31034;ESN&#20316;&#20026;&#19968;&#20010;&#33258;&#20027;&#21160;&#21147;&#31995;&#32479;&#65292;&#33021;&#22815;&#20570;&#20986;&#30701;&#26399;&#39044;&#27979;&#65292;&#21487;&#36798;&#20960;&#20010;Lyapunov&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;&#21462;&#20915;&#20110;&#21021;&#22987;&#26465;&#20214;-&#36825;&#26159;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#35270;&#35282;&#30340;&#20998;&#24067;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#32479;&#35745;&#25351;&#26631;&#23558;ESN&#39044;&#27979;&#30340;&#38271;&#26399;&#21160;&#21147;&#23398;&#19982;&#25968;&#20540;&#27169;&#25311;&#25110;&#23454;&#39564;&#21160;&#21147;&#23398;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35266;&#23519;&#21040;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#34920;&#26126;ESN&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Study of dynamical systems using partial state observation is an important problem due to its applicability to many real-world systems. We address the problem by proposing an echo state network (ESN) framework with partial state input with partial or full state output. Application to the Lorenz system and Chua's oscillator (both numerically simulated and experimental systems) demonstrate the effectiveness of our method. We show that the ESN, as an autonomous dynamical system, is capable of making short-term predictions up to a few Lyapunov times. However, the prediction horizon has high variability depending on the initial condition - an aspect that we explore in detail using the distribution of the prediction horizon. Further, using a variety of statistical metrics to compare the long-term dynamics of the ESN predictions with numerically simulated or experimental dynamics and observed similar results, we show that the ESN can effectively learn the system's dynamics even when trained w
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26500;&#36896;&#20102;&#38024;&#23545;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;&#65292;&#33021;&#22815;&#23454;&#29616;&#31561;&#21464;&#19988;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.07350</link><description>&lt;p&gt;
G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
G-invariant diffusion maps. (arXiv:2306.07350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07350
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26500;&#36896;&#20102;&#38024;&#23545;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;&#65292;&#33021;&#22815;&#23454;&#29616;&#31561;&#21464;&#19988;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25193;&#25955;&#22320;&#22270;&#22312;&#20174;&#38477;&#32500;&#21644;&#32858;&#31867;&#21040;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21040;&#20174;&#19968;&#20010;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#26082;&#31561;&#21464;&#21448;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26500;&#36896;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diffusion maps embedding of data lying on a manifold have shown success in tasks ranging from dimensionality reduction and clustering, to data visualization. In this work, we consider embedding data sets which were sampled from a manifold which is closed under the action of a continuous matrix group. An example of such a data set are images who's planar rotations are arbitrary. The G-invariant graph Laplacian, introduced in a previous work of the authors, admits eigenfunctions in the form of tensor products between the elements of the irreducible unitary representations of the group and eigenvectors of certain matrices. We employ these eigenfunctions to derive diffusion maps that intrinsically account for the group action on the data. In particular, we construct both equivariant and invariant embeddings which can be used naturally to cluster and align the data points. We demonstrate the effectiveness of our construction with simulated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#39044;&#23450;&#21464;&#25442;&#32676;&#19979;&#23398;&#20064;&#19981;&#21464;&#30340;&#23383;&#20856;&#38382;&#39064;&#12290;&#21033;&#29992;&#38750;&#38463;&#36125;&#23572;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#29702;&#35299;&#20026;&#26576;&#20123;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.19557</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#34920;&#31034;&#23398;&#20064;&#23545;&#31216;&#19979;&#30340;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dictionary Learning under Symmetries via Group Representations. (arXiv:2305.19557v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#39044;&#23450;&#21464;&#25442;&#32676;&#19979;&#23398;&#20064;&#19981;&#21464;&#30340;&#23383;&#20856;&#38382;&#39064;&#12290;&#21033;&#29992;&#38750;&#38463;&#36125;&#23572;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#29702;&#35299;&#20026;&#26576;&#20123;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21512;&#36866;&#30340;&#21464;&#25442;&#65292;&#20197;&#20415;&#36890;&#36807;&#31034;&#20363;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#23450;&#30340;&#21464;&#25442;&#32676;&#19979;&#23398;&#20064;&#19981;&#21464;&#30340;&#23383;&#20856;&#38382;&#39064;&#12290;&#33258;&#28982;&#30340;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#20919;&#20923;&#30005;&#38236;&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#12289;&#21516;&#27493;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;&#25105;&#20204;&#29305;&#21035;&#20174;&#25968;&#23398;&#34920;&#31034;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#38463;&#36125;&#23572;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#31526;&#21512;&#36825;&#20123;&#19981;&#21464;&#24615;&#30340;&#23383;&#20856;&#23398;&#20064;&#25552;&#20379;&#20102;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#28982;&#30028;&#20013;&#30340;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#33258;&#28982;&#34987;&#24314;&#27169;&#20026;&#26080;&#38480;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#19982;&#30456;&#20851;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#36825;&#24517;&#28982;&#26159;&#26377;&#38480;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#29702;&#35299;&#20026;&#26576;&#20123;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Fourier-DeepONet&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#20855;&#26377;&#23545;&#22320;&#38663;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#28304;&#30340;FWI&#12290;</title><link>http://arxiv.org/abs/2305.17289</link><description>&lt;p&gt;
Fourier-DeepONet: &#22686;&#24378;&#20613;&#37324;&#21494;&#31639;&#23376;&#28145;&#24230;&#32593;&#32476;&#30340;&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#25552;&#39640;&#20102;&#31934;&#24230;&#65292;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness. (arXiv:2305.17289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Fourier-DeepONet&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#20855;&#26377;&#23545;&#22320;&#38663;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#28304;&#30340;FWI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#27874;&#24418;&#21453;&#28436;&#65288;FWI&#65289;&#26159;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20174;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#20013;&#25512;&#26029;&#22320;&#19979;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;FWI&#24050;&#32463;&#36234;&#26469;&#36234;&#34987;&#30740;&#31350;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#28508;&#22312;&#30340;&#35843;&#26597;&#28304;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#28304;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20613;&#37324;&#21494;&#22686;&#24378;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;Fourier-DeepONet&#65289;&#29992;&#20110;&#20840;&#27874;&#24418;&#21453;&#28436;&#65292;&#24182;&#19988;&#20855;&#26377;&#22320;&#38663;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#22320;&#38663;&#28304;&#30340;&#39057;&#29575;&#21644;&#20301;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;DeepONet&#30340;&#35299;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;&#28304;&#21442;&#25968;&#20316;&#20026;Fourier-DeepONet&#30340;&#20854;&#20013;&#19968;&#20010;&#36755;&#20837;&#65292;&#20197;&#23454;&#29616;&#23545;&#20855;&#26377;&#21487;&#21464;&#28304;&#30340;FWI&#30340;&#20998;&#36776;&#29575;&#12290;&#20026;&#20102;&#27979;&#35797;Fourier-DeepONet&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#36924;&#30495;&#30340;FWI&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;FWI-F&#21644;FWI-L&#65289;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#28304;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full waveform inversion (FWI) infers the subsurface structure information from seismic waveform data by solving a non-convex optimization problem. Data-driven FWI has been increasingly studied with various neural network architectures to improve accuracy and computational efficiency. Nevertheless, the applicability of pre-trained neural networks is severely restricted by potential discrepancies between the source function used in the field survey and the one utilized during training. Here, we develop a Fourier-enhanced deep operator network (Fourier-DeepONet) for FWI with the generalization of seismic sources, including the frequencies and locations of sources. Specifically, we employ the Fourier neural operator as the decoder of DeepONet, and we utilize source parameters as one input of Fourier-DeepONet, facilitating the resolution of FWI with variable sources. To test Fourier-DeepONet, we develop two new and realistic FWI benchmark datasets (FWI-F and FWI-L) with varying source frequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21152;&#20837;&#20102;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#20197;&#23454;&#29616;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14675</link><description>&lt;p&gt;
MLP&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#22797;&#20167;
&lt;/p&gt;
&lt;p&gt;
Revenge of MLP in Sequential Recommendation. (arXiv:2305.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21152;&#20837;&#20102;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#20197;&#23454;&#29616;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#23545;&#21382;&#21490;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26356;&#22909;&#22320;&#25512;&#26029;&#21160;&#24577;&#20559;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#24471;&#30410;&#20110;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;RNN&#12289;CNN&#21644;Transformer&#65292;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36814;&#26469;&#20102;&#24555;&#36895;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#36817;&#65292;&#20840;MLP&#27169;&#22411;&#30340;&#30740;&#31350;&#25104;&#26524;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#30340;&#27880;&#24847;&#65292;&#21363;&#36890;&#36807;&#28151;&#21512;&#21382;&#21490;&#34892;&#20026;&#30340;MLP&#23398;&#20064;&#36716;&#25442;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#20840;&#36830;&#25509;&#32467;&#26500;&#20801;&#35768;&#19981;&#21463;&#38480;&#21046;&#30340;&#36328;&#34892;&#20026;&#38388;&#36890;&#20449;&#24182;&#24573;&#30053;&#20102;&#26102;&#38388;&#39034;&#24207;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#23558;&#28151;&#21512;MLP&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#65292;&#25913;&#36827;&#21518;&#30340;MLP&#36171;&#20104;&#20102;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#12290;&#30001;&#20110;MLP&#20013;&#30340;&#36328;&#26631;&#35760;&#20132;&#20114;&#23454;&#38469;&#19978;&#26159;&#30697;&#38453;...
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation models sequences of historical user-item interactive behaviors (or referred as token) to better infer dynamic preferences. Fueled by the improved neural network architectures such as RNN, CNN and Transformer, this field has enjoyed rapid performance boost in the past years. Recent progress on all-MLP models lights on an efficient method with less intensive computation, token-mixing MLP, to learn the transformation patterns among historical behaviors. However, due to the inherent fully-connection design that allows the unrestricted cross-token communication and ignores the chronological order, we find that directly applying token-mixing MLP into sequential recommendation leads to subpar performance. In this paper, we present a purely MLP-based sequential recommendation architecture TriMLP with a novel \underline{Tri}angular Mixer where the modified \underline{MLP} endows tokens with ordered interactions. As the cross-token interaction in MLP is actually matrix 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#26032;&#30340;&#26410;&#27880;&#24847;&#21040;&#30340;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2305.13127</link><description>&lt;p&gt;
&#20160;&#20040;&#30151;&#29366;&#20197;&#21450;&#25345;&#32493;&#22810;&#20037;&#65311;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media. (arXiv:2305.13127v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#26032;&#30340;&#26410;&#27880;&#24847;&#21040;&#30340;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#26368;&#24120;&#35265;&#21644;&#20005;&#37325;&#30340;&#31934;&#31070;&#30142;&#30149;&#65292;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#23545;&#20110;&#26089;&#26399;&#24178;&#39044;&#20197;&#20943;&#36731;&#36825;&#20123;&#21518;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#26412;&#36136;&#19978;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#25233;&#37057;&#30151;&#26816;&#27979;&#30740;&#31350;&#35797;&#22270;&#22522;&#20110;&#37325;&#35201;&#24615;&#20998;&#25968;&#25110;&#20851;&#27880;&#26435;&#37325;&#35299;&#37322;&#20915;&#31574;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#19982;&#20020;&#24202;&#25233;&#37057;&#30151;&#35786;&#26029;&#26631;&#20934;&#19981;&#19968;&#33268;&#65292;&#21518;&#32773;&#22522;&#20110;&#25233;&#37057;&#30151;&#29366;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36981;&#24490;&#35745;&#31639;&#35774;&#35745;&#31185;&#23398;&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21407;&#22411;&#32593;&#32476;(MSTPNet)&#12290;MSTPNet&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#12290;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;MSTPNet&#22312;F1&#20998;&#25968;0.851&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#12290;&#36825;&#20010;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#26410;&#22312;&#35843;&#26597;&#26041;&#27861;&#20013;&#27880;&#24847;&#21040;&#30340;&#26032;&#30151;&#29366;&#65292;&#20363;&#22914;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is the most prevalent and serious mental illness, which induces grave financial and societal ramifications. Depression detection is key for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few depression detection studies attempt to explain the decision based on the importance score or attention weights, these explanations misalign with the clinical depression diagnosis criterion that is based on depressive symptoms. To fill this gap, we follow the computational design science paradigm to develop a novel Multi-Scale Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and interprets depressive symptoms as well as how long they last. Extensive empirical analyses using a large-scale dataset show that MSTPNet outperforms state-of-the-art depression detection methods with an F1-score of 0.851. This result also reveals new symptoms that are unnoted in the survey approach, such as shari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#65292;&#20108;&#26159;&#23558;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#31561;&#35270;&#35273;&#30740;&#31350;&#21551;&#21457;&#30340;&#25216;&#26415;&#24341;&#20837;&#21040;SWF&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02164</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#26465;&#20214;&#21644;&#23616;&#37096;&#36830;&#25509;&#20999;&#29255;Wasserstein&#27969;&#37327;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#65292;&#20108;&#26159;&#23558;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#31561;&#35270;&#35273;&#30740;&#31350;&#21551;&#21457;&#30340;&#25216;&#26415;&#24341;&#20837;&#21040;SWF&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#27969;&#65288;SWF&#65289;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#29983;&#25104;&#24314;&#27169;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#21457;&#29983;&#29983;&#25104;&#36136;&#37327;&#30340;&#20122;&#20248;&#24615;&#21644;&#32570;&#20047;&#26465;&#20214;&#24314;&#27169;&#33021;&#21147;&#32780;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#19968;&#20010;&#24841;&#24742;&#30340;&#35266;&#23519;&#65288;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65289;&#65292;&#32852;&#21512;&#20998;&#24067;&#30340;SWF&#19982;&#26465;&#20214;&#20998;&#24067;&#30340;SWF&#30456;&#31526;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#36825;&#26159;SWF&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#24403;&#30340;&#22270;&#20687;&#24402;&#32435;&#20559;&#32622;&#21040;SWF&#20013;&#65292;&#29992;&#20004;&#20010;&#25216;&#26415;&#21463;&#21040;&#35270;&#35273;&#30740;&#31350;&#20013;&#30340;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#36890;&#36807;&#20840;&#37096;&#25913;&#36827;&#65292;&#22312;&#36827;&#34892;&#32431;&#38750;&#21442;&#25968;&#24314;&#27169;&#30340;&#21516;&#26102;&#65292;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#35768;&#22810;&#28145;&#24230;&#21442;&#25968;&#21270;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>B2Opt&#26159;&#19968;&#31181;&#23398;&#20064;&#20248;&#21270;&#40657;&#31665;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23569;&#37327;&#39044;&#31639;&#19979;&#23454;&#29616;&#26356;&#24555;&#32780;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#23398;&#20064;&#24182;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#24265;&#20215;&#20195;&#29702;&#20989;&#25968;&#25351;&#23548;&#39640;&#25928;&#20248;&#21270;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;B2Opt&#20855;&#26377;&#26356;&#24378;&#30340;&#20248;&#21270;&#31574;&#30053;&#34920;&#31034;&#33021;&#21147;&#24182;&#33021;&#23454;&#29616;&#22810;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.11787</link><description>&lt;p&gt;
B2Opt: &#29992;&#23569;&#37327;&#39044;&#31639;&#23398;&#20064;&#20248;&#21270;&#40657;&#31665;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
B2Opt: Learning to Optimize Black-box Optimization with Little Budget. (arXiv:2304.11787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11787
&lt;/p&gt;
&lt;p&gt;
B2Opt&#26159;&#19968;&#31181;&#23398;&#20064;&#20248;&#21270;&#40657;&#31665;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23569;&#37327;&#39044;&#31639;&#19979;&#23454;&#29616;&#26356;&#24555;&#32780;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#23398;&#20064;&#24182;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#24265;&#20215;&#20195;&#29702;&#20989;&#25968;&#25351;&#23548;&#39640;&#25928;&#20248;&#21270;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;B2Opt&#20855;&#26377;&#26356;&#24378;&#30340;&#20248;&#21270;&#31574;&#30053;&#34920;&#31034;&#33021;&#21147;&#24182;&#33021;&#23454;&#29616;&#22810;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#26114;&#36149;&#40657;&#31665;&#20248;&#21270;&#65288;BBO&#65289;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20197;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#25104;&#26412;&#26356;&#24555;&#22320;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#38382;&#39064;&#30340;&#26412;&#36136;&#22312;&#20110;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#25928;&#20248;&#21270;&#31574;&#30053;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#20174;&#30446;&#26631;&#25110;&#24265;&#20215;&#20195;&#29702;&#20219;&#21153;&#20013;&#33258;&#21160;&#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#31574;&#30053;&#30340;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;1&#65289;&#20511;&#37492;&#36951;&#20256;&#31639;&#27861;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;B2Opt&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#36866;&#32773;&#29983;&#23384;&#36873;&#25321;&#21407;&#29702;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#20248;&#21270;&#31574;&#30053;&#34920;&#31034;&#33021;&#21147;&#65307;2&#65289;B2Opt&#21487;&#20197;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#24265;&#20215;&#20195;&#29702;&#20989;&#25968;&#25351;&#23548;&#39640;&#25928;&#20248;&#21270;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;BBO&#22522;&#20934;&#30456;&#27604;&#65292;B2Opt&#21487;&#20197;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core challenge of high-dimensional and expensive black-box optimization (BBO) is how to obtain better performance faster with little function evaluation cost. The essence of the problem is how to design an efficient optimization strategy tailored to the target task. This paper designs a powerful optimization framework to automatically learn the optimization strategies from the target or cheap surrogate task without human intervention. However, current methods are weak for this due to poor representation of optimization strategy. To achieve this, 1) drawing on the mechanism of genetic algorithm, we propose a deep neural network framework called B2Opt, which has a stronger representation of optimization strategies based on survival of the fittest; 2) B2Opt can utilize the cheap surrogate functions of the target task to guide the design of the efficient optimization strategies. Compared to the state-of-the-art BBO baselines, B2Opt can achieve multiple orders of magnitude performance i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;XIL&#65289;&#26469;&#20462;&#27491;&#27169;&#22411;&#65292;&#20294;&#21516;&#26102;&#21457;&#29616;"&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL"&#65292;&#24314;&#35758;&#32771;&#34385;&#22810;&#31181;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.07136</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL
&lt;/p&gt;
&lt;p&gt;
One Explanation Does Not Fit XIL. (arXiv:2304.07136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;XIL&#65289;&#26469;&#20462;&#27491;&#27169;&#22411;&#65292;&#20294;&#21516;&#26102;&#21457;&#29616;"&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL"&#65292;&#24314;&#35758;&#32771;&#34385;&#22810;&#31181;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#30528;&#24555;&#25463;&#23398;&#20064;&#21644;&#38169;&#35823;&#30456;&#20851;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;XIL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#21453;&#39304;&#26469;&#20462;&#27491;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20013;&#20351;&#29992;&#30340;&#35299;&#37322;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#21516;&#26102;&#27169;&#22411;&#20462;&#27491;&#65292;&#20174;&#32780;&#21457;&#29616;"&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL" &#65292;&#24182;&#24314;&#35758;&#22312;&#36890;&#36807;XIL&#36827;&#34892;&#27169;&#22411;&#20462;&#27491;&#26102;&#32771;&#34385;&#22810;&#31181;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning models produce outstanding results in many areas but, at the same time, suffer from shortcut learning and spurious correlations. To address such flaws, the explanatory interactive machine learning (XIL) framework has been proposed to revise a model by employing user feedback on a model's explanation. This work sheds light on the explanations used within this framework. In particular, we investigate simultaneous model revision through multiple explanation methods. To this end, we identified that \textit{one explanation does not fit XIL} and propose considering multiple ones when revising models via XIL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04759</link><description>&lt;p&gt;
&#21387;&#32553;&#32034;&#24341;&#23454;&#29616;&#30636;&#38388;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Similarity search in the blink of an eye with compressed indices. (arXiv:2304.04759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25968;&#25454;&#20197;&#21521;&#37327;&#34920;&#31034;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#23547;&#25214;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20284;&#30340;&#21521;&#37327;&#26159;&#19968;&#39033;&#24191;&#27867;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#24314;&#26356;&#24555;&#12289;&#26356;&#23567;&#30340;&#32034;&#24341;&#20197;&#36816;&#34892;&#36825;&#20123;&#25628;&#32034;&#30340;&#26032;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#65292;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#23427;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#25628;&#32034;&#24615;&#33021;&#65292;&#23545;&#25628;&#32034;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;LVQ&#34987;&#35774;&#35745;&#20026;&#19982;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#19968;&#36215;&#24037;&#20316;&#20197;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#31995;&#32479;&#20013;&#38024;&#23545;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#36827;&#34892;&#20851;&#38190;&#20248;&#21270;&#21518;&#65292;LVQ&#30340;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#22312;&#22788;&#29702;&#25968;&#21313;&#20159;&#20010;&#21521;&#37327;&#26102;&#65292;LVQ&#36229;&#36807;&#31532;&#20108;&#20339;&#26041;&#26696;&#65306;
&lt;/p&gt;
&lt;p&gt;
Nowadays, data is represented by vectors. Retrieving those vectors, among millions and billions, that are similar to a given query is a ubiquitous problem of relevance for a wide range of applications. In this work, we present new techniques for creating faster and smaller indices to run these searches. To this end, we introduce a novel vector compression method, Locally-adaptive Vector Quantization (LVQ), that simultaneously reduces memory footprint and improves search performance, with minimal impact on search accuracy. LVQ is designed to work optimally in conjunction with graph-based indices, reducing their effective bandwidth while enabling random-access-friendly fast similarity computations. Our experimental results show that LVQ, combined with key optimizations for graph-based indices in modern datacenter systems, establishes the new state of the art in terms of performance and memory footprint. For billions of vectors, LVQ outcompetes the second-best alternatives: (1) in the low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#22270;&#29983;&#25104;&#27169;&#22411;FairGen&#65292;&#36890;&#36807;&#26631;&#31614;&#25439;&#22833;&#21644;&#20844;&#24179;&#25439;&#22833;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#38597;&#21487;&#27604;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#36924;&#30495;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17743</link><description>&lt;p&gt;
FairGen: &#36808;&#21521;&#20844;&#24179;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FairGen: Towards Fair Graph Generation. (arXiv:2303.17743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#22270;&#29983;&#25104;&#27169;&#22411;FairGen&#65292;&#36890;&#36807;&#26631;&#31614;&#25439;&#22833;&#21644;&#20844;&#24179;&#25439;&#22833;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#38597;&#21487;&#27604;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#24418;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#35745;&#31639;&#26426;&#32593;&#32476;&#65292;&#20174;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21040;&#22312;&#32447;&#20132;&#26131;&#32593;&#32476;&#12290;&#23613;&#31649;&#36825;&#20123;&#24037;&#20316;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#27809;&#26377;&#30417;&#30563;&#24615;&#36136;&#65292;&#36890;&#24120;&#26159;&#22312;&#35757;&#32451;&#26102;&#26368;&#23567;&#21270;&#39044;&#26399;&#30340;&#22270;&#24418;&#37325;&#26500;&#25439;&#22833;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#29983;&#25104;&#30340;&#22270;&#24418;&#20013;&#20986;&#29616;&#34920;&#31034;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;(&#36890;&#24120;&#26159;&#23569;&#25968;&#32676;&#20307;)&#23545;&#30446;&#26631;&#36129;&#29486;&#26356;&#23569;&#65292;&#22240;&#27492;&#20250;&#36973;&#21463;&#31995;&#32479;&#24615;&#26356;&#39640;&#30340;&#35823;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#21644;&#29992;&#25143;&#39318;&#36873;&#30340;&#24179;&#31561;&#32422;&#26463;&#65292;&#23558;&#22270;&#29983;&#25104;&#35843;&#25972;&#20026;&#19979;&#28216;&#25366;&#25496;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#25506;&#31350;&#22270;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairGen&#30340;&#20844;&#24179;&#24863;&#30693;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32852;&#21512;&#20102;&#26631;&#31614;&#25439;&#22833;&#21644;&#20844;&#24179;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#38597;&#21487;&#27604;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#24471;&#21040;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been tremendous efforts over the past decades dedicated to the generation of realistic graphs in a variety of domains, ranging from social networks to computer networks, from gene regulatory networks to online transaction networks. Despite the remarkable success, the vast majority of these works are unsupervised in nature and are typically trained to minimize the expected graph reconstruction loss, which would result in the representation disparity issue in the generated graphs, i.e., the protected groups (often minorities) contribute less to the objective and thus suffer from systematically higher errors. In this paper, we aim to tailor graph generation to downstream mining tasks by leveraging label information and user-preferred parity constraint. In particular, we start from the investigation of representation disparity in the context of graph generative models. To mitigate the disparity, we propose a fairness-aware graph generative model named FairGen. Our model jointly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17245</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#20943;&#36731;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;MvC&#65289;&#26088;&#22312;&#25506;&#32034;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31867;&#21035;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#26631;&#31614;&#30417;&#30563;&#12290;&#22810;&#35270;&#22270;&#27604;&#21333;&#35270;&#22270;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#26524;&#35270;&#22270;&#22024;&#26434;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#36864;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#30740;&#31350;&#20102;&#22024;&#26434;&#35270;&#22270;&#30340;&#32570;&#28857;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;&#31216;&#20026;MvCAN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MvC&#30446;&#26631;&#65292;&#20351;&#24471;&#19981;&#20849;&#20139;&#21442;&#25968;&#21644;&#19981;&#19968;&#33268;&#30340;&#32858;&#31867;&#39044;&#27979;&#21487;&#20197;&#36328;&#36234;&#22810;&#20010;&#35270;&#22270;&#65292;&#20197;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36845;&#20195;&#36807;&#31243;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#25366;&#25496;&#22810;&#20010;&#35270;&#22270;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MvCAN&#30340;&#24037;&#20316;&#26159;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#65292;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21518;&#65292;&#23545;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26032;&#25910;&#38598;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MvCAN&#22312;&#22788;&#29702;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#22024;&#26434;&#35270;&#22270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
&lt;/p&gt;</description></item><item><title>marl-jax&#26159;&#19968;&#20010;&#22522;&#20110;DeepMind&#30340;JAX&#29983;&#24577;&#31995;&#21644;RL&#29983;&#24577;&#31995;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#22312;&#22810;&#26679;&#24615;&#32972;&#26223;&#19979;&#30340;&#31038;&#20250;&#26222;&#36866;&#24615;&#65292;&#25552;&#20379;&#21629;&#20196;&#34892;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21512;&#20316;&#19982;&#31454;&#20105;&#28216;&#25103;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.13808</link><description>&lt;p&gt;
marl-jax&#65306;&#29992;&#20110;&#31038;&#20250;&#26222;&#36866;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
marl-jax: Multi-agent Reinforcement Leaning framework for Social Generalization. (arXiv:2303.13808v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13808
&lt;/p&gt;
&lt;p&gt;
marl-jax&#26159;&#19968;&#20010;&#22522;&#20110;DeepMind&#30340;JAX&#29983;&#24577;&#31995;&#21644;RL&#29983;&#24577;&#31995;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#22312;&#22810;&#26679;&#24615;&#32972;&#26223;&#19979;&#30340;&#31038;&#20250;&#26222;&#36866;&#24615;&#65292;&#25552;&#20379;&#21629;&#20196;&#34892;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21512;&#20316;&#19982;&#31454;&#20105;&#28216;&#25103;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#36827;&#23637;&#26159;&#30001;&#31639;&#27861;&#21644;&#24037;&#31243;&#26041;&#38754;&#30340;&#25913;&#36827;&#39537;&#21160;&#30340;&#65292;&#23548;&#33268;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;marl-jax&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#30340;&#31038;&#20250;&#26222;&#36866;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36719;&#20214;&#21253;&#12290;&#35813;&#21253;&#26088;&#22312;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#19968;&#32452;&#20195;&#29702;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22810;&#26679;&#21270;&#32972;&#26223;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#24314;&#31435;&#22312;DeepMind&#30340;JAX&#29983;&#24577;&#31995;&#32479;&#19978;&#65292;&#24182;&#21033;&#29992;&#30001;DeepMind&#24320;&#21457;&#30340;RL&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;marl-jax&#26694;&#26550;&#33021;&#22815;&#22312;&#22810;&#20010;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12289;&#21516;&#26102;&#34892;&#21160;&#30340;&#29615;&#22659;&#20013;&#24037;&#20316;&#12290;&#35813;&#21253;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#21629;&#20196;&#34892;&#30028;&#38754;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#24182;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#24635;&#20043;&#65292;marl-jax&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents. We present marl-jax, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework marl-jax is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities. In conclusion, marl-jax provides a valuable resource for researchers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.05101</link><description>&lt;p&gt;
&#38750;&#23545;&#35282;&#24230;&#37327;&#20013;&#30340;&#21487;&#25193;&#23637;&#38543;&#26426;&#26799;&#24230;&#37324;&#26364;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics. (arXiv:2303.05101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#35266;&#23519;&#21040;&#65292;&#21253;&#21547;&#24494;&#20998;&#20960;&#20309;&#27010;&#24565;&#30340;&#26041;&#27861;&#24448;&#24448;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#37324;&#26364;&#24230;&#37327;&#36890;&#36807;&#32771;&#34385;&#23616;&#37096;&#26354;&#29575;&#26469;&#25913;&#21892;&#21518;&#39564;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#35282;&#24230;&#37327;&#20197;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#36825;&#20250;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#25910;&#25947;&#24615;&#21644;&#25506;&#32034;&#24615;&#65292;&#22312;&#23545;&#27604;&#23545;&#35282;&#24230;&#37327;&#21482;&#26377;&#36731;&#24494;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#12290;&#23545;&#20110;&#20854;&#20182;&#19968;&#20123;&#36873;&#25321;&#65292;&#21518;&#39564;&#20998;&#24067;&#22312;&#31616;&#21333;&#24230;&#37327;&#19979;&#20063;&#36275;&#22815;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#19988;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#26368;&#20027;&#23548;&#30340;&#39057;&#29575;&#36827;&#34892;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02034</link><description>&lt;p&gt;
&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#26368;&#20027;&#23548;&#30340;&#39057;&#29575;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies. (arXiv:2303.02034v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#19988;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#26368;&#20027;&#23548;&#30340;&#39057;&#29575;&#36827;&#34892;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29702;&#35770;&#30340;&#19968;&#20010;&#31361;&#30772;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#28436;&#21464;&#26159;&#30001;&#25968;&#25454;&#38598;&#32467;&#26500;&#21644;&#21367;&#31215;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25152;&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#38750;&#32447;&#24615;&#12289;&#26377;&#24207;&#12289;&#38454;&#27573;&#24615;&#30340;&#36716;&#21464;&#26469;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#19988;&#21457;&#29616;&#21457;&#29616;&#30340;&#36895;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#21644;&#21367;&#31215;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#26159;&#25105;&#20204;&#25152;&#31216;&#30340;&#8220;&#20027;&#23548;&#39057;&#29575;&#20559;&#24046;&#8221;&#30340;&#26680;&#24515;&#65292;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#32467;&#26500;&#37096;&#20998;&#30340;&#20027;&#23548;&#39057;&#29575;&#26469;&#36827;&#34892;&#36825;&#20123;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We here present a stepping stone towards a deeper understanding of convolutional neural networks (CNNs) in the form of a theory of learning in linear CNNs. Through analyzing the gradient descent equations, we discover that the evolution of the network during training is determined by the interplay between the dataset structure and the convolutional network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, ordered, stage-like transitions, and that the speed of discovery changes depending on the relationship between the dataset and the convolutional network structure. Moreover, we find that this interplay lies at the heart of what we call the ``dominant frequency bias'', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset. We furthermore provide experiments that show how our theory relates to deep, non-linear CNNs used in practice. Our findings shed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;Transformer&#24341;&#23548;&#30340;&#25193;&#25955;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#36924;&#30495;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2302.13434</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;Transformer&#24341;&#23548;&#30340;&#25193;&#25955;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition. (arXiv:2302.13434v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;Transformer&#24341;&#23548;&#30340;&#25193;&#25955;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#36924;&#30495;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#25104;&#20026;&#20102;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#22240;&#20026;&#20154;&#20307;&#39592;&#39612;&#30340;&#32039;&#20945;&#34920;&#36798;&#32473;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#27880;&#20837;&#20102;&#26032;&#27963;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#22987;&#27880;&#24847;&#21040;&#20351;&#29992;RGB&#25110;&#20854;&#20182;&#20256;&#24863;&#22120;&#26469;&#36890;&#36807;&#25552;&#21462;&#39592;&#39612;&#20449;&#24687;&#20998;&#26512;&#20154;&#20307;&#21160;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24635;&#26159;&#38656;&#35201;&#39640;&#36136;&#37327;&#21644;&#20805;&#36275;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#38656;&#35201;&#39640;&#26114;&#30340;&#36153;&#29992;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#21160;&#20316;&#12290;&#20026;&#20102;&#33719;&#24471;&#33258;&#28982;&#21644;&#36924;&#30495;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#19968;&#31995;&#21015;&#21512;&#25104;&#21160;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action seque
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#24694;&#24847;&#21442;&#25968;&#24178;&#25200;&#20840;&#23616;&#27169;&#22411;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#36896;&#25104;&#26174;&#33879;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#38450;&#27490;&#25915;&#20987;&#25152;&#38656;&#30340;&#20445;&#25252;&#38544;&#31169;&#22122;&#22768;&#20250;&#20005;&#37325;&#25439;&#23475;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.12685</link><description>&lt;p&gt;
&#20197;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Active Membership Inference Attack under Local Differential Privacy in Federated Learning. (arXiv:2302.12685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12685
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#24694;&#24847;&#21442;&#25968;&#24178;&#25200;&#20840;&#23616;&#27169;&#22411;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#36896;&#25104;&#26174;&#33879;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#38450;&#27490;&#25915;&#20987;&#25152;&#38656;&#30340;&#20445;&#25252;&#38544;&#31169;&#22122;&#22768;&#20250;&#20005;&#37325;&#25439;&#23475;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26368;&#21021;&#34987;&#35270;&#20026;&#22312;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#35843;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#19981;&#35802;&#23454;&#26381;&#21153;&#22120;&#22312;FL&#20013;&#36827;&#34892;&#30340;&#26032;&#22411;&#20027;&#21160;&#25104;&#21592;&#25512;&#26029;&#65288;AMI&#65289;&#25915;&#20987;&#12290;&#22312;AMI&#25915;&#20987;&#20013;&#65292;&#26381;&#21153;&#22120;&#21046;&#36896;&#24182;&#23884;&#20837;&#24694;&#24847;&#21442;&#25968;&#21040;&#20840;&#23616;&#27169;&#22411;&#20013;&#65292;&#20197;&#26377;&#25928;&#25512;&#26029;&#30446;&#26631;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36890;&#36807;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#65292;AMI&#25915;&#20987;&#22312;&#20005;&#26684;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#20445;&#25252;&#19979;&#21487;&#20197;&#23454;&#29616;&#26497;&#39640;&#30340;&#25104;&#21151;&#29575;&#65292;&#20174;&#32780;&#20351;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#38754;&#20020;&#26174;&#33879;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#38450;&#27490;&#25105;&#20204;&#30340;&#25915;&#20987;&#32780;&#28155;&#21152;&#36275;&#22815;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#22122;&#22768;&#20250;&#26174;&#33879;&#25439;&#23475;FL&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client's private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients' training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL's model utility.
&lt;/p&gt;</description></item><item><title>&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.11007</link><description>&lt;p&gt;
&#27969;&#34892;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11007
&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#37319;&#29992;&#20102;&#20998;&#25968;&#24494;&#31215;&#20998;&#30340;Mittag-Leffler&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#32039;&#20945;&#30340;&#21151;&#33021;&#24418;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#20943;&#36731;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25152;&#25552;&#20986;&#30340;&#38376;&#25511;&#34920;&#31034;&#25193;&#23637;&#20102;&#22266;&#23450;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#33258;&#36866;&#24212;&#23545;&#24212;&#29289;&#65292;&#20854;&#24418;&#29366;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23548;&#25968;&#20063;&#21487;&#20197;&#29992;Mittag-Leffler&#20989;&#25968;&#34920;&#31034;&#65292;&#22240;&#27492;&#23427;&#26159;&#26799;&#24230;&#19979;&#38477;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#21512;&#36866;&#20505;&#36873;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#32479;&#19968;&#30340;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#34920;&#31034;&#20026;&#21508;&#31181;&#20869;&#32622;&#23454;&#29616;&#30340;&#32463;&#27982;&#30340;&#21644;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26126;&#30830;&#38750;&#28176;&#36817;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#20840;&#23616;&#25910;&#25947;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#28151;&#21512;&#36817;&#31471;&#22806;&#26799;&#24230;&#27861;&#32467;&#26500;&#21644;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#26469;&#26356;&#26032;Hessian&#36924;&#36817;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2302.08580</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#24341;&#23548;&#30340;&#26354;&#29575;&#36924;&#36817;&#65306;&#20855;&#26377;&#20840;&#23616;&#38750;&#28176;&#36817;&#36229;&#32447;&#24615;&#25910;&#25947;&#30340;&#25311;&#29275;&#39039;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence. (arXiv:2302.08580v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26126;&#30830;&#38750;&#28176;&#36817;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#20840;&#23616;&#25910;&#25947;&#25311;&#29275;&#39039;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#28151;&#21512;&#36817;&#31471;&#22806;&#26799;&#24230;&#27861;&#32467;&#26500;&#21644;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#26469;&#26356;&#26032;Hessian&#36924;&#36817;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25311;&#29275;&#39039;&#31639;&#27861;&#26159;&#35299;&#20915;&#26080;&#32422;&#26463;&#26368;&#23567;&#21270;&#38382;&#39064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#36845;&#20195;&#26041;&#27861;&#20043;&#19968;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#33391;&#22909;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#32467;&#26524;&#23384;&#22312;&#38480;&#21046;&#65292;&#35201;&#20040;&#25552;&#20379;&#20102;&#20855;&#26377;&#28176;&#36817;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#35201;&#20040;&#20165;&#22312;&#21021;&#22987;&#28857;&#21644;&#21021;&#22987;Hessian&#36924;&#36817;&#36873;&#25321;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#23616;&#37096;&#38750;&#28176;&#36817;&#36229;&#32447;&#24615;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#30446;&#21069;&#27809;&#26377;&#25311;&#29275;&#39039;&#26041;&#27861;&#30340;&#20998;&#26512;&#20445;&#35777;&#20102;&#20855;&#26377;&#26126;&#30830;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26126;&#30830;&#38750;&#28176;&#36817;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#20840;&#23616;&#25910;&#25947;&#25311;&#29275;&#39039;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#25311;&#29275;&#39039;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22522;&#20110;&#28151;&#21512;&#36817;&#31471;&#22806;&#26799;&#24230;&#27861;&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#26469;&#26356;&#26032;Hessian&#36924;&#36817;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quasi-Newton algorithms are among the most popular iterative methods for solving unconstrained minimization problems, largely due to their favorable superlinear convergence property. However, existing results for these algorithms are limited as they provide either (i) a global convergence guarantee with an asymptotic superlinear convergence rate, or (ii) a local non-asymptotic superlinear rate for the case that the initial point and the initial Hessian approximation are chosen properly. In particular, no current analysis for quasi-Newton methods guarantees global convergence with an explicit superlinear convergence rate. In this paper, we close this gap and present the first globally convergent quasi-Newton method with an explicit non-asymptotic superlinear convergence rate. Unlike classical quasi-Newton methods, we build our algorithm upon the hybrid proximal extragradient method and propose a novel online learning framework for updating the Hessian approximation matrices. Specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02119</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#25112;&#23454;&#29616;&#22810;&#26679;&#21270;&#35825;&#23548;&#30340;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29615;&#22659;&#20998;&#24067;&#35774;&#35745;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20986;&#35757;&#32451;&#26377;&#25928;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#30340;&#21069;&#26223;&#12290;&#23427;&#30340;&#25104;&#21151;&#37096;&#20998;&#22312;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#35838;&#31243;&#23398;&#20064;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#32463;&#24120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21457;&#29616;&#26377;&#25928;&#32423;&#21035;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#39640;&#25104;&#26412;&#20132;&#20114;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26694;&#26550;&#20013;&#24341;&#20837;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#32473;&#23450;&#32423;&#21035;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#20004;&#20010;&#32423;&#21035;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#65292;&#20351;&#24471;&#29615;&#22659;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Davis-Yin&#20998;&#35010;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#19978;&#36731;&#26494;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.13395</link><description>&lt;p&gt;
&#20351;&#29992;Davis-Yin&#20998;&#35010;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Faster Predict-and-Optimize with Davis-Yin Splitting. (arXiv:2301.13395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Davis-Yin&#20998;&#35010;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#19978;&#36731;&#26494;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#20855;&#26377;&#30456;&#20284;&#20294;&#19981;&#21516;&#21442;&#25968;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;$w$&#24182;&#38750;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#65307;&#21482;&#26377;&#19982;$w$&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;$d$&#21487;&#29992;&#12290;&#25105;&#20204;&#24456;&#23481;&#26131;&#23601;&#20250;&#24819;&#21040;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26681;&#25454;$d$&#39044;&#27979;$w$&#65292;&#20294;&#26159;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#23558;&#32452;&#21512;&#20248;&#21270;&#30340;&#31163;&#25955;&#24615;&#19982;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#24403;&#25152;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#26102;&#65292;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#32771;&#34385;&#32452;&#21512;&#38382;&#39064;&#30340;&#36830;&#32493;&#25918;&#26494;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#23567;&#22411;&#38382;&#39064;&#65288;10-100&#20010;&#21464;&#37327;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#39640;&#24230;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22823;&#22411;&#38382;&#39064;&#19978;&#25193;&#23637;&#33021;&#21147;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#32593;&#32476;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters $w$ are not directly observed; only contextual data $d$ that correlates with $w$ is available. It is tempting to use a neural network to predict $w$ given $d$, but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. When the problem in question is an Integer Linear Program (ILP), one approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem. While existing methods utilizing this approach have shown to be highly effective on small problems (10-100 variables), they do not scale well to large problems. In this work, we draw on ideas from modern convex optimization to design a network and training scheme which scales effortlessly to problems with thousands of variables.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#65288;SCOD&#65289;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#65292;&#26377;&#25928;&#24615;&#39640;&#65292;&#21487;&#20197;&#25512;&#24191;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12386</link><description>&lt;p&gt;
&#20351;&#29992;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Plugin estimators for selective classification with out-of-distribution detection. (arXiv:2301.12386v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#65288;SCOD&#65289;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#65292;&#26377;&#25928;&#24615;&#39640;&#65292;&#21487;&#20197;&#25512;&#24191;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#21463;&#30410;&#20110;&#22312;&#23545;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#26679;&#26412;&#36827;&#34892;&#39044;&#27979;&#26102;&#36873;&#25321;&#24615;&#22320;&#25918;&#24323;&#12290;&#36825;&#31181;&#24323;&#26435;&#23545;&#20110;&#25509;&#36817;&#23398;&#20064;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#25110;&#32773;&#30456;&#23545;&#20110;&#35757;&#32451;&#26679;&#26412;&#26469;&#35828;&#26159;&#24322;&#24120;&#20540;&#30340;&#26679;&#26412;&#29305;&#21035;&#26377;&#29992;&#12290;&#36825;&#20123;&#35774;&#32622;&#24050;&#32463;&#22312;&#36873;&#25321;&#24615;&#20998;&#31867;(SC)&#21644;&#22806;&#37096;&#20998;&#24067;(OOD)&#26816;&#27979;&#30340;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#20294;&#19981;&#20851;&#32852;&#30340;&#30740;&#31350;&#25152;&#30740;&#31350;&#12290;&#26368;&#36817;&#26377;&#20851;&#36873;&#25321;&#24615;&#20998;&#31867;&#19982;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;(SCOD)&#30340;&#30740;&#31350;&#35748;&#20026;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#32479;&#19968;&#30740;&#31350;&#26159;&#26377;&#24517;&#35201;&#30340;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#27491;&#24335;&#22522;&#30784;&#20173;&#28982;&#19981;&#25104;&#29087;&#65292;&#29616;&#26377;&#30340;&#25216;&#26415;&#20063;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#65292;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#26681;&#25454;&#30340;&#12289;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#27867;&#21270;&#20102;SC&#21644;OOD&#26816;&#27979;&#39046;&#22495;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#35828;&#26126;&#20102;&#23545;&#29616;&#26377;&#30340;SC&#21644;OOD&#26816;&#27979;&#22522;&#32447;&#30340;&#22825;&#30495;&#20351;&#29992;&#21487;&#33021;&#19981;&#36275;&#20197;&#36866;&#24212;SCOD&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world classifiers can benefit from the option of abstaining from predicting on samples where they have low confidence. Such abstention is particularly useful on samples which are close to the learned decision boundary, or which are outliers with respect to the training sample. These settings have been the subject of extensive but disjoint study in the selective classification (SC) and out-of-distribution (OOD) detection literature. Recent work on selective classification with OOD detection (SCOD) has argued for the unified study of these problems; however, the formal underpinnings of this problem are still nascent, and existing techniques are heuristic in nature. In this paper, we propose new plugin estimators for SCOD that are theoretically grounded, effective, and generalise existing approaches from the SC and OOD detection literature. In the course of our analysis, we formally explicate how na\"{i}ve use of existing SC and OOD detection baselines may be inadequate for SCOD. We 
&lt;/p&gt;</description></item><item><title>A$^2$-UAV&#26159;&#19968;&#31181;&#36793;&#32536;&#36741;&#21161;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#24863;&#30693;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#22810;&#36339;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#24102;&#23485;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#19982;&#22270;&#20687;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#26080;&#20154;&#26426;&#30340;&#33021;&#37327;&#21644;&#20301;&#32622;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2301.06363</link><description>&lt;p&gt;
A$^2$-UAV&#65306;&#36793;&#32536;&#36741;&#21161;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#24212;&#29992;&#24863;&#30693;&#20869;&#23481;&#21644;&#32593;&#32476;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems. (arXiv:2301.06363v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06363
&lt;/p&gt;
&lt;p&gt;
A$^2$-UAV&#26159;&#19968;&#31181;&#36793;&#32536;&#36741;&#21161;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#24863;&#30693;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#22810;&#36339;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#24102;&#23485;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#19982;&#22270;&#20687;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#26080;&#20154;&#26426;&#30340;&#33021;&#37327;&#21644;&#20301;&#32622;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#39640;&#32423;&#30417;&#35270;&#65292;&#26080;&#20154;&#26426;&#38656;&#35201;&#25191;&#34892;&#36793;&#32536;&#36741;&#21161;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#22312;&#22810;&#36339;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#20005;&#37325;&#30340;&#24102;&#23485;&#38480;&#21046;&#65292;&#23558;&#36825;&#20123;&#20219;&#21153;&#25104;&#21151;&#20256;&#36755;&#21040;&#36793;&#32536;&#38754;&#20020;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;A$^2$-UAV&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#36793;&#32536;&#19978;&#27491;&#30830;&#25191;&#34892;&#30340;&#20219;&#21153;&#25968;&#37327;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#24212;&#29992;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#24212;&#29992;&#24863;&#30693;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#65288;A$^2$-TPP&#65289;&#65292;&#35813;&#38382;&#39064;&#32771;&#34385;&#20102;&#65288;i&#65289;&#22522;&#20110;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#24863;&#20852;&#36259;&#31867;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20934;&#30830;&#24615;&#19982;&#22270;&#20687;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#30446;&#26631;&#20301;&#32622;&#65292;&#65288;iii&#65289;&#26080;&#20154;&#26426;&#30340;&#24403;&#21069;&#33021;&#37327;/&#20301;&#32622;&#65292;&#20197;&#20248;&#21270;&#27599;&#20010;&#26080;&#20154;&#26426;&#30340;&#36335;&#30001;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#30446;&#26631;&#20998;&#37197;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;A$^2$-TPP&#26159;NP-&#38590;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#39640;&#25928;&#22320;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#23545;A$^2$-UAV&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
To perform advanced surveillance, Unmanned Aerial Vehicles (UAVs) require the execution of edge-assisted computer vision (CV) tasks. In multi-hop UAV networks, the successful transmission of these tasks to the edge is severely challenged due to severe bandwidth constraints. For this reason, we propose a novel A$^2$-UAV framework to optimize the number of correctly executed tasks at the edge. In stark contrast with existing art, we take an application-aware approach and formulate a novel pplication-Aware Task Planning Problem (A$^2$-TPP) that takes into account (i) the relationship between deep neural network (DNN) accuracy and image compression for the classes of interest based on the available dataset, (ii) the target positions, (iii) the current energy/position of the UAVs to optimize routing, data pre-processing and target assignment for each UAV. We demonstrate A$^2$-TPP is NP-Hard and propose a polynomial-time algorithm to solve it efficiently. We extensively evaluate A$^2$-UAV th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#26694;&#26550;&#65292;&#20801;&#35768;&#23558;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#19982;&#25209;&#37327;&#21098;&#20999;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#25171;&#20081;&#31561;&#37319;&#26679;&#25216;&#26415;&#12290;&#20316;&#32773;&#30340;DP&#20998;&#26512;&#20351;&#29992;&#20102;$f$-DP&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#31616;&#21333;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#32676;&#20307;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#24037;&#20316;&#26102;&#26399;&#21644;&#32676;&#20307;&#22823;&#23567;&#65292;&#26694;&#26550;&#20855;&#26377;$\sqrt{g E}$ &#30340;&#24046;&#20998;&#38544;&#31169;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.05796</link><description>&lt;p&gt;
&#20351;&#29992;&#25171;&#20081;&#21644;&#25209;&#37327;&#21098;&#20999;&#30340;&#26041;&#27861;&#25512;&#24191;DP-SGD&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalizing DP-SGD with Shuffling and Batch Clipping. (arXiv:2212.05796v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#26694;&#26550;&#65292;&#20801;&#35768;&#23558;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#19982;&#25209;&#37327;&#21098;&#20999;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#25171;&#20081;&#31561;&#37319;&#26679;&#25216;&#26415;&#12290;&#20316;&#32773;&#30340;DP&#20998;&#26512;&#20351;&#29992;&#20102;$f$-DP&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#31616;&#21333;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#32676;&#20307;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#24037;&#20316;&#26102;&#26399;&#21644;&#32676;&#20307;&#22823;&#23567;&#65292;&#26694;&#26550;&#20855;&#26377;$\sqrt{g E}$ &#30340;&#24046;&#20998;&#38544;&#31169;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24046;&#20998;&#38544;&#31169;DP-SGD&#31639;&#27861;&#20351;&#29992;&#38543;&#26426;&#23376;&#37319;&#26679;&#23454;&#29616;&#20010;&#20307;&#21098;&#20999;&#65292;&#20174;&#32780;&#24378;&#21046;&#23454;&#29616;&#23567;&#25209;&#37327;SGD&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#26694;&#26550;&#65292;&#36229;&#36234;&#20102;DP-SGD&#31639;&#27861;&#65292;&#20801;&#35768;&#20219;&#20309;&#21487;&#33021;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120;&#65288;&#22914;&#32463;&#20856;&#30340;SGD&#21644;&#22522;&#20110;&#21160;&#37327;&#30340;SGD&#26041;&#27861;&#65289;&#19982;&#25209;&#37327;&#21098;&#20999;&#30456;&#32467;&#21512;&#65292;&#23427;&#21098;&#20999;&#20102;&#35745;&#31639;&#26799;&#24230;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#23545;&#21098;&#20999;&#26799;&#24230;&#27714;&#21644;&#65288;&#20010;&#20307;&#21098;&#20999;&#65289;&#12290;&#35813;&#26694;&#26550;&#36824;&#20801;&#35768;&#20351;&#29992;&#38500;&#38543;&#26426;&#23376;&#37319;&#26679;&#20043;&#22806;&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#22914;&#25171;&#20081;&#12290;&#25105;&#20204;&#30340;DP&#20998;&#26512;&#36981;&#24490;$f$-DP&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35777;&#26126;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#31616;&#21333;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#23545;&#32676;&#20307;&#38544;&#31169;&#36827;&#34892;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;$E$&#20010;&#26102;&#26399;&#30340;&#24037;&#20316;&#21644;&#22823;&#23567;&#20026;$g$&#30340;&#32676;&#20307;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#25171;&#20081;&#21644;&#25209;&#37327;&#21098;&#20999;&#30340;$\sqrt{g E}$ DP&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical differential private DP-SGD implements individual clipping with random subsampling, which forces a mini-batch SGD approach. We provide a general differential private algorithmic framework that goes beyond DP-SGD and allows any possible first order optimizers (e.g., classical SGD and momentum based SGD approaches) in combination with batch clipping, which clips an aggregate of computed gradients rather than summing clipped gradients (as is done in individual clipping). The framework also admits sampling techniques beyond random subsampling such as shuffling. Our DP analysis follows the $f$-DP approach and introduces a new proof technique which allows us to derive simple closed form expressions and to also analyse group privacy. In particular, for $E$ epochs work and groups of size $g$, we show a $\sqrt{g E}$ DP dependency for batch clipping with shuffling.
&lt;/p&gt;</description></item><item><title>FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07160</link><description>&lt;p&gt;
FedTracker&#65306;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07160
&lt;/p&gt;
&lt;p&gt;
FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#38656;&#35201;&#23558;&#27169;&#22411;&#26292;&#38706;&#32473;&#21508;&#31181;&#21442;&#19982;&#32773;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24694;&#24847;&#23458;&#25143;&#31471;&#26410;&#32463;&#25480;&#26435;&#22320;&#20998;&#21457;&#25110;&#36716;&#21806;&#27169;&#22411;&#65292;&#20174;&#32780;&#25439;&#23475;FL&#22242;&#38431;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#20102;&#38459;&#27490;&#36825;&#31181;&#19981;&#24403;&#34892;&#20026;&#65292;&#24314;&#31435;&#19968;&#31181;&#39564;&#35777;&#27169;&#22411;&#25152;&#26377;&#26435;&#24182;&#36861;&#28335;&#27844;&#38706;&#32773;&#30340;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedTracker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;FL&#27169;&#22411;&#20445;&#25252;&#26694;&#26550;&#12290;FedTracker&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#21253;&#25324;&#20840;&#23616;&#27700;&#21360;&#26426;&#21046;&#21644;&#26412;&#22320;&#25351;&#32441;&#26426;&#21046;&#12290;&#21069;&#32773;&#29992;&#20110;&#39564;&#35777;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#65292;&#32780;&#21518;&#32773;&#29992;&#20110;&#35782;&#21035;&#35813;&#27169;&#22411;&#26469;&#33258;&#21738;&#20010;&#23458;&#25143;&#31471;&#12290;FedTracker&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21407;&#21017;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TEFL (Turbo Explainable Federated Learning) &#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;6G&#21487;&#20449;&#38646;&#35302;&#30896;&#32593;&#32476;&#20999;&#29255;&#30340;&#36879;&#26126;&#24615;&#21644;SLA&#24863;&#30693;&#30340;&#38646;&#35302;&#30896;&#26381;&#21153;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2210.10147</link><description>&lt;p&gt;
TEFL: &#29992;&#20110;6G&#21487;&#20449;&#38646;&#35302;&#30896;&#32593;&#32476;&#20999;&#29255;&#30340;Turbo&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TEFL: Turbo Explainable Federated Learning for 6G Trustworthy Zero-Touch Network Slicing. (arXiv:2210.10147v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TEFL (Turbo Explainable Federated Learning) &#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;6G&#21487;&#20449;&#38646;&#35302;&#30896;&#32593;&#32476;&#20999;&#29255;&#30340;&#36879;&#26126;&#24615;&#21644;SLA&#24863;&#30693;&#30340;&#38646;&#35302;&#30896;&#26381;&#21153;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20845;&#20195;&#65288;6G&#65289;&#32593;&#32476;&#26399;&#26395;&#26234;&#33021;&#22320;&#25903;&#25345;&#22823;&#37327;&#21327;&#21516;&#23384;&#22312;&#30340;&#12289;&#19982;&#21508;&#31181;&#22402;&#30452;&#24212;&#29992;&#26696;&#20363;&#30456;&#20851;&#30340;&#24322;&#26500;&#20999;&#29255;&#12290;&#36825;&#31181;&#32972;&#26223;&#20419;&#20351;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#38646;&#35302;&#30896;&#31649;&#29702;&#21644;&#32534;&#25490;&#65288;MANO&#65289;&#65292;&#20197;&#28385;&#36275;&#20005;&#26684;&#30340;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#65288;SLAs&#65289;&#19979;&#30340;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#20999;&#29255;&#38656;&#27714;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#24037;&#20855;&#23454;&#29616;AI&#40657;&#30418;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#21487;&#20449;&#24230;&#65292;&#20197;&#24314;&#31435;&#20999;&#29255;&#29983;&#24577;&#31995;&#32479;&#20013;&#20132;&#20114;&#21442;&#19982;&#32773;&#65288;&#22914;&#31199;&#25143;&#12289;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21830;&#21644;&#36816;&#33829;&#21830;&#65289;&#20043;&#38388;&#30340;&#36879;&#26126;&#24230;&#12290;&#21463;Turbo&#21407;&#29702;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#24335;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21463;&#38480;&#36164;&#28304;&#20998;&#37197;&#27169;&#22411;&#21644;&#19968;&#20010;&#35299;&#37322;&#22120;&#20132;&#25442;&#65292;&#20197;&#23454;&#29616;&#23545;&#29305;&#24449;&#30340;&#36879;&#26126;&#24615;&#21644;SLA&#24863;&#30693;&#30340;&#38646;&#35302;&#30896;&#26381;&#21153;&#31649;&#29702;&#65288;ZSM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sixth-generation (6G) networks anticipate intelligently supporting a massive number of coexisting and heterogeneous slices associated with various vertical use cases. Such a context urges the adoption of artificial intelligence (AI)-driven zero-touch management and orchestration (MANO) of the end-to-end (E2E) slices under stringent service level agreements (SLAs). Specifically, the trustworthiness of the AI black-boxes in real deployment can be achieved by explainable AI (XAI) tools to build transparency between the interacting actors in the slicing ecosystem, such as tenants, infrastructure providers and operators. Inspired by the turbo principle, this paper presents a novel iterative explainable federated learning (FL) approach where a constrained resource allocation model and an \emph{explainer} exchange -- in a closed loop (CL) fashion -- soft attributions of the features as well as inference predictions to achieve a transparent and SLA-aware zero-touch service management (ZSM) of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06433</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#20135;&#29983;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35266;&#23519;&#23545;&#35937;&#21644;&#22330;&#26223;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#26102;&#38388;&#29702;&#35299;&#30340;&#29305;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#38745;&#24577;&#22270;&#20687;&#39044;&#35757;&#32451;&#20173;&#28982;&#26159;&#23398;&#20064;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#19981;&#21305;&#37197;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#19988;&#38382;&#26159;&#21542;&#35270;&#39057;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65306;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#12289;&#23545;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31579;&#36873;&#35270;&#39057;&#30340;&#26032;&#39062;&#31243;&#24207;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#27604;&#24615;&#26694;&#26550;&#65292;&#20174;&#20854;&#20013;&#30340;&#22797;&#26434;&#36716;&#25442;&#20013;&#23398;&#20064;&#12290;&#36825;&#31181;&#20174;&#35270;&#39057;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#31616;&#21333;&#33539;&#24335;&#34987;&#31216;&#20026;VITO&#65292;&#23427;&#20135;&#29983;&#30340;&#19968;&#33324;&#34920;&#31034;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#19978;&#36828;&#36828;&#20248;&#20110;&#20808;&#21069;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;VITO&#34920;&#31034;&#23545;&#33258;&#28982;&#21644;&#21512;&#25104;&#24418;&#21464;&#30340;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20215;&#26684;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#38656;&#27714;&#23398;&#20064;&#26041;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#22312;&#32447;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#35268;&#33539;&#21270;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2207.11159</link><description>&lt;p&gt;
&#22522;&#20110;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing. (arXiv:2207.11159v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20215;&#26684;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#38656;&#27714;&#23398;&#20064;&#26041;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#22312;&#32447;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#35268;&#33539;&#21270;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#26368;&#22823;&#21270;&#24635;&#25910;&#30410;&#22806;&#65292;&#24456;&#22810;&#34892;&#19994;&#30340;&#20915;&#31574;&#32773;&#36824;&#24076;&#26395;&#30830;&#20445;&#19981;&#21516;&#36164;&#28304;&#20043;&#38388;&#28040;&#32791;&#30340;&#24179;&#34913;&#12290;&#20363;&#22914;&#65292;&#22312;&#38646;&#21806;&#34892;&#19994;&#20013;&#65292;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#36164;&#28304;&#24179;&#34913;&#28040;&#32791;&#26377;&#21161;&#20110;&#25552;&#39640;&#20844;&#24179;&#24615;&#24182;&#32500;&#25345;&#33391;&#22909;&#30340;&#28192;&#36947;&#20851;&#31995;&#65307;&#22312;&#20113;&#35745;&#31639;&#34892;&#19994;&#20013;&#65292;&#36164;&#28304;&#28040;&#32791;&#30340;&#24179;&#34913;&#26377;&#21161;&#20110;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#24182;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#38024;&#23545;&#36825;&#20123;&#23454;&#38469;&#38656;&#27714;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20215;&#26684;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35268;&#33539;&#21270;&#25910;&#30410;&#30340;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#24179;&#34913;&#27491;&#21017;&#21270;&#23558;&#20844;&#24179;&#30340;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#32435;&#20837;&#21040;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#30446;&#26631;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#32622;&#20449;&#30028;&#38480;&#65288;UCB&#65289;&#38656;&#27714;&#23398;&#20064;&#26041;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#22312;&#32447;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#35268;&#33539;&#21270;&#25910;&#30410;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#20010;&#21019;&#26032;&#26041;&#27861;&#26469;&#24212;&#23545;&#38656;&#27714;&#23398;&#20064;&#21644;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a good channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innov
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE&#23545;&#36890;&#29992;&#21160;&#21147;&#23398;&#36827;&#34892;&#26368;&#20248;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#21644;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.14284</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE&#23545;&#36890;&#29992;&#21160;&#21147;&#23398;&#36827;&#34892;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump ODEs. (arXiv:2206.14284v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36335;&#24452;&#30456;&#20851;&#30340;&#31070;&#32463;&#36339;&#36291;ODE&#23545;&#36890;&#29992;&#21160;&#21147;&#23398;&#36827;&#34892;&#26368;&#20248;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#21644;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#36339;&#36291;ODE&#65288;NJ-ODE&#65289;&#26694;&#26550;&#30340;&#36335;&#24452;&#30456;&#20851;&#25193;&#23637;&#26469;&#39044;&#27979;&#19968;&#33324;&#38543;&#26426;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;NJ-ODE&#26159;&#31532;&#19968;&#20010;&#24314;&#31435;&#36215;&#38024;&#23545;&#19981;&#35268;&#21017;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#26469;&#33258;&#20855;&#26377;&#23436;&#25972;&#35266;&#27979;&#30340;It\^o&#25193;&#25955;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#25152;&#26377;&#22352;&#26631;&#21516;&#26102;&#35266;&#27979;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31614;&#21517;&#21464;&#25442;&#30340;&#37325;&#26500;&#24615;&#36136;&#23558;&#36825;&#20123;&#32467;&#26524;&#25512;&#24191;&#21040;&#36890;&#29992;&#30340;&#12289;&#21487;&#33021;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#25110;&#19981;&#36830;&#32493;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36335;&#24452;&#30456;&#20851;&#30340;NJ-ODE&#20248;&#20110;&#21407;&#22987;NJ-ODE&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;PD-NJ-ODE&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#32463;&#20856;&#30340;&#38543;&#26426;&#28388;&#27874;&#38382;&#39064;&#21644;&#38480;&#20215;&#35746;&#21333;&#31807;&#65288;LOB&#65289;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of forecasting general stochastic processes using a path-dependent extension of the Neural Jump ODE (NJ-ODE) framework. While NJ-ODE was the first framework to establish convergence guarantees for the prediction of irregularly observed time series, these results were limited to data stemming from It\^o-diffusions with complete observations, in particular Markov processes where all coordinates are observed simultaneously. In this work, we generalise these results to generic, possibly non-Markovian or discontinuous, stochastic processes with incomplete observations, by utilising the reconstruction properties of the signature transform. These theoretical results are supported by empirical studies, where it is shown that the path-dependent NJ-ODE outperforms the original NJ-ODE framework in the case of non-Markovian data. Moreover, we show that PD-NJ-ODE can be applied successfully to classical stochastic filtering problems and to limit order book (LOB) data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;DEfect-AwaRe&#32852;&#37030;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;DearFSAC&#65289;&#65292;&#29992;&#20110;&#22312;&#30005;&#21147;&#25209;&#21457;&#24066;&#22330;&#20013;&#40065;&#26834;&#22320;&#35757;&#32451;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#32780;&#39044;&#27979;&#31934;&#30830;&#30340;&#30701;&#26399;&#20844;&#29992;&#30005;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.11715</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#32852;&#37030;&#23398;&#20064;&#22312;&#30005;&#21147;&#25209;&#21457;&#24066;&#22330;&#20013;&#30340;&#40065;&#26834;&#30701;&#26399;&#29992;&#30005;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Assisted Federated Learning for Robust Short-term Utility Demand Forecasting in Electricity Wholesale Markets. (arXiv:2206.11715v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;DEfect-AwaRe&#32852;&#37030;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;DearFSAC&#65289;&#65292;&#29992;&#20110;&#22312;&#30005;&#21147;&#25209;&#21457;&#24066;&#22330;&#20013;&#40065;&#26834;&#22320;&#35757;&#32451;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#32780;&#39044;&#27979;&#31934;&#30830;&#30340;&#30701;&#26399;&#20844;&#29992;&#30005;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#22312;&#30005;&#21147;&#20132;&#26131;&#24066;&#22330;&#30340;&#36816;&#20316;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#32771;&#34385;&#21040;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#20026;&#20844;&#29992;&#20107;&#19994;&#20844;&#21496;&#65288;UCs&#65289;&#35757;&#32451;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#25209;&#21457;&#24066;&#22330;&#20013;&#65292;&#30001;&#20110;&#30005;&#21378;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;UCs&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#33719;&#21462;&#20934;&#30830;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24615;&#21644;UCs&#20043;&#38388;&#30340;&#28608;&#28872;&#31454;&#20105;&#65292;&#32570;&#38519;&#26085;&#30410;&#22686;&#22810;&#65292;&#23548;&#33268;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20165;&#20165;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26159;&#19981;&#22815;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DEfect-AwaRe&#32852;&#37030;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;DearFSAC&#65289;&#30340;DRL&#36741;&#21161;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#40065;&#26834;&#22320;&#35757;&#32451;&#19968;&#20010;&#20934;&#30830;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31934;&#30830;&#30340;&#30701;&#26399;&#20844;&#29992;&#30005;&#38656;&#27714;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#21382;&#21490;&#36127;&#33655;&#25968;&#25454;&#21644;&#26102;&#38388;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term load forecasting (STLF) plays a significant role in the operation of electricity trading markets. Considering the growing concern of data privacy, federated learning (FL) is increasingly adopted to train STLF models for utility companies (UCs) in recent research. Inspiringly, in wholesale markets, as it is not realistic for power plants (PPs) to access UCs' data directly, FL is definitely a feasible solution of obtaining an accurate STLF model for PPs. However, due to FL's distributed nature and intense competition among UCs, defects increasingly occur and lead to poor performance of the STLF model, indicating that simply adopting FL is not enough. In this paper, we propose a DRL-assisted FL approach, DEfect-AwaRe federated soft actor-critic (DearFSAC), to robustly train an accurate STLF model for PPs to forecast precise short-term utility electricity demand. Firstly. we design a STLF model based on long short-term memory (LSTM) using just historical load data and time data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROI&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35782;&#21035;&#25509;&#25910;&#20010;&#20154;&#25968;&#25454;&#30340;&#32452;&#32455;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;10,000&#20010;&#23433;&#21331;&#24212;&#29992;&#25581;&#31034;&#20102;&#36825;&#20123;&#32452;&#32455;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;95.71%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2204.09495</link><description>&lt;p&gt;
ROI&#65306;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25509;&#25910;&#20010;&#20154;&#25968;&#25454;&#30340;&#32452;&#32455;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ROI: A method for identifying organizations receiving personal data. (arXiv:2204.09495v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROI&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35782;&#21035;&#25509;&#25910;&#20010;&#20154;&#25968;&#25454;&#30340;&#32452;&#32455;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;10,000&#20010;&#23433;&#21331;&#24212;&#29992;&#25581;&#31034;&#20102;&#36825;&#20123;&#32452;&#32455;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;95.71%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#25581;&#31034;&#20102;&#36890;&#36807;&#32593;&#31449;&#12289;&#31227;&#21160;&#24212;&#29992;&#25110;&#26234;&#33021;&#35774;&#22791;&#31561;&#26041;&#24335;&#22312;&#25968;&#23383;&#29983;&#24577;&#31995;&#32479;&#20013;&#23545;&#20010;&#20154;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#25910;&#38598;&#12290;&#22823;&#22810;&#25968;&#29992;&#25143;&#37117;&#27809;&#26377;&#24847;&#35782;&#21040;&#36825;&#19968;&#20107;&#23454;&#65292;&#24182;&#19988;&#20063;&#26410;&#24847;&#35782;&#21040;&#36825;&#20123;&#25910;&#38598;&#32773;&#19982;&#20840;&#29699;&#33539;&#22260;&#20869;&#35768;&#22810;&#19981;&#21516;&#30340;&#32452;&#32455;&#20849;&#20139;&#20182;&#20204;&#30340;&#20010;&#20154;&#25968;&#25454;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#26377;&#25216;&#26415;&#20013;&#35782;&#21035;&#25509;&#25910;&#20010;&#20154;&#25968;&#25454;&#30340;&#32452;&#32455;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROI&#65288;&#25509;&#25910;&#32452;&#32455;&#35782;&#21035;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#23454;&#29616;&#23545;&#25509;&#25910;&#20010;&#20154;&#25968;&#25454;&#30340;&#32452;&#32455;&#36827;&#34892;&#20934;&#30830;&#29575;&#36798;&#21040;95.71%&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;10,000&#20010;&#23433;&#21331;&#24212;&#29992;&#24182;&#25581;&#31034;&#25509;&#25910;&#29992;&#25143;&#20010;&#20154;&#25968;&#25454;&#30340;&#32452;&#32455;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many studies have exposed the massive collection of personal data in the digital ecosystem through, for instance, websites, mobile apps, or smart devices. This fact goes unnoticed by most users, who are also unaware that the collectors are sharing their personal data with many different organizations around the globe. This paper assesses techniques available in the state of the art to identify the organizations receiving this personal data. Based on our findings, we propose ROI (Receiver Organization Identifier), a fully automated method that combines different techniques to achieve a 95.71% precision score in identifying an organization receiving personal data. We demonstrate our method in the wild by evaluating 10,000 Android apps and exposing the organizations that receive users' personal data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31995;&#32479;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#27969;&#31243;&#20013;&#39640;&#25928;&#38598;&#25104;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22235;&#26059;&#32764;&#31561;&#28789;&#27963;&#26426;&#22120;&#20154;&#24179;&#21488;&#39640;&#24615;&#33021;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2203.07747</link><description>&lt;p&gt;
&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65306;&#38754;&#21521;&#22235;&#26059;&#32764;&#21644;&#28789;&#27963;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms. (arXiv:2203.07747v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31995;&#32479;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#27969;&#31243;&#20013;&#39640;&#25928;&#38598;&#25104;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22235;&#26059;&#32764;&#31561;&#28789;&#27963;&#26426;&#22120;&#20154;&#24179;&#21488;&#39640;&#24615;&#33021;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#24050;&#25104;&#20026;&#39640;&#24615;&#33021;&#33258;&#20027;&#31995;&#32479;&#23884;&#20837;&#24335;&#25511;&#21046;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#35201;&#20351;&#29992;MPC&#23454;&#29616;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#31934;&#30830;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#26159;&#20851;&#38190;&#12290;&#20026;&#20102;&#20445;&#25345;&#23454;&#26102;&#25805;&#20316;&#65292;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#20351;&#29992;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#19968;&#33324;&#38480;&#20110;&#31616;&#21333;&#30340;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#65292;&#36825;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#19982;&#36825;&#31181;&#31616;&#21333;&#27169;&#22411;&#30456;&#21453;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#21160;&#24577;&#25928;&#24212;&#65292;&#20294;&#23427;&#20204;&#30340;&#24040;&#22823;&#35745;&#31639;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#19982;&#24555;&#36895;&#23454;&#26102;&#36845;&#20195;&#24490;&#29615;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#26102;&#31070;&#32463;MPC&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#27969;&#31243;&#20013;&#39640;&#25928;&#22320;&#38598;&#25104;&#22823;&#22411;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#39640;&#24230;&#28789;&#27963;&#30340;&#22235;&#26059;&#32764;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25551;&#36848;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#39640;&#24615;&#33021;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control (MPC) has become a popular framework in embedded control for high-performance autonomous systems. However, to achieve good control performance using MPC, an accurate dynamics model is key. To maintain real-time operation, the dynamics models used on embedded systems have been limited to simple first-principle models, which substantially limits their representative power. In contrast to such simple models, machine learning approaches, specifically neural networks, have been shown to accurately model even complex dynamic effects, but their large computational complexity hindered combination with fast real-time iteration loops. With this work, we present Real-time Neural MPC, a framework to efficiently integrate large, complex neural network architectures as dynamics models within a model-predictive control pipeline. Our experiments, performed in simulation and the real world onboard a highly agile quadrotor platform, demonstrate the capabilities of the described 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#33218;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#39640;&#32500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#26435;&#37325;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.03167</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#33218;
&lt;/p&gt;
&lt;p&gt;
Bayesian Non-stationary Linear Bandits for Large-Scale Recommender Systems. (arXiv:2202.03167v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#33218;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#39640;&#32500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#26435;&#37325;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#33021;&#20250;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#36825;&#31181;&#38468;&#21152;&#20449;&#24687;&#36890;&#24120;&#20855;&#26377;&#22810;&#20010;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#23454;&#26102;&#22788;&#29702;&#36825;&#31181;&#39640;&#32500;&#19978;&#19979;&#25991;&#30340;&#20915;&#31574;&#31639;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#20915;&#31574;&#32773;&#38656;&#35201;&#25512;&#33616;&#22810;&#31181;&#29289;&#21697;&#26102;&#65292;&#36825;&#20855;&#26377;&#29305;&#27530;&#30340;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#25110;&#29992;&#25143;&#30340;&#20559;&#22909;&#21464;&#21270;&#21487;&#33021;&#20250;&#30001;&#20110;&#29615;&#22659;&#20013;&#20998;&#24067;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#32780;&#24433;&#21709;&#24050;&#37096;&#32626;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32447;&#24615;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#39640;&#32500;&#29305;&#24449;&#21521;&#37327;&#12289;&#22823;&#37327;&#33218;&#21644;&#38750;&#24179;&#31283;&#29983;&#25104;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#20943;&#23569;&#20102;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#65292;&#24182;&#20351;&#29992;&#20102;&#25351;&#25968;&#22686;&#38271;&#30340;&#26435;&#37325;&#26469;&#36866;&#24212;&#38750;&#24179;&#31283;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taking advantage of contextual information can potentially boost the performance of recommender systems. In the era of big data, such side information often has several dimensions. Thus, developing decision-making algorithms to cope with such a high-dimensional context in real time is essential. That is specifically challenging when the decision-maker has a variety of items to recommend. In addition, changes in items' popularity or users' preferences can hinder the performance of the deployed recommender system due to a lack of robustness to distribution shifts in the environment. In this paper, we build upon the linear contextual multi-armed bandit framework to address this problem. We develop a decision-making policy for a linear bandit problem with high-dimensional feature vectors, a large set of arms, and non-stationary reward-generating processes. Our Thompson sampling-based policy reduces the dimension of feature vectors using random projection and uses exponentially increasing w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09736</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24352;&#37327;&#21644;&#30697;&#38453;&#20302;&#31209;&#20540;&#20989;&#25968;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning. (arXiv:2201.09736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#65288;VF&#65289;&#30340;&#36817;&#20284;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;VF&#20272;&#35745;&#22312;&#32500;&#24230;&#28798;&#38590;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20154;&#20204;&#37319;&#29992;&#20102;&#31616;&#27905;&#30340;&#21442;&#25968;&#27169;&#22411;&#26469;&#36817;&#20284;VF&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#19978;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#31616;&#27905;&#30340;&#38750;&#21442;&#25968;&#8221;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#20197;&#22312;&#32447;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#20272;&#35745;VF&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;VF&#24448;&#24448;&#26159;&#22810;&#32500;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#24352;&#37327;&#65288;&#22810;&#32500;&#25968;&#32452;&#65289;&#34920;&#31034;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;VF&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;PARAFAC&#20998;&#35299;&#26469;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-function (VF) approximation is a central problem in Reinforcement Learning (RL). Classical non-parametric VF estimation suffers from the curse of dimensionality. As a result, parsimonious parametric models have been adopted to approximate VFs in high-dimensional spaces, with most efforts being focused on linear and neural-network-based approaches. Differently, this paper puts forth a a \emph{parsimonious non-parametric} approach, where we use \emph{stochastic low-rank algorithms} to estimate the VF matrix in an online and model-free fashion. Furthermore, as VFs tend to be multi-dimensional, we propose replacing the classical VF matrix representation with a tensor (multi-way array) representation and, then, use the PARAFAC decomposition to design an online model-free tensor low-rank algorithm. Different versions of the algorithms are proposed, their complexity is analyzed, and their performance is assessed numerically using standardized RL environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#26368;&#20248;&#33218;&#35782;&#21035;&#30340;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#31639;&#27861;&#65292;&#20854;&#21305;&#37197;&#20102;&#19979;&#30028;&#65292;&#21482;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2111.09885</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#26368;&#20248;&#33218;&#35782;&#21035;&#20013;&#30340;&#26368;&#20248;&#31616;&#21333;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Optimal Simple Regret in Bayesian Best Arm Identification. (arXiv:2111.09885v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#26368;&#20248;&#33218;&#35782;&#21035;&#30340;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#31639;&#27861;&#65292;&#20854;&#21305;&#37197;&#20102;&#19979;&#30028;&#65292;&#21482;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#22312;&#20808;&#39564;&#26465;&#20214;&#20855;&#26377;&#19968;&#23450;&#30340;&#36830;&#32493;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#36125;&#21494;&#26031;&#31616;&#21333;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#19982;&#36125;&#21494;&#26031;&#36951;&#25022;&#26368;&#23567;&#21270;&#19981;&#21516;&#65292;&#36125;&#21494;&#26031;&#31616;&#21333;&#36951;&#25022;&#30340;&#20027;&#23548;&#39033;&#26469;&#28304;&#20110;&#26368;&#20248;&#33218;&#21644;&#27425;&#20248;&#33218;&#20043;&#38388;&#38388;&#38553;&#23567;&#20110;$\sqrt{\frac{\log T}{T}}$&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#20854;&#20027;&#23548;&#39033;&#21305;&#37197;&#20102;&#19979;&#30028;&#65292;&#21482;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#65307;&#27169;&#25311;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading term in the Bayesian simple regret derives from the region where the gap between optimal and suboptimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#20247;&#21253;&#21333;&#26631;&#31614;&#24773;&#24863;&#20998;&#26512;&#20013;&#35299;&#20915;&#27880;&#37322;&#32773;&#20559;&#24046;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#20559;&#24046;&#24314;&#27169;&#21644;&#30495;&#23454;&#20540;&#20272;&#35745;&#26469;&#25913;&#21892;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#26679;&#26412;&#21482;&#30001;&#21333;&#20010;&#27880;&#37322;&#32773;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2111.02326</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#20247;&#21253;&#21333;&#26631;&#31614;&#24773;&#24863;&#20998;&#26512;&#20013;&#31471;&#21040;&#31471;&#30340;&#27880;&#37322;&#32773;&#20559;&#24046;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis. (arXiv:2111.02326v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#20247;&#21253;&#21333;&#26631;&#31614;&#24773;&#24863;&#20998;&#26512;&#20013;&#35299;&#20915;&#27880;&#37322;&#32773;&#20559;&#24046;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#20559;&#24046;&#24314;&#27169;&#21644;&#30495;&#23454;&#20540;&#20272;&#35745;&#26469;&#25913;&#21892;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#26679;&#26412;&#21482;&#30001;&#21333;&#20010;&#27880;&#37322;&#32773;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#36890;&#24120;&#26159;&#19968;&#20010;&#23481;&#26131;&#21463;&#21040;&#20247;&#22810;&#27880;&#37322;&#32773;&#20027;&#35266;&#26631;&#31614;&#24433;&#21709;&#30340;&#20247;&#21253;&#20219;&#21153;&#12290;&#30446;&#21069;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#22914;&#20309;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#27491;&#30830;&#22320;&#24314;&#27169;&#27599;&#20010;&#27880;&#37322;&#32773;&#30340;&#27880;&#37322;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#35299;&#20915;&#27880;&#37322;&#32773;&#20559;&#24046;&#26159;&#29702;&#35299;&#27880;&#37322;&#32773;&#26631;&#27880;&#34892;&#20026;&#24182;&#25104;&#21151;&#35299;&#20915;&#30456;&#24212;&#30340;&#20010;&#20307;&#35823;&#35299;&#21644;&#38169;&#35823;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#23545;&#31934;&#30830;&#30340;&#31471;&#21040;&#31471;&#20559;&#24046;&#24314;&#27169;&#21644;&#30495;&#23454;&#20540;&#20272;&#35745;&#36827;&#34892;&#35299;&#37322;&#21644;&#25913;&#36827;&#65292;&#20174;&#32780;&#20943;&#23569;&#29616;&#26377;&#20808;&#36827;&#26041;&#27861;&#20013;&#28041;&#21450;&#30340;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#20998;&#31867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#20165;&#30001;&#21333;&#20010;&#27880;&#37322;&#32773;&#26631;&#27880;&#30340;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#25972;&#20010;&#28304;&#20195;&#30721;&#65292;&#24182;&#21457;&#24067;&#19968;&#20010;&#21253;&#21547;&#35752;&#35770;&#26377;&#26426;&#39135;&#21697;&#20135;&#21697;&#30340;10,000&#20010;&#21477;&#23376;&#30340;&#39046;&#22495;&#29305;&#23450;&#24773;&#24863;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#21477;&#23376;&#26159;&#20174;&#31038;&#20132;&#23186;&#20307;&#25235;&#21462;&#32780;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art methods. However, resolving annotator bias precisely and reliably is the key to understand annotators' labeling behavior and to successfully resolve corresponding individual misconceptions and wrongdoings regarding the annotation task. Our contribution is an explanation and improvement for precise neural end-to-end bias modeling and ground truth estimation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has potential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products. These are crawled from social me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22797;&#26434;&#30340;&#19987;&#23478;&#27880;&#35299;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#28040;&#36153;&#32773;&#20449;&#24565;&#38472;&#36848;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#36739;&#20102;&#32454;&#31890;&#24230;&#21644;&#25277;&#35937;&#31867;&#21035;&#30340;&#26631;&#31614;&#65292;&#24182;&#35828;&#26126;&#22797;&#26434;&#19987;&#23478;&#27880;&#35299;&#22312;&#39640;&#24230;&#29305;&#23450;&#30340;&#24847;&#35265;&#25366;&#25496;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2106.15498</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#28040;&#36153;&#32773;&#20449;&#24565;&#38472;&#36848;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22797;&#26434;&#30340;&#19987;&#23478;&#27880;&#35299;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#28040;&#36153;&#32773;&#20449;&#24565;&#38472;&#36848;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#36739;&#20102;&#32454;&#31890;&#24230;&#21644;&#25277;&#35937;&#31867;&#21035;&#30340;&#26631;&#31614;&#65292;&#24182;&#35828;&#26126;&#22797;&#26434;&#19987;&#23478;&#27880;&#35299;&#22312;&#39640;&#24230;&#29305;&#23450;&#30340;&#24847;&#35265;&#25366;&#25496;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#21487;&#20197;&#36827;&#34892;&#24066;&#22330;&#35843;&#30740;&#65292;&#20197;&#28385;&#36275;&#23458;&#25143;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#31867;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#26500;&#24314;&#22797;&#26434;&#32454;&#31890;&#24230;&#30340;&#31867;&#21035;&#32467;&#26500;&#26469;&#36827;&#34892;&#24066;&#22330;&#35843;&#30740;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#37327;&#36739;&#23569;&#19988;&#27880;&#35299;&#22797;&#26434;&#12290;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#20173;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#24403;&#19987;&#23478;&#27880;&#35299;&#34987;&#24212;&#29992;&#20110;a) &#35768;&#22810;&#32454;&#31890;&#24230;&#31867;&#21035;&#21644;b) &#23569;&#25968;&#25277;&#35937;&#31867;&#21035;&#26102;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;&#22330;&#26223;b)&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#39046;&#22495;&#19987;&#23478;&#32473;&#20986;&#30340;&#25277;&#35937;&#31867;&#21035;&#26631;&#31614;&#65288;&#22522;&#20934;&#65289;&#21644;&#33258;&#21160;&#20998;&#23618;&#32858;&#31867;&#32473;&#20986;&#30340;&#25277;&#35937;&#31867;&#21035;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#21478;&#19968;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#22522;&#20934;&#20351;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#26041;&#27861;&#32473;&#20986;&#25972;&#20010;&#31867;&#21035;&#32467;&#26500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#20316;&#20026;&#22797;&#26434;&#19987;&#23478;&#27880;&#35299;&#22914;&#20309;&#22312;&#39640;&#24230;&#29305;&#23450;&#30340;&#24847;&#35265;&#25366;&#25496;&#20013;&#21457;&#25381;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#20197;&#26368;&#20248;&#21270;&#30340;&#26041;&#24335;&#21033;&#29992;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media offer plenty of information to perform market research in order to meet the requirements of customers. One way how this research is conducted is that a domain expert gathers and categorizes user-generated content into a complex and fine-grained class structure. In many of such cases, little data meets complex annotations. It is not yet fully understood how this can be leveraged successfully for classification. We examine the classification accuracy of expert labels when used with a) many fine-grained classes and b) few abstract classes. For scenario b) we compare abstract class labels given by the domain expert as baseline and by automatic hierarchical clustering. We compare this to another baseline where the entire class structure is given by a completely unsupervised clustering approach. By doing so, this work can serve as an example of how complex expert annotations are potentially beneficial and can be utilized in the most optimal way for opinion mining in highly speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#38169;&#35823;&#23450;&#20301;&#20960;&#20309;&#20998;&#26512;&#31639;&#27861;&#21644;&#36830;&#32493;&#23376;&#31354;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#32422;&#26463;&#31561;&#24322;&#24615;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20840;&#23616;&#26368;&#20248;&#35299;&#19982;&#23616;&#37096;&#35299;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2105.08232</link><description>&lt;p&gt;
&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#30340;&#20960;&#20309;&#20998;&#26512;&#22312;&#31934;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#21306;&#38388;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes. (arXiv:2105.08232v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#38169;&#35823;&#23450;&#20301;&#20960;&#20309;&#20998;&#26512;&#31639;&#27861;&#21644;&#36830;&#32493;&#23376;&#31354;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#32422;&#26463;&#31561;&#24322;&#24615;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20840;&#23616;&#26368;&#20248;&#35299;&#19982;&#23616;&#37096;&#35299;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#20302;&#31209;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#30697;&#38453;&#34917;&#20840;&#12289;&#30456;&#20301;&#21516;&#27493;/&#24674;&#22797;&#12289;&#31283;&#20581;PCA&#21644;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#31561;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32447;&#24615;&#27979;&#37327;&#25439;&#22351;&#30340;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25628;&#32034;&#31209;r&#31561;&#20110;&#26410;&#30693;&#30495;&#23454;&#31209;r*&#30340;&#24773;&#20917;&#65288;&#31934;&#30830;&#21442;&#25968;&#21270;&#24773;&#20917;&#65289;&#65292;&#20197;&#21450;r&#22823;&#20110;r*&#30340;&#24773;&#20917;&#65288;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#65289;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#32422;&#26463;&#31561;&#24322;&#24615;&#24615;&#36136;&#65288;restricted isometry property&#65292;RIP&#65289;&#22312;&#22609;&#36896;&#38750;&#20984;&#20998;&#35299;&#20844;&#24335;&#30340;&#25972;&#20307;&#26223;&#35266;&#21644;&#24110;&#21161;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#25104;&#21151;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;RIP&#24120;&#25968;&#23567;&#20110; 1/(1+sqrt(r*/r))&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#38750;&#20984;&#38382;&#39064;&#30340;&#20219;&#24847;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#36827;&#34892;&#20102;&#20840;&#23616;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#38169;&#35823;&#23450;&#20301;&#20960;&#20309;&#20998;&#26512;&#65288;Robust Error-Locating Geometric Analysis&#65292;RELGA&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#12290;RELGA&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#38169;&#35823;&#23450;&#20301;&#26426;&#21046;&#21644;&#20960;&#20309;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#22122;&#22768;&#27700;&#24179;&#30456;&#23545;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#30697;&#38453;&#24674;&#22797;&#12290;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#36830;&#32493;&#23376;&#31354;&#38388;&#20248;&#21270;&#65288;Successive Subspace Optimization&#65292;SSO&#65289;&#31639;&#27861;&#65292;&#22312;&#22122;&#22768;&#27700;&#24179;&#21644;RIP&#24120;&#25968;&#30340;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#30495;&#23454;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SSO&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#21021;&#22987;&#21270;&#12289;&#38750;&#36864;&#21270;&#24615;&#21644;&#20960;&#20309;&#26465;&#20214;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The matrix sensing problem is an important low-rank optimization problem that has found a wide range of applications, such as matrix completion, phase synchornization/retrieval, robust PCA, and power system state estimation. In this work, we focus on the general matrix sensing problem with linear measurements that are corrupted by random noise. We investigate the scenario where the search rank $r$ is equal to the true rank $r^*$ of the unknown ground truth (the exact parametrized case), as well as the scenario where $r$ is greater than $r^*$ (the overparametrized case). We quantify the role of the restricted isometry property (RIP) in shaping the landscape of the non-convex factorized formulation and assisting with the success of local search algorithms. First, we develop a global guarantee on the maximum distance between an arbitrary local minimizer of the non-convex problem and the ground truth under the assumption that the RIP constant is smaller than $1/(1+\sqrt{r^*/r})$. We then p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#24341;&#20837;&#26377;&#38480;&#20307;&#31215;&#26041;&#27861;&#21644;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#38750;&#31283;&#24577;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20165;&#20351;&#29992;&#20004;&#20010;&#20808;&#21069;&#30340;&#22330;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#22330;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#27969;&#22330;&#22270;&#20687;&#12290;&#27492;&#27169;&#22411;&#22312;&#38750;&#21453;&#24212;&#27969;&#21644;&#21453;&#24212;&#27969;&#27169;&#25311;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.03332</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#38750;&#31283;&#24577;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#30340;&#26377;&#38480;&#20307;&#31215;&#26041;&#27861;&#32593;&#32476;&#65306;&#38750;&#21453;&#24212;&#21644;&#21453;&#24212;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Finite volume method network for acceleration of unsteady computational fluid dynamics: non-reacting and reacting flows. (arXiv:2105.03332v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#24341;&#20837;&#26377;&#38480;&#20307;&#31215;&#26041;&#27861;&#21644;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#38750;&#31283;&#24577;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20165;&#20351;&#29992;&#20004;&#20010;&#20808;&#21069;&#30340;&#22330;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#22330;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#27969;&#22330;&#22270;&#20687;&#12290;&#27492;&#27169;&#22411;&#22312;&#38750;&#21453;&#24212;&#27969;&#21644;&#21453;&#24212;&#27969;&#27169;&#25311;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#22830;&#22788;&#29702;&#22120;&#65288;CPU&#65289;&#30340;&#24615;&#33021;&#36805;&#36895;&#25552;&#21319;&#65292;&#20294;&#20351;&#29992;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#27969;&#21160;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20173;&#19981;&#21487;&#34892;&#12290;&#34429;&#28982;&#24050;&#30740;&#31350;&#20102;&#23558;&#19987;&#38376;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24212;&#29992;&#20110;&#27969;&#22330;&#39044;&#27979;&#65292;&#20294;&#26368;&#36817;&#20986;&#29616;&#20102;&#20026;CFD&#35774;&#35745;&#36866;&#24212;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#24341;&#20837;&#26377;&#38480;&#20307;&#31215;&#26041;&#27861;&#65288;FVM&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#21152;&#36895;CFD&#27169;&#25311;&#12290;&#24320;&#21457;&#30340;&#32593;&#32476;&#27169;&#22411;&#32771;&#34385;&#20102;CFD&#27969;&#22330;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#25152;&#26377;&#32593;&#26684;&#37117;&#24212;&#29992;&#30456;&#21516;&#30340;&#25511;&#21046;&#26041;&#31243;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#20004;&#20010;&#20808;&#21069;&#30340;&#22330;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22330;&#65292;&#32780;&#19981;&#20687;CNN&#37027;&#26679;&#38656;&#35201;&#35768;&#22810;&#22330;&#22270;&#20687;&#65288;&gt;10,000&#65289;&#12290;&#20351;&#29992;&#38750;&#21453;&#24212;&#27969;&#21644;&#21453;&#24212;&#27969;&#27169;&#25311;&#30340;CFD&#26102;&#24207;&#25968;&#25454;&#35780;&#20272;&#20102;&#36825;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;
&lt;/p&gt;
&lt;p&gt;
Despite rapid improvements in the performance of central processing unit (CPU), the calculation cost of simulating chemically reacting flow using CFD remains infeasible in many cases. The application of the convolutional neural networks (CNNs) specialized in image processing in flow field prediction has been studied, but the need to develop a neural netweork design fitted for CFD is recently emerged. In this study, a neural network model introducing the finite volume method (FVM) with a unique network architecture and physics-informed loss function was developed to accelerate CFD simulations. The developed network model, considering the nature of the CFD flow field where the identical governing equations are applied to all grids, can predict the future fields with only two previous fields unlike the CNNs requiring many field images (&gt;10,000). The performance of this baseline model was evaluated using CFD time series data from non-reacting flow and reacting flow simulation; counterflow 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22797;&#21046;&#26041;&#27861;&#20272;&#35745;&#20102;&#20855;&#26377;&#39532;&#23572;&#31185;&#22827;&#25110;&#38544;&#39532;&#23572;&#31185;&#22827;&#20449;&#21495;&#20808;&#39564;&#30340;&#32447;&#24615;&#27169;&#22411;&#30340;&#33258;&#30001;&#33021;&#12289;&#24179;&#22343;&#20114;&#20449;&#24687;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#22120;&#19979;&#65292;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#20998;&#35299;&#20026;&#20855;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#21333;&#36755;&#20837;AWGN&#20449;&#36947;&#65292;&#32780;&#29366;&#24577;&#20998;&#24067;&#36981;&#24490;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#38543;&#26426;&#30697;&#38453;&#30340;&#24038;Perron-Frobenius&#29305;&#24449;&#21521;&#37327;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#65292;&#36890;&#36807;&#22797;&#21046;&#26041;&#27861;&#24471;&#21040;&#30340;&#32467;&#26524;&#19982;Metropolis-Hastings&#31639;&#27861;&#25110;&#20854;&#20182;&#36817;&#20284;&#20256;&#36882;&#31639;&#27861;&#30340;&#32467;&#26524;&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>http://arxiv.org/abs/2009.13370</link><description>&lt;p&gt;
&#20855;&#26377;&#39532;&#23572;&#31185;&#22827;&#25110;&#38544;&#39532;&#23572;&#31185;&#22827;&#20449;&#21495;&#20808;&#39564;&#30340;&#32447;&#24615;&#27169;&#22411;&#30340;&#22797;&#21046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v5 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.13370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22797;&#21046;&#26041;&#27861;&#20272;&#35745;&#20102;&#20855;&#26377;&#39532;&#23572;&#31185;&#22827;&#25110;&#38544;&#39532;&#23572;&#31185;&#22827;&#20449;&#21495;&#20808;&#39564;&#30340;&#32447;&#24615;&#27169;&#22411;&#30340;&#33258;&#30001;&#33021;&#12289;&#24179;&#22343;&#20114;&#20449;&#24687;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#22120;&#19979;&#65292;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#20998;&#35299;&#20026;&#20855;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#21333;&#36755;&#20837;AWGN&#20449;&#36947;&#65292;&#32780;&#29366;&#24577;&#20998;&#24067;&#36981;&#24490;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#38543;&#26426;&#30697;&#38453;&#30340;&#24038;Perron-Frobenius&#29305;&#24449;&#21521;&#37327;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#65292;&#36890;&#36807;&#22797;&#21046;&#26041;&#27861;&#24471;&#21040;&#30340;&#32467;&#26524;&#19982;Metropolis-Hastings&#31639;&#27861;&#25110;&#20854;&#20182;&#36817;&#20284;&#20256;&#36882;&#31639;&#27861;&#30340;&#32467;&#26524;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22797;&#21046;&#26041;&#27861;&#65292;&#20272;&#35745;&#20102;&#22312;&#32447;&#24615;&#27169;&#22411;&#19979;&#30340;&#33258;&#30001;&#33021;&#12289;&#24179;&#22343;&#20114;&#20449;&#24687;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#65292;&#24182;&#20551;&#35774;&#20102;&#20004;&#20010;&#26465;&#20214;&#65306;&#65288;1&#65289;&#28304;&#30001;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#65292;&#65288;2&#65289;&#28304;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#26159;&#22522;&#20110;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#34920;&#26126;&#20855;&#26377;&#39532;&#23572;&#31185;&#22827;&#28304;&#25110;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#28304;&#30340;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#20998;&#35299;&#20026;&#20855;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#21333;&#36755;&#20837;AWGN&#20449;&#36947;&#65292;&#20854;&#20013;&#29366;&#24577;&#20998;&#24067;&#36981;&#24490;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#38543;&#26426;&#30697;&#38453;&#30340;&#24038;Perron-Frobenius&#29305;&#24449;&#21521;&#37327;&#65292;&#20854;&#26364;&#21704;&#39039;&#33539;&#25968;&#20026;1&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22797;&#21046;&#26041;&#27861;&#33719;&#24471;&#30340;&#33258;&#30001;&#33021;&#21644;&#22343;&#26041;&#35823;&#24046;&#19982;&#30740;&#31350;&#25991;&#29486;&#20013;Metropolis-Hastings&#31639;&#27861;&#25110;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#36817;&#20284;&#20256;&#36882;&#31639;&#27861;&#30340;&#23545;&#24212;&#32467;&#26524;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper estimates free energy, average mutual information, and minimum mean square error (MMSE) of a linear model under two assumptions: (1) the source is generated by a Markov chain, (2) the source is generated via a hidden Markov model. Our estimates are based on the replica method in statistical physics. We show that under the posterior mean estimator, the linear model with Markov sources or hidden Markov sources is decoupled into single-input AWGN channels with state information available at both encoder and decoder where the state distribution follows the left Perron-Frobenius eigenvector with unit Manhattan norm of the stochastic matrix of Markov chains. Numerical results show that the free energies and MSEs obtained via the replica method are closely approximate to their counterparts achieved by the Metropolis-Hastings algorithm or some well-known approximate message passing algorithms in the research literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#20803;&#23398;&#20064;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2003.03229</link><description>&lt;p&gt;
&#20855;&#26377;&#31867;&#20154;&#31867;&#26641;&#31361;&#28608;&#27963;&#30340;&#38750;&#32447;&#24615;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Non-linear Neurons with Human-like Apical Dendrite Activations. (arXiv:2003.03229v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#20803;&#23398;&#20064;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#32447;&#24615;&#19981;&#21487;&#20998;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#24120;&#23558;&#31070;&#32463;&#20803;&#32452;&#32455;&#25104;&#33267;&#23569;&#21253;&#21547;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#20803;&#23398;&#20064;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26631;&#20934;&#31070;&#32463;&#20803;&#25509;&#19978;&#25105;&#20204;&#30340;&#26032;&#22411;&#26641;&#31361;&#28608;&#27963;&#20989;&#25968;&#65288;ADA&#65289;&#21487;&#20197;&#20197;100%&#30340;&#20934;&#30830;&#29575;&#23398;&#20064;XOR&#36923;&#36753;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21363;MOROCO&#12289;UTKFace&#12289;CREMA-D&#12289;Fashion-MNIST&#12289;Tiny ImageNet&#21644;ImageNet&#65292;&#32467;&#26524;&#26174;&#31034;ADA&#21644;&#28431;&#30005;ADA&#20989;&#25968;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65288;&#22914;&#19968;&#23618;&#25110;&#20004;&#23618;&#38544;&#34255;&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#19978;&#20248;&#20110;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#12289;&#28431;&#30005;ReLU&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#21644;Swish&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to classify linearly non-separable data, neurons are typically organized into multi-layer neural networks that are equipped with at least one hidden layer. Inspired by some recent discoveries in neuroscience, we propose a new model of artificial neuron along with a novel activation function enabling the learning of nonlinear decision boundaries using a single neuron. We show that a standard neuron followed by our novel apical dendrite activation (ADA) can learn the XOR logical function with 100% accuracy. Furthermore, we conduct experiments on six benchmark data sets from computer vision, signal processing and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST, Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and Swish, for various neural network architectures, e.g. one-hidden-layer or two-hidden-layer multi-layer perceptrons (MLPs) and convolutional ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#32039;&#33268;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#20102;&#19968;&#31181;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#65292;&#35813;&#21464;&#25442;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#25955;&#23556;&#21464;&#25442;&#65292;&#20855;&#26377;&#23616;&#37096;&#21516;&#26500;&#30340;&#19981;&#21464;&#24615;&#21644;&#26576;&#20123;&#31867;&#22411;&#30340;&#24494;&#20998;&#21516;&#32986;&#30340;&#31283;&#23450;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#20960;&#20309;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/1905.10448</link><description>&lt;p&gt;
&#22312;&#32039;&#33268;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20960;&#20309;&#23567;&#27874;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds. (arXiv:1905.10448v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#32039;&#33268;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#20102;&#19968;&#31181;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#65292;&#35813;&#21464;&#25442;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#25955;&#23556;&#21464;&#25442;&#65292;&#20855;&#26377;&#23616;&#37096;&#21516;&#26500;&#30340;&#19981;&#21464;&#24615;&#21644;&#26576;&#20123;&#31867;&#22411;&#30340;&#24494;&#20998;&#21516;&#32986;&#30340;&#31283;&#23450;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#20960;&#20309;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#21069;&#65292;&#27431;&#20960;&#37324;&#24471;&#25955;&#23556;&#21464;&#25442;&#34987;&#24341;&#20837;&#20197;&#25913;&#21892;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#29702;&#35299;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#35813;&#23398;&#31185;&#26088;&#22312;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#27969;&#24418;&#21644;&#22270;&#32467;&#26500;&#22495;&#65292;&#25105;&#20204;&#22312;&#27969;&#24418;&#19978;&#23450;&#20041;&#20102;&#19968;&#31181;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#12290;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#25955;&#23556;&#21464;&#25442;&#65292;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#22522;&#20110;&#19968;&#31995;&#21015;&#23567;&#27874;&#28388;&#27874;&#22120;&#21644;&#36880;&#28857;&#38750;&#32447;&#24615;&#12290;&#23427;&#23545;&#23616;&#37096;&#21516;&#26500;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#24494;&#20998;&#21516;&#32986;&#20855;&#26377;&#31283;&#23450;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#23427;&#22312;&#20960;&#20309;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#20102;&#27431;&#20960;&#37324;&#24471;&#25955;&#23556;&#30340;&#21464;&#24418;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#20351;&#29992;&#30340;&#28388;&#27874;&#22120;&#32467;&#26500;&#19982;&#25968;&#25454;&#30340;&#22522;&#30784;&#20960;&#20309;&#32852;&#31995;&#36215;&#26469;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Euclidean scattering transform was introduced nearly a decade ago to improve the mathematical understanding of convolutional neural networks. Inspired by recent interest in geometric deep learning, which aims to generalize convolutional neural networks to manifold and graph-structured domains, we define a geometric scattering transform on manifolds. Similar to the Euclidean scattering transform, the geometric scattering transform is based on a cascade of wavelet filters and pointwise nonlinearities. It is invariant to local isometries and stable to certain types of diffeomorphisms. Empirical results demonstrate its utility on several geometric learning tasks. Our results generalize the deformation stability and local translation invariance of Euclidean scattering, and demonstrate the importance of linking the used filter structures to the underlying geometry of the data.
&lt;/p&gt;</description></item></channel></rss>