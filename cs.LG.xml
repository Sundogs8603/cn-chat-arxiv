<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00983</link><description>&lt;p&gt;
&#20248;&#21270;&#24211;&#23384;&#37197;&#36865;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65288;IRP&#65289;&#26159;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#22312;&#32771;&#34385;&#24211;&#23384;&#38656;&#27714;&#35268;&#21010;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26377;&#25928;&#30340;&#36335;&#24452;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;IRP&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#38656;&#27714;&#65292;&#28982;&#21518;&#20351;&#29992;&#20248;&#21270;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#37197;&#36865;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#23436;&#32654;&#20934;&#30830;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#24211;&#23384;&#27700;&#24179;&#21463;&#21160;&#24577;&#19994;&#21153;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#19968;&#38454;&#27573;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;IRP&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#20102;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12781</link><description>&lt;p&gt;
&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#22312;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#21363;&#20010;&#20307;&#29992;&#25143;&#20449;&#24687;&#19981;&#20250;&#27844;&#38706;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21521;&#20445;&#23494;&#25968;&#25454;&#27880;&#20837;&#26657;&#20934;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#25110;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#20250;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#38590;&#20197;&#23545;&#22522;&#30784;&#26426;&#23494;&#25968;&#25454;&#30340;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#20316;&#20026;&#19968;&#32452;&#28789;&#27963;&#30340;&#20998;&#24067;&#26469;&#36817;&#20284;&#32473;&#23450;&#35266;&#27979;&#21040;&#30340;&#31169;&#26377;&#26597;&#35810;&#32467;&#26524;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20256;&#26579;&#30149;&#27169;&#22411;&#19979;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20197;&#21450;&#26222;&#36890;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.10835</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#21487;&#35777;&#26126;&#30340;&#27010;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Provable Probabilistic Imaging using Score-Based Generative Priors. (arXiv:2310.10835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26102;&#65292;&#20272;&#35745;&#39640;&#36136;&#37327;&#22270;&#20687;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#26159;&#22270;&#20687;&#37325;&#24314;&#31639;&#27861;&#20013;&#30340;&#20004;&#20010;&#29702;&#24819;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#65288;PMC&#65289;&#20316;&#20026;&#19968;&#31181;&#23545;&#19968;&#33324;&#21453;&#38382;&#39064;&#21487;&#33021;&#35299;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;PMC&#33021;&#22815;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#32467;&#21512;&#20016;&#23500;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#36827;&#34892;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;PMC&#31639;&#27861;&#65292;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#25554;&#20837;&#24335;&#20808;&#39564;&#65288;PnP&#65289;&#21644;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;RED&#65289;&#31639;&#27861;&#30340;&#37319;&#26679;&#27169;&#25311;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#23545;PMC&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20004;&#31181;&#31639;&#27861;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#38750;&#23545;&#25968;&#20985;&#20284;&#28982;&#21644;&#19981;&#23436;&#32654;&#24471;&#20998;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>COPlanner&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21010;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;COPlanner&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#35299;&#20915;&#21160;&#21147;&#23398;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.07220</link><description>&lt;p&gt;
COPlanner: &#20445;&#23432;&#30340;&#27169;&#22411;&#35268;&#21010;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#20026;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL. (arXiv:2310.07220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07220
&lt;/p&gt;
&lt;p&gt;
COPlanner&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21010;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;COPlanner&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#35299;&#20915;&#21160;&#21147;&#23398;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dyna-style&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21253;&#21547;&#20004;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#30340;&#27169;&#22411;&#28436;&#31639;&#38454;&#27573;&#21644;&#20351;&#29992;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#30495;&#23454;&#29615;&#22659;&#25506;&#32034;&#20197;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#65292;&#38590;&#20813;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#20855;&#26377;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#30340;&#19981;&#23436;&#32654;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36827;&#32780;&#21487;&#33021;&#35823;&#23548;&#31574;&#30053;&#23398;&#20064;&#24182;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COPlanner&#30340;&#22522;&#20110;&#35268;&#21010;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#20934;&#30830;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#38382;&#39064;&#65292;&#20854;&#37319;&#29992;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#12290;COPlanner&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#24341;&#23548;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;UP-MPC&#65289;&#32452;&#20214;&#36827;&#34892;&#22810;&#27493;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#35268;&#21010;&#12290;&#36825;&#20010;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#27169;&#22411;&#28436;&#31639;&#26399;&#38388;&#20316;&#20026;&#24809;&#32602;&#22240;&#32032;&#65292;&#22312;&#30495;&#23454;&#29615;&#22659;&#25506;&#32034;&#26399;&#38388;&#20316;&#20026;&#22870;&#21169;&#22240;&#32032;&#65292;&#20197;&#36873;&#25321;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;COPlanner&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#20445;&#23432;&#30340;&#27169;&#22411;&#28436;&#31639;&#21644;&#20048;&#35266;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.15366</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#65306;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15366
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#21183;&#26159;&#20854;&#20801;&#35768;&#23545;&#26681;&#25454;&#24191;&#27867;&#27010;&#29575;&#27979;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#32479;&#19968;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#27979;&#24230;&#20256;&#36882;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#19977;&#35282;&#20256;&#36882;&#26144;&#23556;&#30340;&#20351;&#29992;&#65292;&#20316;&#20026;&#25903;&#25345;&#29983;&#29289;&#31185;&#23398;&#30740;&#31350;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#22312;&#36752;&#23556;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#31995;&#21015;&#65288;&#31232;&#30095;&#30340;&#65289;&#33258;&#36866;&#24212;&#20256;&#36882;&#26144;&#23556;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#36825;&#20123;&#26144;&#23556;&#26159;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#31995;&#21015;&#21487;&#29992;&#25968;&#25454;&#26679;&#26412;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#32771;&#34385;&#30340;&#36752;&#23556;&#29983;&#29289;&#23398;&#24212;&#29992;&#20013;&#65292;&#27492;&#26041;&#27861;&#20026;&#29983;&#25104;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#35760;&#24405;&#30340;&#27493;&#24577;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;&#30340;&#25932;&#23545;&#27169;&#22411;&#12290;&#30740;&#31350;&#35843;&#26597;&#20102;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#26412;IMUGait&#27169;&#24335;&#30340;&#23383;&#20856;&#65292;&#29992;&#20110;&#21457;&#21160;&#25915;&#20987;&#25110;&#25214;&#21040;&#33021;&#22815;&#21305;&#37197;&#30446;&#26631;IMUGait&#27169;&#24335;&#30340;&#27169;&#20223;&#32773;&#12290;&#36890;&#36807;&#23545;&#38169;&#35823;&#29575;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;IMUGait&#27169;&#24335;&#30340;&#35748;&#35777;&#31995;&#32479;&#26159;&#21542;&#26368;&#22256;&#38590;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.11766</link><description>&lt;p&gt;
&#22522;&#20110;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#30340;&#27493;&#24577;&#35748;&#35777;&#30340;&#23383;&#20856;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#35760;&#24405;&#30340;&#27493;&#24577;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;&#30340;&#25932;&#23545;&#27169;&#22411;&#12290;&#30740;&#31350;&#35843;&#26597;&#20102;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#26412;IMUGait&#27169;&#24335;&#30340;&#23383;&#20856;&#65292;&#29992;&#20110;&#21457;&#21160;&#25915;&#20987;&#25110;&#25214;&#21040;&#33021;&#22815;&#21305;&#37197;&#30446;&#26631;IMUGait&#27169;&#24335;&#30340;&#27169;&#20223;&#32773;&#12290;&#36890;&#36807;&#23545;&#38169;&#35823;&#29575;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;IMUGait&#27169;&#24335;&#30340;&#35748;&#35777;&#31995;&#32479;&#26159;&#21542;&#26368;&#22256;&#38590;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20869;&#32622;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#35760;&#24405;&#30340;&#27493;&#24577;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;&#30340;&#25932;&#23545;&#27169;&#22411;&#12290;&#25915;&#20987;&#24605;&#36335;&#21463;&#21040;&#23383;&#20856;&#25915;&#20987;&#30693;&#35782;(PIN&#25110;&#23494;&#30721;)&#22522;&#30784;&#35748;&#35777;&#31995;&#32479;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#24182;&#20197;&#27492;&#21629;&#21517;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#26412;IMUGait&#27169;&#24335;&#30340;&#23383;&#20856;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#21457;&#21160;&#25915;&#20987;&#65292;&#25110;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#20027;&#21160;&#22797;&#21046;&#19982;&#30446;&#26631;IMUGait&#27169;&#24335;&#21305;&#37197;&#30340;&#27169;&#20223;&#32773;&#12290;&#20061;&#20010;&#36523;&#20307;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#22810;&#26679;&#21270;&#30340;&#20010;&#20307;&#20197;&#19981;&#21516;&#27700;&#24179;&#30340;&#22235;&#20010;&#39044;&#23450;&#20041;&#21487;&#25511;&#21644;&#21487;&#35843;&#33410;&#30340;&#27493;&#24577;&#22240;&#32032;(&#36895;&#24230;&#12289;&#27493;&#38271;&#12289;&#27493;&#23485;&#21644;&#22823;&#33151;&#25260;&#36215;)&#34892;&#36208;&#65292;&#20135;&#29983;&#20102;178&#31181;&#29420;&#29305;&#30340;IMUGait&#27169;&#24335;&#12290;&#27599;&#20010;&#27169;&#24335;&#37117;&#20250;&#25915;&#20987;&#21508;&#31181;&#29992;&#25143;&#35748;&#35777;&#27169;&#22411;&#12290;&#23545;&#38169;&#35823;&#29575;&#30340;&#28145;&#20837;&#20998;&#26512;(&#25915;&#20987;&#21069;&#21518;)&#25361;&#25112;&#20102;&#22522;&#20110;IMUGait&#27169;&#24335;&#30340;&#35748;&#35777;&#31995;&#32479;&#26159;&#26368;&#22256;&#38590;&#30340;&#36825;&#31181;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel adversarial model for authentication systems that use gait patterns recorded by the inertial measurement unit (IMU) built into smartphones. The attack idea is inspired by and named after the concept of a dictionary attack on knowledge (PIN or password) based authentication systems. In particular, this work investigates whether it is possible to build a dictionary of IMUGait patterns and use it to launch an attack or find an imitator who can actively reproduce IMUGait patterns that match the target's IMUGait pattern. Nine physically and demographically diverse individuals walked at various levels of four predefined controllable and adaptable gait factors (speed, step length, step width, and thigh-lift), producing 178 unique IMUGait patterns. Each pattern attacked a wide variety of user authentication models. The deeper analysis of error rates (before and after the attack) challenges the belief that authentication systems based on IMUGait patterns are the most difficul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#26469;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#65292;&#24182;&#22312;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.03842</link><description>&lt;p&gt;
&#38544;&#24615;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#36716;&#21521;&#39044;&#35686;
&lt;/p&gt;
&lt;p&gt;
Early warning via transitions in latent stochastic dynamical systems. (arXiv:2309.03842v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#26469;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#65292;&#24182;&#22312;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#22240;&#31361;&#21464;&#12289;&#33041;&#30142;&#30149;&#12289;&#33258;&#28982;&#28798;&#23475;&#12289;&#37329;&#34701;&#21361;&#26426;&#21644;&#24037;&#31243;&#21487;&#38752;&#24615;&#65292;&#23545;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#36827;&#34892;&#26089;&#26399;&#35686;&#25253;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#65292;&#23427;&#25429;&#25417;&#20102;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#25214;&#21040;&#20102;&#36866;&#24403;&#30340;&#26377;&#25928;&#22352;&#26631;&#65292;&#24182;&#25512;&#23548;&#20986;&#33021;&#22815;&#26816;&#27979;&#29366;&#24577;&#36716;&#21464;&#20013;&#20020;&#30028;&#28857;&#30340;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28508;&#22312;&#21160;&#24577;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23494;&#24230;&#21644;&#36716;&#21464;&#27010;&#29575;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31532;&#20108;&#20010;&#22352;&#26631;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#20013;&#20445;&#25345;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#26500;&#36896;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#35843;&#25972;&#26694;&#26550;&#30340;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03537</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#22312;&#20855;&#26377;&#32039;&#33268;&#25903;&#25345;&#21644;&#28176;&#28040;&#30952;&#30340;&#22270;&#19978;
&lt;/p&gt;
&lt;p&gt;
Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments. (arXiv:2309.03537v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#26500;&#36896;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#35843;&#25972;&#26694;&#26550;&#30340;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#31995;&#21015;&#20998;&#23618;&#20998;&#21306;&#26500;&#24314;&#20855;&#26377;&#32039;&#33268;&#25903;&#25345;&#30340;&#22270;&#19978;&#30340;&#32039;&#26694;&#26550;&#12290;&#20174;&#25105;&#20204;&#30340;&#25277;&#35937;&#26500;&#36896;&#24320;&#22987;&#65292;&#25105;&#20204;&#33021;&#22815;&#28789;&#27963;&#22320;&#23558;&#23376;&#22270;Laplacians&#32435;&#20837;&#21040;&#25105;&#20204;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#20801;&#35768;&#35843;&#25972;&#26694;&#26550;&#30340;&#65288;&#23376;&#22270;&#65289;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#22914;&#26041;&#21521;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#26126;&#30830;&#23450;&#20041;&#24182;&#27979;&#35797;&#20102;&#20960;&#20010;&#21464;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22270;&#26694;&#26550;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we proposed a novel and general method to construct tight frames on graphs with compact supports based on a series of hierarchical partitions. Starting from our abstract construction that generalizes previous methods based on partition trees, we are able to flexibly incorporate subgraph Laplacians into our design of graph frames. Consequently, our general methods permit adjusting the (subgraph) vanishing moments of the framelets and extra properties, such as directionality, for efficiently representing graph signals with path-like supports. Several variants are explicitly defined and tested. Experimental results show our proposed graph frames perform superiorly in non-linear approximation tasks.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02332</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#22788;&#29702;&#65306;&#25968;&#25454;&#21644;&#25805;&#20316;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v1 [q-bio.NC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02332
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21754;&#20083;&#21160;&#29289;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30340;&#22797;&#26434;&#32467;&#26500;&#20013;&#65292;&#31070;&#32463;&#20803;&#24418;&#25104;&#32676;&#20307;&#12290;&#36724;&#32034;&#26463;&#36890;&#36807;&#33033;&#20914;&#21015;&#20316;&#20026;&#23186;&#20171;&#22312;&#36825;&#20123;&#32676;&#38598;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#30340;&#31934;&#30830;&#32534;&#30721;&#21644;&#25805;&#20316;&#36824;&#26377;&#24453;&#21457;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#20986;&#21457;&#28857;&#26159;&#19968;&#20010;&#20855;&#26377;&#21487;&#22609;&#24615;&#30340;&#36890;&#29992;&#31070;&#32463;&#20803;&#30340;&#20808;&#36827;&#30340;&#26426;&#26800;&#27169;&#22411;&#12290;&#20174;&#36825;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#28145;&#21051;&#30340;&#25968;&#23398;&#26500;&#36896;&#65306;&#36890;&#36807;&#26377;&#38480;&#20984;&#38181;&#30340;&#20195;&#25968;&#21487;&#20197;&#20934;&#30830;&#22320;&#25551;&#36848;&#20449;&#24687;&#30340;&#34920;&#31034;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#19981;&#20165;&#20165;&#26159;&#34987;&#21160;&#20256;&#36755;&#32773;&#12290;&#23427;&#20204;&#22312;&#36825;&#20010;&#20195;&#25968;&#32467;&#26500;&#20013;&#25198;&#28436;&#30528;&#36816;&#31639;&#31526;&#30340;&#35282;&#33394;&#65292;&#21453;&#26144;&#20102;&#20302;&#32423;&#32534;&#31243;&#35821;&#35328;&#30340;&#21151;&#33021;&#12290;&#24403;&#36825;&#20123;&#32676;&#20307;&#20114;&#36830;&#26102;&#65292;&#23427;&#20204;&#20855;&#26377;&#31616;&#27905;&#32780;&#24378;&#22823;&#30340;&#20195;&#25968;&#34920;&#36798;&#24335;&#12290;&#36825;&#20123;&#32593;&#32476;&#20351;&#23427;&#20204;&#33021;&#22815;&#23454;&#29616;&#35768;&#22810;&#25805;&#20316;&#65292;&#22914;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#12289;&#32500;&#24230;&#38477;&#20302;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensiona
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15821</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#35774;&#22791;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#30340;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#23398;&#20064;&#38477;&#32423;&#21644;&#24930;&#25910;&#25947;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#22320;&#37319;&#29992;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#27010;&#24565;&#65292;&#21482;&#20801;&#35768;&#22312;&#27599;&#20010;&#32452;&#20869;&#32858;&#21512;&#27169;&#22411;&#26435;&#37325;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#27169;&#22411;&#26799;&#24230;&#25110;&#25512;&#29702;&#36755;&#20986;&#20316;&#20026;&#23458;&#25143;&#31471;&#20998;&#21306;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#30446;&#30340;&#26159;&#23558;&#30456;&#20284;&#35774;&#22791;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20294;&#27599;&#20010;&#32858;&#31867;&#20869;&#37096;&#20173;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30740;&#31350;&#25506;&#32034;&#30830;&#23450;&#32858;&#31867;&#30340;&#36866;&#24403;&#26102;&#38388;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#23548;&#33268;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21040;&#20854;&#33258;&#24049;&#30340;&#29420;&#31435;&#32858;&#31867;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#38750;ind&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained significant attention due to its groundbreaking ability to enable distributed learning while maintaining privacy constraints. However, as a consequence of data heterogeneity among decentralized devices, it inherently experiences significant learning degradation and slow convergence speed. Therefore, it is natural to employ the concept of clustering homogeneous clients into the same group, allowing only the model weights within each group to be aggregated. While most existing clustered federated learning methods employ either model gradients or inference outputs as metrics for client partitioning, with the goal of grouping similar devices together, may still have heterogeneity within each cluster. Moreover, there is a scarcity of research exploring the underlying reasons for determining the appropriate timing for clustering, resulting in the common practice of assigning each client to its own individual cluster, particularly in the context of highly non ind
&lt;/p&gt;</description></item><item><title>ULDP-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#29992;&#20110;&#36328;&#36793;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#23494;&#30721;&#23398;&#26500;&#20214;&#22686;&#24378;&#20102;&#20854;&#25928;&#29992;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12210</link><description>&lt;p&gt;
ULDP-FL:&#20855;&#26377;&#36328;&#36793;&#30028;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy. (arXiv:2308.12210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12210
&lt;/p&gt;
&lt;p&gt;
ULDP-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#29992;&#20110;&#36328;&#36793;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#23494;&#30721;&#23398;&#26500;&#20214;&#22686;&#24378;&#20102;&#20854;&#25928;&#29992;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DP-FL&#65289;&#20316;&#20026;&#19968;&#31181;&#30830;&#20445;&#24418;&#24335;&#38544;&#31169;&#30340;&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;DP-FL&#26041;&#27861;&#30830;&#20445;&#22312;&#27599;&#20010;&#36793;&#30028;&#20869;&#20197;&#35760;&#24405;&#32423;&#21035;&#30340;DP&#36827;&#34892;&#36328;&#36793;&#30028;FL&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#21487;&#33021;&#24310;&#20280;&#21040;&#22810;&#20010;&#36793;&#30028;&#65292;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#29992;&#25143;&#32423;DP&#20445;&#35777;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ULDP-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21333;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#21487;&#33021;&#23646;&#20110;&#22810;&#20010;&#36793;&#30028;&#30340;&#36328;&#36793;&#30028;FL&#20013;&#20445;&#35777;&#29992;&#25143;&#32423;DP&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;DP&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#32452;&#38544;&#31169;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#23398;&#26500;&#20214;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#25928;&#29992;&#24182;&#23637;&#31034;&#20102;&#20854;&#31169;&#23494;&#23454;&#29616;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under us
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07931</link><description>&lt;p&gt;
&#31934;&#31616;&#29305;&#24449;&#22330;&#20351;&#24471;&#35821;&#35328;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21644;&#35821;&#35328;&#30417;&#30563;&#30340;&#22270;&#20687;&#27169;&#22411;&#21253;&#21547;&#20102;&#19990;&#30028;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#23545;&#20110;&#27867;&#21270;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#38656;&#35201;&#23545; 3D &#20960;&#20309;&#30340;&#35814;&#32454;&#29702;&#35299;&#65292;&#36825;&#22312; 2D &#22270;&#20687;&#29305;&#24449;&#20013;&#24448;&#24448;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340; 3D &#20960;&#20309;&#19982; 2D &#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#26469;&#24357;&#21512;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340; 2D &#21040; 3D &#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545; 6 &#33258;&#30001;&#24230;&#25235;&#21462;&#21644;&#25918;&#32622;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#36890;&#36807;&#20174;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; CLIP &#20013;&#31934;&#31616;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#26032;&#39062;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#30340;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26410;&#35265;&#36807;&#30340;&#34920;&#36798;&#21644;&#26032;&#39062;&#31867;&#21035;&#30340;&#29289;&#20307;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#38899;&#32032;&#24187;&#35273;&#22120;&#8221;&#30340;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#30701;&#26102;&#38388;&#30446;&#26631;&#35828;&#35805;&#20154;&#35821;&#38899;&#21363;&#21487;&#29983;&#25104;&#22810;&#26679;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#38899;&#32032;&#65292;&#24182;&#29992;&#20110;&#22522;&#20110;&#37051;&#23621;&#30340;&#35821;&#38899;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.06382</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#23454;&#29616;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#30340;&#38899;&#32032;&#24187;&#35273;&#22120;
&lt;/p&gt;
&lt;p&gt;
Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion. (arXiv:2308.06382v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#38899;&#32032;&#24187;&#35273;&#22120;&#8221;&#30340;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#30701;&#26102;&#38388;&#30446;&#26631;&#35828;&#35805;&#20154;&#35821;&#38899;&#21363;&#21487;&#29983;&#25104;&#22810;&#26679;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#38899;&#32032;&#65292;&#24182;&#29992;&#20110;&#22522;&#20110;&#37051;&#23621;&#30340;&#35821;&#38899;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#26088;&#22312;&#25913;&#21464;&#19968;&#20010;&#20154;&#30340;&#22768;&#38899;&#65292;&#20351;&#20854;&#21548;&#36215;&#26469;&#19982;&#21478;&#19968;&#20010;&#20154;&#30340;&#22768;&#38899;&#30456;&#20284;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20869;&#23481;&#21487;&#29702;&#35299;&#24615;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#20043;&#38388;&#23384;&#22312;&#22256;&#22659;&#65307;&#21363;&#20855;&#26377;&#26356;&#39640;&#21487;&#29702;&#35299;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#65292;&#32780;&#20855;&#26377;&#26356;&#39640;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#25968;&#25454;&#26469;&#23454;&#29616;&#39640;&#21487;&#29702;&#35299;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#38899;&#32032;&#24187;&#35273;&#22120;&#8221;&#65292;&#23427;&#20860;&#20855;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#38899;&#32032;&#24187;&#35273;&#22120;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65307;&#23427;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#20165;&#22522;&#20110;&#36739;&#30701;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#35821;&#38899;&#65288;&#20363;&#22914;3&#31186;&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#38899;&#32032;&#12290;&#28982;&#21518;&#21033;&#29992;&#29983;&#25104;&#30340;&#38899;&#32032;&#36827;&#34892;&#22522;&#20110;&#37051;&#23621;&#30340;&#35821;&#38899;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#19968;&#31181;&#26080;&#38656;&#25991;&#26412;&#27880;&#37322;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#25903;&#25345;&#36716;&#25442;&#21040;&#20219;&#20309;&#26410;&#30693;&#35828;&#35805;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#21152;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#21534;&#21520;&#37327;&#12290;&#35813;&#31574;&#30053;&#32500;&#25252;&#19968;&#20010;&#20010;&#20307;&#38431;&#21015;&#65292;&#24182;&#22312;&#36866;&#24403;&#25968;&#37327;&#30340;&#20010;&#20307;&#34987;&#35780;&#20272;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04102</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24322;&#27493;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Evolution of Deep Neural Network Architectures. (arXiv:2308.04102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#21152;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#21534;&#21520;&#37327;&#12290;&#35813;&#31574;&#30053;&#32500;&#25252;&#19968;&#20010;&#20010;&#20307;&#38431;&#21015;&#65292;&#24182;&#22312;&#36866;&#24403;&#25968;&#37327;&#30340;&#20010;&#20307;&#34987;&#35780;&#20272;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36827;&#21270;&#31639;&#27861;(EAs)&#21033;&#29992;&#20505;&#36873;&#35299;&#30340;&#24182;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#35780;&#20272;&#26102;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#35768;&#22810;&#24037;&#20316;&#33410;&#28857;(&#21363;&#35745;&#31639;&#23458;&#25143;&#31471;)&#22823;&#37096;&#20998;&#26102;&#38388;&#37117;&#22788;&#20110;&#38386;&#32622;&#29366;&#24577;&#65292;&#31561;&#24453;&#19979;&#19968;&#20195;&#30340;&#21019;&#24314;&#12290;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(ENAS)&#26159;&#19968;&#31867;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;EA&#65292;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;(AES)&#65292;&#28982;&#21518;&#23558;&#20854;&#36866;&#37197;&#21040;ENAS&#19978;&#12290;AES&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#22810;&#36798;$K$&#20010;&#20010;&#20307;&#30340;&#38431;&#21015;&#65292;&#36825;&#20123;&#20010;&#20307;&#24050;&#20934;&#22791;&#22909;&#34987;&#21457;&#36865;&#21040;&#24037;&#20316;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#30001;&#24037;&#20316;&#22120;&#35780;&#20272;&#20102;$M&lt;&lt;K$&#20010;&#20010;&#20307;&#20043;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#12290;&#21512;&#36866;&#30340;$M$&#20540;&#26159;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#30340;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;AES&#30340;&#26222;&#36866;&#24615;&#21644;&#33021;&#21147;&#65292;&#39318;&#20808;&#22312;11&#20301;&#22810;&#36335;&#22797;&#29992;&#22120;&#35774;&#35745;(&#19968;&#20010;&#21333;&#20010;&#31181;&#32676;&#21487;&#39564;&#35777;&#30340;&#21457;&#29616;&#20219;&#21153;)&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M&lt;&lt;K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery ta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23574;&#23792;Wishart&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#19968;&#31867;&#23376;&#31354;&#38388;&#24182;&#38598;&#27169;&#22411;&#25429;&#25417;&#20449;&#21495;&#32467;&#26500;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#32479;&#35745;&#21644;&#35745;&#31639;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#28982;&#30340;&#25237;&#24433;&#21151;&#29575;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#32479;&#35745;&#36817;&#20284;&#26368;&#20248;&#37051;&#22495;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20855;&#20307;&#26696;&#20363;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;&#35745;&#31639;&#38590;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22522;&#26412;&#31232;&#30095;PCA&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#22312;&#20854;&#32467;&#26500;&#21270;&#23545;&#24212;&#29289;&#20013;&#20063;&#21516;&#26679;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.13535</link><description>&lt;p&gt;
&#31639;&#27861;&#21644;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#38556;&#30861;&#26159;&#21542;&#36866;&#29992;&#20110;&#20854;&#20182;&#32467;&#26500;&#35774;&#32622;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do algorithms and barriers for sparse principal component analysis extend to other structured settings?. (arXiv:2307.13535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23574;&#23792;Wishart&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#19968;&#31867;&#23376;&#31354;&#38388;&#24182;&#38598;&#27169;&#22411;&#25429;&#25417;&#20449;&#21495;&#32467;&#26500;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#32479;&#35745;&#21644;&#35745;&#31639;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#28982;&#30340;&#25237;&#24433;&#21151;&#29575;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#32479;&#35745;&#36817;&#20284;&#26368;&#20248;&#37051;&#22495;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20855;&#20307;&#26696;&#20363;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;&#35745;&#31639;&#38590;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22522;&#26412;&#31232;&#30095;PCA&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#22312;&#20854;&#32467;&#26500;&#21270;&#23545;&#24212;&#29289;&#20013;&#20063;&#21516;&#26679;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23574;&#23792;Wishart&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#20854;&#20013;&#20449;&#21495;&#20013;&#30340;&#32467;&#26500;&#36890;&#36807;&#19968;&#31867;&#23376;&#31354;&#38388;&#24182;&#38598;&#27169;&#22411;&#26469;&#25429;&#25417;&#12290;&#36825;&#20010;&#36890;&#29992;&#31867;&#21035;&#21253;&#25324;&#22522;&#26412;&#31232;&#30095;PCA&#20197;&#21450;&#24102;&#26377;&#22270;&#31232;&#30095;&#24615;&#30340;&#21464;&#20307;&#12290;&#20026;&#20102;&#22312;&#32479;&#35745;&#21644;&#35745;&#31639;&#30340;&#32479;&#19968;&#35270;&#35282;&#19979;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#38382;&#39064;&#23454;&#20363;&#30340;&#20960;&#20309;&#26377;&#20851;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#28982;&#30340;&#25237;&#24433;&#21151;&#29575;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#32479;&#35745;&#36817;&#20284;&#26368;&#20248;&#37051;&#22495;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26222;&#36866;&#22522;&#30784;&#20013;&#36335;&#24452;&#31232;&#30095;&#24615;&#21644;&#26641;&#31232;&#30095;&#24615;&#30340;&#20004;&#31181;&#37325;&#35201;&#29305;&#27530;&#24773;&#20917;&#36827;&#34892;&#31471;&#21040;&#31471;&#20998;&#26512;&#65292;&#34917;&#20805;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#30456;&#21305;&#37197;&#30340;&#35745;&#31639;&#38590;&#24230;&#35777;&#25454;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22522;&#26412;&#31232;&#30095;PCA&#35266;&#23519;&#21040;&#30340;&#20960;&#20010;&#29616;&#35937;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#20854;&#32467;&#26500;&#21270;&#23545;&#24212;&#29289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#26059;&#36716;&#21644;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#65292;&#36890;&#36807;&#20248;&#21270;&#21943;&#21475;&#25968;&#37327;&#21644;&#20301;&#32622;&#65292;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#30340;&#20027;&#21160;&#25511;&#21046;&#65292;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#21644;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;</title><link>http://arxiv.org/abs/2307.12083</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#21943;&#21475;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#36827;&#34892;&#20027;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning. (arXiv:2307.12083v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#26059;&#36716;&#21644;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#65292;&#36890;&#36807;&#20248;&#21270;&#21943;&#21475;&#25968;&#37327;&#21644;&#20301;&#32622;&#65292;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#30340;&#20027;&#21160;&#25511;&#21046;&#65292;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#21644;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#30495;&#27491;&#23041;&#21147;&#20307;&#29616;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#20854;&#21160;&#24577;&#24615;&#36136;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35745;&#31639;&#21644;&#29289;&#29702;&#26041;&#38754;&#26356;&#20026;&#22797;&#26434;&#12290;&#26059;&#36716;&#21644;&#21943;&#23556;&#24050;&#34987;&#35777;&#23454;&#26159;&#20943;&#23569;&#38045;&#20307;&#25152;&#21463;&#38459;&#21147;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#20027;&#21160;&#27969;&#21160;&#25511;&#21046;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#23558;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#26469;&#28155;&#21152;&#26059;&#36716;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#20197;&#23454;&#29616;&#26368;&#22823;&#21487;&#33021;&#30340;&#38459;&#21147;&#25233;&#21046;&#12290;&#23558;&#20171;&#32461;DRL&#20195;&#30721;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#25511;&#21046;&#21442;&#25968;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#29992;&#20110;&#26059;&#36716;&#30340;DRL&#32593;&#32476;&#30340;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#20248;&#21270;&#21943;&#21475;&#30340;&#25968;&#37327;&#21644;&#20301;&#32622;&#12289;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#26059;&#36716;&#19982;DRL&#24037;&#20855;&#30456;&#32467;&#21512;&#26159;&#26377;&#24076;&#26395;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#65292;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection have been a proven way of active flow control to reduce the drag force exerted on blunt bodies. Rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, sensors location, and maximum allowed flow rate to jets in the form of maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation with the DRL tools is promising, since it suppresses the vortex shedding, stabilizes the Karman vortex stre
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.15546</link><description>&lt;p&gt;
&#24403;&#22522;&#30784;&#27169;&#22411;&#36935;&#21040;&#32852;&#37030;&#23398;&#20064;&#65306;&#21160;&#26426;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22312;AI&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#35299;&#20915;&#20102;AI&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;FL&#25193;&#23637;&#20102;FM&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#20849;&#20139;&#65292;&#20998;&#25955;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#36731;&#20102;FL&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#23427;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;FM&#21457;&#23637;&#65292;&#27665;&#20027;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#21253;&#23481;&#24615;&#21644;&#21019;&#26032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;FM&#20197;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;FL&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36215;&#28857;&#65292;&#20419;&#36827;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;FM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20016;&#23500;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20943;&#23569;&#36807;&#25311;&#21512;&#65292;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#30740;&#31350;FL&#21644;FM&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#21152;&#28145;&#23545;&#23427;&#20204;&#21327;&#21516;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#24378;&#35843;&#21160;&#26426;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14872</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#24179;&#34913;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#20960;&#20309;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits. (arXiv:2306.14872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#23454;&#35777;&#24615;&#33021;&#19982;&#24754;&#35266;&#29702;&#35770;&#21518;&#24724;&#30028;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#21253;&#25324;&#36138;&#24515;&#12289;OFUL&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#31639;&#27861;&#31867;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#22312;&#20445;&#30041;&#22522;&#26412;&#31639;&#27861;&#22823;&#37096;&#20998;&#20248;&#33391;&#29305;&#24615;&#30340;&#21516;&#26102;&#8220;&#26657;&#27491;&#8221;&#22522;&#26412;&#31639;&#27861;&#22312;&#26576;&#20123;&#23454;&#20363;&#20013;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#20248;&#21518;&#24724;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is motivated by recent developments in the linear bandit literature, which have revealed a discrepancy between the promising empirical performance of algorithms such as Thompson sampling and Greedy, when compared to their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometry of the uncertainty ellipsoid, enabling us to establish an instance-dependent frequentist regret bound for a broad class of algorithms, including Greedy, OFUL, and Thompson sampling. This result empowers us to identify and ``course-correct" instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$, while retaining most of the desirable properties of the base algorithms. We present sim
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#39044;&#27979;&#30340;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#27491;&#27493;&#39588;&#20197;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21040;&#21709;&#24212;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#30340;&#26041;&#27861;&#25104;&#21151;&#25511;&#21046;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#30830;&#21629;&#21517;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20854;&#23384;&#22312;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13746</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#20043;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Valid inference after prediction. (arXiv:2306.13746v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13746
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#39044;&#27979;&#30340;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#27491;&#27493;&#39588;&#20197;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21040;&#21709;&#24212;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#30340;&#26041;&#27861;&#25104;&#21151;&#25511;&#21046;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#30830;&#21629;&#21517;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20854;&#23384;&#22312;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#39044;&#27979;&#30340;&#25512;&#26029;&#65292;&#21363;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#21464;&#37327;&#65292;&#28982;&#21518;&#23545;&#35813;&#39044;&#27979;&#21709;&#24212;&#19982;&#26576;&#20123;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#23558;&#26631;&#20934;&#25512;&#26029;&#26041;&#27861;&#24212;&#29992;&#20110;&#35813;&#36807;&#31243;&#24182;&#19981;&#33021;&#20934;&#30830;&#37327;&#21270;&#26410;&#35266;&#27979;&#21040;&#65288;&#32780;&#38750;&#39044;&#27979;&#21040;&#65289;&#21709;&#24212;&#19982;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;Wang&#31561;&#20154;&#65288;2020&#65289;&#21644;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#20102;&#20462;&#27491;&#65288;ii&#65289;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21040;&#21709;&#24212;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25512;&#26029;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25511;&#21046;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#27491;&#30830;&#21629;&#21517;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#26080;&#35770;&#39044;&#20808;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#30340;&#36136;&#37327;&#22914;&#20309;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has focused on the very common practice of prediction-based inference: that is, (i) using a pre-trained machine learning model to predict an unobserved response variable, and then (ii) conducting inference on the association between that predicted response and some covariates. As pointed out by Wang et al. [2020], applying a standard inferential approach in (ii) does not accurately quantify the association between the unobserved (as opposed to the predicted) response and the covariates. In recent work, Wang et al. [2020] and Angelopoulos et al. [2023] propose corrections to step (ii) in order to enable valid inference on the association between the unobserved response and the covariates. Here, we show that the method proposed by Angelopoulos et al. [2023] successfully controls the type 1 error rate and provides confidence intervals with correct nominal coverage, regardless of the quality of the pre-trained machine learning model used to predict the unobserved response. Howe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00354</link><description>&lt;p&gt;
&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23427;&#22312;&#21516;&#26102;&#28085;&#30422;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#21435;&#22122;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#22320;&#65292;MTL&#26377;&#26102;&#20250;&#23548;&#33268;&#20247;&#25152;&#21608;&#30693;&#30340;$\textit{&#36127;&#36801;&#31227;}$&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#32780;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#25193;&#25955;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;$\textbf{(O1)}$ &#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#24046;&#36317;&#21152;&#22823;&#65292;&#21435;&#22122;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#20146;&#21644;&#21147;&#20943;&#24369;&#65292; $\textbf{(O2)}$ &#22312;&#25193;&#25955;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#36801;&#31227;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#36731;&#36127;&#36801;&#31227;&#26469;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12289;&#20855;&#20307;&#26159;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26469;&#40723;&#21169;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#24182;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#21435;&#22122;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.17282</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#21644;Nagata&#32500;&#24230;&#20013;k-NN&#35268;&#21017;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;(II)
&lt;/p&gt;
&lt;p&gt;
Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32487;&#32493;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#30740;&#31350;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#12290;&#30001;&#20110;C\'erou&#21644;Guyader(2006)&#20197;&#21450;Preiss(1983)&#30340;&#32467;&#26524;&#65292;&#24050;&#30693;&#35813;&#35268;&#21017;&#22312;&#27599;&#20010;Nagata&#24847;&#20041;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#24230;&#37327;&#31354;&#38388;X&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24179;&#23616;&#24773;&#20917;&#19979;&#27492;&#35268;&#21017;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;Devroye&#65292;Gy\"{o}rfi&#65292;Krzy\.{z}ak&#21644;Lugosi&#65288;1994&#65289;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#25171;&#30772;&#24179;&#23616;&#31574;&#30053;&#19979;&#65292;&#25105;&#20204;&#35774;&#27861;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#65288;&#21363;Nagata&#32500;&#24230;&#20026;&#38646;&#30340;&#31354;&#38388;&#65289;&#20013;&#23637;&#31034;&#20102;&#24378;&#26222;&#36941;&#19968;&#33268;&#24615;&#12290;&#32467;&#21512;C\'erou&#21644;Guyader&#30340;&#23450;&#29702;&#21644;Assouad&#21644;Quentin de Gromard (2006)&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#20986;$k$-NN&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;$k$-NN&#35268;&#21017;&#22312;Heisenberg&#32676;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#32780;&#35813;&#32676;&#24182;&#38750;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\ell_1$-TCL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#36801;&#31227;&#21644;Lasso&#22238;&#24402;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09126</link><description>&lt;p&gt;
&#30693;&#35782;&#36801;&#31227;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;: &#36716;&#31227;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer. (arXiv:2305.09126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\ell_1$-TCL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#36801;&#31227;&#21644;Lasso&#22238;&#24402;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#30456;&#21516;&#30340;&#21327;&#21464;&#37327;&#65288;&#25110;&#29305;&#24449;&#65289;&#31354;&#38388;&#35774;&#32622;&#19979;&#36890;&#36807;&#30693;&#35782;&#36801;&#31227;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#65292;&#21363;&#21516;&#31867;&#21035;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#65292;&#23558;&#20854;&#31216;&#20026;&#36716;&#31227;&#22240;&#26524;&#23398;&#20064;&#65288;TCL&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;$\ell_1$-TCL&#65292;&#20854;&#20013;&#21253;&#21547;$\ell_1$&#27491;&#21017;&#21270;TL&#26469;&#36827;&#34892;&#33510;&#20107;&#21442;&#25968;&#20272;&#35745;&#21644;&#19979;&#28216;&#25554;&#20214;ACE&#20272;&#35745;&#22120;&#65292;&#21253;&#25324;&#32467;&#26524;&#22238;&#24402;&#12289;&#36870;&#27010;&#29575;&#21152;&#26435;&#21644;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#20511;&#21161;&#20110;Lasso&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#24674;&#22797;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel problem of improving causal effect estimation accuracy with the help of knowledge transfer under the same covariate (or feature) space setting, i.e., homogeneous transfer learning (TL), is studied, referred to as the Transfer Causal Learning (TCL) problem. While most recent efforts in adapting TL techniques to estimate average causal effect (ACE) have been focused on the heterogeneous covariate space setting, those methods are inadequate for tackling the TCL problem since their algorithm designs are based on the decomposition into shared and domain-specific covariate spaces. To address this issue, we propose a generic framework called \texttt{$\ell_1$-TCL}, which incorporates $\ell_1$ regularized TL for nuisance parameter estimation and downstream plug-in ACE estimators, including outcome regression, inverse probability weighted, and doubly robust estimators. Most importantly, with the help of Lasso for high-dimensional regression, we establish non-asymptotic recovery guarantee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;&#65292;DeepStock&#36890;&#36807;&#26597;&#30475;&#32929;&#31080;&#20215;&#26684;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Resnet&#21644;logits&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#26410;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#24066;&#22330;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#21033;&#28070;&#12290;</title><link>http://arxiv.org/abs/2304.14870</link><description>&lt;p&gt;
Deep Stock: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Deep Stock: training and trading scheme using deep learning. (arXiv:2304.14870v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;&#65292;DeepStock&#36890;&#36807;&#26597;&#30475;&#32929;&#31080;&#20215;&#26684;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Resnet&#21644;logits&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#26410;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#24066;&#22330;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#25928;&#24066;&#22330;&#20551;&#35828;&#23384;&#22312;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#32929;&#31080;&#24066;&#22330;&#23384;&#22312;&#22833;&#28789;&#29616;&#35937;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#19968;&#20123;&#33021;&#22815;&#33719;&#24471;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#25216;&#26415;&#65292;&#21363;alpha&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#31995;&#32479;&#24615;&#20132;&#26131;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#20998;&#26512;&#21644;&#39044;&#27979;&#24066;&#22330;&#34892;&#20026;&#30340;&#24378;&#22823;&#24037;&#20855;&#24050;&#32463;&#24320;&#22987;&#23853;&#38706;&#22836;&#35282;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#19987;&#19994;&#20132;&#26131;&#21592;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26597;&#30475;&#20808;&#21069;&#30340;600&#22825;&#30340;&#32929;&#31080;&#20215;&#26684;&#65292;&#24182;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#25509;&#19979;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;DeepStock&#65292;&#20351;&#29992;Resnet&#30340;&#36339;&#36291;&#36830;&#25509;&#21644;logits&#26469;&#22686;&#21152;&#27169;&#22411;&#22312;&#20132;&#26131;&#26041;&#26696;&#20013;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#32929;&#31080;&#24066;&#22330;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38889;&#22269;&#24066;&#22330;&#19978;&#33719;&#24471;&#20102;N&#65285;&#30340;&#21033;&#28070;&#65292;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;M&#65285;&#65292;&#24182;&#22312;&#32654;&#22269;&#24066;&#22330;&#19978;&#33719;&#24471;&#20102;A&#65285;&#30340;&#21033;&#28070;&#65292;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;B&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the efficient market hypothesis, many studies suggest the existence of inefficiencies in the stock market, leading to the development of techniques to gain above-market returns, known as alpha. Systematic trading has undergone significant advances in recent decades, with deep learning emerging as a powerful tool for analyzing and predicting market behavior. In this paper, we propose a model inspired by professional traders that look at stock prices of the previous 600 days and predicts whether the stock price rises or falls by a certain percentage within the next D days. Our model, called DeepStock, uses Resnet's skip connections and logits to increase the probability of a model in a trading scheme. We test our model on both the Korean and US stock markets and achieve a profit of N\% on Korea market, which is M\% above the market return, and profit of A\% on US market, which is B\% above the market return.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;Hopfield&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#34429;&#28982;&#19968;&#20123;&#26465;&#20214;&#23545;&#20110;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#36825;&#31181;&#23398;&#20064;&#27169;&#24335;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.13710</link><description>&lt;p&gt;
&#24102;&#31181;&#26893;&#27169;&#24335;&#30340;Hopfield&#27169;&#22411;&#65306;&#19968;&#31181;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hopfield model with planted patterns: a teacher-student self-supervised learning model. (arXiv:2304.13710v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;Hopfield&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#34429;&#28982;&#19968;&#20123;&#26465;&#20214;&#23545;&#20110;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#36825;&#31181;&#23398;&#20064;&#27169;&#24335;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Hopfield&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#35760;&#24518;&#23384;&#20648;&#21644;&#26816;&#32034;&#30340;&#20856;&#22411;&#27169;&#22411;&#65292;&#20294;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#21270;&#27169;&#24335;&#30340;Hopfield&#27169;&#22411;&#30340;&#36866;&#24403;&#25512;&#24191;&#26469;&#26500;&#24314;Boltzmann&#26426;&#30340;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#33258;&#26059;&#21464;&#37327;&#26159;&#26426;&#22120;&#26435;&#37325;&#65292;&#27169;&#24335;&#23545;&#24212;&#20110;&#35757;&#32451;&#38598;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#22270;&#26469;&#20998;&#26512;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#20123;&#30456;&#22270;&#26159;&#36890;&#36807;&#35757;&#32451;&#38598;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22122;&#22768;&#21644;&#25512;&#26029;&#28201;&#24230;&#65288;&#21363;&#26435;&#37325;&#27491;&#21017;&#21270;&#65289;&#26469;&#26500;&#24314;&#30340;&#12290;&#20351;&#29992;&#23567;&#32780;&#23500;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#26426;&#22120;&#21487;&#20197;&#36890;&#36807;&#35760;&#24518;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#65292;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#31034;&#20363;&#25968;&#20197;&#36229;&#36807;&#20020;&#30028;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#65292;&#31995;&#32479;&#30340;&#23384;&#20648;&#38480;&#21046;&#25104;&#20026;&#20135;&#29983;&#19968;&#31181;&#23398;&#20064;&#27169;&#24335;&#30340;&#26426;&#20250;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#31995;&#32479;&#21487;&#20197;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Hopfield networks are known as paradigmatic models for memory storage and retrieval, modern artificial intelligence systems mainly stand on the machine learning paradigm. We show that it is possible to formulate a teacher-student self-supervised learning problem with Boltzmann machines in terms of a suitable generalization of the Hopfield model with structured patterns, where the spin variables are the machine weights and patterns correspond to the training set's examples. We analyze the learning performance by studying the phase diagram in terms of the training set size, the dataset noise and the inference temperature (i.e. the weight regularization). With a small but informative dataset the machine can learn by memorization. With a noisy dataset, an extensive number of examples above a critical threshold is needed. In this regime the memory storage limits of the system becomes an opportunity for the occurrence of a learning regime in which the system can generalize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#21442;&#25968;&#21270;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22266;&#23450;&#20808;&#39564;&#20998;&#24067;&#32570;&#20047;&#20449;&#24687;&#21644;&#20248;&#21270;&#26368;&#20339;&#20998;&#24067;&#26114;&#36149;&#19981;&#31283;&#23450;&#30340;&#23616;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.13586</link><description>&lt;p&gt;
&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Sliced Wasserstein Distance. (arXiv:2304.13586v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#21442;&#25968;&#21270;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22266;&#23450;&#20808;&#39564;&#20998;&#24067;&#32570;&#20047;&#20449;&#24687;&#21644;&#20248;&#21270;&#26368;&#20339;&#20998;&#24067;&#26114;&#36149;&#19981;&#31283;&#23450;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#19968;&#31181;&#32479;&#35745;&#26377;&#25928;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#24230;&#37327;&#12290;SW&#36317;&#31163;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#20999;&#29255;&#20998;&#24067;&#12290;&#30446;&#21069;&#26377;&#20004;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#36825;&#20010;&#20998;&#24067;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#31532;&#20108;&#31181;&#26159;&#20248;&#21270;&#24402;&#23646;&#20110;&#21442;&#25968;&#20998;&#24067;&#26063;&#30340;&#26368;&#20339;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#22266;&#23450;&#30340;&#20808;&#39564;&#20998;&#24067;&#22312;&#31361;&#20986;&#33021;&#22815;&#21306;&#20998;&#20004;&#20010;&#24120;&#35268;&#27010;&#29575;&#27979;&#24230;&#30340;&#25237;&#24433;&#26041;&#21521;&#26041;&#38754;&#32570;&#20047;&#20449;&#24687;&#12290;&#32780;&#20248;&#21270;&#26368;&#20339;&#20998;&#24067;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#19981;&#31283;&#23450;&#30340;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20505;&#36873;&#20998;&#24067;&#30340;&#21442;&#25968;&#20998;&#24067;&#26063;&#21487;&#33021;&#20250;&#24456;&#23481;&#26131;&#34987;&#38169;&#35823;&#25351;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20999;&#29255;&#20998;&#24067;&#35774;&#35745;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#21442;&#25968;&#21270;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#21152;&#36890;&#29992;&#32780;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.08842</link><description>&lt;p&gt;
UDTIRI:&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30475;&#21040;&#22312;&#22478;&#24066;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#20013;&#21033;&#29992;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#36947;&#36335;&#26816;&#26597;&#39046;&#22495;&#65292;&#30446;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Urban Digital Twins Intelligent Road Inspection (UDTIRI)&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#40784;&#20840;&#30340;&#36947;&#36335;&#22353;&#27934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#35753;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#35753;&#31639;&#27861;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#22330;&#26223;&#24182;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#29031;&#21644;&#28287;&#24230;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23545;UDTIRI&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.03365</link><description>&lt;p&gt;
&#22870;&#21169;&#36716;&#31227;&#30340;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20915;&#31574;&#37325;&#28857;&#65288;Decision-focused&#65292;DF&#65289;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#34987;&#20171;&#32461;&#20026;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#19987;&#27880;&#20110;&#23398;&#20064;&#26368;&#26377;&#21033;&#20110;&#33719;&#24471;&#39640;&#25253;&#37228;&#30340;MDP&#21160;&#24577;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#30452;&#25509;&#20248;&#21270;&#25253;&#37228;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#20294;&#20174;MLE&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;DF&#35299;&#30340;&#38750;&#35782;&#21035;&#24615;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29609;&#20855;&#31034;&#20363;&#21644;&#21307;&#30103;&#27169;&#25311;&#22120;&#19978;&#23637;&#31034;&#20102;RDF&#26174;&#30528;&#22686;&#21152;&#20102;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16212</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65306;&#39640;&#25928;&#24555;&#36895;
&lt;/p&gt;
&lt;p&gt;
An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect. (arXiv:2303.16212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#21487;&#20197;&#24179;&#34913;&#32593;&#32476;&#30340;&#21098;&#26525;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;&#31181;&#32676;&#30340;&#29305;&#24615;&#65292;&#23427;&#32463;&#24120;&#21463;&#21040;&#22797;&#26434;&#30340;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#21644;&#39640;&#24230;&#36164;&#28304;&#28040;&#32791;&#30340;&#21098;&#26525;&#32467;&#26500;&#39564;&#35777;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65288;EMO-PMS&#65289;&#65292;&#20197;&#20943;&#23569;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#36825;&#31181;&#20998;&#35299;&#20943;&#23569;&#20102;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#24182;&#38477;&#20302;&#20102;&#20248;&#21270;&#38590;&#24230;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36739;&#23567;&#30340;&#32593;&#32476;&#32467;&#26500;&#25910;&#25947;&#26356;&#24555;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The network pruning algorithm based on evolutionary multi-objective (EMO) can balance the pruning rate and performance of the network. However, its population-based nature often suffers from the complex pruning optimization space and the highly resource-consuming pruning structure verification process, which limits its application. To this end, this paper proposes an EMO joint pruning with multiple sub-networks (EMO-PMS) to reduce space complexity and resource consumption. First, a divide-and-conquer EMO network pruning framework is proposed, which decomposes the complex EMO pruning task on the whole network into easier sub-tasks on multiple sub-networks. On the one hand, this decomposition reduces the pruning optimization space and decreases the optimization difficulty; on the other hand, the smaller network structure converges faster, so the computational resource consumption of the proposed algorithm is lower. Secondly, a sub-network training method based on cross-network constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#33719;&#21462;&#20840;&#38754;&#23545;&#31216;&#30340;&#28857;&#31890;&#23376;&#32676;&#20307;&#65288;&#22914;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26377;&#38480;&#23376;&#38598;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.14770</link><description>&lt;p&gt;
&#21407;&#23376;&#32467;&#26500;&#34920;&#24449;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
Completeness of Atomic Structure Representations. (arXiv:2302.14770v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33719;&#21462;&#20840;&#38754;&#23545;&#31216;&#30340;&#28857;&#31890;&#23376;&#32676;&#20307;&#65288;&#22914;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26377;&#38480;&#23376;&#38598;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33719;&#21462;&#20840;&#38754;&#23545;&#31216;&#30340;&#28857;&#31890;&#23376;&#32676;&#20307;&#65288;&#22914;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#65289;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#29289;&#29702;&#23398;&#21644;&#29702;&#35770;&#21270;&#23398;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#31185;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#25903;&#25745;&#20102;&#27169;&#22411;&#20934;&#30830;&#22797;&#29616;&#29289;&#29702;&#20851;&#31995;&#24182;&#19982;&#22522;&#26412;&#23545;&#31216;&#24615;&#21644;&#23432;&#24658;&#23450;&#24459;&#19968;&#33268;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#29992;&#20110;&#34920;&#31034;&#28857;&#20113;&#30340;&#25551;&#36848;&#31526;&#65288;&#23588;&#20854;&#26159;&#29992;&#20110;&#25551;&#36848;&#21407;&#23376;&#23610;&#24230;&#19978;&#29289;&#36136;&#30340;&#25551;&#36848;&#31526;&#65289;&#26080;&#27861;&#21306;&#20998;&#29305;&#27530;&#30340;&#31890;&#23376;&#25490;&#21015;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#23398;&#20064;&#23427;&#20204;&#30340;&#23646;&#24615;&#12290;&#23384;&#22312;&#21487;&#35777;&#26126;&#23436;&#22791;&#24615;&#30340;&#26694;&#26550;&#65292;&#20294;&#20165;&#22312;&#21516;&#26102;&#25551;&#36848;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#25165;&#26159;&#22914;&#27492;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26377;&#38480;&#23376;&#38598;&#25551;&#36848;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenge of obtaining a comprehensive and symmetric representation of point particle groups, such as atoms in a molecule, which is crucial in physics and theoretical chemistry. The problem has become even more important with the widespread adoption of machine-learning techniques in science, as it underpins the capacity of models to accurately reproduce physical relationships while being consistent with fundamental symmetries and conservation laws. However, the descriptors that are commonly used to represent point clouds -- most notably those adopted to describe matter at the atomic scale -- are unable to distinguish between special arrangements of particles. This makes it impossible to machine learn their properties. Frameworks that are provably complete exist but are only so in the limit in which they simultaneously describe the mutual relationship between all atoms, which is impractical. We present a novel approach to construct descriptors of finite cor
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.04062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04062
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#22810;&#31181;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#20302;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27424;&#25311;&#21512;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#30417;&#31649;&#38382;&#39064;&#38590;&#20197;&#35775;&#38382;&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20197;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26080;&#27861;&#20570;&#21040;&#30340;&#26041;&#24335;&#36827;&#34892;&#20849;&#20139;&#21644;&#20351;&#29992;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24037;&#20316;&#65306;&#65288;i&#65289;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65307;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65307;&#65288;iii&#65289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65288;RDT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#26029;&#35266;&#23519;&#29289;&#32852;&#32593;&#23454;&#20307;&#30340;&#30495;&#23454;&#23545;&#24212;&#29289;&#26469;&#33258;&#21160;&#29983;&#25104;&#36890;&#29992;&#22411;&#25968;&#23383;&#23402;&#29983;&#65292;&#24182;&#35843;&#25972;&#20854;&#34892;&#20026;&#27169;&#22411;&#12290;&#26694;&#26550;&#21033;&#29992;&#29289;&#32852;&#32593;&#20043;&#29289;&#65288;WoT&#65289;&#25552;&#20379;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2301.07390</link><description>&lt;p&gt;
&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65306;&#23558;&#29289;&#32852;&#32593;&#24341;&#20837;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Relativistic Digital Twin: Bringing the IoT to the Future. (arXiv:2301.07390v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65288;RDT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#26029;&#35266;&#23519;&#29289;&#32852;&#32593;&#23454;&#20307;&#30340;&#30495;&#23454;&#23545;&#24212;&#29289;&#26469;&#33258;&#21160;&#29983;&#25104;&#36890;&#29992;&#22411;&#25968;&#23383;&#23402;&#29983;&#65292;&#24182;&#35843;&#25972;&#20854;&#34892;&#20026;&#27169;&#22411;&#12290;&#26694;&#26550;&#21033;&#29992;&#29289;&#32852;&#32593;&#20043;&#29289;&#65288;WoT&#65289;&#25552;&#20379;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#20854;&#29289;&#29702;&#36164;&#20135;&#30340;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26469;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#21644;&#27169;&#25311;&#8220;&#20551;&#35774;&#8221;&#24773;&#26223;&#12290;&#25968;&#23383;&#23402;&#29983;&#33021;&#22815;&#22797;&#21046;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36866;&#24212;&#20854;&#34892;&#20026;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#20013;&#30340;&#25968;&#23383;&#23402;&#29983;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#29992;&#20363;&#65292;&#26080;&#27861;&#26080;&#32541;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#22659;&#12290;&#27492;&#22806;&#65292;&#29289;&#32852;&#32593;&#30340;&#30862;&#29255;&#21270;&#20351;&#24471;&#22312;&#20855;&#26377;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#29289;&#32852;&#32593;&#32593;&#32476;&#21327;&#35758;&#30340;&#24322;&#26500;&#24773;&#26223;&#20013;&#37096;&#32626;&#25968;&#23383;&#23402;&#29983;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65288;RDT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25105;&#20204;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#29289;&#32852;&#32593;&#23454;&#20307;&#30340;&#36890;&#29992;&#22411;&#25968;&#23383;&#23402;&#29983;&#65292;&#24182;&#36890;&#36807;&#19981;&#26029;&#35266;&#23519;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#26469;&#35843;&#25972;&#20854;&#34892;&#20026;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;&#36890;&#36807;&#29289;&#32852;&#32593;&#20043;&#29289;&#65288;WoT&#65289;&#26469;&#36827;&#34892;&#23545;&#35937;&#34920;&#31034;&#65292;&#20026;&#27599;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#21450;&#20854;&#25968;&#23383;&#23402;&#29983;&#25552;&#20379;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex IoT ecosystems often require the usage of Digital Twins (DTs) of their physical assets in order to perform predictive analytics and simulate what-if scenarios. DTs are able to replicate IoT devices and adapt over time to their behavioral changes. However, DTs in IoT are typically tailored to a specific use case, without the possibility to seamlessly adapt to different scenarios. Further, the fragmentation of IoT poses additional challenges on how to deploy DTs in heterogeneous scenarios characterized by the usage of multiple data formats and IoT network protocols. In this paper, we propose the Relativistic Digital Twin (RDT) framework, through which we automatically generate general-purpose DTs of IoT entities and tune their behavioral models over time by constantly observing their real counterparts. The framework relies on the object representation via the Web of Things (WoT), to offer a standardized interface to each of the IoT devices as well as to their DTs. To this purpose
&lt;/p&gt;</description></item><item><title>&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#65288;MSW&#65289;&#36317;&#31163;&#26159;&#19968;&#31181;&#26032;&#30340;SW&#36317;&#31163;&#23478;&#26063;&#65292;&#36890;&#36807;&#22312;&#25237;&#24433;&#26041;&#21521;&#19978;&#26045;&#21152;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20013;&#29420;&#31435;&#25237;&#24433;&#23548;&#33268;&#30340;&#20887;&#20313;&#25237;&#24433;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#65288;found in translation&#65289;</title><link>http://arxiv.org/abs/2301.03749</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65306;&#36229;&#36234;&#29420;&#31435;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Markovian Sliced Wasserstein Distances: Beyond Independent Projections. (arXiv:2301.03749v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03749
&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#65288;MSW&#65289;&#36317;&#31163;&#26159;&#19968;&#31181;&#26032;&#30340;SW&#36317;&#31163;&#23478;&#26063;&#65292;&#36890;&#36807;&#22312;&#25237;&#24433;&#26041;&#21521;&#19978;&#26045;&#21152;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20013;&#29420;&#31435;&#25237;&#24433;&#23548;&#33268;&#30340;&#20887;&#20313;&#25237;&#24433;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#65288;found in translation&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#30001;&#20110;&#29420;&#31435;&#30340;&#22343;&#21248;&#38543;&#26426;&#25237;&#24433;&#26041;&#21521;&#32780;&#23548;&#33268;&#20887;&#20313;&#25237;&#24433;&#12290;&#20026;&#20102;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#22823;K&#20999;&#29255;Wasserstein&#65288;Max-K-SW&#65289;&#36317;&#31163;&#65288;$K\geq1$&#65289;&#23547;&#27714;&#26368;&#20339;&#30340;&#21306;&#20998;&#27491;&#20132;&#25237;&#24433;&#26041;&#21521;&#12290;&#23613;&#31649;&#33021;&#22815;&#20943;&#23569;&#25237;&#24433;&#25968;&#37327;&#65292;&#20294;Max-K-SW&#30340;&#24230;&#37327;&#24615;&#22312;&#23454;&#36341;&#20013;&#19981;&#33021;&#20445;&#35777;&#65292;&#21407;&#22240;&#26159;&#20248;&#21270;&#30340;&#38750;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#27491;&#20132;&#32422;&#26463;&#20063;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21487;&#33021;&#19981;&#22826;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;SW&#36317;&#31163;&#23478;&#26063;&#65292;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#65288;MSW&#65289;&#36317;&#31163;&#65292;&#23427;&#22312;&#25237;&#24433;&#26041;&#21521;&#19978;&#26045;&#21152;&#20102;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#25351;&#23450;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65292;&#21253;&#25324;&#20808;&#39564;&#20998;&#24067;&#12289;&#36716;&#31227;&#20998;&#24067;&#20197;&#21450;&#29123;&#28903;&#21644;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#35752;&#35770;&#20102;MSW&#30340;&#21508;&#31181;&#25104;&#21592;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;MSW&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#25299;&#25169;&#24615;&#36136;&#65288;found in translation&#65289;
&lt;/p&gt;
&lt;p&gt;
Sliced Wasserstein (SW) distance suffers from redundant projections due to independent uniform random projecting directions. To partially overcome the issue, max K sliced Wasserstein (Max-K-SW) distance ($K\geq 1$), seeks the best discriminative orthogonal projecting directions. Despite being able to reduce the number of projections, the metricity of Max-K-SW cannot be guaranteed in practice due to the non-optimality of the optimization. Moreover, the orthogonality constraint is also computationally expensive and might not be effective. To address the problem, we introduce a new family of SW distances, named Markovian sliced Wasserstein (MSW) distance, which imposes a first-order Markov structure on projecting directions. We discuss various members of MSW by specifying the Markov structure including the prior distribution, the transition distribution, and the burning and thinning technique. Moreover, we investigate the theoretical properties of MSW including topological properties (met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;(DPDMs)&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;DP-SGD&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2210.09929</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Diffusion Models. (arXiv:2210.09929v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;(DPDMs)&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;DP-SGD&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#22312;&#28041;&#21450;&#38544;&#31169;&#30340;&#39046;&#22495;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#32469;&#36807;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20379;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;&#26412;&#25991;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;(DPDMs)&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(DP-SGD)&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;DPDM&#20013;&#30340;&#21442;&#25968;&#21270;&#21644;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22122;&#22768;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;DM&#35757;&#32451;&#30340;&#24378;&#22823;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26032;&#39062;DPDMs&#65292;&#24182;&#22312;&#25152;&#26377;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;DPDM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#34920;&#29616;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;DP-SGD&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#24403;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#34987;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06950</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#25442;&#32534;&#30721;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20449;&#24687;&#29109;&#32534;&#30721;&#65292;&#28982;&#21518;&#20877;&#26144;&#23556;&#22238;&#25968;&#25454;&#31354;&#38388;&#36827;&#34892;&#37325;&#26500;&#12290;&#19982;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#31070;&#32463;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#20869;&#23481;&#8221;&#28508;&#21464;&#37327;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20250;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#21464;&#37327;&#23384;&#20648;&#22270;&#20687;&#20449;&#24687;&#12290;&#20915;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#21097;&#20313;&#8220;&#32441;&#29702;&#8221;&#21464;&#37327;&#20250;&#22312;&#35299;&#30721;&#26102;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#24230;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#28041;&#21450;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.09894</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#20154;&#20063;&#33021;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#24515;&#21270;&#21098;&#35009;&#30340;&#34928;&#33853;
&lt;/p&gt;
&lt;p&gt;
Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26694;&#26550;&#30001;&#20110;&#22312;&#24191;&#27867;&#30340;&#21327;&#20316;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#36215;&#20102;&#26576;&#20123;&#23433;&#20840;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#39118;&#38505;&#26159;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#24694;&#24847;&#23458;&#25143;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;FL &#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#26159;&#28040;&#38500; Byzantine attacks &#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#30830;&#20445;&#26368;&#32456;&#27169;&#22411;&#26159;&#21487;&#20449;&#30340;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;/&#26356;&#26032;&#20043;&#38388;&#30340;&#26041;&#24046;&#36234;&#22823;&#65292;&#38544;&#34255; Byzantine attacks &#30340;&#31354;&#38388;&#23601;&#36234;&#22823;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#26041;&#24046;&#65292;&#21487;&#20197;&#21066;&#24369;&#24050;&#30693; Byzantine attacks &#30340;&#21147;&#37327;&#12290;&#20013;&#24515;&#21270;&#21098;&#35009; (CC) &#26694;&#26550;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19978;&#19968;&#27425;&#30340;&#21160;&#37327;&#39033;&#38500;&#20102;&#20943;&#23569;&#26041;&#24046;&#22806;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21442;&#32771;&#28857;&#26356;&#22909;&#22320;&#28040;&#38500; Byzantine attacks&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#30340;&#24694;&#24847;&#20195;&#29702;&#26377;&#19981;&#21516;&#30446;&#26631;&#26102; CC &#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21098;&#35009;&#31639;&#27861;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC)&#65292;&#20197;&#20811;&#26381;&#36825;&#31181;&#33030;&#24369;&#24615;&#12290;MRPC &#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26469;&#28040;&#38500;&#19987;&#38376;&#35774;&#35745;&#20197;&#32469;&#36807; CC &#26041;&#27861;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks, and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients' models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that, the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#23884;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.14653</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#31995;&#21015;&#20013;&#30340;&#38598;&#25104;&#39044;&#27979;&#65306;&#20185;&#22659;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland. (arXiv:2207.14653v2 [math-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#23884;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28023;&#27915;&#25110;&#22823;&#27668;&#27969;&#31561;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#38598;&#21512;&#20272;&#35745;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#23558;&#35813;&#21160;&#21147;&#31995;&#32479;&#23884;&#20837;&#30001;&#21160;&#21147;&#39537;&#21160;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#20013;&#12290;&#36825;&#20010;&#23478;&#26063;&#22240;&#20854;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#32780;&#34987;&#21629;&#21517;&#20026;&#20185;&#22659;&#12290;&#22312;&#20185;&#22659;&#20013;&#65292;Koopman&#21644;Perron-Frobenius&#31639;&#23376;&#26159;&#37193;&#30340;&#21644;&#19968;&#33268;&#36830;&#32493;&#30340;&#12290;&#36825;&#20010;&#23646;&#24615;&#20445;&#35777;&#23427;&#20204;&#21487;&#20197;&#29992;&#23545;&#35282;&#21270;&#26377;&#30028;&#26080;&#31351;&#23567;&#29983;&#25104;&#22120;&#30340;&#25351;&#25968;&#32423;&#32423;&#25968;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#30452;&#25509;&#33719;&#24471;&#23545;Lyapunov&#25351;&#25968;&#21644;&#20999;&#32447;&#32447;&#24615;&#21160;&#24577;&#30340;&#31934;&#30830;&#38598;&#21512;&#24335;&#34920;&#36798;&#24335;&#12290;&#20185;&#22659;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#20986;&#26497;&#20854;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#36712;&#36857;&#26679;&#26412;&#30340;&#24658;&#23450;&#26102;&#38388;&#32447;&#24615;&#32452;&#21512;&#26469;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;&#36825;&#31181;&#20196;&#20154;&#23604;&#23596;&#30340;&#31616;&#21333;&#31574;&#30053;&#24471;&#20197;&#23454;&#29616;&#65292;&#26159;&#36890;&#36807;Hilbert&#31354;&#38388;&#35774;&#32622;&#20013;&#20999;&#32447;&#32447;&#24615;&#21160;&#21147;&#30340;&#23436;&#20840;&#21512;&#29702;&#30340;&#21472;&#21152;&#21407;&#29702;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A methodological framework for ensemble-based estimation and simulation of high dimensional dynamical systems such as the oceanic or atmospheric flows is proposed. To that end, the dynamical system is embedded in a family of reproducing kernel Hilbert spaces with kernel functions driven by the dynamics. This family is nicknamed Wonderland for its appealing properties. In Wonderland the Koopman and Perron-Frobenius operators are unitary and uniformly continuous. This property warrants they can be expressed in exponential series of diagonalizable bounded infinitesimal generators. Access to Lyapunov exponents and to exact ensemble based expressions of the tangent linear dynamics are directly available as well. Wonderland enables us the devise of strikingly simple ensemble data assimilation methods for trajectory reconstructions in terms of constant-in-time linear combinations of trajectory samples. Such an embarrassingly simple strategy is made possible through a fully justified superposi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-SDE&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#24930;-&#24555;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26377;&#25928;&#38477;&#32500;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#31163;&#25955;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#31995;&#32479;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.04151</link><description>&lt;p&gt;
Auto-SDE:&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#38477;&#32500;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems. (arXiv:2205.04151v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-SDE&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#24930;-&#24555;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26377;&#25928;&#38477;&#32500;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#31163;&#25955;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#31995;&#32479;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25551;&#32472;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#29616;&#35937;&#65292;&#22810;&#23610;&#24230;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#21644;&#24037;&#31243;&#38382;&#39064;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#24930;-&#24555;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26377;&#25928;&#38477;&#32500;&#21160;&#21147;&#23398;&#12290;&#32473;&#23450;&#28385;&#36275;&#26576;&#20123;&#26410;&#30693;&#24930;-&#24555;&#38543;&#26426;&#31995;&#32479;&#30340;&#30701;&#26399;&#35266;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#25324;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;Auto-SDE&#30340;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;&#19981;&#21464;&#30340;&#24930;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#19968;&#31995;&#21015;&#26102;&#38388;&#30456;&#20851;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#25439;&#22833;&#20989;&#25968;&#36890;&#36807;&#31163;&#25955;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26500;&#36896;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19979;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale stochastic dynamical systems have been widely adopted to scientific and engineering problems due to their capability of depicting complex phenomena in many real world applications. This work is devoted to investigating the effective reduced dynamics for a slow-fast stochastic dynamical system. Given observation data on a short-term period satisfying some unknown slow-fast stochastic system, we propose a novel algorithm including a neural network called Auto-SDE to learn invariant slow manifold. Our approach captures the evolutionary nature of a series of time-dependent autoencoder neural networks with the loss constructed from a discretized stochastic differential equation. Our algorithm is also proved to be accurate, stable and effective through numerical experiments under various evaluation metrics.
&lt;/p&gt;</description></item></channel></rss>