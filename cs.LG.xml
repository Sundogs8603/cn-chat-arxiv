<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#26032;&#30340;&#39640;&#24230;&#21487;&#20998;&#24067;&#21644;&#33258;&#21160;&#21487;&#24494;&#20998;&#30340;&#26041;&#21521;&#23567;&#27874;&#21464;&#25442;&#65292;&#22312;&#29699;&#38754;&#21644;&#29699;&#19978;&#30340;&#20449;&#21495;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01282</link><description>&lt;p&gt;
&#22312;&#29699;&#38754;&#21644;&#29699;&#19978;&#30340;&#21487;&#24494;&#20998;&#21644;&#21152;&#36895;&#23567;&#27874;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Differentiable and accelerated wavelet transforms on the sphere and ball
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#26032;&#30340;&#39640;&#24230;&#21487;&#20998;&#24067;&#21644;&#33258;&#21160;&#21487;&#24494;&#20998;&#30340;&#26041;&#21521;&#23567;&#27874;&#21464;&#25442;&#65292;&#22312;&#29699;&#38754;&#21644;&#29699;&#19978;&#30340;&#20449;&#21495;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#21521;&#23567;&#27874;&#23383;&#20856;&#26159;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#20998;&#21106;&#21508;&#31181;&#23610;&#24230;&#12289;&#20301;&#32622;&#21644;&#26041;&#21521;&#19978;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#23545;&#20110;&#29289;&#29702;&#20449;&#21495;&#20855;&#26377;&#29305;&#27530;&#30340;&#20146;&#21644;&#24615;&#65292;&#22240;&#20026;&#29289;&#29702;&#20449;&#21495;&#36890;&#24120;&#20855;&#26377;&#39640;&#21508;&#21521;&#24322;&#24615;&#12289;&#23616;&#37096;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#35768;&#22810;&#37325;&#35201;&#30340;&#29289;&#29702;&#20449;&#21495;&#22312;&#29699;&#24418;&#22495;&#19978;&#35266;&#27979;&#65292;&#20363;&#22914;&#23431;&#23449;&#23398;&#20013;&#30340;&#22825;&#31354;&#12290;&#20511;&#21161;&#26368;&#36817;&#22312;&#35745;&#31639;&#35856;&#27874;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#39640;&#24230;&#21487;&#20998;&#24067;&#21644;&#33258;&#21160;&#21487;&#24494;&#20998;&#30340;&#26041;&#21521;&#23567;&#27874;&#21464;&#25442;&#65292;&#24212;&#29992;&#20110;&#20108;&#32500;&#29699;&#38754;S^2&#21644;&#19977;&#32500;&#29699;&#20307;B^3=R^+ x S^2&#65288;&#36890;&#36807;&#23558;&#29699;&#38754;&#19982;&#24452;&#21521;&#21322;&#32447;&#32467;&#21512;&#32780;&#24418;&#25104;&#30340;&#31354;&#38388;&#65289;&#12290;&#19982;&#29616;&#26377;&#36719;&#20214;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29699;&#38754;&#21644;&#29699;&#19978;&#20449;&#21495;&#30340;&#21152;&#36895;&#27604;&#20998;&#21035;&#39640;&#36798;300&#20493;&#21644;21800&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;64&#20301;&#26426;&#22120;&#31934;&#24230;&#12290;&#36825;&#20123;&#31639;&#27861;&#19981;&#20165;&#22312;&#21152;&#36895;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#24494;&#20998;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directional wavelet dictionaries are hierarchical representations which efficiently capture and segment information across scale, location and orientation. Such representations demonstrate a particular affinity to physical signals, which often exhibit highly anisotropic, localised multiscale structure. Many physically important signals are observed over spherical domains, such as the celestial sky in cosmology. Leveraging recent advances in computational harmonic analysis, we design new highly distributable and automatically differentiable directional wavelet transforms on the $2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 = \mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere with the radial half-line). We observe up to a $300$-fold and $21800$-fold acceleration for signals on the sphere and ball, respectively, compared to existing software, whilst maintaining 64-bit machine precision. Not only do these algorithms dramatically acce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09629</link><description>&lt;p&gt;
Quiet-STaR: &#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#24049;&#23398;&#20250;&#24605;&#32771;&#21518;&#20877;&#35828;&#35805;
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09629
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#21644;&#20132;&#35848;&#26102;&#65292;&#20154;&#20204;&#26377;&#26102;&#20250;&#20572;&#19979;&#26469;&#24605;&#32771;&#12290;&#23613;&#31649;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#25512;&#29702;&#26694;&#23450;&#20026;&#22238;&#31572;&#38382;&#39064;&#25110;&#23436;&#25104;&#20195;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#25512;&#29702;&#20960;&#20046;&#37117;&#38544;&#21547;&#22312;&#25152;&#26377;&#20070;&#38754;&#25991;&#26412;&#20013;&#12290;&#20363;&#22914;&#65292;&#36825;&#36866;&#29992;&#20110;&#35777;&#26126;&#20013;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#27493;&#39588;&#65292;&#20197;&#21450;&#25903;&#25745;&#23545;&#35805;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#22312;&#33258;&#23398;&#20064;&#25512;&#29702;&#32773;&#65288;STaR&#65292;Zelikman&#31561;&#65292;2022&#65289;&#20013;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#25512;&#26029;&#26469;&#33258;&#38382;&#31572;&#20013;&#26377;&#29992;&#30340;&#24605;&#32771;&#65292;&#24182;&#23398;&#20064;&#37027;&#20123;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#24605;&#32771;&#12290;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;--&#29702;&#24819;&#24773;&#20917;&#19979;, &#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#20174;&#20219;&#24847;&#25991;&#26412;&#20013;&#25512;&#26029;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#24605;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;Quiet-STaR&#65292;&#36825;&#26159;STaR&#30340;&#19968;&#20010;&#27867;&#21270;&#29256;&#26412;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;1&#65289;&#29983;&#25104;&#36830;&#32493;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
&lt;/p&gt;</description></item><item><title>Make-Your-3D&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#23450;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;5&#20998;&#38047;&#20869;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#23454;&#29616;&#39640;&#20445;&#30495;&#12289;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#29983;&#25104;&#65292;&#36890;&#36807;&#21327;&#35843;&#19981;&#21516;&#27169;&#22411;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;</title><link>https://arxiv.org/abs/2403.09625</link><description>&lt;p&gt;
Make-Your-3D: &#24555;&#36895;&#24182;&#19968;&#33268;&#30340;&#20027;&#20307;&#39537;&#21160;3D&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09625
&lt;/p&gt;
&lt;p&gt;
Make-Your-3D&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#23450;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;5&#20998;&#38047;&#20869;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#23454;&#29616;&#39640;&#20445;&#30495;&#12289;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#29983;&#25104;&#65292;&#36890;&#36807;&#21327;&#35843;&#19981;&#21516;&#27169;&#22411;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#35843;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#22823;&#21147;&#37327;&#65292;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21333;&#20010;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;3D&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#26032;&#27700;&#24179;&#30340;&#21019;&#36896;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;3D&#29983;&#25104;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#36328;&#19981;&#21516;&#25552;&#31034;&#21019;&#24314;&#20027;&#20307;&#39537;&#21160;3D&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Make-Your-3D&#30340;&#26032;&#39062;3D&#23450;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20165;&#19968;&#24352;&#20027;&#39064;&#30340;&#21333;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#20010;&#24615;&#21270;&#22320;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#65292;&#20165;&#38656;5&#20998;&#38047;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21327;&#35843;&#22810;&#35270;&#25193;&#25955;&#27169;&#22411;&#21644;&#29305;&#23450;&#36523;&#20221;&#30340;2D&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#24067;&#65292;&#23558;&#23427;&#20204;&#19982;&#25152;&#38656;&#30340;3D&#20027;&#20307;&#30340;&#20998;&#24067;&#23545;&#40784;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20849;&#21516;&#36827;&#21270;&#26694;&#26550;&#26469;&#20943;&#23569;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#36890;&#36807;&#36523;&#20221;&#24863;&#30693;&#20248;&#21270;&#20114;&#30456;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09625v1 Announce Type: cross  Abstract: Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts. In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimizatio
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20026;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#24102;&#26469;&#26032;&#39062;&#35270;&#35282;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20989;&#25968;&#36924;&#36817;&#30340;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.09621</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09621
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20026;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#24102;&#26469;&#26032;&#39062;&#35270;&#35282;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20989;&#25968;&#36924;&#36817;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23547;&#27714;&#38024;&#23545;&#29615;&#22659;&#25200;&#21160;&#30340;&#40065;&#26834;&#31574;&#30053;&#35757;&#32451;&#65292;&#36890;&#36807;&#24314;&#27169;&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#26469;&#35843;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#24403;&#38754;&#23545;&#24222;&#22823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#26102;&#65292;&#36825;&#31181;RL&#38656;&#35201;&#32771;&#34385;&#21040;&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#22522;&#26412;&#30340;&#38750;&#32447;&#24615;&#21644;&#35745;&#31639;&#36127;&#25285;&#65292;&#36825;&#32473;&#20998;&#26512;&#21644;&#23454;&#38469;&#24212;&#29992;&#20989;&#25968;&#36924;&#36817;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#22312;&#22522;&#26412;&#35774;&#32622;&#19979;&#65292;&#25552;&#35758;&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#22312;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#32972;&#26223;&#19979;&#21551;&#21160;&#23545;&#23454;&#20363;&#30456;&#20851;&#27425;&#20248;&#24615;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#40065;&#26834;&#31163;&#32447;RL&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#26412;&#36136;&#19978;&#19982;&#26631;&#20934;&#31163;&#32447;RL&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#26377;&#26126;&#26174;&#21306;&#21035;&#65292;&#21487;&#33021;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#29702;&#35770;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#22320;&#20381;&#36182;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09621v1 Announce Type: cross  Abstract: Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depen
&lt;/p&gt;</description></item><item><title>&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#39044;&#26399;&#34892;&#20026;&#65292;&#33021;&#22815;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#65292;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.09613</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#35757;&#32451;&#37325;&#26032;&#21796;&#37266;&#30693;&#35782;&#65306;&#20174;&#28798;&#38590;&#24615;&#24178;&#25200;&#20013;&#36827;&#34892;&#39044;&#26399;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09613
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#39044;&#26399;&#34892;&#20026;&#65292;&#33021;&#22815;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#65292;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20854;&#20013;&#25991;&#26723;&#20197;&#22266;&#23450;&#37325;&#22797;&#24207;&#21015;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#31995;&#21015;&#25991;&#26723;&#19978;&#35757;&#32451;&#26102;&#65292;&#32593;&#32476;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#19968;&#31181;&#22855;&#29305;&#19988;&#21331;&#36234;&#30340;&#29305;&#24615;&#65306;&#23427;&#20204;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#65292;&#22312;&#20877;&#27425;&#36935;&#21040;&#20043;&#21069;&#30340;&#25991;&#26723;&#26102;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#36807;&#26469;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#26550;&#26500;&#25193;&#23637;&#20854;&#21442;&#25968;&#25968;&#37327;&#26102;&#36880;&#28176;&#20986;&#29616;&#24182;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09613v1 Announce Type: cross  Abstract: We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
&lt;/p&gt;</description></item><item><title>&#20809;&#23398;&#20449;&#21495;&#22788;&#29702;&#33021;&#22815;&#25552;&#39640;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26816;&#27979;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#31354;&#38388;&#37325;&#26032;&#20998;&#37197;&#20809;&#23398;&#20449;&#21495;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#22122;&#22768;&#21644;&#24369;&#20449;&#21495;&#29615;&#22659;&#19979;&#30340;&#31070;&#32463;&#35745;&#31639;&#20219;&#21153;&#24615;&#33021;&#29942;&#39048;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.09612</link><description>&lt;p&gt;
&#20808;&#31639;&#27861;&#35745;&#31639;&#30340;&#20809;&#23398;&#26816;&#27979;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#30340;&#35270;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Compute-first optical detection for noise-resilient visual perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09612
&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#20449;&#21495;&#22788;&#29702;&#33021;&#22815;&#25552;&#39640;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26816;&#27979;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#31354;&#38388;&#37325;&#26032;&#20998;&#37197;&#20809;&#23398;&#20449;&#21495;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#22122;&#22768;&#21644;&#24369;&#20449;&#21495;&#29615;&#22659;&#19979;&#30340;&#31070;&#32463;&#35745;&#31639;&#20219;&#21153;&#24615;&#33021;&#29942;&#39048;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#24863;&#30693;&#30340;&#32972;&#26223;&#19979;&#65292;&#22330;&#26223;&#20013;&#30340;&#20809;&#23398;&#20449;&#21495;&#36890;&#36807;&#25506;&#27979;&#22120;&#36716;&#25442;&#20026;&#30005;&#23376;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#22788;&#29702;&#20197;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#22122;&#22768;&#21644;&#24369;&#20449;&#21495;&#29615;&#22659;&#65288;&#22914;&#22812;&#35270;&#24212;&#29992;&#20013;&#30340;&#28909;&#25104;&#20687;&#65289;&#20013;&#65292;&#31070;&#32463;&#35745;&#31639;&#20219;&#21153;&#30340;&#24615;&#33021;&#38754;&#20020;&#30528;&#19968;&#20010;&#26174;&#33879;&#30340;&#29942;&#39048;&#65292;&#21407;&#22240;&#26159;&#22312;&#22024;&#26434;&#26816;&#27979;&#26102;&#25968;&#25454;&#36136;&#37327;&#22266;&#26377;&#30340;&#38477;&#32423;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26816;&#27979;&#20043;&#21069;&#36827;&#34892;&#20809;&#23398;&#20449;&#21495;&#22788;&#29702;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;&#31354;&#38388;&#37325;&#26032;&#20998;&#37197;&#20809;&#23398;&#20449;&#21495;&#21487;&#20197;&#22686;&#24378;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26816;&#27979;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#20197;MNIST&#20998;&#31867;&#20026;&#22522;&#20934;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#24471;&#21040;&#20102;&#37327;&#21270;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#20449;&#21495;&#38598;&#20013;&#19982;&#22122;&#22768;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20854;&#23454;&#38469;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09612v1 Announce Type: cross  Abstract: In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information. In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection. Here, we propose a concept of optical signal processing before detection to address this issue. We demonstrate that spatially redistributing optical signals through a properly designed linear transformer can enhance the detection noise resilience of visual perception tasks, as benchmarked with the MNIST classification. Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09603</link><description>&lt;p&gt;
&#25511;&#21046;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#36827;&#34892;&#20048;&#35266;&#21487;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Optimistic Verifiable Training by Controlling Hardware Nondeterminism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#26085;&#30410;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#23548;&#33268;&#20102;&#20026;&#32570;&#20047;&#24517;&#35201;&#36164;&#28304;&#30340;&#23458;&#25143;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26381;&#21153;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#35757;&#32451;&#30340;&#27491;&#30830;&#24615;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#20363;&#22914;&#25968;&#25454;&#27602;&#21270;&#65292;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#39564;&#35777;&#35757;&#32451;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#38656;&#35201;&#21152;&#23494;&#25216;&#26415;&#32780;&#38590;&#20197;&#25193;&#23637;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#19968;&#20010;&#21487;&#20449;&#31532;&#19977;&#26041;&#23457;&#35745;&#21592;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#30340;&#8220;&#20048;&#35266;&#8221;&#26041;&#27861;&#12290; &#21518;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;GPU&#31867;&#22411;&#20043;&#38388;&#30340;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#38459;&#27490;&#23457;&#35745;&#21592;&#31934;&#30830;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#26041;&#26696;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#19979;&#36827;&#34892;&#65292;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#22235;&#33293;&#20116;&#20837;&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28151;&#21512;Mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#26377;&#26080;&#23614;&#34521;&#22768;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#22810;&#26631;&#31614;&#31034;&#20363;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09598</link><description>&lt;p&gt;
&#28151;&#21512;Mixup&#29992;&#20110;&#31232;&#26377;&#26080;&#23614;&#34521;&#22768;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28151;&#21512;Mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#26377;&#26080;&#23614;&#34521;&#22768;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#22810;&#26631;&#31614;&#31034;&#20363;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#19981;&#24179;&#34913;&#20998;&#31867;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#21160;&#29289;&#22768;&#38899;&#32463;&#24120;&#21516;&#26102;&#20986;&#29616;&#65292;&#32780;&#26576;&#20123;&#22768;&#38899;&#27604;&#20854;&#20182;&#22768;&#38899;&#35201;&#23569;&#24471;&#22810;&#12290;&#26412;&#25991;&#38024;&#23545;&#20351;&#29992;&#21253;&#21547;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#22810;&#26631;&#31614;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;AnuraSet&#65292;&#19987;&#27880;&#20110;&#20998;&#31867;&#26080;&#23614;&#30446;&#29289;&#31181;&#22768;&#38899;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Mixture of Mixups (Mix2)&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;Mixup&#12289;Manifold Mixup&#21644;MultiMix&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21333;&#29420;&#20351;&#29992;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65307;&#28982;&#32780;&#65292;&#24403;&#38543;&#26426;&#24212;&#29992;&#23427;&#20204;&#26102;&#65292;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#36873;&#21462;&#19968;&#20010;&#26041;&#27861;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#25552;&#21040;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21457;&#29983;&#27425;&#25968;&#36739;&#23569;&#30340;&#31232;&#26377;&#31867;&#21035;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;Mix2&#22312;&#36328;&#21508;&#31181;&#31867;&#21035;&#21516;&#26102;&#20986;&#29616;&#27700;&#24179;&#19978;&#20063;&#33021;&#26377;&#25928;&#20998;&#31867;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09598v1 Announce Type: cross  Abstract: Multi-label imbalanced classification poses a significant challenge in machine learning, particularly evident in bioacoustics where animal sounds often co-occur, and certain sounds are much less frequent than others. This paper focuses on the specific case of classifying anuran species sounds using the dataset AnuraSet, that contains both class imbalance and multi-label examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a framework that leverages mixing regularization methods Mixup, Manifold Mixup, and MultiMix. Experimental results show that these methods, individually, may lead to suboptimal results; however, when applied randomly, with one selected at each training iteration, they prove effective in addressing the mentioned challenges, particularly for rare classes with few occurrences. Further analysis reveals that Mix2 is also proficient in classifying sounds across various levels of class co-occurrences
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25968;&#25454;&#24211;&#21551;&#21457;&#30340;&#22312;&#32447;&#25968;&#25454;&#27969;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#36951;&#24536;&#36807;&#26102;&#25968;&#25454;&#31890;&#23376;&#65292;&#20197;&#20445;&#25345;&#21482;&#26377;&#26368;&#36817;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20302;&#24310;&#36831;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.09588</link><description>&lt;p&gt;
&#36845;&#20195;&#36951;&#24536;&#65306;&#20351;&#29992;&#25968;&#25454;&#24211;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#31890;&#21270;&#36827;&#34892;&#22312;&#32447;&#25968;&#25454;&#27969;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25968;&#25454;&#24211;&#21551;&#21457;&#30340;&#22312;&#32447;&#25968;&#25454;&#27969;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#36951;&#24536;&#36807;&#26102;&#25968;&#25454;&#31890;&#23376;&#65292;&#20197;&#20445;&#25345;&#21482;&#26377;&#26368;&#36817;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20302;&#24310;&#36831;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#31995;&#32479;&#65292;&#22914;&#37329;&#34701;&#12289;&#20132;&#36890;&#21644;&#30005;&#20449;&#31995;&#32479;&#65292;&#22312;&#26102;&#38388;&#19978;&#25935;&#24863;&#65292;&#35201;&#27714;&#23454;&#26102;&#20915;&#31574;&#38656;&#35201;&#20302;&#24310;&#36831;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#24517;&#39035;&#22788;&#29702;&#36830;&#32493;&#30340;&#26080;&#30028;&#25968;&#25454;&#27969;&#20197;&#21450;&#27010;&#24565;&#28418;&#31227;&#65292;&#36825;&#26159;&#20256;&#32479;&#22238;&#24402;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#25361;&#25112;&#24615;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25968;&#25454;&#24211;&#21551;&#21457;&#30340;&#25968;&#25454;&#27969;&#22238;&#24402;&#27169;&#22411;&#65292;&#23427;&#65288;a&#65289;&#20511;&#37492;&#20102;R*-&#26641;&#30340;&#28789;&#24863;&#65292;&#20174;&#20256;&#20837;&#30340;&#25968;&#25454;&#27969;&#20013;&#21019;&#24314;&#31890;&#23376;&#65292;&#20197;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#65292;&#65288;b&#65289;&#36845;&#20195;&#24615;&#22320;&#36951;&#24536;&#34987;&#35748;&#20026;&#24050;&#36807;&#26102;&#30340;&#31890;&#23376;&#65292;&#20174;&#32780;&#32500;&#25252;&#20165;&#21253;&#21547;&#26368;&#36817;&#30340;&#30456;&#20851;&#31890;&#23376;&#21015;&#34920;&#65292;&#65288;c&#65289;&#20351;&#29992;&#26368;&#36817;&#30340;&#25968;&#25454;&#21644;&#31890;&#23376;&#25552;&#20379;&#20302;&#24310;&#36831;&#39044;&#27979;&#12290;&#36825;&#31181;&#21463;R*-&#26641;&#21551;&#21457;&#30340;&#26041;&#27861;&#36824;&#20351;&#31639;&#27861;&#20855;&#26377;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09588v1 Announce Type: new  Abstract: Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making. Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to. There exists a need to create novel data stream regression methods that can handle these scenarios. We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions. The R*-tree-inspired approach also makes the algorithm amen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.09580</link><description>&lt;p&gt;
&#31639;&#27861;&#21477;&#27861;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Algorithmic syntactic causal identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09580
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#20013;&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20855;&#65292;&#20801;&#35768;&#20174;&#29702;&#35770;&#19978;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#30340;&#35266;&#27979;&#20998;&#24067;&#25512;&#23548;&#24178;&#39044;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#35782;&#21035;&#24418;&#24335;&#65292;&#22914;&#20351;&#29992;d&#20998;&#31163;&#21644;do-&#28436;&#31639;&#30340;&#25216;&#26415;&#37117;&#26159;&#22312;CBN&#19978;&#21033;&#29992;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25968;&#23398;&#35821;&#35328;&#34920;&#36798;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#65292;&#27010;&#29575;&#35770;&#21644;&#22240;&#27492;&#30446;&#21069;&#30340;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#19981;&#36866;&#29992;&#65292;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#25968;&#25454;&#27969;&#31243;&#24207;&#65288;&#20363;&#22914;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65289;&#12289;&#20998;&#24067;&#24335;&#31995;&#32479;&#21644;&#22823;&#22810;&#25968;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#20844;&#29702;&#22522;&#30784;&#26469;&#28040;&#38500;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#36825;&#31181;&#26367;&#20195;&#20844;&#29702;&#21270;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#33719;&#24471;&#19968;&#20010;&#26126;&#30830;&#19988;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09580v1 Announce Type: new  Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean
&lt;/p&gt;</description></item><item><title>uaMix-MAE&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;ID&#35843;&#25972;&#31574;&#30053;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#38899;&#39057;&#28151;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;MAEs&#36827;&#34892;&#23545;&#27604;&#35843;&#25972;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#35821;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.09579</link><description>&lt;p&gt;
uaMix-MAE: &#20351;&#29992;&#26080;&#30417;&#30563;&#38899;&#39057;&#28151;&#21512;&#39640;&#25928;&#35843;&#25972;&#39044;&#35757;&#32451;&#38899;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09579
&lt;/p&gt;
&lt;p&gt;
uaMix-MAE&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;ID&#35843;&#25972;&#31574;&#30053;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#38899;&#39057;&#28151;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;MAEs&#36827;&#34892;&#23545;&#27604;&#35843;&#25972;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#33021;&#22815;&#20174;&#26080;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#27880;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#32780;&#23454;&#20363;&#21306;&#20998;&#65288;ID&#65289;&#21017;&#24378;&#35843;&#39640;&#32423;&#35821;&#20041;&#65292;&#20026;&#20943;&#36731;MAEs&#20013;&#30340;&#27880;&#37322;&#38656;&#27714;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;uaMix-MAE&#30340;&#39640;&#25928;ID&#35843;&#25972;&#31574;&#30053;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#38899;&#39057;&#28151;&#21512;&#12290;&#36890;&#36807;&#23545;&#27604;&#35843;&#25972;&#65292;uaMix-MAE&#20351;&#39044;&#35757;&#32451;MAEs&#30340;&#34920;&#31034;&#23545;&#40784;&#65292;&#20174;&#32780;&#20419;&#36827;&#26377;&#25928;&#22320;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#35821;&#20041;&#12290;&#20026;&#20102;&#22312;&#23567;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#20248;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;&#28151;&#21512;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#21644;&#34394;&#25311;&#26631;&#31614;&#20013;&#25805;&#20316;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09579v1 Announce Type: cross  Abstract: Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#35270;&#36710;&#36742;&#30340;&#34892;&#20026;&#21644;&#29366;&#24577;&#20449;&#24687;&#26469;&#33258;&#21160;&#35782;&#21035;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#26080;&#38656;&#36710;&#36742;&#20027;&#21160;&#36890;&#30693;&#12290;</title><link>https://arxiv.org/abs/2403.09571</link><description>&lt;p&gt;
&#20320;&#26159;&#26426;&#22120;&#20154;&#21527;&#65311;&#20174;&#34892;&#20026;&#20998;&#26512;&#20013;&#26816;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;
&lt;/p&gt;
&lt;p&gt;
Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#35270;&#36710;&#36742;&#30340;&#34892;&#20026;&#21644;&#29366;&#24577;&#20449;&#24687;&#26469;&#33258;&#21160;&#35782;&#21035;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#26080;&#38656;&#36710;&#36742;&#20027;&#21160;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#24040;&#22823;&#28909;&#28526;&#24613;&#20999;&#22320;&#21628;&#21796;&#26032;&#20852;&#21644;&#21019;&#26032;&#25216;&#26415;&#65292;&#20197;&#25903;&#25345;&#20808;&#36827;&#30340;&#31227;&#21160;&#24615;&#20351;&#29992;&#26696;&#20363;&#12290;&#38543;&#30528;&#27773;&#36710;&#21046;&#36896;&#21830;&#19981;&#26029;&#24320;&#21457;SAE 3&#32423;&#21450;&#20197;&#19978;&#31995;&#32479;&#26469;&#25552;&#39640;&#20056;&#23458;&#30340;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24615;&#65292;&#20132;&#36890;&#31649;&#29702;&#26426;&#26500;&#38656;&#35201;&#24314;&#31435;&#26032;&#31243;&#24207;&#26469;&#31649;&#29702;&#20174;&#20154;&#24037;&#39550;&#39542;&#21040;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#36807;&#28193;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#21453;&#39304;&#26426;&#21046;&#26469;&#24494;&#35843;&#35774;&#24819;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#33258;&#21160;&#20998;&#26512;&#24182;&#23558;&#20854;&#19982;&#20154;&#24037;&#39550;&#39542;&#36710;&#36742;&#21306;&#20998;&#24320;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#35270;&#20351;&#29992;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#29366;&#24577;&#20449;&#24687;&#30340;&#27963;&#21160;&#36710;&#36742;&#65292;&#20197;&#30830;&#23450;&#36710;&#36742;&#26159;&#21542;&#20026;&#33258;&#21160;&#39550;&#39542;&#65292;&#32780;&#26080;&#38656;&#36710;&#36742;&#20027;&#21160;&#36890;&#30693;&#12290;&#22522;&#26412;&#19978;&#65292;&#23427;&#20381;&#36182;&#36710;&#36742;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#36825;&#20123;&#36710;&#36742;&#20849;&#20139;&#22312;&#36947;&#36335;&#19978;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#20379;&#26426;&#22120;&#23398;&#20064;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09571v1 Announce Type: cross  Abstract: The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09570</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21450;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30340;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#35774;&#35745;&#32773;&#38754;&#20020;&#19968;&#31995;&#21015;&#20248;&#21270;&#20219;&#21153;&#65292;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26114;&#36149;&#35780;&#20272;&#30340;&#40657;&#30418;&#20989;&#25968;&#24418;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#38656;&#35201;&#33719;&#21462;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#20248;&#20540;&#25110;&#35299;&#30340;&#20449;&#24687;&#21644;&#36890;&#36807;&#21442;&#25968;&#30340;&#36716;&#31227;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09570v1 Announce Type: new  Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27965;&#21407;&#29702;&#30340;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09560</link><description>&lt;p&gt;
&#33258;&#27965;&#35757;&#32451;&#29992;&#20110;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistency Training for Hamiltonian Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27965;&#21407;&#29702;&#30340;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09560v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25552;&#35201;: &#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#26159;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#20998;&#23376;&#31185;&#23398;&#38382;&#39064;&#30340;&#22810;&#21151;&#33021;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21704;&#23494;&#39039;&#37327;&#39044;&#27979;&#20855;&#26377;&#33258;&#27965;&#21407;&#29702;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#31934;&#30830;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#19968;&#20248;&#28857;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#22256;&#38590;&#65292;&#24182;&#23558;&#35813;&#20219;&#21153;&#19982;&#20854;&#20182;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#30340;&#23646;&#24615;&#39044;&#27979;&#20844;&#24335;&#21306;&#20998;&#24320;&#65306;&#65288;1&#65289;&#33258;&#27965;&#35757;&#32451;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#22240;&#27492;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65307;&#65288;2&#65289;&#33258;&#27965;&#35757;&#32451;&#27604;&#20351;&#29992;DFT&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#26159;&#23545;&#19968;&#32452;&#20998;&#23376;&#32467;&#26500;&#19978;&#30340;DFT&#35745;&#31639;&#30340;&#25674;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#20998;&#24067;&#20043;&#22806;&#30340;&#24773;&#20917;&#19979;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09560v1 Announce Type: new  Abstract: Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios
&lt;/p&gt;</description></item><item><title>&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09549</link><description>&lt;p&gt;
&#23558;&#21435;&#22122;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#20197;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09549
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;3D&#21407;&#23376;&#20307;&#31995;&#20013;&#30340;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#20174;&#22836;&#31639;&#35745;&#31639;&#65292;&#22240;&#27492;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21435;&#22122;&#38750;&#24179;&#34913;&#32467;&#26500;&#65288;DeNS&#65289;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;DeNS&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21521;&#20854;3D&#22352;&#26631;&#28155;&#21152;&#22122;&#22768;&#26469;&#30772;&#22351;3D&#32467;&#26500;&#65292;&#28982;&#21518;&#39044;&#27979;&#22122;&#22768;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#20165;&#38480;&#20110;&#24179;&#34913;&#32467;&#26500;&#30340;&#21435;&#22122;&#24037;&#20316;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#21435;&#22122;&#27867;&#21270;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#38750;&#24179;&#34913;&#32467;&#26500;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#38750;&#24179;&#34913;&#32467;&#26500;&#19981;&#23545;&#24212;&#20110;&#23616;&#37096;&#33021;&#37327;&#26368;&#23567;&#20540;&#65292;&#20855;&#26377;&#38750;&#38646;&#21147;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#21407;&#23376;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09549v1 Announce Type: cross  Abstract: Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic posit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#20851;&#27880;&#25552;&#39640;&#21484;&#22238;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#37325;&#28857;&#20943;&#23569;&#20551;&#38452;&#24615;&#21644;&#20351;&#29992; SHAP &#36827;&#34892;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#20851;&#27880;&#25552;&#39640;&#21484;&#22238;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19990;&#30028;&#19978;&#22842;&#36208;&#26368;&#22810;&#22899;&#24615;&#29983;&#21629;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#20854;&#20013;&#20083;&#33146;&#30284;&#21344;&#25454;&#20102;&#30284;&#30151;&#30149;&#20363;&#21644;&#27515;&#20129;&#20154;&#25968;&#26368;&#39640;&#30340;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#21487;&#20197;&#39044;&#38450;&#20083;&#33146;&#30284;&#65292;&#20174;&#32780;&#36827;&#34892;&#26089;&#26399;&#27835;&#30103;&#12290;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22312;&#30284;&#30151;&#39044;&#27979;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#20294;&#26377;&#26102;&#20165;&#20381;&#38752;&#20934;&#30830;&#24615;&#21487;&#33021;&#24182;&#38750;&#22987;&#32456;&#21487;&#38752;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#25552;&#21319;&#25216;&#26415;&#22522;&#20110;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#20083;&#33146;&#30284;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#24615;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#26816;&#27979;&#21307;&#23398;&#30142;&#30149;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#21033;&#29992;&#21152;&#24030;&#22823;&#23398;&#23572;&#28286;&#20998;&#26657; (UCI) &#25968;&#25454;&#38598;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#33258;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09548v1 Announce Type: new  Abstract: Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GitHub&#19978;185&#20010;&#24320;&#28304;&#39033;&#30446;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#36890;&#24120;&#38656;&#35201;&#26356;&#38271;&#30340;&#26500;&#24314;&#26102;&#38388;&#65292;&#20013;&#31561;&#35268;&#27169;&#30340;&#39033;&#30446;&#27979;&#35797;&#35206;&#30422;&#29575;&#36739;&#20302;&#65292;&#24182;&#21576;&#29616;&#20986;&#26500;&#24314;&#26102;&#38388;&#36235;&#21183;&#22686;&#38271;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.09547</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#22914;&#20309;&#20351;&#29992;&#25345;&#32493;&#38598;&#25104;&#23454;&#36341;&#65311;GitHub Actions &#19978;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GitHub&#19978;185&#20010;&#24320;&#28304;&#39033;&#30446;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#36890;&#24120;&#38656;&#35201;&#26356;&#38271;&#30340;&#26500;&#24314;&#26102;&#38388;&#65292;&#20013;&#31561;&#35268;&#27169;&#30340;&#39033;&#30446;&#27979;&#35797;&#35206;&#30422;&#29575;&#36739;&#20302;&#65292;&#24182;&#21576;&#29616;&#20986;&#26500;&#24314;&#26102;&#38388;&#36235;&#21183;&#22686;&#38271;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38598;&#25104;&#65288;CI&#65289;&#26159;&#20256;&#32479;&#36719;&#20214;&#24320;&#21457;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#23454;&#36341;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39033;&#30446;&#39046;&#22495;&#20013;&#30340;&#32454;&#33410;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;ML&#24320;&#21457;&#30340;&#29420;&#29305;&#24615;&#65292;&#20102;&#35299;CI&#23454;&#36341;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#30340;&#37319;&#29992;&#23545;&#20110;&#35843;&#25972;&#26377;&#25928;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#23545;GitHub&#19978;&#30340;185&#20010;&#24320;&#28304;&#39033;&#30446;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65288;93&#20010;ML&#39033;&#30446;&#21644;92&#20010;&#38750;ML&#39033;&#30446;&#65289;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#32500;&#24230;&#65292;&#26088;&#22312;&#25581;&#31034;ML&#21644;&#38750;ML&#39033;&#30446;&#20043;&#38388;CI&#37319;&#29992;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ML&#39033;&#30446;&#36890;&#24120;&#38656;&#35201;&#26356;&#38271;&#30340;&#26500;&#24314;&#26102;&#38388;&#65292;&#24182;&#19988;&#20013;&#31561;&#35268;&#27169;&#30340;ML&#39033;&#30446;&#19982;&#38750;ML&#39033;&#30446;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#27979;&#35797;&#35206;&#30422;&#29575;&#12290;&#27492;&#22806;&#65292;&#23567;&#22411;&#21644;&#20013;&#31561;&#35268;&#27169;&#30340;ML&#39033;&#30446;&#26174;&#31034;&#20986;&#27604;&#20854;&#38750;ML&#23545;&#24212;&#39033;&#30446;&#26356;&#22810;&#30340;&#22686;&#38271;&#26500;&#24314;&#26102;&#38388;&#36235;&#21183;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09547v1 Announce Type: cross  Abstract: Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build durations, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualita
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#32034;&#20102;&#32441;&#29702;&#23398;&#20064;&#30340;&#20851;&#38190;&#65292;&#36890;&#36807;&#32441;&#29702;-&#23545;&#35937;&#20851;&#32852;&#25581;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#32441;&#29702;&#21644;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#32441;&#29702;&#23398;&#20064;&#23545;&#20110;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#26032;&#21487;&#33021;&#24615;&#21644;&#21457;&#29616;&#24847;&#24819;&#19981;&#21040;&#20559;&#35265;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09543</link><description>&lt;p&gt;
&#32441;&#29702;&#23398;&#20064;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations in Texture Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09543
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#32034;&#20102;&#32441;&#29702;&#23398;&#20064;&#30340;&#20851;&#38190;&#65292;&#36890;&#36807;&#32441;&#29702;-&#23545;&#35937;&#20851;&#32852;&#25581;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#32441;&#29702;&#21644;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#32441;&#29702;&#23398;&#20064;&#23545;&#20110;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#26032;&#21487;&#33021;&#24615;&#21644;&#21457;&#29616;&#24847;&#24819;&#19981;&#21040;&#20559;&#35265;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\textit{&#32441;&#29702;&#23398;&#20064;}&#65306;&#21363;&#36890;&#36807;&#23545;&#35937;&#20998;&#31867;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32441;&#29702;&#30340;&#35782;&#21035;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#36825;&#20123;&#32441;&#29702;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#32441;&#29702;-&#23545;&#35937;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#32441;&#29702;&#21644;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#31867;&#32467;&#26524;&#65306;&#24378;&#22823;&#19988;&#39044;&#26399;&#30340;&#20851;&#32852;&#12289;&#24378;&#22823;&#20294;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#32852;&#65292;&#20197;&#21450;&#39044;&#26399;&#20294;&#19981;&#23384;&#22312;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23545;&#32441;&#29702;&#23398;&#20064;&#30340;&#25506;&#31350;&#20351;&#24471;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#26032;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#26377;&#28508;&#21147;&#21457;&#29616;&#24847;&#24819;&#19981;&#21040;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09543v1 Announce Type: cross  Abstract: In this work, we investigate \textit{texture learning}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.09516</link><description>&lt;p&gt;
&#21033;&#29992;&#20856;&#22411;&#34920;&#31034;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#32780;&#19981;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAFair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#27492;&#31867;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20856;&#22411;&#25991;&#26412;&#65292;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;&#27169;&#22411;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#27880;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#24120;&#35265;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
&lt;/p&gt;</description></item><item><title>STPA&#22312;&#27773;&#36710;&#39046;&#22495;&#30340;&#24212;&#29992;&#38754;&#20020;&#21487;&#36861;&#28335;&#24615;&#25361;&#25112;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#27773;&#36710;&#34892;&#19994;&#30340;STPA&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2403.09509</link><description>&lt;p&gt;
&#22312;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#20998;&#24067;&#24335;&#24320;&#21457;&#20013;&#30340;STPA&#65306;&#19968;&#39033;&#35775;&#35848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09509
&lt;/p&gt;
&lt;p&gt;
STPA&#22312;&#27773;&#36710;&#39046;&#22495;&#30340;&#24212;&#29992;&#38754;&#20020;&#21487;&#36861;&#28335;&#24615;&#25361;&#25112;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#27773;&#36710;&#34892;&#19994;&#30340;STPA&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20998;&#26512;&#29992;&#20110;&#22312;&#23433;&#20840;&#30456;&#20851;&#21151;&#33021;&#30340;&#35774;&#35745;&#38454;&#27573;&#35782;&#21035;&#21361;&#38505;&#24182;&#24314;&#31435;&#30693;&#35782;&#12290;&#36825;&#22312;&#22797;&#26434;&#30340;AI-enabled&#21644;&#36719;&#20214;&#23494;&#38598;&#22411;&#31995;&#32479;&#22914;&#33258;&#20027;&#39550;&#39542;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512; (STPA) &#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38450;&#24481;&#21644;&#33322;&#31354;&#33322;&#22825;&#31561;&#23433;&#20840;&#30456;&#20851;&#39046;&#22495;&#65292;&#20063;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#26085;&#30410;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;STPA &#20551;&#35774;&#22312;&#20855;&#26377;&#20998;&#24067;&#24335;&#31995;&#32479;&#24320;&#21457;&#21644;&#22810;&#25277;&#35937;&#35774;&#35745;&#23618;&#27425;&#30340;&#27773;&#36710;&#31995;&#32479;&#24037;&#31243;&#20013;&#24182;&#38750;&#23436;&#20840;&#26377;&#25928;&#12290;&#36825;&#23558;&#38459;&#30861;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;STPA&#20998;&#26512;&#20182;&#20204;&#30340;&#36719;&#20214;&#20316;&#20026;&#26356;&#22823;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#23548;&#33268;&#32570;&#20047;&#21487;&#36861;&#28335;&#24615;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#25345;&#32493;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#30340;&#21487;&#32500;&#25252;&#24615;&#25361;&#25112; (DevOps)&#12290;&#26412;&#25991;&#39318;&#20808;&#27604;&#36739;&#20102;&#38754;&#21521;&#27773;&#36710;&#34892;&#19994;&#30340;STPA&#30340;&#19981;&#21516;&#25351;&#21335;&#65292;&#20363;&#22914;J31887/ISO21448/STPA&#25163;&#20876;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09509v1 Announce Type: cross  Abstract: Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD). System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability. This can be seen as a maintainability challenge in continuous development and deployment (DevOps). In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;</title><link>https://arxiv.org/abs/2403.09506</link><description>&lt;p&gt;
&#19981;&#35201;&#20197;&#22806;&#34920;&#21028;&#26029;: &#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#35757;&#32451;&#27969;&#31243;&#22312;&#25968;&#25454;&#22686;&#24378;&#26102;&#24573;&#30053;&#20102;&#33394;&#35843;&#25238;&#21160;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#20250;&#24102;&#26469;&#23545;&#20998;&#31867;&#26377;&#23475;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20063;&#26159;&#20302;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33394;&#35843;&#21464;&#21270;&#22312;&#35270;&#39057;&#35782;&#21035;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#21464;&#21270;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#23545;&#20110;&#21253;&#21547;&#36816;&#21160;&#20449;&#24687;&#30340;&#35270;&#39057;&#26469;&#35828;&#65292;&#38745;&#24577;&#22806;&#35266;&#19981;&#26159;&#37027;&#20040;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#65292;&#23427;&#22312;&#35270;&#39057;&#20013;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#65292;&#38544;&#24335;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SwapMix&#30340;&#25805;&#20316;&#65292;&#29992;&#20110;&#39640;&#25928;&#20462;&#25913;&#35270;&#39057;&#26679;&#26412;&#30340;&#22806;&#35266;&#65292;&#24182;&#24341;&#20837;&#20102;&#21464;&#24322;&#23545;&#40784;&#65288;VA&#65289;&#26469;&#35299;&#20915;SwapMix&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36843;&#20351;&#27169;&#22411;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09506v1 Announce Type: cross  Abstract: Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to le
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.09502</link><description>&lt;p&gt;
EquiAV: &#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#38899;&#39057;-&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#25429;&#25417;&#20016;&#23500;&#32508;&#21512;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#35768;&#22810;&#23398;&#20064;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#65292;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#20173;&#28982;&#24456;&#38590;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#65292;&#22240;&#20026;&#22686;&#24378;&#21487;&#33021;&#20250;&#36731;&#26131;&#30772;&#22351;&#36755;&#20837;&#23545;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EquiAV&#65292;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25193;&#23637;&#31561;&#21464;&#24615;&#24320;&#22987;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#20419;&#36827;&#12290;&#23427;&#20351;&#24471;&#26469;&#33258;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#33021;&#22815;&#32858;&#21512;&#21040;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#20013;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09499</link><description>&lt;p&gt;
&#20351;&#29992;Q&#23398;&#20064;&#30340;&#22902;&#29275;&#20859;&#27542;&#22330;&#30005;&#27744;&#31649;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22902;&#29275;&#20859;&#27542;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#65292;&#26159;&#20892;&#19994;&#20013;&#19968;&#20010;&#33021;&#28304;&#23494;&#38598;&#22411;&#30340;&#37096;&#38376;&#12290;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#38598;&#25104;&#21040;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#30005;&#27744;&#31649;&#29702;&#23545;&#20110;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#33267;&#20851;&#37325;&#35201;&#12290;&#31649;&#29702;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#30001;&#20110;&#30005;&#33021;&#28040;&#32791;&#30340;&#27874;&#21160;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#38388;&#27463;&#24615;&#20197;&#21450;&#33021;&#28304;&#20215;&#26684;&#30340;&#27874;&#21160;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#65292;&#28982;&#32780;&#22312;&#36825;&#19968;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20197;&#29233;&#23572;&#20848;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#20197;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#20026;&#26680;&#24515;&#30340;2030&#24180;&#33021;&#28304;&#25112;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23433;&#25490;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25705;&#25176;&#36710;&#30896;&#25758;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#25506;&#35752;&#20102;&#21487;&#38752;&#22320;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#30896;&#25758;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09491</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25705;&#25176;&#36710;&#30896;&#25758;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
On using Machine Learning Algorithms for Motorcycle Collision Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25705;&#25176;&#36710;&#30896;&#25758;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#25506;&#35752;&#20102;&#21487;&#38752;&#22320;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#30896;&#25758;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#25705;&#25176;&#36710;&#21560;&#24341;&#20102;&#22823;&#37327;&#19981;&#21516;&#31867;&#22411;&#30340;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25705;&#25176;&#36710;&#20107;&#25925;&#20013;&#20005;&#37325;&#20260;&#20129;&#29575;&#36828;&#36828;&#36229;&#36807;&#23458;&#36710;&#20107;&#25925;&#65292;&#20154;&#20204;&#19968;&#30452;&#33268;&#21147;&#20110;&#22686;&#21152;&#34987;&#21160;&#23433;&#20840;&#31995;&#32479;&#12290;&#30896;&#25758;&#27169;&#25311;&#26174;&#31034;&#65292;&#22914;&#26524;&#25705;&#25176;&#36710;&#37197;&#22791;&#20102;&#35832;&#22914;&#27668;&#22218;&#21644;&#23433;&#20840;&#24102;&#31561;&#34987;&#21160;&#23433;&#20840;&#25514;&#26045;&#65292;&#37027;&#20040;&#22312;&#25705;&#25176;&#36710;&#19982;&#27773;&#36710;&#30896;&#25758;&#26102;&#65292;&#20005;&#37325;&#21463;&#20260;&#25110;&#27515;&#20129;&#30340;&#39118;&#38505;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#12290;&#20026;&#20102;&#20351;&#34987;&#21160;&#23433;&#20840;&#31995;&#32479;&#34987;&#28608;&#27963;&#65292;&#24517;&#39035;&#22312;&#27627;&#31186;&#20869;&#26816;&#27979;&#21040;&#30896;&#25758;&#65292;&#28041;&#21450;&#21508;&#31181;&#30896;&#25758;&#37197;&#32622;&#65292;&#20294;&#32477;&#19981;&#33021;&#35823;&#35302;&#21457;&#12290;&#20026;&#20102;&#21487;&#38752;&#22320;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#30896;&#25758;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36866;&#29992;&#24615;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20107;&#25925;&#21644;&#39550;&#39542;&#25805;&#20316;&#27169;&#25311;&#26469;&#25910;&#38598;&#25968;&#25454;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09491v1 Announce Type: new  Abstract: Globally, motorcycles attract vast and varied users. However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems. Impact simulations show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts. For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered. For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms. First, a series of simulations of accidents and driving operation is introduced to collect data to train machine learning classification m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20174;&#22522;&#26412;&#25216;&#33021;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#32423;&#35838;&#31243;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09479</link><description>&lt;p&gt;
&#20174;&#22522;&#26412;&#25216;&#33021;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65306;&#20808;&#25171;&#22909;&#22522;&#30784;&#20877;&#35828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20174;&#22522;&#26412;&#25216;&#33021;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#32423;&#35838;&#31243;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#20204;&#21457;&#23637;&#22522;&#26412;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#38656;&#35201;&#32467;&#21512;&#35832;&#22914;&#31639;&#26415;&#21644;&#21333;&#20301;&#36716;&#25442;&#31561;&#22522;&#26412;&#25216;&#33021;&#30340;&#26356;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24448;&#24448;&#22256;&#38590;&#37325;&#37325;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25506;&#27979;&#26694;&#26550;&#65292;&#26469;&#30740;&#31350;&#22522;&#26412;&#25216;&#33021;&#33021;&#21542;&#33258;&#21457;&#22320;&#25512;&#24191;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#32423;&#35838;&#31243;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25216;&#33021;&#25512;&#24191;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#26412;&#25216;&#33021;&#19981;&#33021;&#33258;&#21457;&#22320;&#25512;&#24191;&#21040;&#21512;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20998;&#23618;&#35838;&#31243;&#23398;&#20064;&#65292;&#25105;&#20204;&#25104;&#21151;&#35825;&#23548;&#20102;&#25512;&#24191;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09479v1 Announce Type: new  Abstract: Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks. Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks. Pr
&lt;/p&gt;</description></item><item><title>VIRUS-NeRF&#26159;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#25972;&#21512;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#36798;&#21040;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09477</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#8212;&#8212;VIRUS-NeRF
&lt;/p&gt;
&lt;p&gt;
VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09477
&lt;/p&gt;
&lt;p&gt;
VIRUS-NeRF&#26159;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#25972;&#21512;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#36798;&#21040;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#29616;&#20195;&#24037;&#21378;&#21644;&#20179;&#24211;&#25805;&#20316;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38556;&#30861;&#29289;&#26816;&#27979;&#12289;&#22238;&#36991;&#21644;&#36335;&#24452;&#35268;&#21010;&#26159;&#20851;&#38190;&#30340;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;LiDAR&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20302;&#20998;&#36776;&#29575;&#27979;&#36317;&#20256;&#24863;&#22120;&#65292;&#22914;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#26102;&#38388;&#39134;&#34892;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;(VIRUS-NeRF)&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;VIRUS-NeRF&#26500;&#24314;&#22312;&#30636;&#26102;&#31070;&#32463;&#22270;&#24418;&#22522;&#20803;&#19982;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;(Instant-NGP)&#30340;&#22522;&#30784;&#19978;&#65292;&#34701;&#21512;&#20102;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#26356;&#26032;&#29992;&#20110;&#20809;&#32447;&#36319;&#36394;&#30340;&#21344;&#25454;&#32593;&#26684;&#12290;&#22312;2D&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;VIRUS-NeRF&#23454;&#29616;&#20102;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#23567;&#22411;&#29615;&#22659;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;LiDAR&#27979;&#37327;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09477v1 Announce Type: cross  Abstract: Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, whi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09472</link><description>&lt;p&gt;
&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65306;&#36229;&#36234;&#20154;&#31867;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#30340;&#28436;&#31034;&#25110;&#21028;&#26029;&#65292;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;AI&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#33021;&#21147;&#23558;&#21463;&#21040;&#20154;&#31867;&#33021;&#21147;&#30340;&#19978;&#30028;&#38480;&#21046;&#12290;&#36825;&#23601;&#24102;&#26469;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24403;&#31995;&#32479;&#30340;&#33021;&#21147;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#32487;&#32493;&#25913;&#36827;&#36825;&#20123;&#31995;&#32479;&#65311;&#26412;&#25991;&#22312;&#35299;&#20915;&#38590;&#24230;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;4-5&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#30340;&#32972;&#26223;&#19979;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#65288;&#22914;1-3&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#20013;&#23398;&#20064;&#20154;&#31867;&#27880;&#37322;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#8221;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19968;&#20010;&#22312;&#26356;&#31616;&#21333;&#20219;&#21153;&#30340;&#30417;&#30563;&#19979;&#35757;&#32451;&#30340;&#35780;&#20272;&#22120;&#65288;&#22870;&#21169;&#27169;&#22411;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35780;&#20998;&#26356;&#38590;&#20219;&#21153;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#20419;&#36827;&#22312;&#19981;&#21516;&#38590;&#24230;&#20219;&#21153;&#38388;&#30340;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#22788;&#29702;&#30563;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise
&lt;/p&gt;</description></item><item><title>&#23558;&#19968;&#32500;&#30340;&#35299;&#27861;&#25512;&#24191;&#21040;&#22810;&#32500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20803;&#22810;&#39033;&#24335;&#22238;&#24402;&#20013;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09465</link><description>&lt;p&gt;
&#24322;&#24120;&#20540;&#40065;&#26834;&#30340;&#22810;&#20803;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Outlier Robust Multivariate Polynomial Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09465
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#32500;&#30340;&#35299;&#27861;&#25512;&#24191;&#21040;&#22810;&#32500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20803;&#22810;&#39033;&#24335;&#22238;&#24402;&#20013;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24322;&#24120;&#20540;&#40065;&#26834;&#30340;&#22810;&#20803;&#22810;&#39033;&#24335;&#22238;&#24402;&#38382;&#39064;&#65306;&#35774;$p\colon\mathbb{R}^n\to\mathbb{R}$&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#22312;&#27599;&#20010;&#21464;&#37327;&#19978;&#33267;&#22810;&#20026;$d$&#27425;&#30340;$n$&#20803;&#22810;&#39033;&#24335;&#12290;&#32473;&#23450;&#19968;&#32452;&#38543;&#26426;&#26679;&#26412;$(\mathbf{x}_i,y_i) \in [-1,1]^n \times \mathbb{R}$&#65292;&#23427;&#20204;&#26159;$(\mathbf{x}_i,p(\mathbf{x}_i))$&#30340;&#24102;&#22122;&#22768;&#29256;&#26412;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#27599;&#20010;$\mathbf{x}_i$&#29420;&#31435;&#22320;&#20174;$[-1,1]^n$&#19978;&#30340;&#26576;&#20010;&#20998;&#24067;$\chi$&#20013;&#37319;&#26679;&#65292;&#23545;&#27599;&#20010;$i$&#65292;$y_i$&#20197;&#27010;&#29575;&#33267;&#22810;$\rho &lt; 1/2$&#26159;&#20219;&#24847;&#30340;&#65288;&#21363;&#24322;&#24120;&#20540;&#65289;&#65292;&#21542;&#21017;&#28385;&#36275;$|y_i-p(\mathbf{x}_i)|\leq\sigma$&#12290;&#30446;&#26631;&#26159;&#36755;&#20986;&#19968;&#20010;&#22810;&#20803;&#22810;&#39033;&#24335;$\hat{p}$&#65292;&#22312;&#27599;&#20010;&#21464;&#37327;&#19978;&#33267;&#22810;&#20026;$d$&#27425;&#65292;&#22312;$\ell_\infty$&#33539;&#25968;&#19979;&#19982;$p$&#30340;&#36317;&#31163;&#33267;&#22810;&#20026;$O(\sigma)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09465v1 Announce Type: cross  Abstract: We study the problem of robust multivariate polynomial regression: let $p\colon\mathbb{R}^n\to\mathbb{R}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable. We are given as input a set of random samples $(\mathbf{x}_i,y_i) \in [-1,1]^n \times \mathbb{R}$ that are noisy versions of $(\mathbf{x}_i,p(\mathbf{x}_i))$. More precisely, each $\mathbf{x}_i$ is sampled independently from some distribution $\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\rho &lt; 1/2$, and otherwise satisfies $|y_i-p(\mathbf{x}_i)|\leq\sigma$. The goal is to output a polynomial $\hat{p}$, of degree at most $d$ in each variable, within an $\ell_\infty$-distance of at most $O(\sigma)$ from $p$.   Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24433;&#21709;&#21306;&#22495;&#27010;&#24565;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#36845;&#20195;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#26500;&#35774;&#35745;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36830;&#32493;&#26753;&#31995;&#32479;&#30340;&#25130;&#38754;&#35201;&#27714;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09454</link><description>&lt;p&gt;
&#36890;&#36807;&#24433;&#21709;&#21306;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#36830;&#32493;&#26753;&#31995;&#32479;&#30340;&#32467;&#26500;&#35774;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Machine learning for structural design models of continuous beam systems via influence zones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09454
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24433;&#21709;&#21306;&#22495;&#27010;&#24565;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#36845;&#20195;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#26500;&#35774;&#35745;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36830;&#32493;&#26753;&#31995;&#32479;&#30340;&#25130;&#38754;&#35201;&#27714;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20174;&#36870;&#38382;&#39064;&#30340;&#35282;&#24230;&#20026;&#36830;&#32493;&#26753;&#31995;&#32479;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#26500;&#35774;&#35745;&#27169;&#22411;&#12290;&#22312;&#21010;&#20998;&#21069;&#21521;&#12289;&#20248;&#21270;&#21644;&#36870;&#21521;&#26426;&#22120;&#23398;&#20064;&#31639;&#23376;&#20043;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#21457;&#23637;&#30340;&#24433;&#21709;&#21306;&#22495;&#27010;&#24565;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#32467;&#26500;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20195;&#34920;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#26041;&#27861;&#21464;&#38761;&#12290;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#26500;&#24605;&#19968;&#20010;&#38750;&#36845;&#20195;&#30340;&#32467;&#26500;&#35774;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20219;&#24847;&#31995;&#32479;&#22823;&#23567;&#30340;&#36830;&#32493;&#26753;&#31995;&#32479;&#30340;&#25130;&#38754;&#35201;&#27714;&#12290;&#22312;&#29983;&#25104;&#24050;&#30693;&#35299;&#30340;&#25968;&#25454;&#38598;&#20043;&#21518;&#65292;&#30830;&#23450;&#20102;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#38024;&#23545;&#26410;&#30693;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;&#25130;&#38754;&#23646;&#24615;&#39044;&#27979;&#65292;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#27979;&#35797;&#35823;&#24046;&#20026;1.6%&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#29992;&#20110;&#19981;&#21516;&#35268;&#27169;&#30340;&#32467;&#26500;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09454v1 Announce Type: new  Abstract: This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective. After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods. The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size. After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data. The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable si
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#20250;&#22686;&#21152;&#29983;&#25104;&#38544;&#31169;&#39118;&#38505;&#65292;&#29978;&#33267;&#20351;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#25552;&#39640;5.4&#65285;&#65292;&#24182;&#21487;&#23558;&#25552;&#21462;&#30340;&#31169;&#26377;&#26679;&#26412;&#25968;&#37327;&#20174;&#20960;&#20046;0&#20010;&#22686;&#21152;&#21040;&#24179;&#22343;16.3&#20010;&#12290;</title><link>https://arxiv.org/abs/2403.09450</link><description>&lt;p&gt;
&#38663;&#21160;&#27844;&#23494;&#65306;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#22686;&#21152;&#29983;&#25104;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09450
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#20250;&#22686;&#21152;&#29983;&#25104;&#38544;&#31169;&#39118;&#38505;&#65292;&#29978;&#33267;&#20351;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#25552;&#39640;5.4&#65285;&#65292;&#24182;&#21487;&#23558;&#25552;&#21462;&#30340;&#31169;&#26377;&#26679;&#26412;&#25968;&#37327;&#20174;&#20960;&#20046;0&#20010;&#22686;&#21152;&#21040;&#24179;&#22343;16.3&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#22312;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20063;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65306;&#24050;&#21457;&#24067;&#30340;&#27169;&#22411;&#25110;API&#21487;&#33021;&#29983;&#25104;&#35757;&#32451;&#22270;&#20687;&#65292;&#20174;&#32780;&#27844;&#38706;&#28041;&#21450;&#38544;&#31169;&#30340;&#35757;&#32451;&#20449;&#24687;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#39118;&#38505;&#65292;&#21363;Shake-to-Leak (S2L)&#65292;&#21363;&#20351;&#29992;&#25805;&#32437;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#22686;&#21152;&#29616;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#21487;&#33021;&#21457;&#29983;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#31574;&#30053;&#20013;&#65292;&#21253;&#25324;&#27010;&#24565;&#27880;&#20837;&#26041;&#27861;&#65288;DreamBooth&#21644;Textual Inversion&#65289;&#21644;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65288;LoRA&#21644;Hypernetwork&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#22312;&#26368;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;S2L&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#19978;&#30340;&#26368;&#26032;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#30340;AUC&#25552;&#39640;5.4&#65285;&#65288;&#32477;&#23545;&#24046;&#24322;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#27599;&#20010;&#30446;&#26631;&#22495;&#30340;&#25552;&#21462;&#31169;&#26377;&#26679;&#26412;&#20174;&#20960;&#20046;0&#20010;&#26679;&#26412;&#22686;&#21152;&#21040;&#24179;&#22343;16.3&#20010;&#26679;&#26412;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09450v1 Announce Type: new  Abstract: While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#23545;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09441</link><description>&lt;p&gt;
&#23545;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20849;&#21516;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#23545;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#65292;&#20351;&#20854;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861; -- &#32467;&#26500;&#21270;&#26435;&#37325;&#21098;&#26525;&#21644;&#37327;&#21270;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#23545;&#21387;&#32553;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09441v1 Announce Type: new  Abstract: As deep learning (DL) models are increasingly being integrated into our everyday lives, ensuring their safety by making them robust against adversarial attacks has become increasingly critical. DL models have been found to be susceptible to adversarial attacks which can be achieved by introducing small, targeted perturbations to disrupt the input data. Adversarial training has been presented as a mitigation strategy which can result in more robust models. This adversarial robustness comes with additional computational costs required to design adversarial attacks during training. The two objectives -- adversarial robustness and computational efficiency -- then appear to be in conflict of each other. In this work, we explore the effects of two different model compression methods -- structured weight pruning and quantization -- on adversarial robustness. We specifically explore the effects of fine-tuning on compressed models, and present th
&lt;/p&gt;</description></item><item><title>VISA&#26041;&#27861;&#36890;&#36807;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#36817;&#20284;&#25512;&#26029;&#65292;&#33021;&#22815;&#22312;&#20445;&#23432;&#36873;&#25321;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#20197;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#36798;&#21040;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#24403;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.09429</link><description>&lt;p&gt;
&#20855;&#26377;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Sequential Sample-Average Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09429
&lt;/p&gt;
&lt;p&gt;
VISA&#26041;&#27861;&#36890;&#36807;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#36817;&#20284;&#25512;&#26029;&#65292;&#33021;&#22815;&#22312;&#20445;&#23432;&#36873;&#25321;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#20197;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#36798;&#21040;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#24403;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39034;&#24207;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#65288;VISA&#65289;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#36817;&#20284;&#25512;&#26029;&#65292;&#20363;&#22914;&#22522;&#20110;&#25968;&#20540;&#27169;&#25311;&#30340;&#27169;&#22411;&#12290;VISA&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#26679;&#26412;&#22343;&#20540;&#36924;&#36817;&#26469;&#25193;&#23637;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#21069;&#21521;KL&#21464;&#20998;&#25512;&#26029;&#65292;&#36825;&#20123;&#36924;&#36817;&#22312;&#20449;&#20219;&#21306;&#22495;&#20869;&#34987;&#35270;&#20026;&#26377;&#25928;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#22810;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#37325;&#22797;&#20351;&#29992;&#27169;&#22411;&#35780;&#20272;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#12289;Lotka-Volterra&#21160;&#21147;&#23398;&#21644;Pickover&#21560;&#24341;&#23376;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;VISA&#21487;&#20197;&#22312;&#36873;&#25321;&#20445;&#23432;&#30340;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20004;&#20493;&#25110;&#26356;&#39640;&#30340;&#35745;&#31639;&#33410;&#32422;&#36798;&#21040;&#19982;&#26631;&#20934;&#37325;&#35201;&#24615;&#21152;&#26435;&#21069;&#21521;KL&#21464;&#20998;&#25512;&#26029;&#30456;&#24403;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09429v1 Announce Type: cross  Abstract: We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09428</link><description>&lt;p&gt;
&#19982;&#37051;&#23621;&#20511;&#23453;&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35299;&#20915;&#32570;&#22833;&#27169;&#24577;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#20851;&#20110;&#32570;&#22833;&#27169;&#24577;&#30340;&#30740;&#31350;&#25299;&#23637;&#21040;&#20302;&#25968;&#25454;&#24773;&#22659;&#65292;&#21363;&#19968;&#20010;&#19979;&#28216;&#20219;&#21153;&#26082;&#23384;&#22312;&#32570;&#22833;&#27169;&#24577;&#21448;&#23384;&#22312;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#37322;&#25918;&#21464;&#21387;&#22120;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#29616;&#26377;&#30340;&#20840;&#27169;&#24577;&#25968;&#25454;&#30340;&#20215;&#20540;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#25361;&#25112;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09428v1 Announce Type: new  Abstract: Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples. We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a high
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30001;&#28459;&#28216;&#30524;&#21160;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#29992;&#25143;&#35782;&#21035;&#30740;&#31350;&#65292;&#24182;&#22312;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09415</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30001;&#28459;&#28216;&#30524;&#21160;&#25968;&#25454;&#36827;&#34892;&#29992;&#25143;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
User Identification via Free Roaming Eye Tracking Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09415
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30001;&#28459;&#28216;&#30524;&#21160;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#29992;&#25143;&#35782;&#21035;&#30740;&#31350;&#65292;&#24182;&#22312;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#8220;&#33258;&#30001;&#28459;&#28216;&#8221;(FR)&#21644;&#8220;&#23450;&#21521;&#28459;&#28216;&#8221;(TR)&#65306;41&#21517;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#22312;&#22823;&#23398;&#26657;&#22253;&#37324;&#22235;&#22788;&#36208;&#21160;(FR)&#65292;&#25110;&#32773;&#34987;&#35201;&#27714;&#22312;&#22270;&#20070;&#39302;&#20869;&#25214;&#21040;&#29305;&#23450;&#25151;&#38388;(TR)&#12290;&#20351;&#29992;Pupil Labs Neon&#30524;&#21160;&#35774;&#22791;&#35760;&#24405;&#30524;&#21160;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20197;&#21069;&#24050;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#36827;&#34892;&#29992;&#25143;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;&#20351;&#29992;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;(RBFN)&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;FR&#21644;TR&#20013;&#33719;&#24471;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;87.3%&#21644;89.4%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#19982;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;95.3%&#36827;&#34892;&#27604;&#36739;(&#35813;&#20934;&#30830;&#29575;&#26159;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#20351;&#29992;BioEye 2015&#31454;&#36187;&#25968;&#25454;&#38598;&#30340;&#8220;RAN&#8221;&#21050;&#28608;&#33719;&#24471;&#30340;)&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#30740;&#31350;&#29992;&#25143;&#35782;&#21035;&#30340;&#32467;&#26524;&#65307;&#36825;&#31181;&#29615;&#22659;&#36890;&#24120;&#27604;&#23454;&#39564;&#23460;&#29615;&#22659;&#26356;&#20855;&#21487;&#34892;&#24615;&#65292;&#21487;&#33021;&#21253;&#21547;&#26356;&#22810;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09415v1 Announce Type: new  Abstract: We present a new dataset of "free roaming" (FR) and "targeted roaming" (TR): a pool of 41 participants is asked to walk around a university campus (FR) or is asked to find a particular room within a library (TR). Eye movements are recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On this dataset we investigate the accuracy of user identification using a previously known machine learning pipeline where a Radial Basis Function Network (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR and 89.4% for TR. This should be compared to 95.3% which is the (corresponding) highest accuracy we are aware of (achieved in a laboratory setting using the "RAN" stimulus of the BioEye 2015 competition dataset). To the best of our knowledge, our results are the first that study user identification in a non laboratory setting; such settings are often more feasible than laboratory settings and may include further ad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LM2D&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#26032;&#39062;&#27010;&#29575;&#26550;&#26500;&#65292;&#26088;&#22312;&#22312;&#38899;&#20048;&#21644;&#27468;&#35789;&#26465;&#20214;&#19979;&#36827;&#34892;&#33310;&#36424;&#29983;&#25104;&#65307;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#28085;&#30422;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;3D&#33310;&#36424;&#36816;&#21160;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.09407</link><description>&lt;p&gt;
LM2D&#65306;&#20197;&#27468;&#35789;&#21644;&#38899;&#20048;&#39537;&#21160;&#30340;&#33310;&#36424;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
LM2D: Lyrics- and Music-Driven Dance Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LM2D&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#26032;&#39062;&#27010;&#29575;&#26550;&#26500;&#65292;&#26088;&#22312;&#22312;&#38899;&#20048;&#21644;&#27468;&#35789;&#26465;&#20214;&#19979;&#36827;&#34892;&#33310;&#36424;&#29983;&#25104;&#65307;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#28085;&#30422;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;3D&#33310;&#36424;&#36816;&#21160;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33310;&#36424;&#36890;&#24120;&#28041;&#21450;&#19987;&#19994;&#32534;&#33310;&#65292;&#21253;&#21547;&#25353;&#29031;&#38899;&#20048;&#33410;&#22863;&#36827;&#34892;&#30340;&#22797;&#26434;&#21160;&#20316;&#65292;&#36824;&#21487;&#33021;&#21463;&#21040;&#27468;&#35789;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#23558;&#27468;&#35789;&#25972;&#21512;&#21040;&#21548;&#35273;&#32500;&#24230;&#20043;&#22806;&#65292;&#20016;&#23500;&#20102;&#22522;&#30784;&#38899;&#33394;&#65292;&#24182;&#20351;&#36816;&#21160;&#29983;&#25104;&#26356;&#26131;&#20110;&#35821;&#20041;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33310;&#36424;&#21512;&#25104;&#26041;&#27861;&#24448;&#24448;&#21482;&#24314;&#27169;&#20110;&#38899;&#39057;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LM2D&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#26032;&#39062;&#27010;&#29575;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#25193;&#25955;&#29983;&#25104;&#27493;&#39588;&#22312;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;&#26465;&#20214;&#19979;&#21019;&#24314;&#33310;&#36424;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#28085;&#30422;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;3D&#33310;&#36424;&#36816;&#21160;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23039;&#21183;&#20272;&#35745;&#25216;&#26415;&#33719;&#24471;&#12290;&#25105;&#20204;&#36890;&#36807;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20215;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20165;&#26377;&#38899;&#20048;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09407v1 Announce Type: cross  Abstract: Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, i
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#25910;&#25947;&#31639;&#27861;&#30340;&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.09389</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#23398;&#20064;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning to optimize with convergence guarantees using nonlinear system theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09389
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#25910;&#25947;&#31639;&#27861;&#30340;&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#25968;&#23383;&#26041;&#27861;&#26469;&#25511;&#21046;&#21160;&#24577;&#31995;&#32479;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#35774;&#35745;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#36941;&#21382;&#22797;&#26434;&#20248;&#21270;&#31354;&#38388;&#30340;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23545;&#20984;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#20984;&#38382;&#39064;&#21017;&#38656;&#35201;&#31934;&#32454;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23398;&#20064;&#20248;&#21270;(L2O)&#30340;&#26032;&#20852;&#33539;&#24335;&#33258;&#21160;&#21457;&#29616;&#20855;&#26377;&#20248;&#21270;&#24615;&#33021;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#65292;&#20294;&#32570;&#20047;&#20998;&#26512;&#25152;&#23398;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#25910;&#25947;&#31639;&#27861;&#30340;&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#30452;&#25509;&#20860;&#23481;&#65292;&#30830;&#20445;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09389v1 Announce Type: cross  Abstract: The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensurin
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;pantypes&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#19968;&#32452;&#31232;&#30095;&#23545;&#35937;&#25429;&#33719;&#36755;&#20837;&#20998;&#24067;&#30340;&#20840;&#37096;&#22810;&#26679;&#24615;&#30340;&#21407;&#22411;&#23545;&#35937;&#23478;&#26063;&#65292;&#21487;&#20197;&#22823;&#22823;&#22686;&#24378;&#21407;&#22411;&#33258;&#35299;&#37322;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09383</link><description>&lt;p&gt;
Pantypes: &#20195;&#34920;&#21508;&#31181;&#31867;&#22411;&#30340;&#33258;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pantypes: Diverse Representatives for Self-Explainable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09383
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;pantypes&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#19968;&#32452;&#31232;&#30095;&#23545;&#35937;&#25429;&#33719;&#36755;&#20837;&#20998;&#24067;&#30340;&#20840;&#37096;&#22810;&#26679;&#24615;&#30340;&#21407;&#22411;&#23545;&#35937;&#23478;&#26063;&#65292;&#21487;&#20197;&#22823;&#22823;&#22686;&#24378;&#21407;&#22411;&#33258;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#65292;&#21407;&#22411;&#33258;&#35299;&#37322;&#20998;&#31867;&#22120;&#24050;&#32463;&#20986;&#29616;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#19982;&#23398;&#20064;&#30340;&#21407;&#22411;&#23545;&#35937;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#22312;&#20915;&#31574;&#20013;&#34701;&#20837;&#39640;&#36879;&#26126;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#35774;&#35745;&#26102;&#32771;&#34385;&#20102;&#22810;&#26679;&#24615;&#65292;&#20294;&#23398;&#20064;&#30340;&#21407;&#22411;&#23545;&#35937;&#36890;&#24120;&#24182;&#19981;&#36275;&#20197;&#20805;&#20998;&#20195;&#34920;&#25152;&#26377;&#36755;&#20837;&#20998;&#24067;&#30340;&#26041;&#38754;&#65292;&#23588;&#20854;&#26159;&#20302;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#26041;&#38754;&#12290;&#36825;&#31181;&#19981;&#36275;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#21363;&#34920;&#31034;&#20559;&#24046;&#65292;&#24050;&#32463;&#19982;&#19982;&#26426;&#22120;&#23398;&#20064;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#21508;&#31181;&#26377;&#23475;&#29305;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;pantypes&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#19968;&#32452;&#31232;&#30095;&#23545;&#35937;&#25429;&#33719;&#36755;&#20837;&#20998;&#24067;&#30340;&#20840;&#37096;&#22810;&#26679;&#24615;&#30340;&#21407;&#22411;&#23545;&#35937;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;pantypes&#21487;&#20197;&#36890;&#36807;&#21344;&#25454;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#22686;&#24378;&#21407;&#22411;&#33258;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09383v1 Announce Type: cross  Abstract: Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thu
&lt;/p&gt;</description></item><item><title>BurstAttention&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.09347</link><description>&lt;p&gt;
BurstAttention&#65306;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09347
&lt;/p&gt;
&lt;p&gt;
BurstAttention&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#22312;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#36825;&#20123;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#20063;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;BurstAttention&#8221;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#20840;&#23616;&#38598;&#32676;&#21644;&#26412;&#22320;&#35774;&#22791;&#32423;&#21035;&#30340;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09347v1 Announce Type: cross  Abstract: Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence proce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#32423;&#34701;&#21512;&#37327;&#23376;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#65288;HQFNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27169;&#31946;&#25104;&#21592;&#20989;&#25968;&#65292;&#22312;&#19981;&#30830;&#23450;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.09318</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#23618;&#32423;&#34701;&#21512;&#37327;&#23376;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Fused Quantum Fuzzy Neural Network for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#32423;&#34701;&#21512;&#37327;&#23376;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#65288;HQFNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27169;&#31946;&#25104;&#21592;&#20989;&#25968;&#65292;&#22312;&#19981;&#30830;&#23450;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#25968;&#25454;&#26102;&#20195;&#25968;&#25454;&#29305;&#24449;&#23398;&#20064;&#30340;&#24378;&#22823;&#23398;&#20064;&#33539;&#24335;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#24573;&#30053;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#30830;&#23450;&#24615;&#27169;&#22411;&#12290;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;FDNN&#26159;&#23618;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#27169;&#31946;&#21644;&#31070;&#32463;&#34920;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#28982;&#21518;&#34701;&#21512;&#36825;&#20123;&#34920;&#31034;&#24418;&#25104;&#24453;&#20998;&#31867;&#30340;&#34920;&#31034;&#12290;FDNN&#22312;&#19981;&#30830;&#23450;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#32423;&#34701;&#21512;&#37327;&#23376;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#65288;HQFNN&#65289;&#12290;&#19982;&#32463;&#20856;&#30340;FDNN&#19981;&#21516;&#65292;HQFNN&#20351;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#31946;&#25104;&#21592;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#65288;Dirty-MNIST&#21644;15-Scene&#65289;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09318v1 Announce Type: cross  Abstract: Neural network is a powerful learning paradigm for data feature learning in the era of big data. However, most neural network models are deterministic models that ignore the uncertainty of data. Fuzzy neural networks are proposed to address this problem. FDNN is a hierarchical deep neural network that derives information from both fuzzy and neural representations, the representations are then fused to form representation to be classified. FDNN perform well on uncertain data classification tasks. In this paper, we proposed a novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership functions in fuzzy neural network. We conducted simulated experiment on two types of datasets (Dirty-MNIST and 15-Scene), the results show that the proposed model can outperform several existing methods. In addition, we demonstrate the robustness of the proposed q
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.09303</link><description>&lt;p&gt;
&#29992;&#29702;&#35770;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#20165;&#20351;&#29992;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#24322;&#24120;&#21457;&#29616;&#65292;&#23545;&#20581;&#24247;&#31579;&#26597;&#21644;&#35782;&#21035;&#32597;&#35265;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#65288;AEs&#65289;&#30340;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#23427;&#20204;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#24037;&#20316;&#65306;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#35757;&#32451;&#30340;AEs&#19981;&#33021;&#24456;&#22909;&#22320;&#37325;&#24314;&#30475;&#19981;&#35265;&#30340;&#24322;&#24120;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#22522;&#20110;&#37325;&#24314;&#38169;&#35823;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#24314;&#35757;&#32451;&#30446;&#26631;&#19982;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#19981;&#22815;&#21512;&#29702;&#12290;&#35813;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#22522;&#20110;AE&#30340;&#37325;&#24314;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21407;&#21017;&#65292;&#24182;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09303v1 Announce Type: new  Abstract: Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the informati
&lt;/p&gt;</description></item><item><title>StainFuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23558;&#26579;&#33394;&#26631;&#20934;&#21270;&#38382;&#39064;&#35270;&#20026;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#65292;&#22312;2&#30334;&#19975;&#22810;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#32467;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09302</link><description>&lt;p&gt;
StainFuser&#65306;&#22312;&#22810;&#21513;&#21152;&#20687;&#32032;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#25511;&#21046;&#25193;&#25955;&#20197;&#21152;&#24555;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09302
&lt;/p&gt;
&lt;p&gt;
StainFuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23558;&#26579;&#33394;&#26631;&#20934;&#21270;&#38382;&#39064;&#35270;&#20026;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#65292;&#22312;2&#30334;&#19975;&#22810;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#32467;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#26631;&#20934;&#21270;&#31639;&#27861;&#26088;&#22312;&#23558;&#28304;&#22810;&#21513;&#21152;&#20687;&#32032;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#24378;&#24230;&#29305;&#24449;&#36716;&#25442;&#20026;&#19982;&#30446;&#26631;&#22270;&#20687;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#20943;&#36731;&#22270;&#20687;&#20013;&#29992;&#20110;&#31361;&#20986;&#26174;&#31034;&#32454;&#32990;&#32452;&#20998;&#30340;&#26579;&#33394;&#21058;&#22806;&#35266;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;StainFuser&#65292;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#39640;&#36136;&#37327;&#36716;&#25442;&#31579;&#36873;&#20102;&#36804;&#20170;&#20026;&#27490;&#21253;&#21547;&#36229;&#36807;200&#19975;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#26368;&#22823;&#26579;&#33394;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;SPI-2M&#65292;&#24182;&#36827;&#34892;&#20102;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#35757;&#32451;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;StainFuser&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GAN&#21644;&#25163;&#24037;&#21046;&#20316;&#26041;&#27861;&#30340;&#26631;&#20934;&#21270;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#29992;&#20316;te&#26102;&#65292;&#23427;&#25913;&#21892;&#20102;&#32454;&#32990;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09302v1 Announce Type: cross  Abstract: Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a style transfer task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural style transfer for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art GAN and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a te
&lt;/p&gt;</description></item><item><title>&#21487;&#31227;&#38500;&#21464;&#37327;&#30340;&#27010;&#24565;&#20801;&#35768;&#36882;&#24402;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#36890;&#36807;&#36880;&#27493;&#20943;&#23569;&#38382;&#39064;&#35268;&#27169;&#26469;&#24110;&#21161;&#35299;&#20915;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09300</link><description>&lt;p&gt;
&#36882;&#24402;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Recursive Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09300
&lt;/p&gt;
&lt;p&gt;
&#21487;&#31227;&#38500;&#21464;&#37327;&#30340;&#27010;&#24565;&#20801;&#35768;&#36882;&#24402;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#36890;&#36807;&#36880;&#27493;&#20943;&#23569;&#38382;&#39064;&#35268;&#27169;&#26469;&#24110;&#21161;&#35299;&#20915;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09300v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22240;&#26524;&#21457;&#29616;&#65292;&#21363;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#36890;&#24120;&#26159;&#35782;&#21035;&#21644;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#31532;&#19968;&#27493;&#65292;&#36825;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#22240;&#26524;&#21457;&#29616;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#26377;&#38480;&#30340;&#25968;&#25454;&#23548;&#33268;&#32479;&#35745;&#26816;&#39564;&#38169;&#35823;&#65292;&#23398;&#20064;&#20219;&#21153;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20196;&#20154;&#26395;&#32780;&#21364;&#27493;&#12290;&#26412;&#25991;&#22522;&#20110;&#24182;&#25193;&#23637;&#20102;&#25105;&#20204;&#20808;&#21069;&#21457;&#34920;&#30340;&#22235;&#31687;&#35770;&#25991;&#65288;Mokhtarian&#31561;&#65292;2021&#24180;&#65307;Akbari&#31561;&#65292;2021&#24180;&#65307;Mokhtarian&#31561;&#65292;2022&#24180;&#65292;2023a&#24180;&#65289;&#12290;&#36825;&#20123;&#20316;&#21697;&#24341;&#20837;&#20102;&#21487;&#31227;&#38500;&#21464;&#37327;&#30340;&#27010;&#24565;&#65292;&#36825;&#20123;&#21464;&#37327;&#26159;&#21807;&#19968;&#21487;&#20197;&#36882;&#24402;&#31227;&#38500;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;&#30340;&#21464;&#37327;&#12290;&#21487;&#31227;&#38500;&#21464;&#37327;&#30340;&#23384;&#22312;&#21644;&#35782;&#21035;&#20801;&#35768;&#22240;&#26524;&#21457;&#29616;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36880;&#27493;&#20943;&#23569;&#38382;&#39064;&#35268;&#27169;&#26469;&#24110;&#21161;&#35299;&#20915;&#21069;&#36848;&#25361;&#25112;&#12290;&#36825;&#31181;&#32553;&#20943;&#19981;&#20165;&#22312;&#27599;&#20010;&#26465;&#20214;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20102;&#26465;&#20214;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09300v1 Announce Type: new  Abstract: Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#25506;&#32034;&#20102;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.09298</link><description>&lt;p&gt;
&#36229;&#36234;&#35328;&#35821;&#65306;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
More than words: Advancements and challenges in speech recognition for singing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#25506;&#32034;&#20102;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#26631;&#20934;&#35821;&#38899;&#35782;&#21035;&#23436;&#20840;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#27468;&#21809;&#21253;&#21547;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#38899;&#39640;&#21464;&#21270;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#20048;&#39118;&#26684;&#20197;&#21450;&#32972;&#26223;&#38899;&#20048;&#24178;&#25200;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35832;&#22914;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#25105;&#23558;&#25551;&#36848;&#19968;&#20123;&#25105;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36827;&#34892;&#30740;&#31350;&#26102;&#30340;&#32463;&#21382;&#65292;&#23601;&#22312;&#23427;&#20204;&#24320;&#22987;&#23853;&#38706;&#22836;&#35282;&#30340;&#26102;&#20505;&#65292;&#20294;&#20063;&#20250;&#23637;&#31034;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#36827;&#23637;&#22914;&#20309;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#25105;&#30340;&#30446;&#26631;&#26159;&#38416;&#26126;&#23558;&#35821;&#38899;&#35782;&#21035;&#24212;&#29992;&#20110;&#27468;&#21809;&#26102;&#30340;&#22797;&#26434;&#24615;&#65292;&#35780;&#20272;&#24403;&#21069;&#30340;&#33021;&#21147;&#65292;&#24182;&#27010;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09298v1 Announce Type: cross  Abstract: This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SELECTOR&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#65292;&#21033;&#29992;&#21367;&#31215;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.09290</link><description>&lt;p&gt;
SELECTOR&#65306;&#20855;&#26377;&#21367;&#31215;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#30284;&#30151;&#29983;&#23384;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SELECTOR&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#65292;&#21033;&#29992;&#21367;&#31215;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#23545;&#20110;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21046;&#23450;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#38477;&#20302;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#21307;&#30103;&#36153;&#29992;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#22810;&#27169;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20840;&#38754;&#12289;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#19982;&#32570;&#22833;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#27169;&#24577;&#20869;&#20449;&#24687;&#20132;&#20114;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SELECTOR&#65292;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#25513;&#30721;&#32534;&#30721;&#22120;&#30340;&#24322;&#26500;&#22270;&#24863;&#30693;&#32593;&#32476;&#65292;&#29992;&#20110;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#39044;&#27979;&#12290;SELECTOR&#21253;&#25324;&#29305;&#24449;&#36793;&#37325;&#26500;&#12289;&#21367;&#31215;&#25513;&#30721;&#32534;&#30721;&#22120;&#12289;&#29305;&#24449;&#20132;&#21449;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#29983;&#23384;&#39044;&#27979;&#27169;&#22359;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#24322;&#26500;&#22270;&#65292;&#24182;&#37319;&#29992;&#20803;&#36335;&#24452;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#36793;&#37325;&#26500;&#65292;&#30830;&#20445;&#29305;&#24449;&#20449;&#24687;&#30340;&#20840;&#38754;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09290v1 Announce Type: cross  Abstract: Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life. Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules. Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#20851;&#32852;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65288;DA-PFL&#65289;&#65292;&#37319;&#29992;&#20114;&#34917;&#30340;&#20851;&#32852;&#24230;&#37327;&#21644;&#21160;&#24577;&#32858;&#21512;&#31574;&#30053;&#65292;&#22312;&#27599;&#36718;&#21160;&#24577;&#32858;&#21512;&#23458;&#25143;&#31471;&#20197;&#20943;&#23569;&#31867;&#19981;&#24179;&#34913;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09284</link><description>&lt;p&gt;
DA-PFL: &#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#21160;&#24577;&#20851;&#32852;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09284
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#20851;&#32852;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65288;DA-PFL&#65289;&#65292;&#37319;&#29992;&#20114;&#34917;&#30340;&#20851;&#32852;&#24230;&#37327;&#21644;&#21160;&#24577;&#32858;&#21512;&#31574;&#30053;&#65292;&#22312;&#27599;&#36718;&#21160;&#24577;&#32858;&#21512;&#23458;&#25143;&#31471;&#20197;&#20943;&#23569;&#31867;&#19981;&#24179;&#34913;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20542;&#21521;&#20110;&#32858;&#21512;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#20197;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#21152;&#21095;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#20851;&#32852;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65288;DA-PFL&#65289;&#65292;&#20197;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#20114;&#34917;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#19968;&#31181;&#20851;&#32852;&#24230;&#37327;&#26469;&#24341;&#23548;&#21738;&#20123;&#23458;&#25143;&#31471;&#24212;&#35813;&#34987;&#32858;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#32858;&#21512;&#31574;&#30053;&#65292;&#26681;&#25454;&#27599;&#19968;&#36718;&#30340;&#20851;&#32852;&#24230;&#37327;&#21160;&#24577;&#32858;&#21512;&#23458;&#25143;&#31471;&#65292;&#20197;&#20943;&#23569;&#31867;&#19981;&#24179;&#34913;&#39118;&#38505;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DA-PFL&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09284v1 Announce Type: new  Abstract: Personalized federated learning becomes a hot research topic that can learn a personalized learning model for each client. Existing personalized federated learning models prefer to aggregate similar clients with similar data distribution to improve the performance of learning models. However, similaritybased personalized federated learning methods may exacerbate the class imbalanced problem. In this paper, we propose a novel Dynamic Affinity-based Personalized Federated Learning model (DA-PFL) to alleviate the class imbalanced problem during federated learning. Specifically, we build an affinity metric from a complementary perspective to guide which clients should be aggregated. Then we design a dynamic aggregation strategy to dynamically aggregate clients based on the affinity metric in each round to reduce the class imbalanced risk. Extensive experiments show that the proposed DA-PFL model can significantly improve the accuracy of each
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09267</link><description>&lt;p&gt;
&#28145;&#24230;&#38480;&#20215;&#35746;&#21333;&#31807;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Limit Order Book Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#23574;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#22312;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#19978;&#20132;&#26131;&#30340;&#19968;&#32452;&#24322;&#36136;&#32929;&#31080;&#30340;&#39640;&#39057;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#8220;LOBFrame&#8221;&#65292;&#19968;&#20010;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#65292;&#24182;&#23450;&#37327;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#21452;&#37325;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#32929;&#31080;&#30340;&#24494;&#35266;&#32467;&#26500;&#29305;&#24449;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39640;&#39044;&#27979;&#33021;&#21147;&#19981;&#19968;&#23450;&#23545;&#24212;&#21487;&#25805;&#20316;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;&#38480;&#20215;&#35746;&#21333;&#31807;&#29615;&#22659;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#20934;&#30830;&#39044;&#27979;&#30340;&#27010;&#29575;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;HUDS&#65292;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21477;&#23376;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.09259</link><description>&lt;p&gt;
&#26159;&#21542;&#32473;&#25968;&#25454;&#36148;&#26631;&#31614;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;HUDS&#65292;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21477;&#23376;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36873;&#25321;&#26356;&#23567;&#30340;&#20195;&#34920;&#24615;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#26631;&#35760;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HUDS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;NMT&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#36827;&#34892;&#21477;&#23376;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09259v1 Announce Type: new  Abstract: Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection. HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them. It then clusters sentence embeddings within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36328;&#21463;&#35797;&#32773;&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#24615;&#33021;&#21644;&#36328;&#21463;&#35797;&#32773;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#21516;&#26102;&#21457;&#29616;&#26631;&#20934;CNN&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.09228</link><description>&lt;p&gt;
&#36328;&#21463;&#35797;&#32773;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for cross-subject Motor Imagery classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36328;&#21463;&#35797;&#32773;&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#24615;&#33021;&#21644;&#36328;&#21463;&#35797;&#32773;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#21516;&#26102;&#21457;&#29616;&#26631;&#20934;CNN&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26088;&#22312;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20309;&#26102;&#21487;&#33021;&#20986;&#38169;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25506;&#32034;&#20102;&#30830;&#23450;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#20063;&#31216;&#20026;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#24212;&#19982;&#27867;&#21270;&#35823;&#24046;&#30456;&#23545;&#24212;&#12290;&#36825;&#20123;&#26041;&#27861;&#29702;&#35770;&#19978;&#21487;&#29992;&#20110;&#39044;&#27979;&#30001;&#21463;&#35797;&#32773;&#38388;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#38169;&#20998;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#38169;&#20998;&#12290;&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#24615;&#33021;&#21644;&#36328;&#21463;&#35797;&#32773;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;Softmax&#36755;&#20986;&#30340;&#26631;&#20934;CNN&#20248;&#20110;&#19968;&#20123;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09228v1 Announce Type: new  Abstract: Uncertainty Quantification aims to determine when the prediction from a Machine Learning model is likely to be wrong. Computer Vision research has explored methods for determining epistemic uncertainty (also known as model uncertainty), which should correspond with generalisation error. These methods theoretically allow to predict misclassifications due to inter-subject variability. We applied a variety of Uncertainty Quantification methods to predict misclassifications for a Motor Imagery Brain Computer Interface. Deep Ensembles performed best, both in terms of classification performance and cross-subject Uncertainty Quantification performance. However, we found that standard CNNs with Softmax output performed better than some of the more advanced methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#28151;&#21512;&#36890;&#36947;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#30340;&#25968;&#25454;&#25193;&#23637;&#20248;&#21183;&#65292;&#33021;&#23545;&#25239;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#24536;&#21364;&#65292;&#23454;&#29616;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21019;&#26032;&#27169;&#22411;MCformer&#12290;</title><link>https://arxiv.org/abs/2403.09223</link><description>&lt;p&gt;
MCformer: &#28151;&#21512;&#36890;&#36947;Transformer&#23454;&#29616;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09223
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28151;&#21512;&#36890;&#36947;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#30340;&#25968;&#25454;&#25193;&#23637;&#20248;&#21183;&#65292;&#33021;&#23545;&#25239;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#24536;&#21364;&#65292;&#23454;&#29616;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21019;&#26032;&#27169;&#22411;MCformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22823;&#37327;&#29983;&#25104;&#65292;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#36890;&#36947;&#29420;&#31435;&#65288;CI&#65289;&#31574;&#30053;&#65292;&#20294;&#26159;CI&#31574;&#30053;&#38754;&#20020;&#30528;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#24536;&#21364;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#28151;&#21512;&#36890;&#36947;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;CI&#31574;&#30053;&#30340;&#25968;&#25454;&#25193;&#23637;&#20248;&#21183;&#21644;&#23545;&#25239;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#24536;&#21364;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20010;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MCformer&#65292;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09223v1 Announce Type: new  Abstract: The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting. In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence). Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy. The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features. However, the CI strategy faces the challenge of interchannel correlation forgetting. To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting. Based on this strategy, we introduce MCformer, a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Laplace&#36924;&#36817;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36873;&#25321;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19981;&#20339;&#21644;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26032;&#30340;&#26631;&#20934;&#22312;&#36136;&#37327;&#19978;&#19982;&#40644;&#37329;&#26631;&#20934;&#21160;&#24577;&#23884;&#22871;&#25277;&#26679;&#30456;&#24403;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.09215</link><description>&lt;p&gt;
Laplace&#36924;&#36817;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Laplace Approximation as Model Selection Criterion for Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09215
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Laplace&#36924;&#36817;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36873;&#25321;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19981;&#20339;&#21644;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26032;&#30340;&#26631;&#20934;&#22312;&#36136;&#37327;&#19978;&#19982;&#40644;&#37329;&#26631;&#20934;&#21160;&#24577;&#23884;&#22871;&#25277;&#26679;&#30456;&#24403;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26088;&#22312;&#25214;&#21040;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#25110;&#31616;&#26131;&#24615;&#26041;&#38754;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#26368;&#22909;&#26159;&#19977;&#32773;&#20860;&#39038;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#35780;&#20272;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#21363;&#25214;&#21040;&#19968;&#20010;&#25552;&#20379;&#25152;&#26377;&#36825;&#20123;&#20934;&#21017;&#20043;&#38388;&#26368;&#20339;&#26435;&#34913;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;Laplace&#36924;&#36817;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#26469;&#24212;&#23545;&#20197;&#24448;&#24037;&#20316;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#19981;&#20339;&#25110;&#20855;&#26377;&#20005;&#37325;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#22312;&#36136;&#37327;&#19978;&#19982;&#40644;&#37329;&#26631;&#20934;&#21160;&#24577;&#23884;&#22871;&#25277;&#26679;&#30456;&#24403;&#65292;&#32780;&#21448;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#20351;&#24471;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#19988;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09215v1 Announce Type: cross  Abstract: Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.09209</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#23621;&#20197;&#23454;&#26102;&#26816;&#27979;&#20869;&#37096;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#32452;&#32455;&#38754;&#20020;&#26469;&#33258;&#20869;&#37096;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#20808;&#21069;&#20851;&#20110;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65288;ITD&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#27979;&#24322;&#24120;&#29992;&#25143;&#25110;&#24322;&#24120;&#26102;&#38388;&#27573;&#65288;&#20363;&#22914;&#65292;&#19968;&#21608;&#25110;&#19968;&#22825;&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#33021;&#22312;&#26085;&#24535;&#20013;&#26377;&#25968;&#21313;&#19975;&#26465;&#27963;&#21160;&#65292;&#21363;&#20351;&#22312;&#19968;&#22825;&#20869;&#65292;&#19968;&#20010;&#29992;&#25143;&#20063;&#21487;&#33021;&#23384;&#22312;&#25968;&#21315;&#26465;&#27963;&#21160;&#65292;&#36825;&#38656;&#35201;&#39640;&#26114;&#30340;&#35843;&#26597;&#39044;&#31639;&#26469;&#39564;&#35777;&#24322;&#24120;&#29992;&#25143;&#25110;&#27963;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#26159;&#20107;&#21518;&#26041;&#27861;&#32780;&#19981;&#26159;&#23454;&#26102;&#26816;&#27979;&#65292;&#26080;&#27861;&#21450;&#26102;&#25253;&#21578;&#20869;&#37096;&#23041;&#32961;&#22312;&#24341;&#36215;&#25439;&#22833;&#20043;&#21069;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#27963;&#21160;&#32423;&#21035;&#23454;&#26102;ITD&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;LAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LAN&#21516;&#26102;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#19977;&#23618;&#32447;&#24615;&#32467;&#26500;&#30340;&#37096;&#20998;CBM&#20013;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27010;&#21270;&#38169;&#35823;&#30340;&#19978;&#30028;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#37096;&#20998;CBM&#20248;&#20110;&#26420;&#32032;CBM&#12290;</title><link>https://arxiv.org/abs/2403.09206</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27010;&#21270;&#38169;&#35823;&#22312;&#37096;&#20998;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#19978;&#30028;&#65306;&#37096;&#20998;CBM&#32988;&#36807;&#26420;&#32032;CBM
&lt;/p&gt;
&lt;p&gt;
Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19977;&#23618;&#32447;&#24615;&#32467;&#26500;&#30340;&#37096;&#20998;CBM&#20013;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27010;&#21270;&#38169;&#35823;&#30340;&#19978;&#30028;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#37096;&#20998;CBM&#20248;&#20110;&#26420;&#32032;CBM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.09206v1 &#31867;&#22411;&#36890;&#21578;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#26159;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;CBM&#20013;&#65292;&#23545;&#24212;&#20110;&#36755;&#20986;&#21407;&#22240;&#30340;&#27010;&#24565;&#34987;&#25554;&#20837;&#21040;&#26368;&#21518;&#19968;&#20010;&#20013;&#38388;&#23618;&#20316;&#20026;&#35266;&#23519;&#20540;&#12290;&#20154;&#20204;&#39044;&#26399;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#36755;&#20986;&#21644;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31867;&#20284;&#20110;&#32447;&#24615;&#22238;&#24402;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#37322;&#38656;&#35201;&#35266;&#23519;&#25152;&#26377;&#27010;&#24565;&#65292;&#24182;&#19988;&#38477;&#20302;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#37096;&#20998;CBM&#65288;PCBM&#65289;&#20351;&#29992;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#12290;&#23613;&#31649;&#19968;&#20123;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;PCBM&#30340;&#27867;&#21270;&#24615;&#33021;&#20960;&#20046;&#19982;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#39640;&#65292;&#20294;&#30001;&#20110;PCBM&#26159;&#22855;&#24322;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#20854;&#27867;&#21270;&#38169;&#35823;&#30340;&#29702;&#35770;&#34892;&#20026;&#23578;&#26410;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20855;&#26377;&#19977;&#23618;&#32447;&#24615;&#26550;&#26500;&#30340;PCBM&#20013;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09206v1 Announce Type: cross  Abstract: Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;</title><link>https://arxiv.org/abs/2403.09193</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#32441;&#29702;&#20559;&#35265;&#36824;&#26159;&#24418;&#29366;&#20559;&#35265;&#65292;&#25105;&#20204;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Vision Language Models Texture or Shape Biased and Can We Steer Them?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#30701;&#30701;&#20960;&#24180;&#20869;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26684;&#23616;&#65292;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#24212;&#29992;&#65292;&#20174;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21040;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#20877;&#21040;&#35270;&#35273;&#38382;&#31572;&#12290;&#19982;&#32431;&#35270;&#35273;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35775;&#38382;&#35270;&#35273;&#20869;&#23481;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#24341;&#21457;&#25105;&#20204;&#24605;&#32771;&#23427;&#20204;&#26159;&#21542;&#20063;&#19982;&#20154;&#31867;&#35270;&#35273;&#19968;&#33268; - &#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#26377;&#22810;&#22823;&#31243;&#24230;&#22320;&#37319;&#29992;&#20102;&#20154;&#31867;&#24341;&#23548;&#30340;&#35270;&#35273;&#20559;&#35265;&#65292;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#21482;&#26159;&#20174;&#32431;&#35270;&#35273;&#27169;&#22411;&#20013;&#32487;&#25215;&#20102;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20559;&#35265;&#26159;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21363;&#23616;&#37096;&#20449;&#24687;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;VLMs&#20013;&#30340;&#36825;&#31181;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#36890;&#24120;&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#20110;&#24418;&#29366;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36890;&#36807;&#25991;&#26412;&#36827;&#34892;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#30784;&#25237;&#24433;&#23618;&#65288;BPL&#65289;&#30340;DL&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#34920;&#31034;&#26469;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.09188</link><description>&lt;p&gt;
&#20351;&#29992;GC-MS&#20809;&#35889;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#35774;&#35745;&#29992;&#20110;&#31232;&#30095;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09188
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#30784;&#25237;&#24433;&#23618;&#65288;BPL&#65289;&#30340;DL&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#34920;&#31034;&#26469;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#21253;&#21547;&#25968;&#30334;&#19975;&#29978;&#33267;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#65292;&#24182;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#25968;&#25454;&#26368;&#21021;&#37117;&#20197;&#36866;&#24403;&#30340;&#24418;&#24335;&#23384;&#20648;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#65292;&#20363;&#22914;&#27668;&#30456;&#33394;&#35889;-&#36136;&#35889;&#65288;GC-MS&#65289;&#20809;&#35889;&#21644;DNA&#24207;&#21015;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#35768;&#22810;&#38646;&#20540;&#65292;&#31232;&#30095;&#25968;&#25454;&#24418;&#24335;&#23548;&#33268;&#20102;&#20248;&#21270;DL&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#30784;&#25237;&#24433;&#23618;&#65288;BPL&#65289;&#30340;DL&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#39044;&#26399;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#23558;&#26377;&#21161;&#20110;DL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#21644;&#24494;&#35843;&#36807;&#31243;&#12290;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#20174;GC-MS&#26816;&#27979;&#21040;&#30340;362&#31181;&#29305;&#27530;&#21654;&#21857;&#27668;&#21619;&#20809;&#35889;&#20316;&#20026;&#31232;&#30095;&#25968;&#25454;&#38598;&#30340;&#31034;&#20363;&#12290;BPL&#23618;&#20301;&#20110;DL&#27169;&#22411;&#30340;&#24320;&#22836;&#12290;&#35813;&#23618;&#20013;&#30340;&#21487;&#35843;&#21442;&#25968;&#26159;&#21487;&#23398;&#20064;&#30340;&#25237;&#24433;&#36724;&#65292;&#23427;&#20204;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09188v1 Announce Type: new  Abstract: Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data. However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models. A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation. The transformed data is expected to facilitate the gradient calculation and finetuned process in a DL training process. The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS. The BPL layer was placed at the beginning of the DL model. The tunable parameters in the layer were learnable projected axes that were the bases of a new 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#24191;&#20041;&#30456;&#20851;&#24615;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#30340;&#24212;&#29992;&#65292;&#20197;&#22788;&#29702;&#26684;&#25289;&#26031;&#26364;&#27969;&#24418;&#65292;&#25552;&#20986;&#27169;&#22411;&#36820;&#22238;&#19968;&#32452;&#21407;&#22411;&#23376;&#31354;&#38388;&#21644;&#30456;&#20851;&#24615;&#21521;&#37327;&#65292;&#20026;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.09183</link><description>&lt;p&gt;
&#24191;&#20041;&#30456;&#20851;&#24615;&#23398;&#20064;&#26684;&#25289;&#26031;&#26364;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalized Relevance Learning Grassmann Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#24191;&#20041;&#30456;&#20851;&#24615;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#30340;&#24212;&#29992;&#65292;&#20197;&#22788;&#29702;&#26684;&#25289;&#26031;&#26364;&#27969;&#24418;&#65292;&#25552;&#20986;&#27169;&#22411;&#36820;&#22238;&#19968;&#32452;&#21407;&#22411;&#23376;&#31354;&#38388;&#21644;&#30456;&#20851;&#24615;&#21521;&#37327;&#65292;&#20026;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#30456;&#26426;&#30340;&#36827;&#27493;&#65292;&#23481;&#26131;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#25910;&#38598;&#26469;&#33258;&#23545;&#35937;&#30340;&#22810;&#20010;&#22270;&#20687;&#65288;&#25110;&#35270;&#39057;&#65289;&#12290;&#22240;&#27492;&#65292;&#22270;&#20687;&#38598;&#20998;&#31867;&#24341;&#36215;&#20102;&#26356;&#22810;&#20851;&#27880;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;&#24314;&#27169;&#22270;&#20687;&#38598;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#26159;&#23376;&#31354;&#38388;&#65292;&#23427;&#20204;&#24418;&#25104;&#31216;&#20026;&#26684;&#25289;&#26031;&#26364;&#27969;&#24418;&#30340;&#27969;&#24418;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#24191;&#20041;&#30456;&#20851;&#24615;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#30340;&#24212;&#29992;&#25299;&#23637;&#21040;&#22788;&#29702;&#26684;&#25289;&#26031;&#26364;&#27969;&#24418;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36820;&#22238;&#19968;&#32452;&#21407;&#22411;&#23376;&#31354;&#38388;&#21644;&#19968;&#20010;&#30456;&#20851;&#24615;&#21521;&#37327;&#12290;&#21407;&#22411;&#27169;&#22411;&#20102;&#31867;&#20869;&#30340;&#20856;&#22411;&#34892;&#20026;&#65292;&#32780;&#30456;&#20851;&#24615;&#22240;&#23376;&#25351;&#23450;&#20102;&#20998;&#31867;&#20219;&#21153;&#20013;&#26368;&#20855;&#36776;&#21035;&#24615;&#30340;&#20027;&#35201;&#21521;&#37327;&#65288;&#25110;&#22270;&#20687;&#65289;&#12290;&#23427;&#20204;&#36890;&#36807;&#31361;&#20986;&#24433;&#21709;&#20998;&#31867;&#39044;&#27979;&#30340;&#37325;&#35201;&#22270;&#20687;&#21644;&#20687;&#32032;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#20102;&#21407;&#22411;&#65292;&#26032;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#20063;&#24471;&#21040;&#20102;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09183v1 Announce Type: cross  Abstract: Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions. Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them. A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold. In this contribution, we extend the application of Generalized Relevance Learning Vector Quantization to deal with Grassmann manifold. The proposed model returns a set of prototype subspaces and a relevance vector. While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task. They both provide insights into the model's decisions by highlighting influential images and pixels for predictions. Moreover, due to learning prototypes, the model complexity of the new method during inference 
&lt;/p&gt;</description></item><item><title>ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09171</link><description>&lt;p&gt;
ADEdgeDrop&#65306;&#29992;&#20110;&#24378;&#20581;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09171
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21508;&#31181;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#23637;&#31034;&#20102;&#20174;&#37051;&#36817;&#33410;&#28857;&#25910;&#38598;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#21644;&#20887;&#20313;&#30340;&#22270;&#25968;&#25454;&#36896;&#25104;&#30340;&#24046;&#30340;&#27867;&#21270;&#21644;&#33030;&#24369;&#30340;&#31283;&#20581;&#24615;&#38480;&#21046;&#20102;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;Graph Augmentation Learning&#65288;GAL&#65289;&#20013;&#65292;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;GNNs&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#21024;&#38500;&#36793;&#32536;&#36890;&#24120;&#20250;&#32469;&#36807;&#20851;&#38190;&#36793;&#32536;&#65292;&#20174;&#32780;&#21066;&#24369;&#28040;&#24687;&#20256;&#36882;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65288;ADEdgeDrop&#65289;&#65292;&#21033;&#29992;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#24341;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21040;&#19981;&#21516;&#30340;GNN&#20027;&#24178;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09171v1 Announce Type: cross  Abstract: Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#22312;&#29420;&#31435;&#20195;&#29702;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#20013;&#30830;&#23450;&#23398;&#20064;&#32467;&#26524;&#30340;&#32622;&#20449;&#27700;&#24179;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09141</link><description>&lt;p&gt;
&#22312;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09141
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#22312;&#29420;&#31435;&#20195;&#29702;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#20013;&#30830;&#23450;&#23398;&#20064;&#32467;&#26524;&#30340;&#32622;&#20449;&#27700;&#24179;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#26377;&#38480;&#33258;&#20027;&#22788;&#29702;&#33021;&#21147;&#30340;&#20302;&#21151;&#29575;&#21333;&#20803;&#65292;&#36793;&#32536;&#29289;&#32852;&#32593;&#35774;&#22791;&#38543;&#30528;FPGA&#21644;AI&#21152;&#36895;&#22120;&#30340;&#24341;&#20837;&#32780;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#19968;&#36827;&#27493;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#31361;&#26174;&#20102;&#36793;&#32536;AI&#30340;&#23454;&#29992;&#24615;&#12290;&#36825;&#31181;&#36827;&#27493;&#24341;&#20837;&#20102;&#26032;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#33021;&#28304;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#38480;&#21046;&#20248;&#21270;AI&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#23454;&#29616;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#21327;&#20316;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#20043;&#19968;&#26159;&#35299;&#20915;&#30830;&#23450;&#23398;&#20064;&#32467;&#26524;&#30340;&#32622;&#20449;&#27700;&#24179;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#29420;&#31435;&#20195;&#29702;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#31649;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09141v1 Announce Type: cross  Abstract: Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators. This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI. Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments. Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities. A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents. To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environme
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25361;&#25112;&#30340;&#26368;&#20248;Top-Two&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09123</link><description>&lt;p&gt;
&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#27969;&#20307;&#20998;&#26512;&#30340;&#26368;&#20248;Top-Two&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Top-Two Method for Best Arm Identification and Fluid Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25361;&#25112;&#30340;&#26368;&#20248;Top-Two&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Top-2&#26041;&#27861;&#22312;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#21363;&#22312;&#26377;&#38480;&#25968;&#37327;&#33218;&#20013;&#20855;&#26377;&#26368;&#22823;&#22343;&#20540;&#30340;&#33218;&#65292;&#35813;&#31639;&#27861;&#22312;&#20219;&#20309;&#39034;&#24207;&#27493;&#39588;&#20013;&#29420;&#31435;&#22320;&#20197;&#22266;&#23450;&#27010;&#29575; &#946; &#25289;&#21160;&#32463;&#39564;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#25289;&#21160;&#26368;&#20339;&#25361;&#25112;&#32773;&#33218;&#12290;&#36873;&#25321;&#38169;&#35823;&#30340;&#27010;&#29575;&#20445;&#35777;&#22312;&#25351;&#23450;&#30340;&#948; &gt;0&#20197;&#19979;&#12290;&#23545;&#20110;BAI&#38382;&#39064;&#65292;&#24050;&#30693;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#948; &#8594; 0&#26102;&#19982;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#25554;&#20214;&#26041;&#27861;&#28176;&#36817;&#21305;&#37197;&#12290; &#23545;&#20110;&#20219;&#20309; &#946; &#8712;&#65288;0,1&#65289;&#30340;&#19978;&#36848;Top 2&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22987;&#32456;&#20445;&#25345;&#22312;&#19979;&#30028;&#30340;&#24120;&#25968;&#33539;&#22260;&#20869;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#19982;&#19979;&#30028;&#21305;&#37197;&#30340;&#26368;&#20339; &#946; &#24050;&#34987;&#35777;&#26126;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;Top-2&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20998;&#37197;&#38170;&#28857;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09123v1 Announce Type: new  Abstract: Top-$2$ methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\beta$, and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\delta &gt;0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\delta \rightarrow 0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\beta \in (0,1)$ has sample complexity within a constant of the lower bound. However, determining the optimal $\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;R-PCA&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;PCA&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#38754;&#20248;&#20110;R-PCA&#65292;&#20294;&#22312;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#26041;&#38754;&#34920;&#29616;&#25509;&#36817;&#20934;&#30830;&#24230;&#25968;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.09117</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Randomized Principal Component Analysis for Hyperspectral Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;R-PCA&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;PCA&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#38754;&#20248;&#20110;R-PCA&#65292;&#20294;&#22312;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#26041;&#38754;&#34920;&#29616;&#25509;&#36817;&#20934;&#30830;&#24230;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#32473;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38477;&#32500;&#26159;&#24517;&#35201;&#30340;&#20197;&#20943;&#23567;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#38543;&#26426;&#25237;&#24433;&#20026;&#38477;&#20302;&#32500;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#38543;&#26426;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;R-PCA&#65289;&#12290;&#22312;&#36825;&#39033;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#23558;&#29305;&#24449;&#25968;&#20943;&#23569;&#21040;20&#21644;30&#20197;&#36827;&#34892;&#20004;&#20010;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65288;Indian Pines&#21644;Pavia University&#65289;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;PCA&#22312;SVM&#26041;&#38754;&#20248;&#20110;R-PCA&#65292;&#20294;&#22312;LightGBM&#26041;&#38754;&#33719;&#24471;&#20102;&#25509;&#36817;&#30340;&#20934;&#30830;&#24230;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09117v1 Announce Type: cross  Abstract: The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets. In such a case, dimensionality reduction is necessary to decrease the computational complexity. The random projections open up new ways of dimensionality reduction, especially for large data sets. In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated. In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University). The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM. The highest classification accurac
&lt;/p&gt;</description></item><item><title>AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09113</link><description>&lt;p&gt;
AutoLoRA&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#21160;&#35843;&#25972;&#30697;&#38453;&#31209;&#22312;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09113
&lt;/p&gt;
&lt;p&gt;
AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#21457;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#20043;&#19978;&#24494;&#35843;&#20302;&#31209;&#22686;&#37327;&#26356;&#26032;&#30697;&#38453;&#65292;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;LoRA&#22312;&#25152;&#26377;&#23618;&#20013;&#22343;&#21248;&#20998;&#37197;&#31209;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31351;&#20030;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31209;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24494;&#35843;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoLoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#12290;AutoLoRA&#23558;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#19982;&#36873;&#25321;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#35813;&#21464;&#37327;&#20915;&#23450;&#20102;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26159;&#21542;&#24212;&#35813;&#34987;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
&lt;/p&gt;</description></item><item><title>&#23558;&#31232;&#30095;&#35782;&#21035;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;SINDy-RL&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#25928;&#35299;&#37322;&#24615;&#36824;&#26377;&#22312;&#20302;&#25968;&#25454;&#21046;&#24230;&#19979;&#21019;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09110</link><description>&lt;p&gt;
SINDy-RL: &#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09110
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31232;&#30095;&#35782;&#21035;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;SINDy-RL&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#25928;&#35299;&#37322;&#24615;&#36824;&#26377;&#22312;&#20302;&#25968;&#25454;&#21046;&#24230;&#19979;&#21019;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#26174;&#31034;&#20986;&#22312;&#19982;&#22797;&#26434;&#21160;&#24577;&#29615;&#22659;&#20013;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#25511;&#21046;&#31574;&#30053;&#20013;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#65292;&#20363;&#22914;&#31283;&#23450;&#25176;&#21345;&#39532;&#20811;&#32858;&#21464;&#21453;&#24212;&#22534;&#30340;&#30913;&#27969;&#20307;&#21160;&#21147;&#23398;&#25110;&#20351;&#29289;&#20307;&#22312;&#27969;&#20307;&#27969;&#21160;&#20013;&#21463;&#21040;&#30340;&#38459;&#21147;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#23545;&#35768;&#22810;&#24212;&#29992;&#32780;&#35328;&#25104;&#26412;&#21487;&#33021;&#36807;&#39640;&#12290;&#21478;&#22806;&#65292;&#20381;&#36182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#30340;&#40657;&#30418;&#31574;&#30053;&#65292;&#21487;&#33021;&#22312;&#26576;&#20123;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20351;&#29992;&#26102;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31232;&#30095;&#35782;&#21035;&#65288;SINDy&#65289;&#65292;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#21046;&#24230;&#19979;&#21019;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#21069;&#26223;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;SINDy-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;SINDy&#21644;DRL&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09110v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpret
&lt;/p&gt;</description></item><item><title>S^2MVTC&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#23398;&#20064;&#36328;&#35270;&#22270;&#20869;&#37096;&#21644;&#35270;&#22270;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09107</link><description>&lt;p&gt;
S^2MVTC&#65306;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09107
&lt;/p&gt;
&lt;p&gt;
S^2MVTC&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#23398;&#20064;&#36328;&#35270;&#22270;&#20869;&#37096;&#21644;&#35270;&#22270;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09107v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#35268;&#27169;&#22810;&#35270;&#22270;&#32858;&#31867;&#22240;&#20854;&#22312;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#25928;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#25506;&#32034;&#38170;&#22270;&#25110;&#25237;&#24433;&#30697;&#38453;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#26469;&#23547;&#27714;&#29992;&#20110;&#32858;&#31867;&#30340;&#20849;&#35782;&#23884;&#20837;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#22810;&#35270;&#22270;&#24352;&#37327;&#32858;&#31867;&#65288;S^2MVTC&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#23398;&#20064;&#36328;&#35270;&#22270;&#20869;&#37096;&#21644;&#35270;&#22270;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#19981;&#21516;&#35270;&#22270;&#30340;&#23884;&#20837;&#29305;&#24449;&#21472;&#21152;&#25104;&#24352;&#37327;&#24182;&#23545;&#20854;&#36827;&#34892;&#26059;&#36716;&#26469;&#26500;&#24314;&#23884;&#20837;&#29305;&#24449;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#20302;&#39057;&#36817;&#20284;&#65288;TLFA&#65289;&#31639;&#23376;&#65292;&#23558;&#22270;&#30456;&#20284;&#24615;&#34701;&#20837;&#21040;&#23884;&#20837;&#29305;&#24449;&#23398;&#20064;&#20013;&#65292;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;&#19981;&#21516;&#35270;&#22270;&#20869;&#37096;&#23884;&#20837;&#29305;&#24449;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#19968;&#33268;&#24615;&#32422;&#26463;&#34987;&#24212;&#29992;&#20110;&#23884;&#20837;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09107v1 Announce Type: new  Abstract: Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding featur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23548;&#26631;&#31614;&#32454;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#33258;&#25105;&#32454;&#21270;&#26356;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#24182;&#21160;&#24577;&#23558;&#33258;&#25105;&#33976;&#39311;&#27169;&#22411;&#30340;&#30693;&#35782;&#32435;&#20837;&#24403;&#21069;&#27169;&#22411;&#26469;&#28040;&#38500;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09101</link><description>&lt;p&gt;
&#36719;&#21270;&#20197;&#20445;&#21355;&#65306;&#36890;&#36807;&#33258;&#23548;&#26631;&#31614;&#32454;&#21270;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23548;&#26631;&#31614;&#32454;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#33258;&#25105;&#32454;&#21270;&#26356;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#24182;&#21160;&#24577;&#23558;&#33258;&#25105;&#33976;&#39311;&#27169;&#22411;&#30340;&#30693;&#35782;&#32435;&#20837;&#24403;&#21069;&#27169;&#22411;&#26469;&#28040;&#38500;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#30446;&#21069;&#26159;&#33719;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AT&#26041;&#27861;&#36973;&#21463;&#40065;&#26834;&#36807;&#25311;&#21512;&#65292;&#21363;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26354;&#32447;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#26799;&#24230;&#33539;&#25968;&#30340;&#35282;&#24230;&#35782;&#21035;&#20102;&#40065;&#26834;&#36807;&#25311;&#21512;&#19982;AT&#20013;&#22024;&#26434;&#26631;&#31614;&#36807;&#24230;&#35760;&#24518;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#37492;&#20110;&#27492;&#31867;&#26631;&#31614;&#22122;&#22768;&#20027;&#35201;&#26159;&#30001;&#20998;&#24067;&#19981;&#21305;&#37197;&#21644;&#19981;&#24688;&#24403;&#30340;&#26631;&#31614;&#20998;&#37197;&#23548;&#33268;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AT&#30340;&#26631;&#31614;&#32454;&#21270;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#33258;&#23548;&#26631;&#31614;&#32454;&#21270;&#39318;&#20808;&#20174;&#36807;&#20110;&#33258;&#20449;&#30340;&#30828;&#26631;&#31614;&#20013;&#33258;&#25105;&#32454;&#21270;&#20986;&#26356;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#28982;&#21518;&#36890;&#36807;&#21160;&#24577;&#23558;&#33258;&#25105;&#33976;&#39311;&#27169;&#22411;&#30340;&#30693;&#35782;&#32435;&#20837;&#24403;&#21069;&#27169;&#22411;&#20174;&#32780;&#26080;&#38656;&#22806;&#37096;&#25945;&#24072;&#26469;&#26657;&#20934;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09101v1 Announce Type: new  Abstract: Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirica
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21152;&#26631;&#35760;&#32452;&#32455;&#36827;&#34892;&#34394;&#25311;&#21452;&#25240;&#23556;&#25104;&#20687;&#21644;&#34394;&#25311;&#21018;&#26524;&#32418;&#26579;&#33394;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;&#33258;&#21457;&#33639;&#20809;&#22270;&#20687;&#36716;&#25442;&#20026;&#26126;&#22330;&#21644;&#20559;&#25391;&#20809;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.09100</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21457;&#33639;&#20809;&#26174;&#24494;&#38236;&#21644;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#23545;&#19981;&#21152;&#26631;&#35760;&#32452;&#32455;&#20013;&#28096;&#31881;&#26679;&#27785;&#31215;&#30340;&#34394;&#25311;&#21452;&#25240;&#23556;&#25104;&#20687;&#21644;&#32452;&#32455;&#23398;&#26579;&#33394;
&lt;/p&gt;
&lt;p&gt;
Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09100
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21152;&#26631;&#35760;&#32452;&#32455;&#36827;&#34892;&#34394;&#25311;&#21452;&#25240;&#23556;&#25104;&#20687;&#21644;&#34394;&#25311;&#21018;&#26524;&#32418;&#26579;&#33394;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;&#33258;&#21457;&#33639;&#20809;&#22270;&#20687;&#36716;&#25442;&#20026;&#26126;&#22330;&#21644;&#20559;&#25391;&#20809;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#24615;&#28096;&#31881;&#26679;&#21464;&#24615;&#30149;&#26159;&#19968;&#32452;&#30142;&#30149;&#65292;&#20854;&#29305;&#24449;&#26159;&#21508;&#31181;&#22120;&#23448;&#21644;&#32452;&#32455;&#20013;&#38169;&#25240;&#34507;&#30333;&#30340;&#27785;&#31215;&#65292;&#23548;&#33268;&#22120;&#23448;&#21151;&#33021;&#36880;&#28176;&#21463;&#25439;&#21644;&#34928;&#31469;&#12290;&#21018;&#26524;&#32418;&#26579;&#33394;&#26159;&#22312;&#32452;&#32455;&#20999;&#29255;&#20013;&#21487;&#35270;&#21270;&#28096;&#31881;&#26679;&#27785;&#31215;&#30340;&#37329;&#26631;&#20934;&#21270;&#23398;&#26579;&#33394;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#19982;&#38169;&#25240;&#34507;&#30333;&#24418;&#25104;&#22797;&#21512;&#29289;&#65292;&#22312;&#20559;&#25391;&#20809;&#26174;&#24494;&#38236;&#19979;&#21576;&#29616;&#21452;&#25240;&#23556;&#22270;&#26696;&#12290;&#28982;&#32780;&#65292;&#21018;&#26524;&#32418;&#26579;&#33394;&#26159;&#36153;&#26102;&#19988;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#35823;&#35786;&#65292;&#22240;&#20026;&#28096;&#31881;&#37327;&#12289;&#26579;&#33394;&#36136;&#37327;&#21644;&#19987;&#23478;&#35299;&#37322;&#21463;&#21040;&#21464;&#21270;&#65292;&#38656;&#35201;&#36890;&#36807;&#23545;&#32452;&#32455;&#22312;&#20559;&#25391;&#26174;&#24494;&#38236;&#19979;&#30340;&#25163;&#21160;&#26816;&#26597;&#26469;&#36827;&#34892;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#39318;&#27425;&#28436;&#31034;&#20102;&#23545;&#19981;&#21152;&#26631;&#35760;&#20154;&#31867;&#32452;&#32455;&#30340;&#34394;&#25311;&#21452;&#25240;&#23556;&#25104;&#20687;&#21644;&#34394;&#25311;&#21018;&#26524;&#32418;&#26579;&#33394;&#65292;&#34920;&#26126;&#21333;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24555;&#36895;&#22320;&#23558;&#19981;&#21152;&#26631;&#35760;&#32452;&#32455;&#20999;&#29255;&#30340;&#33258;&#21457;&#33639;&#20809;&#22270;&#20687;&#36716;&#25442;&#20026;&#26126;&#22330;&#21644;&#20559;&#25391;&#20809;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09100v1 Announce Type: cross  Abstract: Systemic amyloidosis is a group of diseases characterized by the deposition of misfolded proteins in various organs and tissues, leading to progressive organ dysfunction and failure. Congo red stain is the gold standard chemical stain for the visualization of amyloid deposits in tissue sections, as it forms complexes with the misfolded proteins and shows a birefringence pattern under polarized light microscopy. However, Congo red staining is tedious and costly to perform, and prone to false diagnoses due to variations in the amount of amyloid, staining quality and expert interpretation through manual examination of tissue under a polarization microscope. Here, we report the first demonstration of virtual birefringence imaging and virtual Congo red staining of label-free human tissue to show that a single trained neural network can rapidly transform autofluorescence images of label-free tissue sections into brightfield and polarized lig
&lt;/p&gt;</description></item><item><title>DGDA&#26041;&#27861;&#36890;&#36807;&#22312;GDA&#26356;&#26032;&#20013;&#24341;&#20837;&#32791;&#25955;&#39033;&#65292;&#35299;&#20915;&#20102;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25391;&#33633;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#30340;&#26356;&#20248;&#36234;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.09090</link><description>&lt;p&gt;
&#32791;&#25955;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65306;&#19968;&#31181;&#21463;&#25511;&#21046;&#35770;&#21551;&#21457;&#30340;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09090
&lt;/p&gt;
&lt;p&gt;
DGDA&#26041;&#27861;&#36890;&#36807;&#22312;GDA&#26356;&#26032;&#20013;&#24341;&#20837;&#32791;&#25955;&#39033;&#65292;&#35299;&#20915;&#20102;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25391;&#33633;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#30340;&#26356;&#20248;&#36234;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;(GDA)&#26041;&#27861;&#29992;&#20110;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20250;&#20135;&#29983;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#30340;&#25391;&#33633;&#34892;&#20026;&#65292;&#20363;&#22914;&#22312;&#21452;&#32447;&#24615;&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;GDA&#26356;&#26032;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#32791;&#25955;&#39033;&#65292;&#20197;&#20943;&#24369;&#36825;&#20123;&#25391;&#33633;&#12290;&#25152;&#25552;&#20986;&#30340;&#32791;&#25955;&#26799;&#24230;&#19979;&#38477;&#65288;DGDA&#65289;&#26041;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#22312;&#19968;&#20010;&#29366;&#24577;&#22686;&#24191;&#21644;&#27491;&#21017;&#21270;&#30340;&#38797;&#28857;&#20989;&#25968;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;GDA&#65292;&#32780;&#19981;&#20005;&#26684;&#24341;&#20837;&#39069;&#22806;&#30340;&#20984;&#24615;/&#20985;&#24615;&#12290;&#25105;&#20204;&#22312;&#21452;&#32447;&#24615;&#21644;&#24378;&#20984;-&#24378;&#20985;&#35774;&#32622;&#20013;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;DGDA&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;DGDA&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;GDA&#12289;Extra-Gradient (EG) &#21644;&#20048;&#35266;&#24615;GDA&#65289;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DGDA&#36229;&#36234;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25968;&#20540;&#31034;&#20363;&#25903;&#25345;&#25105;&#20204;&#30340;&#35828;&#27861;&#65292;&#23637;&#31034;&#20102;DGDA&#22312;&#35299;&#20915;&#38797;&#28857;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09090v1 Announce Type: cross  Abstract: Gradient Descent Ascent (GDA) methods for min-max optimization problems typically produce oscillatory behavior that can lead to instability, e.g., in bilinear settings. To address this problem, we introduce a dissipation term into the GDA updates to dampen these oscillations. The proposed Dissipative GDA (DGDA) method can be seen as performing standard GDA on a state-augmented and regularized saddle function that does not strictly introduce additional convexity/concavity. We theoretically show the linear convergence of DGDA in the bilinear and strongly convex-strongly concave settings and assess its performance by comparing DGDA with other methods such as GDA, Extra-Gradient (EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these methods, achieving superior convergence rates. We support our claims with two numerical examples that showcase DGDA's effectiveness in solving saddle point problems.
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#20174;&#20005;&#37325;&#24310;&#36831;&#30340;&#23458;&#25143;&#31471;&#23398;&#20064;&#65292;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24773;&#20917;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;FARe-DUST&#21644;FeAST-on-MSG&#12290;</title><link>https://arxiv.org/abs/2403.09086</link><description>&lt;p&gt;
&#20174;&#32852;&#37030;&#23398;&#20064;&#20013;&#23398;&#20064;&#36831;&#21040;&#30340;&#23458;&#25143;&#31471;
&lt;/p&gt;
&lt;p&gt;
Learning from straggler clients in federated learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09086
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#20174;&#20005;&#37325;&#24310;&#36831;&#30340;&#23458;&#25143;&#31471;&#23398;&#20064;&#65292;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24773;&#20917;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;FARe-DUST&#21644;FeAST-on-MSG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26377;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#22815;&#20174;&#36820;&#22238;&#20855;&#26377;&#26174;&#33879;&#26102;&#38388;&#24310;&#36831;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#23458;&#25143;&#35774;&#22791;&#20013;&#36827;&#34892;&#23398;&#20064;&#65311;&#23398;&#20064;&#25928;&#26524;&#26159;&#21542;&#21487;&#33021;&#21463;&#21040;&#23458;&#25143;&#31471;&#22312;&#34987;&#23433;&#25490;&#21518;&#20960;&#20998;&#38047;&#12289;&#20960;&#23567;&#26102;&#25110;&#20960;&#22825;&#25165;&#25253;&#21578;&#22238;&#26469;&#30340;&#24433;&#21709;&#65311;&#36890;&#36807;&#24320;&#21457;&#30001;&#23454;&#38469;&#24212;&#29992;&#25351;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#23458;&#25143;&#31471;&#24310;&#36831;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20687;FedAvg&#21644;FedAdam&#36825;&#26679;&#30340;&#21516;&#27493;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#21450;&#24322;&#27493;FedBuff&#31639;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#25152;&#26377;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#37117;&#38590;&#20197;&#20174;&#20005;&#37325;&#24310;&#36831;&#30340;&#23458;&#25143;&#31471;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20462;&#25913;&#65292;&#21253;&#25324;&#33976;&#39311;&#27491;&#21017;&#21270;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;FARe-DUST&#21644;FeAST-on-MSG&#65292;&#20998;&#21035;&#22522;&#20110;&#33976;&#39311;&#21644;&#24179;&#22343;&#21270;&#12290;&#23545;EMNIST&#12289;CIFAR-100&#21644;StackOverflow&#22522;&#20934;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09086v1 Announce Type: new  Abstract: How well do existing federated learning algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo simulations of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including distillation regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow benchmark federated learning
&lt;/p&gt;</description></item><item><title>&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09066</link><description>&lt;p&gt;
Continual Learning&#20013;&#30340;&#36229;&#21442;&#25968;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters in Continual Learning: a Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09066
&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#26088;&#22312;&#22312;CL&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#32531;&#35299;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35843;&#25972;&#27599;&#31181;&#31639;&#27861;&#30340;&#36866;&#24403;&#36229;&#21442;&#25968;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20027;&#24352;&#29616;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#26082;&#19981;&#20999;&#23454;&#38469;&#65292;&#20063;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09054</link><description>&lt;p&gt;
Keyformer&#65306;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#36873;&#25321;&#20943;&#23569;KV&#32531;&#23384;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#29983;&#25104;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#30784;&#26550;&#26500;&#12290;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25512;&#26029;&#36807;&#31243;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#25552;&#31034;&#22788;&#29702;&#21644;&#26631;&#35760;&#29983;&#25104;&#12290;&#26631;&#35760;&#29983;&#25104;&#65292;&#26500;&#25104;&#20102;&#22823;&#37096;&#20998;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20027;&#35201;&#28041;&#21450;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#21644;&#19982;&#38190;-&#20540;(KV)&#32531;&#23384;&#20132;&#20114;&#12290;&#30001;&#20110;&#20174;&#23384;&#20648;&#31995;&#32479;&#20256;&#36755;&#26435;&#37325;&#21644;KV&#32531;&#23384;&#20540;&#21040;&#35745;&#31639;&#21333;&#20803;&#30340;&#24320;&#38144;&#65292;&#36825;&#19968;&#38454;&#27573;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#20869;&#23384;&#29942;&#39048;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#21644;&#22823;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#36825;&#20004;&#32773;&#23545;LLMs&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;  &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#8220;Keyformer&#8221;&#65292;&#20197;&#32531;&#35299;&#19982;KV&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;Keyformer&#21033;&#29992;&#20102;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#22823;&#32422;90
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09053</link><description>&lt;p&gt;
&#36208;&#21521;&#27169;&#22411;&#33976;&#39311;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a theory of model distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33976;&#39311;&#26159;&#23558;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26367;&#25442;&#20026;&#31616;&#21270;&#27169;&#22411;&#26469;&#36817;&#20284;&#21407;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20851;&#20110;&#27169;&#22411;&#33976;&#39311;&#30340;&#31243;&#24230;&#12289;&#25152;&#38656;&#36816;&#34892;&#26102;&#38388;&#21644;&#25968;&#25454;&#37327;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#22823;&#22810;&#26410;&#35299;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#22987;&#20102;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;PAC-&#33976;&#39311; [Val84]&#65292;&#25552;&#20986;&#20102;&#25552;&#21462;&#35757;&#32451;&#26435;&#37325;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#8220;&#32447;&#24615;&#34920;&#31034;&#20551;&#35774;&#8221;&#23558;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#33976;&#39311;&#25104;&#31616;&#26126;&#26126;&#20102;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#65292;&#36824;&#35777;&#26126;&#20102;&#33976;&#39311;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#22312;&#34920;&#24449;&#20854;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;FedPLVM&#36890;&#36807;&#24314;&#31435;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#21644;&#20351;&#29992;&#26032;&#22411;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.09048</link><description>&lt;p&gt;
&#39535;&#26381;&#24322;&#26500;&#25968;&#25454;&#22495;&#20013;&#32852;&#37030;&#21407;&#22411;&#23398;&#20064;&#20013;&#30340;&#36328;&#39046;&#22495;&#34920;&#31034;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09048
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;FedPLVM&#36890;&#36807;&#24314;&#31435;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#21644;&#20351;&#29992;&#26032;&#22411;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#20294;&#29616;&#23454;&#22330;&#26223;&#20013;&#36890;&#24120;&#28041;&#21450;&#24322;&#26500;&#25968;&#25454;&#39046;&#22495;&#12290;&#32852;&#37030;&#21407;&#22411;&#23398;&#20064;&#65288;FedPL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#24179;&#22343;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#21407;&#22411;&#26469;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FedPL&#26041;&#27861;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21019;&#24314;&#30456;&#21516;&#25968;&#37327;&#30340;&#21407;&#22411;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20351;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#20943;&#36731;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedPLVM&#65292;&#23427;&#24314;&#31435;&#20102;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09048v1 Announce Type: new  Abstract: Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and pres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.09039</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#12290;STRIPE&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#20998;&#21035;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09039v1 Announce Type: cross  Abstract: Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;DiTMoS&#65292;&#36890;&#36807;&#36873;&#25321;&#22120;-&#20998;&#31867;&#22120;&#26550;&#26500;&#21644;&#22810;&#26679;&#24615;&#31574;&#30053;&#65292;&#26500;&#24314;&#23567;&#22411;/&#24369;&#27169;&#22411;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.09035</link><description>&lt;p&gt;
&#25506;&#31350;&#24494;&#25511;&#21046;&#22120;&#19978;&#22810;&#26679;&#24494;&#23567;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;DiTMoS&#65292;&#36890;&#36807;&#36873;&#25321;&#22120;-&#20998;&#31867;&#22120;&#26550;&#26500;&#21644;&#22810;&#26679;&#24615;&#31574;&#30053;&#65292;&#26500;&#24314;&#23567;&#22411;/&#24369;&#27169;&#22411;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#33455;&#29255;&#36164;&#28304;&#21463;&#38480;&#12290;&#26412;&#25991;&#20174;&#26500;&#24314;&#23567;&#22411;/&#24369;&#27169;&#22411;&#24182;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#21453;&#21521;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DiTMoS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#20855;&#26377;&#36873;&#25321;&#22120;-&#20998;&#31867;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#36873;&#25321;&#22120;&#23558;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#23450;&#20301;&#21040;&#36866;&#24403;&#30340;&#20998;&#31867;&#22120;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;DiTMoS&#24314;&#31435;&#22312;&#19968;&#20010;&#20851;&#38190;&#27934;&#35265;&#19978;&#65306;&#24369;&#27169;&#22411;&#30340;&#32452;&#21512;&#21487;&#20197;&#34920;&#29616;&#20986;&#39640;&#22810;&#26679;&#24615;&#65292;&#23427;&#20204;&#30340;&#32852;&#21512;&#21487;&#20197;&#26174;&#30528;&#25552;&#21319;&#20934;&#30830;&#24615;&#19978;&#38480;&#12290;&#20026;&#20102;&#25509;&#36817;&#36825;&#20010;&#19978;&#38480;&#65292;DiTMoS&#24341;&#20837;&#20102;&#21253;&#25324;&#22686;&#21152;&#20998;&#31867;&#22120;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#35757;&#32451;&#25968;&#25454;&#25286;&#20998;&#22312;&#20869;&#30340;&#19977;&#31181;&#31574;&#30053;&#65292;&#23545;&#25239;&#36873;&#25321;&#22120;-&#20998;&#31867;&#22120;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09035v1 Announce Type: new  Abstract: Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;</title><link>https://arxiv.org/abs/2403.09032</link><description>&lt;p&gt;
CodeUltraFeedback&#65306;&#19968;&#31181;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09032
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#25991;&#26412;LLMs&#30340;&#36755;&#20986;&#12290;&#29616;&#26377;&#22522;&#20934;&#20208;&#36182;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#65292;&#26410;&#33021;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#21644;LLM&#36755;&#20986;&#20013;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#23545;LLM&#20559;&#22909;&#23545;&#40784;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeUltraFeedback&#65292;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#22797;&#26434;&#25351;&#20196;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;AI&#21453;&#39304;&#26469;&#35843;&#25972;&#21644;&#23545;&#40784;LLMs&#19982;&#32534;&#31243;&#20559;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;14&#31181;&#19981;&#21516;&#30340;LLMs&#23545;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#36827;&#34892;&#27880;&#37322;&#65292;&#20351;&#29992;GPT-3.5&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#26041;&#27861;&#20135;&#29983;&#25968;&#23383;&#21644;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CODAL-Bench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#19982;&#36825;&#20123;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09032v1 Announce Type: cross  Abstract: Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that C
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22768;&#23398;&#20449;&#21495;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#20013;&#30340;&#36724;&#25215;&#25925;&#38556;&#36827;&#34892;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20986;&#33394;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09030</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22768;&#23398;&#20449;&#21495;&#35786;&#26029;&#39118;&#21147;&#28065;&#36718;&#26426;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22768;&#23398;&#20449;&#21495;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#20013;&#30340;&#36724;&#25215;&#25925;&#38556;&#36827;&#34892;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20986;&#33394;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#22768;&#23398;&#20449;&#21495;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#20013;&#30340;&#36724;&#25215;&#25925;&#38556;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20116;&#31181;&#39044;&#23450;&#20041;&#25925;&#38556;&#31867;&#22411;&#30340;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#25104;&#21151;&#26500;&#24314;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#21367;&#31215;LSTM&#27169;&#22411;&#12290;&#20026;&#20102;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#24182;&#22788;&#29702;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#25968;&#25454;&#65292;&#23558;&#20854;&#22788;&#29702;&#25104;&#24103;&#20197;&#25429;&#33719;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#23637;&#29616;&#20986;&#26497;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#25972;&#20307;&#20934;&#30830;&#29575;&#36229;&#36807;99.5&#65285;&#65292;&#27491;&#24120;&#29366;&#24577;&#30340;&#20551;&#38451;&#24615;&#29575;&#20302;&#20110;1&#65285;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#20026;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#20013;&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#21644;&#32500;&#25252;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09030v1 Announce Type: cross  Abstract: This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals. A convolutional LSTM model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation. To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information. The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability. On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status. The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#22312;&#39640;&#25968;&#25454;&#36895;&#29575;&#12289;&#20302;&#24310;&#36831;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#26550;&#26500;&#35201;&#27714;&#65306;&#25152;&#26377;&#21442;&#25968;&#23384;&#20648;&#22312;&#33455;&#29255;&#19978;&#65292;&#38656;&#35201;&#21327;&#21516;&#35774;&#35745;&#33258;&#23450;&#20041;/&#21487;&#37325;&#26500;&#36923;&#36753;&#20197;&#28385;&#36275;&#26497;&#31471;&#30340;&#24310;&#36831;&#21644;&#24102;&#23485;&#32422;&#26463;</title><link>https://arxiv.org/abs/2403.08980</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#23545;&#39640;&#25968;&#25454;&#36895;&#29575;&#12289;&#20302;&#24310;&#36831;&#31185;&#23398;&#24212;&#29992;&#30340;&#26550;&#26500;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08980
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#22312;&#39640;&#25968;&#25454;&#36895;&#29575;&#12289;&#20302;&#24310;&#36831;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#26550;&#26500;&#35201;&#27714;&#65306;&#25152;&#26377;&#21442;&#25968;&#23384;&#20648;&#22312;&#33455;&#29255;&#19978;&#65292;&#38656;&#35201;&#21327;&#21516;&#35774;&#35745;&#33258;&#23450;&#20041;/&#21487;&#37325;&#26500;&#36923;&#36753;&#20197;&#28385;&#36275;&#26497;&#31471;&#30340;&#24310;&#36831;&#21644;&#24102;&#23485;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#39046;&#22495;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22788;&#29702;&#26497;&#39640;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#25152;&#26377;&#21442;&#25968;&#23384;&#20648;&#22312;&#33455;&#29255;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27809;&#26377;&#36275;&#22815;&#30340;&#26102;&#38388;&#21435;&#20174;&#33455;&#29255;&#22806;&#26816;&#32034;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#20363;&#22914;DRAM&#30340;&#33455;&#29255;&#22806;&#20869;&#23384;&#27809;&#26377;&#36275;&#22815;&#30340;&#24102;&#23485;&#26469;&#25353;&#29031;&#25968;&#25454;&#20135;&#29983;&#30340;&#36895;&#24230;&#24555;&#36895;&#22788;&#29702;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;&#65292;&#27599;25&#32435;&#31186;&#65289;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26497;&#31471;&#30340;&#24310;&#36831;&#21644;&#24102;&#23485;&#35201;&#27714;&#23545;&#26088;&#22312;&#36816;&#34892;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#30340;&#30828;&#20214;&#26377;&#26550;&#26500;&#19978;&#30340;&#24433;&#21709;&#65306;1&#65289;&#25152;&#26377;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#24517;&#39035;&#36866;&#21512;&#22312;&#33455;&#29255;&#19978;&#65292;2&#65289;&#36890;&#24120;&#38656;&#35201;&#21327;&#21516;&#35774;&#35745;&#33258;&#23450;&#20041;/&#21487;&#37325;&#26500;&#36923;&#36753;&#20197;&#28385;&#36275;&#36825;&#20123;&#24310;&#36831;&#21644;&#24102;&#23485;&#32422;&#26463;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#31185;&#23398;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#24517;&#39035;&#23436;&#20840;&#22312;&#33455;&#29255;&#19978;&#36816;&#34892;&#65292;&#26497;&#31471;&#24773;&#20917;&#19979;&#38656;&#35201;&#23450;&#21046;&#33455;&#29255;&#26469;&#28385;&#36275;&#22914;&#27492;&#20005;&#26684;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08980v1 Announce Type: new  Abstract: With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip. In many of these applications, there is not enough time to go off-chip and retrieve weights. Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns). As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints. In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints.
&lt;/p&gt;</description></item><item><title>AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08978</link><description>&lt;p&gt;
AutoGuide: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#33258;&#21160;&#29983;&#25104;&#21644;&#36873;&#25321;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08978
&lt;/p&gt;
&lt;p&gt;
AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#21463;&#38480;&#12290;&#36825;&#32473;&#22522;&#20110;LLMs&#30340;&#20195;&#29702;&#24102;&#26469;&#20102;&#37325;&#22823;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30693;&#35782;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AutoGuide&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#32463;&#39564;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#26469;&#24357;&#21512;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoGuide&#36890;&#36807;&#25552;&#21462;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#26377;&#25928;&#22320;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#12290;&#27599;&#20010;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#20197;&#31616;&#27905;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65292;&#24182;&#36981;&#24490;&#26465;&#20214;&#32467;&#26500;&#65292;&#28165;&#26224;&#25551;&#36848;&#36866;&#29992;&#30340;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25351;&#21335;&#20026;&#21521;&#20195;&#29702;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#22823;&#24133;&#39046;&#20808;&#20110;&#31454;&#20105;&#30340;&#22522;&#20110;LLMs&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08978v1 Announce Type: new  Abstract: The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30446;&#30340;&#30340;VR&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;108&#21517;&#21442;&#19982;&#32773;&#22312;VR&#20013;&#23398;&#20064;&#32452;&#35013;&#20004;&#31181;&#19981;&#21516;&#20840;&#23610;&#23544;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.08969</link><description>&lt;p&gt;
&#20840;&#23610;&#23544;&#35013;&#37197;&#27169;&#25311;&#27979;&#35797;&#21488;&#65288;FAST&#65289;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Full-scale Assembly Simulation Testbed (FAST) Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08969
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30446;&#30340;&#30340;VR&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;108&#21517;&#21442;&#19982;&#32773;&#22312;VR&#20013;&#23398;&#20064;&#32452;&#35013;&#20004;&#31181;&#19981;&#21516;&#20840;&#23610;&#23544;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#36319;&#36394;&#21644;&#20132;&#20114;&#25968;&#25454;&#22914;&#20309;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30446;&#30340;&#65292;&#21253;&#25324;&#29992;&#25143;&#35782;&#21035;&#12289;&#39044;&#27979;&#32593;&#32476;&#26197;&#21160;&#30151;&#21644;&#20272;&#31639;&#23398;&#20064;&#22686;&#30410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20110;VR&#30340;&#20840;&#23610;&#23544;&#35013;&#37197;&#27169;&#25311;&#27979;&#35797;&#21488;&#65288;FAST&#65289;&#25429;&#33719;&#30340;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;108&#21517;&#21442;&#19982;&#32773;&#65288;50&#21517;&#22899;&#24615;&#65292;56&#21517;&#30007;&#24615;&#65292;2&#21517;&#38750;&#20108;&#20803;&#24615;&#21035;&#65289;&#23398;&#20064;&#22914;&#20309;&#22312;VR&#20013;&#32452;&#35013;&#20004;&#31181;&#19981;&#21516;&#20840;&#23610;&#23544;&#32467;&#26500;&#26102;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#35299;&#37322;&#25968;&#25454;&#38598;&#26159;&#22914;&#20309;&#25910;&#38598;&#30340;&#24182;&#25551;&#36848;&#21253;&#21547;&#30340;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08969v1 Announce Type: cross  Abstract: In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#24211;&#26222;&#26364;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#20854;&#20840;&#23616;&#32447;&#24615;&#21270;&#25104;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.08965</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24211;&#26222;&#26364;&#29702;&#35770;&#30340;&#36712;&#36947;&#38382;&#39064;&#21160;&#21147;&#23398;&#35782;&#21035;&#19982;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08965
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#24211;&#26222;&#26364;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#20854;&#20840;&#23616;&#32447;&#24615;&#21270;&#25104;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#30740;&#31350;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#21161;&#20110;&#25551;&#36848;&#22825;&#20307;&#21644;&#20154;&#36896;&#21355;&#26143;&#30340;&#36816;&#21160;&#12290;&#38543;&#30528;&#23545;&#21355;&#26143;&#21644;&#21355;&#26143;&#32534;&#38431;&#39134;&#34892;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#23545;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#26377;&#25928;&#30340;&#25511;&#21046;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24211;&#26222;&#26364;&#29702;&#35770;&#23454;&#29616;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#21516;&#26102;&#31995;&#32479;&#35782;&#21035;&#21644;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#21363;&#36890;&#36807;&#32431;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#32447;&#24615;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#20840;&#23616;&#32447;&#24615;&#21270;&#20026;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65288;LTI&#65289;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08965v1 Announce Type: cross  Abstract: The study of the Two-Body and Circular Restricted Three-Body Problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites. With the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important. Global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results. We propose a data-driven framework for simultaneous system identification and global linearization of both the Two-Body Problem and Circular Restricted Three-Body Problem via deep learning-based Koopman Theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (LTI) system. The linear Koopman operator is discovered through purely data-driven training of a Deep Neural Network with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.08955</link><description>&lt;p&gt;
&#26397;&#21521;&#39640;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33258;&#20027;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#20102;&#26399;&#26395;&#22238;&#25253;&#21644;&#39118;&#38505;&#65292;&#20855;&#26377;&#20135;&#29983;&#27010;&#29575;&#40065;&#26834;&#31574;&#30053;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;REINFORCE&#31639;&#27861;&#24182;&#37319;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$\mathcal{O}(\epsilon^{-2})$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20197;&#36798;&#21040;$\epsilon$-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#27604;&#39118;&#38505;&#20013;&#24615;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.08946</link><description>&lt;p&gt;
&#21487;&#29992;&#30340;XAI&#65306;&#22312;LLM&#26102;&#20195;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#30340;10&#20010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08946
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25351;&#30340;&#26159;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27934;&#35265;&#65292;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36816;&#20316;&#26041;&#24335;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;XAI&#30340;&#37325;&#28857;&#27491;&#34987;&#25193;&#23637;&#21040;&#24120;&#24120;&#22240;&#20026;&#19981;&#36879;&#26126;&#32780;&#22791;&#21463;&#25209;&#35780;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#19968;&#25299;&#23637;&#38656;&#35201;&#23545;XAI&#26041;&#27861;&#35770;&#36827;&#34892;&#26174;&#33879;&#36716;&#21464;&#65292;&#22240;&#20026;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;XAI&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;LLMs&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#34892;&#19994;&#24212;&#29992;&#20013;&#65292;XAI&#30340;&#35282;&#33394;&#20174;&#20165;&#20165;&#25171;&#24320;&#8220;&#40657;&#21283;&#23376;&#8221;&#36716;&#21464;&#20026;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20165;&#20316;&#20026;XAI&#27934;&#35265;&#30340;&#34987;&#21160;&#25509;&#21463;&#32773;&#65292;LLMs&#30340;&#29420;&#29305;&#33021;&#21147;&#33021;&#22815;&#30456;&#20114;&#22686;&#24378;XAI&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;1&#65289;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25512;&#26029;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#26368;&#22823;&#21270;&#19982;&#25512;&#26029;&#27169;&#22411;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25512;&#26029;&#27169;&#22411;&#36817;&#20284;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08941</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#26080;&#20851;&#21518;&#39564;&#36924;&#36817;&#30340;&#24555;&#36895;&#20934;&#30830;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08941
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25512;&#26029;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#26368;&#22823;&#21270;&#19982;&#25512;&#26029;&#27169;&#22411;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25512;&#26029;&#27169;&#22411;&#36817;&#20284;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#25512;&#26029;&#21253;&#25324;&#23398;&#20064;&#20004;&#20010;&#27169;&#22411;&#65306;&#65288;1&#65289;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#28508;&#22312;&#31354;&#38388;&#19978;&#30340;&#31616;&#21333;&#20998;&#24067;&#36716;&#25442;&#20026;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25512;&#26029;&#27169;&#22411;&#65292;&#36817;&#20284;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#32534;&#30721;&#21518;&#39564;&#12290;&#36825;&#20004;&#20010;&#32452;&#20214;&#36890;&#36807;&#23545;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#36793;&#38469;&#20284;&#28982;&#30340;&#19979;&#30028;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#12290;&#22312;&#32852;&#21512;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#25512;&#26029;&#27169;&#22411;&#24456;&#24046;&#22320;&#36817;&#20284;&#20102;&#28508;&#22312;&#32534;&#30721;&#21518;&#39564;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20248;&#21270;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#29983;&#25104;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25512;&#26029;&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#27599;&#27425;&#26356;&#26032;&#20043;&#21069;&#26368;&#22823;&#21270;&#19982;&#25512;&#26029;&#27169;&#22411;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36845;&#20195;&#35757;&#32451;&#25928;&#29575;&#20302;&#65292;&#38656;&#35201;&#21551;&#21457;&#24335;&#26631;&#20934;&#26469;&#20174;&#36845;&#20195;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08941v1 Announce Type: cross  Abstract: Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model's log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative
&lt;/p&gt;</description></item><item><title>FogGuard&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38654;&#22825;&#27668;&#26465;&#20214;&#25361;&#25112;&#30340;&#26032;&#22411;&#38654;&#24863;&#30693;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#25910;&#38598;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08939</link><description>&lt;p&gt;
FogGuard: &#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#20445;&#25252;YOLO&#20813;&#21463;&#38654;&#38718;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
FogGuard: guarding YOLO against fog using perceptual loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08939
&lt;/p&gt;
&lt;p&gt;
FogGuard&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38654;&#22825;&#27668;&#26465;&#20214;&#25361;&#25112;&#30340;&#26032;&#22411;&#38654;&#24863;&#30693;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#25910;&#38598;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38654;&#24863;&#30693;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#31216;&#20026;FogGuard&#65292;&#26088;&#22312;&#35299;&#20915;&#38654;&#22825;&#27668;&#26465;&#20214;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#24694;&#21155;&#30340;&#22825;&#27668;&#26465;&#20214;&#20250;&#26174;&#33879;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#38752;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20998;&#20026;&#20004;&#31867;&#65292;1&#65289;&#22270;&#20687;&#22686;&#24378;&#65288;&#22914;IA-YOLO&#65289;&#21644;2&#65289;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#35797;&#22270;&#29983;&#25104;&#26080;&#38654;&#22270;&#20687;&#65292;&#28982;&#32780;&#65292;&#20174;&#26377;&#38654;&#22270;&#20687;&#20013;&#24674;&#22797;&#26080;&#38654;&#22270;&#20687;&#27604;&#22312;&#26377;&#38654;&#22270;&#20687;&#20013;&#26816;&#27979;&#23545;&#35937;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#31867;&#26041;&#27861;&#37117;&#22312;&#23581;&#35797;&#35299;&#20915;&#38382;&#39064;&#30340;&#26356;&#38590;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#23545;&#21407;&#22987;&#26631;&#27880;&#25968;&#25454;&#30340;&#24494;&#35843;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08939v1 Announce Type: cross  Abstract: In this paper, we present a novel fog-aware object detection network called FogGuard, designed to address the challenges posed by foggy weather conditions. Autonomous driving systems heavily rely on accurate object detection algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs).   Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) domain adaptation based approaches. Image enhancement based techniques attempt to generate fog-free image. However, retrieving a fogless image from a foggy image is a much harder problem than detecting objects in a foggy image. Domain-adaptation based approaches, on the other hand, do not make use of labelled datasets in the target domain. Both categories of approaches are attempting to solve a harder version of the problem. Our approach builds over fine-tuning on the   Our framework is specifically designed t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#28385;&#36275;&#19968;&#23450;&#26680;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#35889;&#21644;&#38598;&#20013;&#24615;&#36136;&#30340;&#19968;&#33324;&#31867;&#38382;&#39064;&#65292;&#20855;&#26377;&#19968;&#31181;&#38750;&#28176;&#36817;&#30830;&#23450;&#24615;&#36817;&#20284;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08938</link><description>&lt;p&gt;
Kernel Ridge Regression&#30340;&#38750;&#28176;&#36817;&#29702;&#35770;&#65306;&#30830;&#23450;&#24615;&#31561;&#20215;&#65292;&#27979;&#35797;&#35823;&#24046;&#21644;GCV&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#28385;&#36275;&#19968;&#23450;&#26680;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#35889;&#21644;&#38598;&#20013;&#24615;&#36136;&#30340;&#19968;&#33324;&#31867;&#38382;&#39064;&#65292;&#20855;&#26377;&#19968;&#31181;&#38750;&#28176;&#36817;&#30830;&#23450;&#24615;&#36817;&#20284;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#23398;&#20064;&#26410;&#30693;&#30446;&#26631;&#20989;&#25968;$f_*$&#65292;&#32473;&#23450;i.i.d.&#25968;&#25454;$(u_i,y_i)$&#65292;$i\leq n$&#65292;&#20854;&#20013;$u_i \in U$&#26159;&#19968;&#20010;&#21327;&#21464;&#37327;&#21521;&#37327;&#65292;$y_i = f_* (u_i) +\varepsilon_i \in \mathbb{R}$&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36830;&#32493;&#34920;&#26126;&#65292;KRR&#30340;&#27979;&#35797;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20174;&#26680;&#31639;&#23376;&#30340;&#35889;&#20381;&#36182;&#30340;&#31561;&#20215;&#24207;&#21015;&#27169;&#22411;&#23548;&#20986;&#30340;&#23553;&#38381;&#24418;&#24335;&#20272;&#35745;&#24456;&#22909;&#22320;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27492;&#31561;&#20215;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#36804;&#20170;&#20026;&#27490;&#35201;&#20040;&#20381;&#36182;&#20110;&#38480;&#21046;&#24615;&#20551;&#35774;--&#22914;&#27425;&#39640;&#26031;&#29420;&#31435;&#26412;&#24449;&#20989;&#25968;--&#65292;&#35201;&#20040;&#23545;&#39640;&#32500;&#20855;&#20307;&#26680;&#36827;&#34892;&#28176;&#36817;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08938v1 Announce Type: cross  Abstract: We consider learning an unknown target function $f_*$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\leq n$, where $u_i \in U$ is a covariate vector and $y_i = f_* (u_i) +\varepsilon_i \in \mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent' sequence model that only depends on the spectrum of the kernel operator. However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions -- such as subgaussian independent eigenfunctions -- , or asymptotic derivations for specific kernels in high dimensions.   In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition. Specifically, we establish in this setting a non-asymptotic deterministic approximation for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#19982;&#31169;&#26377;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#24555;&#30340;&#26597;&#35810;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.08917</link><description>&lt;p&gt;
&#39640;&#25928;&#35745;&#31639;&#19982;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficiently Computing Similarities to Private Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08917
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#19982;&#31169;&#26377;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#24555;&#30340;&#26597;&#35810;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24046;&#20998;&#31169;&#26377;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#26597;&#35810;&#28857;&#65288;&#22914;&#20844;&#20849;&#25968;&#25454;&#25110;&#21512;&#25104;&#25968;&#25454;&#65289;&#19982;&#31169;&#26377;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24120;&#35265;&#23376;&#20363;&#31243;&#25277;&#35937;&#20986;&#26469;&#65292;&#24182;&#30740;&#31350;&#20197;&#19979;&#22522;&#26412;&#31639;&#27861;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#30456;&#20284;&#24615;&#20989;&#25968;$f$&#21644;&#19968;&#20010;&#22823;&#30340;&#39640;&#32500;&#31169;&#26377;&#25968;&#25454;&#38598;$X \subset \mathbb{R}^d$&#65292;&#36755;&#20986;&#19968;&#20010;&#24046;&#20998;&#31169;&#26377;&#65288;DP&#65289;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#20309;&#26597;&#35810;$y$&#30340;$\sum_{x \in X} f(x,y)$&#12290;&#25105;&#20204;&#32771;&#34385;$f$&#20026;&#26680;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;$f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$&#65288;&#20063;&#31216;&#20026;&#24046;&#20998;&#31169;&#26377;&#26680;&#23494;&#24230;&#20272;&#35745;&#65289;&#65292;&#25110;&#32773;&#36317;&#31163;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#22914;$f(x,y) = \|x-y\|_2$&#31561;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#20026;&#19968;&#31995;&#21015;&#26680;&#20989;&#25968;&#21644;&#36317;&#31163;&#20989;&#25968;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#24555;&#30340;&#26597;&#35810;&#26102;&#38388;&#12290;&#25105;&#20204;&#32467;&#26524;&#32972;&#21518;&#30340;&#32479;&#19968;&#26041;&#27861;&#26159;&#21033;&#29992;&#8220;&#20302;&#32500;&#32467;&#26500;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08917v1 Announce Type: cross  Abstract: Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \subset \mathbb{R}^d$, output a differentially private (DP) data structure which approximates $\sum_{x \in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \|x-y\|_2$, among others.   Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' pre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08901</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#21457;&#29616;&#21487;&#20449;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#30340;&#25112;&#30053;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08901
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#39640;&#20445;&#30495;&#20223;&#30495;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#24191;&#27867;&#25972;&#21512;&#65292;&#20984;&#26174;&#20102;&#31283;&#20581;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#20445;&#20195;&#29702;&#27169;&#22411;&#21487;&#21487;&#38752;&#22320;&#29992;&#20110;&#37325;&#35201;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;Occam Plausibility Algorithm&#65288;OPAL-surrogate&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#22312;&#22823;&#37327;&#28508;&#22312;&#27169;&#22411;&#65288;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#20197;&#21450;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#65289;&#20013;&#25581;&#31034;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#26694;&#26550;&#22522;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#37319;&#29992;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21407;&#21017;&#65292;OPAL-surrogate&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08901v1 Announce Type: cross  Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and
&lt;/p&gt;</description></item><item><title>&#22312;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;agent&#29420;&#31435;&#36827;&#34892;TD($\lambda$)&#36816;&#31639;&#65292;&#24182;&#26368;&#32456;&#22312;&#32467;&#26524;&#19978;&#36827;&#34892;&#24179;&#22343;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20197;&#24448;&#24037;&#20316;&#26356;&#23569;&#30340;&#36890;&#20449;&#37327;&#35201;&#27714;&#30340;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.08896</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#20998;&#24067;&#24335;TD($\lambda$)
&lt;/p&gt;
&lt;p&gt;
One-Shot Averaging for Distributed TD($\lambda$) Under Markov Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08896
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;agent&#29420;&#31435;&#36827;&#34892;TD($\lambda$)&#36816;&#31639;&#65292;&#24182;&#26368;&#32456;&#22312;&#32467;&#26524;&#19978;&#36827;&#34892;&#24179;&#22343;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20197;&#24448;&#24037;&#20316;&#26356;&#23569;&#30340;&#36890;&#20449;&#37327;&#35201;&#27714;&#30340;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#27599;&#20010;agent&#37117;&#25317;&#26377;&#30456;&#21516;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21103;&#26412;&#65292;&#20294;&#26159;&#36716;&#25442;&#26159;&#29420;&#31435;&#22320;&#20174;&#30456;&#24212;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#30001;&#27599;&#20010;agent&#37319;&#26679;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;TD($\lambda$)&#30340;&#32447;&#24615;&#21152;&#36895;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#29992;&#20110;&#31574;&#30053;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#21363;&#33509;&#30446;&#26631;&#31934;&#24230;&#36275;&#22815;&#23567;&#65292;$N$&#20010;agents&#21487;&#20197;&#20197;$N$&#20493;&#36895;&#24230;&#35780;&#20272;&#19968;&#20010;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#21152;&#36895;&#26159;&#36890;&#36807;&#8220;&#19968;&#27425;&#24615;&#24179;&#22343;&#21270;&#8221;&#23454;&#29616;&#30340;&#65292;&#21363;agent&#20204;&#29420;&#31435;&#22320;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#36816;&#34892;TD($\lambda$)&#65292;&#24182;&#19988;&#20165;&#22312;&#26368;&#21518;&#19968;&#27493;&#20043;&#21518;&#23545;&#20182;&#20204;&#30340;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#12290;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#25152;&#38656;&#30340;&#36890;&#20449;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08896v1 Announce Type: new  Abstract: We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29305;&#24449;&#21487;&#20197;&#26681;&#25454;&#27425;&#35201;&#27169;&#22411;&#24615;&#33021;&#29305;&#24449;&#36827;&#34892;&#36873;&#25321;&#65292;&#20174;&#32780;&#32416;&#27491;&#19982;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#27169;&#22411;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.08880</link><description>&lt;p&gt;
REFRESH: &#30001;SHAP&#20540;&#25351;&#23548;&#30340;&#36127;&#36131;&#20219;&#21644;&#39640;&#25928;&#30340;&#29305;&#24449;&#37325;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#29305;&#24449;&#21487;&#20197;&#26681;&#25454;&#27425;&#35201;&#27169;&#22411;&#24615;&#33021;&#29305;&#24449;&#36827;&#34892;&#36873;&#25321;&#65292;&#20174;&#32780;&#32416;&#27491;&#19982;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#27169;&#22411;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#20197;&#20934;&#30830;&#24615;&#20026;&#30446;&#26631;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#21487;&#33021;&#26082;&#28902;&#29712;&#21448;&#35745;&#31639;&#23494;&#38598;&#12290;&#27169;&#22411;&#24615;&#33021;&#29305;&#24449;&#65292;&#22914;&#20844;&#27491;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#20063;&#23545;&#27169;&#22411;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#27861;&#35268;&#25512;&#21160;&#23545;&#26356;&#21152;&#21487;&#20449;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#32416;&#27491;&#19982;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#27169;&#22411;&#29305;&#24449;&#12290;&#24403;&#29305;&#24449;&#36873;&#25321;&#26159;&#38024;&#23545;&#19968;&#20010;&#27169;&#22411;&#24615;&#33021;&#29305;&#24449;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#65289;&#26102;&#65292;&#20197;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#29305;&#24449;&#65288;&#22914;&#20844;&#27491;&#24615;&#21644;&#31283;&#20581;&#24615;&#65289;&#20026;&#30446;&#26631;&#30340;&#29305;&#24449;&#36873;&#25321;&#23558;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#37325;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#26681;&#25454;&#27425;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08880v1 Announce Type: new  Abstract: Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as fairness and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature \emph{reselection}, so that features can be selected with respect to secondary 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#29615;&#22659;&#19979;&#36827;&#34892;&#22810;&#30446;&#26631;&#12289;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20302;&#35745;&#31639;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.08879</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08879
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#29615;&#22659;&#19979;&#36827;&#34892;&#22810;&#30446;&#26631;&#12289;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20302;&#35745;&#31639;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#29615;&#22659;&#34987;&#35748;&#20026;&#26159;&#21160;&#24577;&#21644;&#20998;&#24067;&#24335;&#30340;&#65292;&#21442;&#19982;&#32773;&#65288;&#36710;&#36742;&#29992;&#25143;&#65292;&#36816;&#33829;&#21830;&#31561;&#65289;&#20855;&#26377;&#22810;&#20010;&#12289;&#19981;&#26029;&#21464;&#21270;&#19988;&#21487;&#33021;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#29992;&#20110;&#20248;&#21270;ITS&#24212;&#29992;&#65292;&#22914;&#36164;&#28304;&#31649;&#29702;&#21644;&#21368;&#36733;&#65292;&#20294;&#22823;&#22810;&#25968;RL&#31639;&#27861;&#19987;&#27880;&#20110;&#21333;&#19968;&#30446;&#26631;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#19968;&#30446;&#26631;&#26159;&#19981;&#21487;&#33021;&#30340;&#12289;&#26840;&#25163;&#30340;&#25110;&#19981;&#36275;&#30340;&#65292;&#36825;&#20351;&#24471;&#36825;&#31181;RL&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20302;&#35745;&#31639;&#35201;&#27714;&#30340;&#22810;&#30446;&#26631;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21160;&#24577;&#12289;&#20998;&#24067;&#24335;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#33258;&#21160;&#35302;&#21457;&#33258;&#36866;&#24212;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#20855;&#26377;&#31232;&#30095;&#21644;&#24310;&#36831;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;ITS&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08879v1 Announce Type: cross  Abstract: The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Moment Pooling&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Deep Sets&#20013;&#30340;&#27714;&#21644;&#27867;&#21270;&#20026;&#20219;&#24847;&#30340;&#22810;&#21464;&#37327;&#30697;&#65292;&#26174;&#33879;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#65292;&#22312;&#22266;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#26377;&#25928;&#28508;&#22312;&#32500;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#20869;&#37096;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08854</link><description>&lt;p&gt;
&#28165;&#26224;&#30636;&#38388;&#65306;&#20351;&#29992;Moment Pooling&#31616;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Moment Pooling&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Deep Sets&#20013;&#30340;&#27714;&#21644;&#27867;&#21270;&#20026;&#20219;&#24847;&#30340;&#22810;&#21464;&#37327;&#30697;&#65292;&#26174;&#33879;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#65292;&#22312;&#22266;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#26377;&#25928;&#28508;&#22312;&#32500;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#20869;&#37096;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#28041;&#21450;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#36890;&#24120;&#26159;&#39640;&#32500;&#19988;&#38590;&#20197;&#30452;&#25509;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;Moment Pooling&#8221;&#65292;&#36825;&#26159;Deep Sets&#32593;&#32476;&#30340;&#19968;&#20010;&#33258;&#28982;&#24310;&#20280;&#65292;&#21487;&#22823;&#24133;&#20943;&#23569;&#36825;&#20123;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#65292;&#21516;&#26102;&#32500;&#25345;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;Moment Pooling&#23558;Deep Sets&#20013;&#30340;&#27714;&#21644;&#27867;&#21270;&#20026;&#20219;&#24847;&#30340;&#22810;&#21464;&#37327;&#30697;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22266;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#26377;&#25928;&#28508;&#22312;&#32500;&#24230;&#12290;&#25105;&#20204;&#23558;Moment Pooling&#24212;&#29992;&#20110;&#22840;&#20811;/&#33014;&#23376;&#21943;&#27880;&#20998;&#31867;&#30340;&#23545;&#25758;&#26426;&#29289;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;Energy Flow Networks&#65288;EFNs&#65289;&#25193;&#23637;&#20026;Moment EFNs&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#23567;&#33267;1&#30340;&#28508;&#22312;&#32500;&#24230;&#30340;Moment EFNs&#34920;&#29616;&#19982;&#20855;&#26377;&#36739;&#39640;&#28508;&#22312;&#32500;&#24230;&#30340;&#26222;&#36890;EFNs&#31867;&#20284;&#12290;&#36825;&#31181;&#23567;&#28508;&#22312;&#32500;&#24230;&#20351;&#20869;&#37096;&#34920;&#31034;&#21487;&#20197;&#30452;&#25509;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08854v1 Announce Type: cross  Abstract: Many machine learning applications involve learning a latent representation of data, which is often high-dimensional and difficult to directly interpret. In this work, we propose "Moment Pooling", a natural extension of Deep Sets networks which drastically decrease latent space dimensionality of these networks while maintaining or even improving performance. Moment Pooling generalizes the summation in Deep Sets to arbitrary multivariate moments, which enables the model to achieve a much higher effective latent dimensionality for a fixed latent dimension. We demonstrate Moment Pooling on the collider physics task of quark/gluon jet classification by extending Energy Flow Networks (EFNs) to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1 perform similarly to ordinary EFNs with higher latent dimension. This small latent dimension allows for the internal representation to be directly visualized and interpreted, w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PAPERCLIP&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08851</link><description>&lt;p&gt;
PAPERCLIP&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#22825;&#25991;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PAPERCLIP&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PAPERCLIP&#65288;Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#30001;&#26395;&#36828;&#38236;&#25104;&#20687;&#30340;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#24494;&#35843;&#32780;&#26469;&#65292;&#20351;&#29992;&#25104;&#21151;&#30340;&#35266;&#27979;&#25552;&#26696;&#25688;&#35201;&#21644;&#30456;&#24212;&#30340;&#19979;&#28216;&#35266;&#27979;&#65292;&#20854;&#20013;&#25688;&#35201;&#21487;&#36873;&#25321;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24341;&#23548;&#29983;&#25104;&#26469;&#36827;&#34892;&#24635;&#32467;&#12290;&#20197;&#21704;&#21187;&#31354;&#38388;&#26395;&#36828;&#38236;&#65288;HST&#65289;&#30340;&#35266;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#30340;&#27169;&#22411;&#36890;&#36807;&#38024;&#23545;&#22270;&#20687;&#26816;&#32034;&#65288;&#21363;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25214;&#21040;&#26368;&#30456;&#20851;&#30340;&#35266;&#27979;&#65289;&#21644;&#25551;&#36848;&#26816;&#32034;&#65288;&#21363;&#26597;&#35810;&#19982;&#22825;&#25991;&#29289;&#20307;&#31867;&#21035;&#21644;&#29992;&#20363;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#65289;&#30340;&#27979;&#35797;&#65292;&#20307;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a 
&lt;/p&gt;</description></item><item><title>JAXbind&#26088;&#22312;&#22823;&#24133;&#20943;&#23569;&#23558;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#30340;&#33258;&#23450;&#20041;&#20989;&#25968;&#32465;&#23450;&#21040;JAX&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#65292;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#23450;&#20041;&#33258;&#23450;&#20041;JAX&#21407;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.08847</link><description>&lt;p&gt;
JAXbind: &#23558;&#20219;&#20309;&#20989;&#25968;&#32465;&#23450;&#21040;JAX
&lt;/p&gt;
&lt;p&gt;
JAXbind: Bind any function to JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08847
&lt;/p&gt;
&lt;p&gt;
JAXbind&#26088;&#22312;&#22823;&#24133;&#20943;&#23569;&#23558;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#30340;&#33258;&#23450;&#20041;&#20989;&#25968;&#32465;&#23450;&#21040;JAX&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#65292;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#23450;&#20041;&#33258;&#23450;&#20041;JAX&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
JAX&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#21518;&#32773;&#32463;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#39640;&#24615;&#33021;&#20195;&#30721;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#23558;&#20854;&#32435;&#20837;JAX&#20013;&#12290;&#37325;&#26032;&#22312;JAX&#20013;&#23454;&#29616;&#29616;&#26377;&#20195;&#30721;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#32780;JAX&#20013;&#29992;&#20110;&#32465;&#23450;&#33258;&#23450;&#20041;&#20195;&#30721;&#30340;&#29616;&#26377;&#25509;&#21475;&#38656;&#35201;&#23545;JAX&#21450;&#20854;C++&#21518;&#31471;&#26377;&#28145;&#20837;&#20102;&#35299;&#12290;JAXbind&#30340;&#30446;&#26631;&#26159;&#22823;&#22823;&#20943;&#23569;&#23558;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#30340;&#33258;&#23450;&#20041;&#20989;&#25968;&#32465;&#23450;&#21040;JAX&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;JAXbind&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#65292;&#29992;&#20110;&#23450;&#20041;&#25903;&#25345;&#20219;&#24847;JAX&#36716;&#25442;&#30340;&#33258;&#23450;&#20041;&#25152;&#35859;JAX&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08847v1 Announce Type: cross  Abstract: JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code requires deep knowledge of JAX and its C++ backend. The goal of JAXbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom so-called JAX primitives that support arbitrary JAX transformations.
&lt;/p&gt;</description></item><item><title>&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.08845</link><description>&lt;p&gt;
&#21333;&#19978;&#19979;&#25991;&#22823;&#25209;&#37327;&#25277;&#26679;&#30340;&#20998;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bifurcated Attention for Single-Context Large-Batch Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08845
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#20887;&#20313;&#30340;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#36825;&#26159;&#39640;&#25209;&#37327;&#22823;&#23567;&#21644;&#38271;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24310;&#36831;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36890;&#36807;&#22312;&#22686;&#37327;&#35299;&#30721;&#26399;&#38388;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;GEMM&#25805;&#20316;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#26469;&#33258;&#39044;&#22635;&#20805;&#30340;KV&#32531;&#23384;&#20197;&#21450;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#31934;&#30830;&#30340;&#35745;&#31639;&#65292;&#24182;&#32500;&#25345;&#24120;&#35268;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35745;&#31639;&#36127;&#36733;&#65288;FLOPs&#65289;&#65292;&#20294;&#20943;&#23569;&#20102;&#20869;&#23384;IO&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36824;&#19982;&#20943;&#23569;KV&#32531;&#23384;&#20869;&#23384;IO&#24050;&#30693;&#30340;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#26356;&#39640;&#30340;&#25209;&#37327;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#30001;&#27492;&#24102;&#26469;&#30340;&#25928;&#29575;&#23548;&#33268;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#25913;&#21892;&#20102;&#23454;&#26102;&#24212;&#29992;&#30340;&#36866;&#29992;&#24615;&#65292;&#20363;&#22914;&#23454;&#29616;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#22823;&#37051;&#22495;&#25628;&#32034;&#20013;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23398;&#20064;&#22686;&#24378;&#30340;&#37051;&#22495;&#36873;&#25321;&#65288;LENS&#65289;&#65292;&#29992;&#20110;&#22312;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#36741;&#21161;&#30830;&#23450;&#35299;&#20915;&#26041;&#26696;&#20013;&#24212;&#35813;&#30772;&#22351;&#21644;&#20462;&#22797;&#30340;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.08839</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#37051;&#22495;&#36873;&#25321;&#29992;&#20110;&#24102;&#26102;&#38388;&#31383;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#22823;&#37051;&#22495;&#25628;&#32034;&#20013;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23398;&#20064;&#22686;&#24378;&#30340;&#37051;&#22495;&#36873;&#25321;&#65288;LENS&#65289;&#65292;&#29992;&#20110;&#22312;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#36741;&#21161;&#30830;&#23450;&#35299;&#20915;&#26041;&#26696;&#20013;&#24212;&#35813;&#30772;&#22351;&#21644;&#20462;&#22797;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37051;&#22495;&#25628;&#32034;&#65288;LNS&#65289;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#24191;&#27867;&#36866;&#29992;&#19988;&#22312;&#23454;&#36341;&#20013;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25972;&#21512;&#21040;LNS&#20013;&#65292;&#20197;&#24110;&#21161;&#20915;&#23450;&#22312;&#27599;&#27425;LNS&#36845;&#20195;&#20013;&#24212;&#30772;&#22351;&#21644;&#20462;&#22797;&#21738;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#23558;&#26032;&#26041;&#27861;&#31216;&#20026;&#23398;&#20064;&#22686;&#24378;&#30340;&#37051;&#22495;&#36873;&#25321;&#65288;LENS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26222;&#36866;&#30340;&#65292;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;LNS&#31639;&#27861;&#20197;&#22686;&#24378;&#30772;&#22351;&#31639;&#27861;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LENS&#22312;&#22522;&#26412;&#30340;&#24102;&#26102;&#38388;&#31383;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPTW&#65289;&#19978;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20026;VRPTW&#23454;&#29616;&#20102;&#19968;&#20010;LNS&#31639;&#27861;&#65292;&#24182;&#25910;&#38598;&#20102;&#20174;&#20247;&#25152;&#21608;&#30693;&#12289;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#34893;&#29983;&#20986;&#30340;&#26032;&#39062;&#35757;&#32451;&#23454;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;LENS&#26041;&#27861;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20004;&#20010;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08839v1 Announce Type: new  Abstract: Large Neighborhood Search (LNS) is a universal approach that is broadly applicable and has proven to be highly efficient in practice for solving optimization problems. We propose to integrate machine learning (ML) into LNS to assist in deciding which parts of the solution should be destroyed and repaired in each iteration of LNS. We refer to our new approach as Learning-Enhanced Neighborhood Selection (LENS for short). Our approach is universally applicable, i.e., it can be applied to any LNS algorithm to amplify the workings of the destroy algorithm. In this paper, we demonstrate the potential of LENS on the fundamental Vehicle Routing Problem with Time Windows (VRPTW). We implemented an LNS algorithm for VRPTW and collected data on generated novel training instances derived from well-known, extensively utilized benchmark datasets. We trained our LENS approach with this data and compared the experimental results of our approach with two
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08838</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33337;&#33334;&#36712;&#36857;&#32858;&#31867;&#26088;&#22312;&#23547;&#25214;&#30456;&#20284;&#30340;&#36712;&#36857;&#27169;&#24335;&#65292;&#22312;&#28023;&#19978;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#21644;&#38408;&#20540;&#26469;&#35782;&#21035;&#31163;&#25955;&#30340;&#33337;&#33334;&#34892;&#20026;&#65292;&#20294;&#23384;&#22312;&#26080;&#27861;&#34920;&#31034;&#28436;&#21464;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#65288;PC-HiV&#65289;&#30340;&#26041;&#27861;&#12290;PC-HiV&#39318;&#20808;&#20351;&#29992;&#20998;&#23618;&#34920;&#31034;&#23558;&#27599;&#26465;&#36712;&#36857;&#36716;&#25442;&#20026;&#34892;&#20026;&#24207;&#21015;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#25139;&#39044;&#27979;&#28436;&#21270;&#12290;&#36890;&#36807;&#24212;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;PC-HiV&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#22312;&#30495;&#23454;AIS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PC-HiV&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25429;&#25417;&#33337;&#33334;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08838v1 Announce Type: cross  Abstract: Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;&#65292;&#36890;&#36807;&#23558;&#24494;&#25209;&#37327;&#25191;&#34892;&#20174;&#21516;&#26102;&#25913;&#20026;&#39034;&#24207;&#25191;&#34892;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#24182;&#34892;&#21270;&#20013;&#28608;&#27963;&#20869;&#23384;&#23792;&#20540;&#21644;&#26799;&#24230;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#33021;&#20943;&#23569;&#25152;&#38656;GPU&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.08837</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#24182;&#34892;&#21270;&#30340;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08837
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;&#65292;&#36890;&#36807;&#23558;&#24494;&#25209;&#37327;&#25191;&#34892;&#20174;&#21516;&#26102;&#25913;&#20026;&#39034;&#24207;&#25191;&#34892;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#24182;&#34892;&#21270;&#20013;&#28608;&#27963;&#20869;&#23384;&#23792;&#20540;&#21644;&#26799;&#24230;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#33021;&#20943;&#23569;&#25152;&#38656;GPU&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#24182;&#34892;&#21270;&#25216;&#26415;&#20197;&#25193;&#23637;&#35268;&#27169;&#12290;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#22914;&#25968;&#25454;&#24182;&#34892;&#24615;&#25110;ZeRO-DP&#65292;&#24494;&#25209;&#37327;&#25968;&#25454;&#34987;&#24182;&#34892;&#22788;&#29702;&#65292;&#36825;&#20135;&#29983;&#20102;&#20004;&#20010;&#32570;&#28857;&#65306;&#22312;&#21069;&#21521;&#20256;&#36882;&#32467;&#26463;&#26102;&#27169;&#22411;&#28608;&#27963;&#25152;&#38656;&#30340;&#24635;&#20869;&#23384;&#23792;&#20540;&#65292;&#24182;&#19988;&#26799;&#24230;&#24517;&#39035;&#22312;&#21453;&#21521;&#20256;&#25773;&#27493;&#39588;&#32467;&#26463;&#26102;&#21516;&#26102;&#24179;&#22343;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#23558;&#24494;&#25209;&#37327;&#30340;&#25191;&#34892;&#20174;&#21516;&#26102;&#21464;&#20026;&#39034;&#24207;&#25191;&#34892;&#65292;&#24102;&#26377;&#22343;&#21248;&#30340;&#24310;&#36831;&#12290;&#20197;&#30053;&#24494;&#26799;&#24230;&#24310;&#36831;&#20026;&#20195;&#20215;&#65292;&#28608;&#27963;&#25152;&#21344;&#30340;&#24635;&#20869;&#23384;&#26159;&#24658;&#23450;&#30340;&#65292;&#24182;&#19988;&#26799;&#24230;&#36890;&#20449;&#22312;&#35757;&#32451;&#27493;&#39588;&#26399;&#38388;&#26159;&#24179;&#34913;&#30340;&#12290;&#36890;&#36807;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;GPU&#25968;&#37327;&#65292;&#36890;&#36807;&#22312;&#24494;&#25209;&#37327;&#20043;&#38388;&#20849;&#20139;GPU&#12290;&#22312;ZeRO-DP&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20801;&#35768;&#20351;&#29992;&#28857;&#23545;&#28857;&#25805;&#20316;&#36827;&#34892;&#27169;&#22411;&#29366;&#24577;&#30340;&#36890;&#20449;&#65292;&#32780;&#38750; t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08837v1 Announce Type: cross  Abstract: Training large deep learning models requires parallelization techniques to scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches of data are processed in parallel, which creates two drawbacks: the total memory required to store the model's activations peaks at the end of the forward pass, and gradients must be simultaneously averaged at the end of the backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm shifting the execution of the micro-batches from simultaneous to sequential, with a uniform delay. At the cost of a slight gradient delay, the total memory taken by activations is constant, and the gradient communications are balanced during the training step. With Model Parallelism, our technique reduces the number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP framework, our technique allows communication of the model states with point-to-point operations rather t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21307;&#23398;&#36807;&#31243;&#30417;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;&#25216;&#26415;&#34701;&#20837;&#26412;&#20307;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08836</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#20301;&#32622;&#32534;&#30721;&#30340;&#21464;&#21387;&#22120;&#22312;&#21307;&#30103;&#36807;&#31243;&#30417;&#27979;&#20013;&#30340;&#30693;&#35782;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21307;&#23398;&#36807;&#31243;&#30417;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;&#25216;&#26415;&#34701;&#20837;&#26412;&#20307;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26159;&#19968;&#39033;&#26088;&#22312;&#39044;&#27979;&#27491;&#22312;&#36816;&#34892;&#30340;&#36807;&#31243;&#36319;&#36394;&#20449;&#24687;&#30340;&#36807;&#31243;&#25366;&#25496;&#20219;&#21153;&#65292;&#20363;&#22914;&#26368;&#27491;&#30830;&#30340;&#19979;&#19968;&#20010;&#35201;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#21487;&#20197;&#22312;&#38750;&#20856;&#22411;&#21644;&#38750;&#24179;&#20961;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;&#25216;&#26415;&#34701;&#20837;&#26412;&#20307;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25991;&#31456;&#20171;&#32461;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#21307;&#23398;&#36807;&#31243;&#30417;&#27979;&#20013;&#27491;&#22312;&#25910;&#38598;&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08836v1 Announce Type: cross  Abstract: Predictive process monitoring is a process mining task aimed at forecasting information about a running process trace, such as the most correct next activity to be executed. In medical domains, predictive process monitoring can provide valuable decision support in atypical and nontrivial situations. Decision support and quality assessment in medicine cannot ignore domain knowledge, in order to be grounded on all the available information (which is not limited to data) and to be really acceptable by end users.   In this paper, we propose a predictive process monitoring approach relying on the use of a {\em transformer}, a deep learning architecture based on the attention mechanism. A major contribution of our work lies in the incorporation of ontological domain-specific knowledge, carried out through a graph positional encoding technique. The paper presents and discusses the encouraging experimental result we are collecting in the domai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36275;&#29699;&#39046;&#22495;&#20013;&#29992;&#20110;&#26816;&#27979;&#39640;&#28508;&#21147;&#29699;&#21592;&#65292;&#30456;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08835</link><description>&lt;p&gt;
&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36275;&#29699;&#29699;&#21592;&#25628;&#23547;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stacking-based deep neural network for player scouting in football 1
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36275;&#29699;&#39046;&#22495;&#20013;&#29992;&#20110;&#26816;&#27979;&#39640;&#28508;&#21147;&#29699;&#21592;&#65292;&#30456;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DataScouting&#26159;&#19987;&#19994;&#20307;&#32946;&#30028;&#26368;&#30693;&#21517;&#30340;&#25968;&#25454;&#24212;&#29992;&#20043;&#19968;&#65292;&#29305;&#21035;&#26159;&#22312;&#36275;&#29699;&#39046;&#22495;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24222;&#22823;&#30340;&#29699;&#21592;&#25968;&#25454;&#24211;&#65292;&#20197;&#20415;&#26816;&#27979;&#28508;&#21147;&#24040;&#22823;&#30340;&#29699;&#21592;&#65292;&#28982;&#21518;&#30001;&#20154;&#31867;&#29699;&#25506;&#36827;&#34892;&#36827;&#19968;&#27493;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#39640;&#28508;&#21147;&#30340;&#36275;&#29699;&#29699;&#21592;&#12290;&#22312;&#24320;&#28304;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08835v1 Announce Type: cross  Abstract: Datascouting is one of the most known data applications in professional sport, and specifically football. Its objective is to analyze huge database of players in order to detect high potentials that can be then individually considered by human scouts. In this paper, we propose a stacking-based deep learning model to detect high potential football players. Applied on open-source database, our model obtains significantly better results that classical statistical methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#34920;&#26684;&#25968;&#25454;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26680;&#30149;&#27835;&#30103;&#32467;&#26524;&#65292;&#24182;&#22312;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08834</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#32467;&#26680;&#30149;&#27835;&#30103;&#32467;&#26524;&#30340;&#39044;&#27979;&#20998;&#26512;&#65306;&#21345;&#32435;&#22612;&#20811;&#37030;&#35268;&#27169;&#30340;&#32467;&#26680;&#30149;&#25968;&#25454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predictive Analysis of Tuberculosis Treatment Outcomes Using Machine Learning: A Karnataka TB Data Study at a Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#34920;&#26684;&#25968;&#25454;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26680;&#30149;&#27835;&#30103;&#32467;&#26524;&#65292;&#24182;&#22312;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26680;&#30149;(TB)&#20173;&#28982;&#26159;&#20840;&#29699;&#20581;&#24247;&#23041;&#32961;&#20043;&#19968;&#65292;&#22312;&#20840;&#29699;&#33268;&#27515;&#21407;&#22240;&#20013;&#25490;&#21517;&#38752;&#21069;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;(ML)&#24050;&#32463;&#25104;&#20026;&#19968;&#32929;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#20026;&#19982;TB&#27835;&#30103;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26680;&#30149;(TB)&#27835;&#30103;&#32467;&#26524;&#12290;&#23427;&#23558;&#36825;&#20010;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20174;&#21360;&#24230;&#30340;&#22269;&#23478;&#32467;&#26680;&#30149;&#25511;&#21046;&#39033;&#30446;NIKSHAY&#20013;&#33719;&#21462;&#30340;&#24739;&#32773;&#25968;&#25454;&#29983;&#25104;&#39118;&#38505;&#20998;&#25968;&#65292;&#35813;&#39033;&#30446;&#21253;&#25324;&#36229;&#36807;50&#19975;&#24739;&#32773;&#35760;&#24405;&#12290;&#25968;&#25454;&#39044;&#22788;&#29702;&#26159;&#30740;&#31350;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;2&#19975;&#21517;&#24739;&#32773;&#35760;&#24405;&#30340;&#39564;&#35777;&#38598;&#19978;&#23454;&#29616;&#20102;98%&#30340;&#21484;&#22238;&#29575;&#21644;0.95&#30340;AUC-ROC&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#22312;&#25913;&#36827;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32463;&#36807;&#20102;&#21508;&#31181;&#32771;&#34385;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08834v1 Announce Type: cross  Abstract: Tuberculosis (TB) remains a global health threat, ranking among the leading causes of mortality worldwide. In this context, machine learning (ML) has emerged as a transformative force, providing innovative solutions to the complexities associated with TB treatment.This study explores how machine learning, especially with tabular data, can be used to predict Tuberculosis (TB) treatment outcomes more accurately. It transforms this prediction task into a binary classification problem, generating risk scores from patient data sourced from NIKSHAY, India's national TB control program, which includes over 500,000 patient records.   Data preprocessing is a critical component of the study, and the model achieved an recall of 98% and an AUC-ROC score of 0.95 on the validation set, which includes 20,000 patient records.We also explore the use of Natural Language Processing (NLP) for improved model learning. Our results, corroborated by various m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#33021;&#26368;&#20248;&#30340;&#26368;&#31616;&#21333;&#31639;&#27861;&#65306;&#36820;&#22238;&#19977;&#20010;ERM&#20998;&#31867;&#22120;&#30340;&#22810;&#25968;&#25237;&#31080;&#65292;&#35777;&#26126;&#20854;&#23454;&#29616;&#20102;&#38169;&#35823;&#30340;&#26399;&#26395;&#26368;&#20248;&#36793;&#30028;&#65292;&#24182;&#24471;&#20986;&#36817;&#20046;&#26368;&#20248;&#27010;&#29575;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.08831</link><description>&lt;p&gt;
&#22810;&#25968;&#19977;&#32773;&#65306;&#26368;&#31616;&#21333;&#30340;&#26368;&#20248;&#23398;&#20064;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Majority-of-Three: The Simplest Optimal Learner?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#33021;&#26368;&#20248;&#30340;&#26368;&#31616;&#21333;&#31639;&#27861;&#65306;&#36820;&#22238;&#19977;&#20010;ERM&#20998;&#31867;&#22120;&#30340;&#22810;&#25968;&#25237;&#31080;&#65292;&#35777;&#26126;&#20854;&#23454;&#29616;&#20102;&#38169;&#35823;&#30340;&#26399;&#26395;&#26368;&#20248;&#36793;&#30028;&#65292;&#24182;&#24471;&#20986;&#36817;&#20046;&#26368;&#20248;&#27010;&#29575;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#29616;&#25968;&#25454;&#35774;&#32622;&#19979;&#21457;&#23637;&#26368;&#20339;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#26159;&#23398;&#20064;&#29702;&#35770;&#20013;&#20960;&#21313;&#24180;&#26469;&#30340;&#19968;&#20010;&#37325;&#22823;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#26159;&#27425;&#20248;&#30340;&#12290;&#20960;&#24180;&#21069;&#65292;Hanneke&#32456;&#20110;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;Hanneke&#30340;&#31639;&#27861;&#30456;&#24403;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#36820;&#22238;&#35768;&#22810;&#32463;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;ERM&#20998;&#31867;&#22120;&#30340;&#22810;&#25968;&#25237;&#31080;&#12290;&#22240;&#27492;&#65292;&#26368;&#33258;&#28982;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26368;&#31616;&#21333;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#26368;&#20248;&#30340;&#26368;&#31616;&#21333;&#31639;&#27861;&#65306;&#36820;&#22238;&#19977;&#20010;ERM&#20998;&#31867;&#22120;&#30340;&#22810;&#25968;&#25237;&#31080;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31639;&#27861;&#23454;&#29616;&#20102;&#20854;&#38169;&#35823;&#30340;&#26399;&#26395;&#26368;&#20248;&#36793;&#30028;&#65292;&#36825;&#26174;&#28982;&#26159;&#21333;&#20010;ERM&#20998;&#31867;&#22120;&#26080;&#27861;&#36798;&#21040;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#38169;&#35823;&#30340;&#36817;&#20046;&#26368;&#20248;&#27010;&#29575;&#36793;&#30028;&#12290;&#25105;&#20204;&#25512;&#27979;&#26356;&#22909;&#30340;&#20998;&#26512;&#23558;&#35777;&#26126;&#36825;&#20010;&#31639;&#27861;&#23454;&#38469;&#19978;&#22312;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08831v1 Announce Type: cross  Abstract: Developing an optimal PAC learning algorithm in the realizable setting, where empirical risk minimization (ERM) is suboptimal, was a major open problem in learning theory for decades. The problem was finally resolved by Hanneke a few years ago. Unfortunately, Hanneke's algorithm is quite complex as it returns the majority vote of many ERM classifiers that are trained on carefully selected subsets of the data. It is thus a natural goal to determine the simplest algorithm that is optimal. In this work we study the arguably simplest algorithm that could be optimal: returning the majority vote of three ERM classifiers. We show that this algorithm achieves the optimal in-expectation bound on its error which is provably unattainable by a single ERM classifier. Furthermore, we prove a near-optimal high-probability bound on this algorithm's error. We conjecture that a better analysis will prove that this algorithm is in fact optimal in the hig
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20551;&#26032;&#38395;&#20013;&#20010;&#20154;&#21644;&#31038;&#20250;&#20559;&#35265;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#20559;&#35265;&#23545;&#38598;&#20307;&#20915;&#31574;&#30340;&#28183;&#36879;&#65292;&#24182;&#35780;&#20272;&#20102;&#33258;&#36866;&#24212;&#32858;&#21512;&#31639;&#27861;&#22312;&#25552;&#39640;&#21028;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08829</link><description>&lt;p&gt;
&#32531;&#35299;&#38598;&#20307;&#20915;&#31574;&#20013;&#30340;&#20559;&#35265;&#65306;&#22312;&#38754;&#23545;&#20551;&#26032;&#38395;&#26102;&#25552;&#21319;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20551;&#26032;&#38395;&#20013;&#20010;&#20154;&#21644;&#31038;&#20250;&#20559;&#35265;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#20559;&#35265;&#23545;&#38598;&#20307;&#20915;&#31574;&#30340;&#28183;&#36879;&#65292;&#24182;&#35780;&#20272;&#20102;&#33258;&#36866;&#24212;&#32858;&#21512;&#31639;&#27861;&#22312;&#25552;&#39640;&#21028;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#21644;&#31038;&#20250;&#20559;&#35265;&#21066;&#24369;&#20102;&#20154;&#31867;&#39038;&#38382;&#30340;&#25928;&#21147;&#65292;&#22240;&#20026;&#23427;&#23548;&#33268;&#21028;&#26029;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#36896;&#25104;&#19981;&#21033;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;&#20551;&#26032;&#38395;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20154;&#31867;&#21442;&#19982;&#32773;&#35782;&#21035;&#34394;&#20551;&#26631;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32858;&#28966;&#28041;&#21450;&#25935;&#24863;&#29305;&#24449;&#30340;&#26631;&#39064;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20154;&#31867;&#21453;&#24212;&#22914;&#20309;&#21463;&#20182;&#20204;&#30340;&#20559;&#35265;&#25152;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#26029;&#20986;&#29616;&#30340;&#20010;&#20154;&#20559;&#35265;&#21450;&#20854;&#28183;&#36879;&#20837;&#38598;&#20307;&#20915;&#31574;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#21475;&#22240;&#32032;&#12289;&#26631;&#39064;&#31867;&#21035;&#20197;&#21450;&#20449;&#24687;&#21576;&#29616;&#26041;&#24335;&#26174;&#33879;&#24433;&#21709;&#20154;&#31867;&#21028;&#26029;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#25968;&#25454;&#20316;&#20026;&#22522;&#20934;&#38382;&#39064;&#65292;&#35780;&#20272;&#33258;&#36866;&#24212;&#32858;&#21512;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#25552;&#39640;&#30340;&#20934;&#30830;&#24615;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#26032;&#31639;&#27861;&#23835;&#36215;&#26102;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08829v1 Announce Type: cross  Abstract: Individual and social biases undermine the effectiveness of human advisers by inducing judgment errors which can disadvantage protected groups. In this paper, we study the influence these biases can have in the pervasive problem of fake news by evaluating human participants' capacity to identify false headlines. By focusing on headlines involving sensitive characteristics, we gather a comprehensive dataset to explore how human responses are shaped by their biases. Our analysis reveals recurring individual biases and their permeation into collective decisions. We show that demographic factors, headline categories, and the manner in which information is presented significantly influence errors in human judgment. We then use our collected data as a benchmark problem on which we evaluate the efficacy of adaptive aggregation algorithms. In addition to their improved accuracy, our results highlight the interactions between the emergence of c
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#20247;&#21253;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#20808;&#21069;&#30495;&#30456;&#25512;&#26029;&#31639;&#27861;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#38271;&#26399;&#21644;&#22312;&#32447;&#30495;&#30456;&#25512;&#26029;&#25552;&#20379;&#20102;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08826</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#22312;&#32447;&#37096;&#32626;&#30340;&#30495;&#30456;&#25512;&#26029;&#31639;&#27861;&#39564;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08826
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#20247;&#21253;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#20808;&#21069;&#30495;&#30456;&#25512;&#26029;&#31639;&#27861;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#38271;&#26399;&#21644;&#22312;&#32447;&#30495;&#30456;&#25512;&#26029;&#25552;&#20379;&#20102;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#26631;&#27880;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#20247;&#21253;&#12290;&#20026;&#20102;&#20445;&#35777;&#25968;&#25454;&#26631;&#27880;&#30340;&#36136;&#37327;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#25910;&#38598;&#22810;&#20010;&#27880;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#30495;&#30456;&#25512;&#26029;&#31639;&#27861;&#26469;&#20934;&#30830;&#25512;&#26029;&#30495;&#23454;&#26631;&#31614;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#21457;&#24067;&#20102;&#29992;&#20110;&#35780;&#20272;&#30495;&#30456;&#25512;&#26029;&#31639;&#27861;&#26377;&#25928;&#24615;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#38598;&#20013;&#22312;&#21333;&#19968;&#31867;&#22411;&#30340;&#20247;&#21253;&#20219;&#21153;&#65292;&#24182;&#24573;&#30053;&#20102;&#19982;&#24037;&#20316;&#32773;&#27880;&#37322;&#27963;&#21160;&#30456;&#20851;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#36825;&#20123;&#38480;&#21046;&#20005;&#37325;&#38480;&#21046;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#21644;&#22312;&#32447;&#30495;&#30456;&#25512;&#26029;&#30340;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20174;&#19968;&#23478;&#30495;&#23454;&#20247;&#21253;&#24179;&#21488;&#25910;&#38598;&#30340;&#22823;&#37327;&#20247;&#21253;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#32422;&#20004;&#21315;&#21517;&#24037;&#20316;&#32773;&#12289;&#19968;&#30334;&#19975;&#20010;&#20219;&#21153;&#12289;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08826v1 Announce Type: cross  Abstract: For the purpose of efficient and cost-effective large-scale data labeling, crowdsourcing is increasingly being utilized. To guarantee the quality of data labeling, multiple annotations need to be collected for each data sample, and truth inference algorithms have been developed to accurately infer the true labels. Despite previous studies having released public datasets to evaluate the efficacy of truth inference algorithms, these have typically focused on a single type of crowdsourcing task and neglected the temporal information associated with workers' annotation activities. These limitations significantly restrict the practical applicability of these algorithms, particularly in the context of long-term and online truth inference. In this paper, we introduce a substantial crowdsourcing annotation dataset collected from a real-world crowdsourcing platform. This dataset comprises approximately two thousand workers, one million tasks, a
&lt;/p&gt;</description></item><item><title>LoRA-SP&#21033;&#29992;&#38543;&#26426;&#21322;&#36873;&#25321;&#21442;&#25968;&#20923;&#32467;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#26377;&#25928;&#24179;&#34913;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20219;&#21153;&#29305;&#23450;&#20248;&#21270;&#30340;&#36866;&#24212;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08822</link><description>&lt;p&gt;
LoRA-SP&#65306;&#29992;&#20110;&#36164;&#28304;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#37096;&#20998;&#21442;&#25968;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08822
&lt;/p&gt;
&lt;p&gt;
LoRA-SP&#21033;&#29992;&#38543;&#26426;&#21322;&#36873;&#25321;&#21442;&#25968;&#20923;&#32467;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#26377;&#25928;&#24179;&#34913;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20219;&#21153;&#29305;&#23450;&#20248;&#21270;&#30340;&#36866;&#24212;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoRA-SP&#65288;&#31616;&#21270;&#37096;&#20998;&#21442;&#25968;&#36866;&#24212;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26694;&#26550;&#20869;&#30340;&#38543;&#26426;&#21322;&#36873;&#25321;&#21442;&#25968;&#20923;&#32467;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20219;&#21153;&#29305;&#23450;&#20248;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#38543;&#26426;&#26426;&#21046;&#65292;LoRA-SP &#30830;&#23450;&#35201;&#26356;&#26032;&#25110;&#20923;&#32467;&#21738;&#20123;&#21442;&#25968;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934; NLP &#20219;&#21153;&#20013;&#35780;&#20272;&#20102; LoRA-SP&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#20256;&#32479;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#30456;&#27604;&#65292;&#33021;&#22815;&#20197;&#22823;&#22823;&#38477;&#20302;&#30340;&#36164;&#28304;&#28040;&#32791;&#23454;&#29616;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;LoRA-SP &#30340;&#21019;&#26032;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#20808;&#36827;&#30340; NLP &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08822v1 Announce Type: cross  Abstract: In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited sett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;PSF&#30340;&#21464;&#21270;&#26469;&#26174;&#33879;&#21152;&#36895;Sharpness-aware Minimization&#65288;SAM&#65289;&#65292;&#23454;&#29616;&#20102;&#19982;SAM&#30456;&#24403;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08821</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#21270;&#20272;&#35745;&#23454;&#29616;&#26377;&#25928;&#30340;&#26799;&#24230;&#26679;&#26412;&#22823;&#23567;&#65292;&#21152;&#36895;&#25935;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Effective Gradient Sample Size via Variation Estimation for Accelerating Sharpness aware Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;PSF&#30340;&#21464;&#21270;&#26469;&#26174;&#33879;&#21152;&#36895;Sharpness-aware Minimization&#65288;SAM&#65289;&#65292;&#23454;&#29616;&#20102;&#19982;SAM&#30456;&#24403;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#20102;&#8220;&#25935;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#8221;&#65288;SAM&#65289;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;SAM&#22312;&#27599;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#37117;&#20250;&#35745;&#31639;&#26799;&#24230;&#20004;&#27425;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#25104;&#26412;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22686;&#21152;&#19968;&#20493;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#36895;SAM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;SAM&#30340;&#26799;&#24230;&#26159;SGD&#26799;&#24230;&#21644;&#31532;&#19968;&#38454;&#26799;&#24230;&#19978;&#30340;&#20108;&#38454;&#26799;&#24230;&#30697;&#38453;&#25237;&#24433;&#65288;PSF&#65289;&#30340;&#32452;&#21512;&#12290;PSF&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#36880;&#28176;&#22686;&#21152;&#30340;&#21464;&#21270;&#39057;&#29575;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#22522;&#20110;PSF&#30340;&#21464;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#23558;&#24050;&#37319;&#26679;&#30340;PSF&#37325;&#29992;&#20110;&#38750;&#25277;&#26679;&#36845;&#20195;&#12290;&#22823;&#37327;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#36798;&#21040;&#20102;&#19982;SAM&#30456;&#24403;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08821v1 Announce Type: cross  Abstract: Sharpness-aware Minimization (SAM) has been proposed recently to improve model generalization ability. However, SAM calculates the gradient twice in each optimization step, thereby doubling the computation costs compared to stochastic gradient descent (SGD). In this paper, we propose a simple yet efficient sampling method to significantly accelerate SAM. Concretely, we discover that the gradient of SAM is a combination of the gradient of SGD and the Projection of the Second-order gradient matrix onto the First-order gradient (PSF). PSF exhibits a gradually increasing frequency of change during the training process. To leverage this observation, we propose an adaptive sampling method based on the variation of PSF, and we reuse the sampled PSF for non-sampling iterations. Extensive empirical results illustrate that the proposed method achieved state-of-the-art accuracies comparable to SAM on diverse network architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26041;&#38754;&#30340;&#33203;&#39135;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;Diet-ODIN&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#33203;&#39135;&#27169;&#24335;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.08820</link><description>&lt;p&gt;
Diet-ODIN&#65306;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#33203;&#39135;&#27169;&#24335;&#19979;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26041;&#38754;&#30340;&#33203;&#39135;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;Diet-ODIN&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#33203;&#39135;&#27169;&#24335;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#19968;&#30452;&#26159;&#32654;&#22269;&#31038;&#20250;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#23613;&#31649;&#33647;&#29289;&#36741;&#21161;&#27835;&#30103;&#65288;MAT&#65289;&#34987;&#35748;&#20026;&#26159;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#21644;&#25104;&#30270;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20294;&#21508;&#31181;&#21103;&#20316;&#29992;&#21487;&#33021;&#24341;&#21457;&#38463;&#29255;&#31867;&#33647;&#29289;&#20877;&#27425;&#28389;&#29992;&#12290;&#38500;MAT&#22806;&#65292;&#33203;&#39135;&#33829;&#20859;&#24178;&#39044;&#22312;&#38450;&#27490;&#21644;&#24247;&#22797;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#33203;&#39135;&#27169;&#24335;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20043;&#38388;&#20196;&#20154;&#25285;&#24551;&#30340;&#20851;&#32852;&#30340;&#30740;&#31350;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#39318;&#27425;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#30456;&#20851;&#30340;&#22823;&#35268;&#27169;&#22810;&#26041;&#38754;&#33203;&#39135;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#21363;&#35299;&#37322;&#24615;&#33203;&#39135;&#27169;&#24335;&#19979;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#26816;&#27979;&#65288;Diet-ODIN&#65289;&#65292;&#29992;&#20110;&#36830;&#25509;&#24322;&#36136;&#22270;&#65288;HG&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#35782;&#21035;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#32773;&#24182;&#35299;&#37322;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08820v1 Announce Type: cross  Abstract: The opioid crisis has been one of the most critical society concerns in the United States. Although the medication assisted treatment (MAT) is recognized as the most effective treatment for opioid misuse and addiction, the various side effects can trigger opioid relapse. In addition to MAT, the dietary nutrition intervention has been demonstrated its importance in opioid misuse prevention and recovery. However, research on the alarming connections between dietary patterns and opioid misuse remain under-explored. In response to this gap, in this paper, we first establish a large-scale multifaceted dietary benchmark dataset related to opioid users at the first attempt and then develop a novel framework - i.e., namely Opioid Misuse Detection with Interpretable Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large language model (LLM) for the identification of users with opioid misuse and the interpretation of their a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08819</link><description>&lt;p&gt;
&#28201;&#24230;&#35745;&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Thermometer: Towards Universal Calibration for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#24178;&#39044;&#25514;&#26045;&#22914;&#25351;&#20196;&#35843;&#25972;&#36890;&#24120;&#20250;&#23548;&#33268;&#26657;&#20934;&#19981;&#20339;&#30340;LLMs&#12290;&#23613;&#31649;&#26657;&#20934;&#22312;&#20256;&#32479;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#35752;&#65292;&#20294;&#23545;LLMs&#36827;&#34892;&#26657;&#20934;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#26469;&#33258;LLMs&#30340;&#20005;&#26684;&#35745;&#31639;&#35201;&#27714;&#65292;&#20063;&#26469;&#33258;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#12290;THERMOMETER&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#12290;&#23427;&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#39640;&#65292;&#20445;&#25345;&#20102;LLM&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#26032;&#20219;&#21153;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#26657;&#20934;&#21709;&#24212;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.08818</link><description>&lt;p&gt;
&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#23558;&#20020;&#24202;&#35760;&#24405;&#21644;&#31508;&#35760;&#19982;&#36229;&#22270;&#21644;LLM&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08818
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#22312;&#36817;&#20960;&#21313;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;EHRs&#36890;&#24120;&#21253;&#21547;&#24322;&#26500;&#20449;&#24687;&#65292;&#22914;&#34920;&#26684;&#24418;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#25991;&#26412;&#31508;&#35760;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;EHRs&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#20379;&#24739;&#32773;&#20581;&#24247;&#29366;&#24577;&#30340;&#26356;&#23436;&#25972;&#22270;&#29255;&#12290;&#23613;&#31649;&#23545;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#19981;&#21516;&#31867;&#22411;EHR&#25968;&#25454;&#30340;&#34701;&#21512;&#65288;&#22810;&#27169;&#24577;&#34701;&#21512;&#65289;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#21307;&#30103;&#32534;&#30721;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#20070;&#38754;&#31508;&#35760;&#20013;&#23384;&#22312;&#30340;&#22122;&#38899;&#21644;&#20887;&#20313;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#23558;EHR&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08818v1 Announce Type: cross  Abstract: Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured data in textual notes. Different types of information in EHRs can complement each other and provide a more complete picture of the health status of a patient. While there has been a lot of research on representation learning of structured EHR data, the fusion of different types of EHR data (multimodal fusion) is not well studied. This is mostly because of the complex medical coding systems used and the noise and redundancy present in the written notes. In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively. Our framework uses a two-level infusion strategy to combine medical concept semantics and clinical note semantics into hypergrap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#36127;&#36733;&#24179;&#34913;&#31995;&#32479;&#65292;&#21033;&#29992;Open-RAN xAPP&#26694;&#26550;&#21644;&#36817;&#23454;&#26102;&#23556;&#39057;&#25509;&#21475;&#25511;&#21046;&#22120;&#65288;near-RT RIC&#65289;&#23454;&#26045;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30446;&#21069;UE&#20351;&#29992;&#30340;&#26368;&#22823;&#20449;&#22122;&#27604;&#65288;MAX-SINR&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#25552;&#20379;&#26356;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08813</link><description>&lt;p&gt;
&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#19982;5G&#36127;&#36733;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Q-Learning and 5G load balancing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#36127;&#36733;&#24179;&#34913;&#31995;&#32479;&#65292;&#21033;&#29992;Open-RAN xAPP&#26694;&#26550;&#21644;&#36817;&#23454;&#26102;&#23556;&#39057;&#25509;&#21475;&#25511;&#21046;&#22120;&#65288;near-RT RIC&#65289;&#23454;&#26045;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30446;&#21069;UE&#20351;&#29992;&#30340;&#26368;&#22823;&#20449;&#22122;&#27604;&#65288;MAX-SINR&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#25552;&#20379;&#26356;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#34562;&#31389;&#32593;&#32476;&#25216;&#26415;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22522;&#31449;&#65288;BS&#65289;&#36127;&#36733;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#20037;&#24615;&#38382;&#39064;&#12290;&#34429;&#28982;&#38598;&#20013;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#26469;&#36890;&#30693;&#27599;&#20010;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#21508;&#20010;BS&#30340;&#36127;&#36733;&#24773;&#20917;&#12290;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#36127;&#36733;&#24179;&#34913;&#20351;&#24471;&#26234;&#33021;UE&#21487;&#20197;&#29420;&#31435;&#36873;&#25321;&#26368;&#20339;BS&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#21521;&#32593;&#32476;&#26292;&#38706;&#30340;&#31169;&#20154;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08813v1 Announce Type: cross  Abstract: Despite advances in cellular network technology, base station (BS) load balancing remains a persistent problem. Although centralized resource allocation methods can address the load balancing problem, it still remains an NP-hard problem. In this research, we study how federated deep Q learning can be used to inform each user equipment (UE) of the each BS's load conditions. Federated deep Q learning's load balancing enables intelligent UEs to independently select the best BS while also limiting the amount of private information exposed to the network.   In this study, we propose and analyze a federated deep Q learning load balancing system, which is implemented using the Open-RAN xAPP framework and the near-Real Time Radio Interface Controller (near-RT RIC). Our simulation results indicate that compared to the maximum Signal-To-Noise-Ratio (MAX-SINR) method currently used by UEs, our proposed deep Q learning model can consistently provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#8220;Gore&#25193;&#25955;LoRA&#27169;&#22411;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#25797;&#38271;&#29983;&#25104;&#36229;&#36924;&#30495;&#26292;&#21147;&#21644;&#27969;&#34880;&#35270;&#35273;&#25928;&#26524;&#30340;&#21019;&#26032;AI&#27169;&#22411;&#65292;&#24378;&#35843;&#22312;&#20854;&#21019;&#20316;&#19982;&#23454;&#26045;&#20013;&#38656;&#35201;&#35748;&#30495;&#35752;&#35770;AI&#12289;&#33402;&#26415;&#21644;&#26292;&#21147;&#30340;&#34701;&#21512;&#65292;&#20027;&#24352;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#36947;&#24503;&#37096;&#32626;&#36825;&#20123;&#24378;&#22823;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.08812</link><description>&lt;p&gt;
Gore&#25193;&#25955;LoRA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gore Diffusion LoRA Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#8220;Gore&#25193;&#25955;LoRA&#27169;&#22411;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#25797;&#38271;&#29983;&#25104;&#36229;&#36924;&#30495;&#26292;&#21147;&#21644;&#27969;&#34880;&#35270;&#35273;&#25928;&#26524;&#30340;&#21019;&#26032;AI&#27169;&#22411;&#65292;&#24378;&#35843;&#22312;&#20854;&#21019;&#20316;&#19982;&#23454;&#26045;&#20013;&#38656;&#35201;&#35748;&#30495;&#35752;&#35770;AI&#12289;&#33402;&#26415;&#21644;&#26292;&#21147;&#30340;&#34701;&#21512;&#65292;&#20027;&#24352;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#36947;&#24503;&#37096;&#32626;&#36825;&#20123;&#24378;&#22823;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08812v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#24433;&#21709;&#20102;&#25105;&#20204;&#19982;&#26292;&#21147;&#30340;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#31639;&#27861;&#21019;&#20316;&#26292;&#21147;&#22270;&#20687;&#30340;&#20262;&#29702;&#35752;&#35770;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#8220;Gore&#25193;&#25955;LoRA&#27169;&#22411;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;AI&#27169;&#22411;&#65292;&#25797;&#38271;&#29983;&#25104;&#34920;&#29616;&#28608;&#28872;&#26292;&#21147;&#21644;&#27969;&#34880;&#30340;&#36229;&#36924;&#30495;&#35270;&#35273;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#25506;&#35752;&#28085;&#30422;&#20102;&#35813;&#27169;&#22411;&#30340;&#25216;&#26415;&#22797;&#26434;&#24615;&#12289;&#21487;&#34892;&#24212;&#29992;&#20197;&#21450;&#20854;&#21033;&#29992;&#20013;&#22266;&#26377;&#30340;&#20262;&#29702;&#22256;&#22659;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#21019;&#20316;&#21644;&#23454;&#26045;&#38656;&#35201;&#35748;&#30495;&#35752;&#35770;AI&#12289;&#33402;&#26415;&#21644;&#26292;&#21147;&#30340;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#65292;&#20513;&#23548;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#36947;&#24503;&#37096;&#32626;&#36825;&#20123;&#24378;&#22823;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08812v1 Announce Type: cross  Abstract: The Emergence of Artificial Intelligence (AI) has significantly impacted our engagement with violence, sparking ethical deliberations regarding the algorithmic creation of violent imagery. This paper scrutinizes the "Gore Diffusion LoRA Model," an innovative AI model proficient in generating hyper-realistic visuals portraying intense violence and bloodshed. Our exploration encompasses the model's technical intricacies, plausible applications, and the ethical quandaries inherent in its utilization. We contend that the creation and implementation of such models warrant a meticulous discourse concerning the convergence of AI, art, and violence. Furthermore, we advocate for a structured framework advocating responsible development and ethical deployment of these potent technologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#36793;&#32536;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#36136;&#37327;&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#29289;&#32852;&#32593;&#26550;&#26500;&#20013;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.08810</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26550;&#26500;&#20013;&#27604;&#36739;&#36793;&#32536;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#21442;&#25968;&#19982;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Comparison of edge computing methods in Internet of Things architectures for efficient estimation of indoor environmental parameters with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#36793;&#32536;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#36136;&#37327;&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#29289;&#32852;&#32593;&#26550;&#26500;&#20013;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#35774;&#22791;&#25968;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#65292;&#21152;&#19978;&#30446;&#21069;&#20174;&#20113;&#35745;&#31639;&#36716;&#21521;&#36793;&#32536;&#35745;&#31639;&#30340;&#36235;&#21183;&#36843;&#20351;&#25105;&#20204;&#38656;&#35201;&#20351;&#29992;&#33021;&#28304;&#39640;&#25928;&#30340;&#35774;&#22791;&#22312;&#25968;&#25454;&#28304;&#38468;&#36817;&#36827;&#34892;&#39640;&#25928;&#21487;&#38752;&#30340;&#25968;&#25454;&#22788;&#29702;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#36793;&#32536;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20197;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#36136;&#37327;&#21442;&#25968;&#65292;&#27604;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#31867;&#22411;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#29289;&#32852;&#32593;&#26550;&#26500;&#23454;&#29616;&#65292;&#36890;&#36807;&#26080;&#32447;&#36830;&#25509;&#65292;&#20849;&#20139;&#21830;&#29992;&#27169;&#22359;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#21644;&#20256;&#24863;&#65292;&#27604;&#22914;&#28201;&#24230;&#12289;&#28287;&#24230;&#12289;&#29031;&#24230;&#12289;CO2&#21644;&#20854;&#20182;&#27668;&#20307;&#20256;&#24863;&#22120;&#12290;&#38598;&#20013;&#24335;&#26041;&#27861;&#20351;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#21644;&#28040;&#24687;&#38431;&#21015;&#36965;&#27979;&#20256;&#36755;&#21327;&#35758;&#65292;&#32780;&#20998;&#24067;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08810v1 Announce Type: cross  Abstract: The large increase in the number of Internet of Things (IoT) devices have revolutionised the way data is processed, which added to the current trend from cloud to edge computing has resulted in the need for efficient and reliable data processing near the data sources using energy-efficient devices. Two methods based on low-cost edge-IoT architectures are proposed to implement lightweight Machine Learning (ML) models that estimate indoor environmental quality (IEQ) parameters, such as Artificial Neural Networks of Multilayer Perceptron type. Their implementation is based on centralised and distributed parallel IoT architectures, connected via wireless, which share commercial off-the-self modules for data acquisition and sensing, such as sensors for temperature, humidity, illuminance, CO2, and other gases. The centralised method uses a Graphics Processing Unit and the Message Queuing Telemetry Transport protocol, but the distributed meth
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#29305;&#24449;&#30456;&#20284;&#24615;&#23398;&#20064;&#26469;&#25552;&#21319;&#23545;&#25239;&#24615;&#24378;&#30340;Deepfake&#26816;&#27979;&#65292;&#20197;&#21306;&#20998;&#30495;&#20551;&#23454;&#20363;&#24182;&#26368;&#22823;&#21270;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#26410;&#25200;&#21160;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08806</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#29305;&#24449;&#30456;&#20284;&#24615;&#23398;&#20064;&#26469;&#25552;&#21319;&#23545;&#25239;&#24615;&#24378;&#30340;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Deepfake Detection via Adversarial Feature Similarity Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08806
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#29305;&#24449;&#30456;&#20284;&#24615;&#23398;&#20064;&#26469;&#25552;&#21319;&#23545;&#25239;&#24615;&#24378;&#30340;Deepfake&#26816;&#27979;&#65292;&#20197;&#21306;&#20998;&#30495;&#20551;&#23454;&#20363;&#24182;&#26368;&#22823;&#21270;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#26410;&#25200;&#21160;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deepfake&#25216;&#26415;&#24341;&#21457;&#20102;&#23545;&#25968;&#23383;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#25285;&#24551;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20256;&#25773;&#30340;Deepfake&#25216;&#26415;&#20063;&#24102;&#26469;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26032;&#25361;&#25112;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#24494;&#23567;&#30340;&#12289;&#23519;&#35273;&#19981;&#21040;&#30340;&#25200;&#21160;&#26469;&#25805;&#32437;Deepfake&#35270;&#39057;&#65292;&#27450;&#39575;&#26816;&#27979;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Adversarial Feature Similarity Learning&#65288;AFSL&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#19977;&#31181;&#22522;&#26412;&#30340;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#33539;&#24335;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#21644;&#26435;&#37325;&#21521;&#37327;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#21306;&#20998;&#30495;&#23454;&#21644;&#34394;&#20551;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#26368;&#22823;&#21270;&#23545;&#25239;&#24615;&#25200;&#21160;&#31034;&#20363;&#21644;&#26410;&#32463;&#25200;&#21160;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#32780;&#19981;&#31649;&#23427;&#20204;&#26159;&#30495;&#23454;&#30340;&#36824;&#26159;&#34394;&#20551;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20854;&#26368;&#22823;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08806v1 Announce Type: cross  Abstract: Deepfake technology has raised concerns about the authenticity of digital content, necessitating the development of effective detection methods. However, the widespread availability of deepfakes has given rise to a new challenge in the form of adversarial attacks. Adversaries can manipulate deepfake videos with small, imperceptible perturbations that can deceive the detection models into producing incorrect outputs. To tackle this critical issue, we introduce Adversarial Feature Similarity Learning (AFSL), which integrates three fundamental deep feature learning paradigms. By optimizing the similarity between samples and weight vectors, our approach aims to distinguish between real and fake instances. Additionally, we aim to maximize the similarity between both adversarially perturbed examples and unperturbed examples, regardless of their real or fake nature. Moreover, we introduce a regularization technique that maximizes the dissimil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#24418;&#24577;&#31639;&#27861;SFDFA&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#32447;&#35745;&#31639;&#20934;&#30830;&#30340;&#23616;&#37096;&#26799;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.08804</link><description>&lt;p&gt;
&#38024;&#23545;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#26799;&#24230;&#20272;&#35745;&#30340;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Forward Direct Feedback Alignment for Online Gradient Estimates of Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#24418;&#24577;&#31639;&#27861;SFDFA&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#32447;&#35745;&#31639;&#20934;&#30830;&#30340;&#23616;&#37096;&#26799;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#19968;&#31181;&#25214;&#21040;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#26356;&#33410;&#33021;&#36873;&#25321;&#30340;&#20852;&#36259;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#24179;&#21488;&#19978;&#20197;&#39640;&#25928;&#33410;&#33021;&#30340;&#26041;&#24335;&#36827;&#34892;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#22312;&#35757;&#32451;&#31639;&#27861;&#30340;&#35774;&#35745;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#22312;&#37027;&#20123;&#24179;&#21488;&#19978;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#24418;&#24577;&#31639;&#27861;&#65292;&#21363;\textit{&#23574;&#38160;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;}&#65288;SFDFA&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#23545;\textit{&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;}&#30340;&#25913;&#36827;&#65292;&#29992;&#20110;&#35757;&#32451;SNN&#12290;SFDFA&#23558;&#36755;&#20986;&#21644;&#38544;&#34255;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#26435;&#37325;&#20272;&#35745;&#20026;&#21453;&#39304;&#36830;&#25509;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25551;&#36848;&#22914;&#20309;&#20934;&#30830;&#35745;&#31639;&#22312;&#32447;&#26041;&#24335;&#20013;&#30340;&#23574;&#23792;&#23616;&#37096;&#26799;&#24230;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#31070;&#32463;&#20803;&#20869;&#30340;&#21518;&#31361;&#35302;&#23574;&#23792;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25512;&#23548;&#20986;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08804v1 Announce Type: cross  Abstract: There is an interest in finding energy efficient alternatives to current state of the art neural network training algorithms. Spiking neural network are a promising approach, because they can be simulated energy efficiently on neuromorphic hardware platforms. However, these platforms come with limitations on the design of the training algorithm. Most importantly, backpropagation cannot be implemented on those. We propose a novel neuromorphic algorithm, the \textit{Spiking Forward Direct Feedback Alignment} (SFDFA) algorithm, an adaption of \textit{Forward Direct Feedback Alignment} to train SNNs. SFDFA estimates the weights between output and hidden neurons as feedback connections. The main contribution of this paper is to describe how exact local gradients of spikes can be computed in an online manner while taking into account the intra-neuron dependencies between post-synaptic spikes and derive a dynamical system for neuromorphic har
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.08802</link><description>&lt;p&gt;
&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of Generative Artificial Intelligence for Companies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#29305;&#21035;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#36805;&#36895;&#36827;&#20837;&#20225;&#19994;&#65292;&#20294;&#32570;&#20047;&#20805;&#20998;&#30340;&#27835;&#29702;&#65292;&#24102;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23613;&#31649;&#23545;GenAI&#20855;&#26377;&#21464;&#38761;&#24615;&#36136;&#21644;&#30417;&#31649;&#25514;&#26045;&#30340;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#28041;&#21450;&#32452;&#32455;&#27835;&#29702;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#19994;&#21153;&#35270;&#35282;&#12290;&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#23427;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#65292;&#36824;&#36890;&#36807;&#21046;&#23450;&#36866;&#29992;&#20110;&#20225;&#19994;&#20869;&#30340;GenAI&#27835;&#29702;&#26694;&#26550;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35814;&#32454;&#25551;&#36848;&#20102;&#33539;&#22260;&#12289;&#30446;&#26631;&#21644;&#27835;&#29702;&#26426;&#21046;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;GenAI&#27835;&#29702;&#30340;&#26041;&#27861;&#65292;&#20026;&#20225;&#19994;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#37319;&#29992;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;&#23545;&#20110;&#25216;&#26415;&#20154;&#21592;&#26469;&#35828;&#65292;&#20063;&#26377;&#21161;&#20110;&#25299;&#23485;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#25439;&#22833;&#20989;&#25968;&#25628;&#32034;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#19977;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;NeuroLoss1&#12289;NeuroLoss2&#21644;NeuroLoss3&#65292;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08793</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#25439;&#22833;&#20989;&#25968;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08793
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#25439;&#22833;&#20989;&#25968;&#25628;&#32034;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#19977;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;NeuroLoss1&#12289;NeuroLoss2&#21644;NeuroLoss3&#65292;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#26469;&#23398;&#20064;&#65292;&#20294;&#26159;&#21364;&#20351;&#29992;&#20934;&#30830;&#24230;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290; &#36825;&#31181;&#24046;&#24322;&#24615;&#20419;&#20351;&#20102;&#31070;&#32463;&#25439;&#22833;&#20989;&#25968;&#25628;&#32034;&#65288;NLFS&#65289;&#65292;&#21363;&#23547;&#25214;&#21487;&#20197;&#26367;&#20195;&#31070;&#32463;&#32593;&#32476;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290; &#25105;&#20204;&#23558;NLFS&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLFS&#25628;&#32034;&#31354;&#38388;&#65292;&#40723;&#21169;&#25506;&#32034;&#26356;&#22810;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#36716;&#25442;&#21040;&#22823;&#35268;&#27169;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#20989;&#25968;&#12290; &#25105;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#36827;&#21270;&#36827;&#34892;&#31354;&#38388;&#25628;&#32034;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#21464;&#24322;&#30340;&#24180;&#40836;&#36951;&#20256;&#31639;&#27861;&#12290; &#32463;&#36807;&#36827;&#21270;&#21644;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#28040;&#38500;&#26041;&#26696;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#24471;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#22810;&#20010;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#20043;&#38388;&#20256;&#36882;&#65292;&#20197;&#35780;&#20272;&#27867;&#21270;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19977;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20998;&#21035;&#31216;&#20026;NeuroLoss1&#12289;NeuroLoss2&#21644;NeuroLoss3&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08793v1 Announce Type: cross  Abstract: For classification, neural networks typically learn by minimizing cross-entropy, but are evaluated and compared using accuracy. This disparity suggests neural loss function search (NLFS), the search for a drop-in replacement loss function of cross-entropy for neural networks. We apply NLFS to image classifier convolutional neural networks. We propose a new search space for NLFS that encourages more diverse loss functions to be explored, and a surrogate function that accurately transfers to large-scale convolutional neural networks. We search the space using regularized evolution, a mutation-only aging genetic algorithm. After evolution and a proposed loss function elimination protocol, we transferred the final loss functions across multiple architectures, datasets, and image augmentation techniques to assess generalization. In the end, we discovered three new loss functions, called NeuroLoss1, NeuroLoss2, and NeuroLoss3 that were able 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19982;&#36793;&#32536;AI&#21152;&#36895;&#22120;&#22312;&#23454;&#26102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;Loihi&#30456;&#36739;&#20110;Coral TPU&#20855;&#26377;&#26356;&#20302;&#30340;&#21151;&#32791;&#21644;&#33021;&#32791;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.08792</link><description>&lt;p&gt;
&#23454;&#26102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65306;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19982;&#36793;&#32536;AI&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Realtime Facial Expression Recognition: Neuromorphic Hardware vs. Edge AI Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19982;&#36793;&#32536;AI&#21152;&#36895;&#22120;&#22312;&#23454;&#26102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;Loihi&#30456;&#36739;&#20110;Coral TPU&#20855;&#26377;&#26356;&#20302;&#30340;&#21151;&#32791;&#21644;&#33021;&#32791;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23454;&#26102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#31995;&#32479;&#65292;&#20316;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;FER&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#20004;&#31181;&#30828;&#20214;&#36873;&#39033;&#65306;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#21644;&#36793;&#32536;AI&#21152;&#36895;&#22120;&#12290;&#30740;&#31350;&#21253;&#25324;&#23545;Intel Loihi&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#21644;&#22235;&#20010;&#19981;&#21516;&#36793;&#32536;&#24179;&#21488;&#65288;Raspberry Pi-4&#12289;Intel&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26834;&#65288;NSC&#65289;&#12289;Jetson Nano&#21644;Coral TPU&#65289;&#36827;&#34892;&#35814;&#23613;&#23454;&#39564;&#65292;&#25552;&#20379;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;Coral TPU&#65292;Loihi&#21487;&#20197;&#23454;&#29616;&#22823;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#21151;&#32791;&#38477;&#20302;&#21644;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#33410;&#33021;&#65292;&#32780; Coral TPU&#21017;&#26159;&#21151;&#32791;&#26368;&#20302;&#12289;&#33021;&#32791;&#26368;&#23569;&#30340;&#36793;&#32536;AI&#21152;&#36895;&#22120;&#12290;&#22312;&#27492;&#38477;&#20302;&#21151;&#32791;&#21644;&#33021;&#32791;&#30340;&#21516;&#26102;&#65292;&#31070;&#32463;&#24418;&#24577;&#35299;&#20915;&#26041;&#26696;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08792v1 Announce Type: cross  Abstract: The paper focuses on real-time facial expression recognition (FER) systems as an important component in various real-world applications such as social robotics. We investigate two hardware options for the deployment of FER machine learning (ML) models at the edge: neuromorphic hardware versus edge AI accelerators. Our study includes exhaustive experiments providing comparative analyses between the Intel Loihi neuromorphic processor and four distinct edge platforms: Raspberry Pi-4, Intel Neural Compute Stick (NSC), Jetson Nano, and Coral TPU. The results obtained show that Loihi can achieve approximately two orders of magnitude reduction in power dissipation and one order of magnitude energy savings compared to Coral TPU which happens to be the least power-intensive and energy-consuming edge AI accelerator. These reductions in power and energy are achieved while the neuromorphic solution maintains a comparable level of accuracy with the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20154;&#33080;&#39564;&#35777;&#31639;&#27861;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;</title><link>https://arxiv.org/abs/2403.08789</link><description>&lt;p&gt;
&#23558;&#20154;&#31867;&#27010;&#24565;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20154;&#33080;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Bridging Human Concepts and Computer Vision for Explainable Face Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20154;&#33080;&#39564;&#35777;&#31639;&#27861;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#24433;&#21709;&#20102;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;(&#22914;&#20154;&#33080;&#39564;&#35777;)&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#22240;&#27492;&#30830;&#20445;&#20915;&#31574;&#30340;&#36879;&#26126;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#38382;&#36131;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23384;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#25216;&#26415;&#29992;&#20110;&#28548;&#28165;AI&#20915;&#31574;&#65292;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#20026;&#20154;&#31867;&#25552;&#20379;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#35270;&#35273;&#65292;&#22686;&#21152;&#20102;&#20154;&#33080;&#39564;&#35777;&#31639;&#27861;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#24863;&#30693;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#20197;&#20102;&#35299;&#26426;&#22120;&#22312;&#20154;&#33080;&#27604;&#36739;&#20219;&#21153;&#26399;&#38388;&#22914;&#20309;&#24863;&#30693;&#20154;&#31867;&#35821;&#20041;&#21306;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;Mediapipe&#25552;&#20379;&#30340;&#20998;&#21106;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#20154;&#31867;&#35821;&#20041;&#38754;&#37096;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#30340;&#24863;&#30693;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#25972;&#20102;&#20004;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#31639;&#27861;&#65292;&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08789v1 Announce Type: cross  Abstract: With Artificial Intelligence (AI) influencing the decision-making process of sensitive applications such as Face Verification, it is fundamental to ensure the transparency, fairness, and accountability of decisions. Although Explainable Artificial Intelligence (XAI) techniques exist to clarify AI decisions, it is equally important to provide interpretability of these decisions to humans. In this paper, we present an approach to combine computer and human vision to increase the explanation's interpretability of a face verification algorithm. In particular, we are inspired by the human perceptual process to understand how machines perceive face's human-semantic areas during face comparison tasks. We use Mediapipe, which provides a segmentation technique that identifies distinct human-semantic facial regions, enabling the machine's perception analysis. Additionally, we adapted two model-agnostic algorithms to provide human-interpretable i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#22270;&#28388;&#27874;&#22120;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#19968;&#33268;&#24615;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#21644;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#65292;&#23454;&#29616;&#23545;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.08787</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#22270;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-view Subspace Clustering via An Adaptive Consensus Graph Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#22270;&#28388;&#27874;&#22120;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#19968;&#33268;&#24615;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#21644;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#65292;&#23454;&#29616;&#23545;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;MVSC&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MVSC&#26041;&#27861;&#39318;&#20808;&#20174;&#19981;&#21516;&#35270;&#35282;&#25910;&#38598;&#20114;&#34917;&#20449;&#24687;&#65292;&#28982;&#21518;&#23548;&#20986;&#19968;&#33268;&#24615;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#20197;&#25351;&#31034;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#20551;&#35774;&#23384;&#22312;&#19968;&#33268;&#24615;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#26500;&#24314;&#19968;&#33268;&#24615;&#22270;&#28388;&#27874;&#22120;&#12290;&#22312;&#27599;&#20010;&#35270;&#35282;&#20013;&#65292;&#28388;&#27874;&#22120;&#29992;&#20110;&#24179;&#28369;&#25968;&#25454;&#24182;&#20026;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#35774;&#35745;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#12290;&#26368;&#21518;&#65292;&#20174;&#19981;&#21516;&#35270;&#35282;&#33719;&#24471;&#30340;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#34987;&#29992;&#26469;&#20026;&#19968;&#33268;&#24615;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#21019;&#24314;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#19968;&#33268;&#24615;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#12289;&#19968;&#33268;&#24615;&#22270;&#28388;&#27874;&#22120;&#21644;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#34987;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08787v1 Announce Type: cross  Abstract: Multiview subspace clustering (MVSC) has attracted an increasing amount of attention in recent years. Most existing MVSC methods first collect complementary information from different views and consequently derive a consensus reconstruction coefficient matrix to indicate the subspace structure of a multi-view data set. In this paper, we initially assume the existence of a consensus reconstruction coefficient matrix and then use it to build a consensus graph filter. In each view, the filter is employed for smoothing the data and designing a regularizer for the reconstruction coefficient matrix. Finally, the obtained reconstruction coefficient matrices from different views are used to create constraints for the consensus reconstruction coefficient matrix. Therefore, in the proposed method, the consensus reconstruction coefficient matrix, the consensus graph filter, and the reconstruction coefficient matrices from different views are inte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08757</link><description>&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Combinatorial Optimization via Heat Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#28909;&#25193;&#25955;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#12290;&#38024;&#23545;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35775;&#38382;&#35299;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33324;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19968;&#31995;&#21015;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#24191;&#27867;&#36935;&#21040;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08664</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#24037;&#20020;&#24202;&#35760;&#24405;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Llama 2 LLM&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#29983;&#25104;&#21487;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#24739;&#32773;&#20449;&#24687;&#30340;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#65292;&#19982;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#24341;&#20837;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.08609</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#21487;&#25193;&#23637;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#30340;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#24341;&#20837;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#20013;&#38656;&#35201;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26159;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#23558;&#26159;&#23558;&#31867;&#20284;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#22120;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#32435;&#20837;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#38656;&#27714;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#19968;&#20123;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#23454;&#29616;&#36825;&#19968;&#23646;&#24615;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#21542;&#30830;&#23454;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08609v1 Announce Type: new  Abstract: Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#25552;&#31034;&#65288;CPrompt&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#25152;&#26377;&#29616;&#26377;&#20998;&#31867;&#22120;&#25509;&#21463;&#25552;&#31034;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#21152;&#23545;&#40784;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.08568</link><description>&lt;p&gt;
&#26080;&#38656;&#22797;&#20064;&#30340;&#19968;&#33268;&#25552;&#31034;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Consistent Prompting for Rehearsal-Free Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#25552;&#31034;&#65288;CPrompt&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#25152;&#26377;&#29616;&#26377;&#20998;&#31867;&#22120;&#25509;&#21463;&#25552;&#31034;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#21152;&#23545;&#40784;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#20027;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#25110;&#25968;&#25454;&#27969;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#26087;&#30693;&#35782;&#12290;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#65292;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#21644;&#20998;&#31867;&#22120;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#25581;&#31034;&#20102;&#20004;&#31181;&#31867;&#22411;&#12290;&#27979;&#35797;&#39044;&#27979;&#26159;&#20174;&#25152;&#26377;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#35757;&#32451;&#21482;&#20851;&#27880;&#24403;&#21069;&#20219;&#21153;&#20998;&#31867;&#22120;&#32780;&#27809;&#26377;&#36827;&#34892;&#25972;&#20307;&#23545;&#40784;&#65292;&#23548;&#33268;&#20998;&#31867;&#22120;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25552;&#31034;&#30340;&#19981;&#19968;&#33268;&#24615;&#34920;&#31034;&#27979;&#35797;&#26399;&#38388;&#36873;&#25321;&#30340;&#25552;&#31034;&#21487;&#33021;&#19982;&#35757;&#32451;&#26399;&#38388;&#19982;&#35813;&#20219;&#21153;&#20851;&#32852;&#30340;&#25552;&#31034;&#19981;&#23545;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#19968;&#33268;&#25552;&#31034;&#65288;CPrompt&#65289;&#65292;&#29992;&#20110;&#26356;&#21152;&#23545;&#40784;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#37117;&#25509;&#21463;&#25552;&#31034;&#35757;&#32451;&#65292;&#20174;&#32780;&#24418;&#25104;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08568v1 Announce Type: cross  Abstract: Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.08309</link><description>&lt;p&gt;
HRLAIF: &#36890;&#36807;AI&#21453;&#39304;&#25913;&#36827;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24110;&#21161;&#24615;&#21644;&#26080;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#30456;&#27604;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#20855;&#26377;&#26356;&#30701;&#30340;&#27880;&#37322;&#21608;&#26399;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#20248;&#21183;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#24555;&#36895;&#31574;&#30053;&#36845;&#20195;&#38454;&#27573;&#38750;&#24120;&#39640;&#25928;&#12290;&#20351;&#29992;ChatGPT&#20316;&#20026;&#26631;&#27880;&#21592;&#65292;&#22312;RLAIF&#35757;&#32451;&#20013;&#20026;&#24320;&#25918;&#22495;&#25552;&#31034;&#25552;&#20379;&#21453;&#39304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#20559;&#22909;&#32988;&#29575;&#22686;&#21152;&#65292;&#20294;&#35780;&#20272;&#32773;&#30340;&#28385;&#24847;&#24230;&#19979;&#38477;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#28385;&#24847;&#24230;&#19979;&#38477;&#20027;&#35201;&#26159;&#22240;&#20026;&#19968;&#20123;&#21709;&#24212;&#21464;&#24471;&#19981;&#22815;&#26377;&#24110;&#21161;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#26041;&#38754;&#65292;&#31361;&#26174;&#20102;&#22522;&#26412;RLAIF&#30340;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;HRLAIF&#65289;&#12290;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#30340;&#24110;&#21161;&#24615;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08265</link><description>&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#20316;&#20026;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Random Search as a Baseline for Sparse Neural Network Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08265
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#19982;&#23494;&#38598;&#32593;&#32476;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#20419;&#20351;&#35768;&#22810;&#24037;&#20316;&#23398;&#20064;&#12289;&#35825;&#23548;&#25110;&#25628;&#32034;&#24615;&#33021;&#39640;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36136;&#37327;&#25110;&#25928;&#29575;&#30340;&#25552;&#21319;&#20540;&#24471;&#27880;&#24847;&#65292;&#20294;&#26631;&#20934;&#22522;&#32447;&#32570;&#20047;&#65292;&#22240;&#27492;&#22952;&#30861;&#20102;&#26041;&#27861;&#20043;&#38388;&#30340;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#25628;&#32034;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#33391;&#22909;&#30340;&#31232;&#30095;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#32593;&#32476;&#30340;&#33410;&#28857;&#31354;&#38388;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#25439;&#22833;&#26223;&#35266;&#20013;&#20301;&#32622;&#26356;&#26377;&#20248;&#21183;&#30340;&#26356;&#22909;&#21021;&#22987;&#21270;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#19981;&#21516;&#31232;&#30095;&#31243;&#24230;&#19979;&#31232;&#30095;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#24615;&#33021;&#65292;&#24182;&#19982;&#23427;&#20204;&#30340;&#23436;&#20840;&#36830;&#25509;&#29238;&#32593;&#32476;&#20197;&#21450;&#38543;&#26426;&#31232;&#30095;&#37197;&#32622;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.07969</link><description>&lt;p&gt;
KnowCoder&#65306;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#30721;&#21040;LLMs&#20013;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KnowCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#65288;UIE&#65289;&#12290;KnowCoder&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24335;&#34920;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;LLMs&#36981;&#24490;&#27169;&#24335;&#24182;&#20934;&#30830;&#25552;&#21462;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;KnowCoder&#24341;&#20837;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24335;&#32479;&#19968;&#36716;&#25442;&#20026;Python&#31867;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;LLM&#21451;&#22909;&#30340;&#26041;&#24335;&#25429;&#25417;UIE&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#32422;&#26463;&#31561;&#22797;&#26434;&#27169;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30,000&#31181;&#30693;&#35782;&#31867;&#22411;&#30340;&#20195;&#30721;&#39118;&#26684;&#27169;&#24335;&#24211;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;UIE&#20013;&#26368;&#22823;&#30340;&#24211;&#12290;&#20026;&#20102;&#31616;&#21270;LLMs&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;KnowCoder&#21253;&#21547;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#39044;&#35757;&#32451;&#22686;&#24378;&#20854;&#27169;&#24335;&#29702;&#35299;&#33021;&#21147;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07969v1 Announce Type: cross  Abstract: In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07917</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#20027;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#30340;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural-Evolutionary Algorithm for Autonomous Transit Network Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07917
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#26159;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#22909;&#22788;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#36335;&#32447;&#32593;&#32476;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#26500;&#24314;&#36335;&#32447;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#22810;&#20010;&#21464;&#24322;&#25805;&#20316;&#31526;&#20043;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#35780;&#20272;&#36825;&#31181;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#29616;&#23454;&#22522;&#20934;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#27604;&#21333;&#29420;&#23398;&#20064;&#30340;&#31574;&#30053;&#39640;&#20986;&#39640;&#36798;20\%&#65292;&#27604;&#31616;&#21333;&#30340;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#39640;&#20986;&#39640;&#36798;53%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07917v1 Announce Type: cross  Abstract: Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on realistic benchmark instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#24320;&#28304;&#36719;&#20214;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;&#24635;&#20307;&#32780;&#35328;&#65292;&#24320;&#31665;&#21363;&#29992;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32988;&#36807;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36824;&#21457;&#29616;&#23558;&#37327;&#23376;&#27169;&#22411;&#20013;&#30340;&#32416;&#32544;&#21435;&#38500;&#36890;&#24120;&#20250;&#23548;&#33268;&#21516;&#26679;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07059</link><description>&lt;p&gt;
&#20248;&#20110;&#32463;&#20856;&#65311;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22937;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
Better than classical? The subtle art of benchmarking quantum machine learning models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07059
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#24320;&#28304;&#36719;&#20214;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;&#24635;&#20307;&#32780;&#35328;&#65292;&#24320;&#31665;&#21363;&#29992;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32988;&#36807;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36824;&#21457;&#29616;&#23558;&#37327;&#23376;&#27169;&#22411;&#20013;&#30340;&#32416;&#32544;&#21435;&#38500;&#36890;&#24120;&#20250;&#23548;&#33268;&#21516;&#26679;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32463;&#20856;&#27169;&#25311;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26159;&#22312;&#27809;&#26377;&#26080;&#22122;&#22768;&#30828;&#20214;&#21487;&#29992;&#20043;&#21069;&#35780;&#20272;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24605;&#24819;&#30340;&#20027;&#35201;&#26041;&#24335;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#35774;&#35745;&#23545;&#32467;&#26524;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#24403;&#21069;&#21487;&#36798;&#21040;&#30340;&#23567;&#35268;&#27169;&#65292;&#20197;&#21450;&#21463;&#37327;&#23376;&#25216;&#26415;&#21830;&#19994;&#21270;&#24433;&#21709;&#30340;&#21465;&#36848;&#20351;&#24471;&#38590;&#20197;&#33719;&#24471;&#31283;&#20581;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;PennyLane&#36719;&#20214;&#26694;&#26550;&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#65292;&#24182;&#20351;&#29992;&#23427;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;12&#31181;&#27969;&#34892;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29992;&#20110;&#21019;&#24314;160&#20010;&#21333;&#29420;&#25968;&#25454;&#38598;&#30340;6&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#24320;&#31665;&#21363;&#29992;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32988;&#36807;&#37327;&#23376;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#20174;&#37327;&#23376;&#27169;&#22411;&#20013;&#31227;&#38500;&#32416;&#32544;&#24448;&#24448;&#20250;&#23548;&#33268;&#21516;&#26679;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#8220;&#37327;&#23376;&#29305;&#24615;&#8221;&#21487;&#33021;&#24182;&#38750;&#20851;&#38190;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07059v1 Announce Type: cross  Abstract: Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that "quantumness" may not be the crucial ingredi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#25552;&#31034;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06870</link><description>&lt;p&gt;
&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Residual Prompts for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06870
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#25552;&#31034;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#20923;&#32467;&#20102;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20391;&#37325;&#20110;&#35757;&#32451;&#19968;&#20123;&#31216;&#20026;&#25552;&#31034;&#30340;&#21442;&#25968;&#21521;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#23558;&#36825;&#20123;&#21521;&#37327;&#32452;&#32455;&#22312;&#19968;&#20010;&#38190;-&#20540;&#23545;&#27744;&#20013;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#25552;&#31034;&#65288;&#20540;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#30001;&#20110;&#38190;&#26159;&#23398;&#20064;&#30340;&#65292;&#25552;&#31034;&#36873;&#25321;&#31574;&#30053;&#26412;&#36523;&#20063;&#20250;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#36873;&#25321;&#31574;&#30053;&#26356;&#21152;&#31283;&#23450;&#65292;&#25105;&#20204;&#35831;&#27714;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#22312;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#20013;&#36873;&#25321;&#25105;&#20204;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#32423;&#21033;&#29992;&#26631;&#20934;&#25991;&#26412;&#25552;&#31034;&#26469;&#35843;&#25972;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24418;&#25104;&#31283;&#23450;&#30340;&#31867;&#21407;&#22411;&#12290;&#32780;&#31532;&#20108;&#32423;&#21017;&#23558;&#36825;&#20123;&#21407;&#22411;&#19982;&#26597;&#35810;&#22270;&#20687;&#19968;&#36215;&#29992;&#20316;&#38190;&#26469;&#32034;&#24341;&#19968;&#20010;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23545;&#27604;&#65288;ProCo&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#29305;&#24449;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23545;&#27604;&#23545;&#36827;&#34892;&#37319;&#26679;</title><link>https://arxiv.org/abs/2403.06726</link><description>&lt;p&gt;
&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contrastive Learning for Long-Tailed Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23545;&#27604;&#65288;ProCo&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#29305;&#24449;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23545;&#27604;&#23545;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#24067;&#32463;&#24120;&#20986;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#20854;&#20013;&#22823;&#37327;&#23569;&#25968;&#31867;&#21035;&#21253;&#21547;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#20102;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31639;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#24179;&#34913;&#30340;&#35757;&#32451;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#22266;&#26377;&#25361;&#25112;&#30340;&#22256;&#25200;&#65306;&#23427;&#38656;&#35201;&#36275;&#22815;&#22823;&#25209;&#27425;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#26500;&#24314;&#28085;&#30422;&#25152;&#26377;&#31867;&#21035;&#30340;&#23545;&#27604;&#23545;&#65292;&#28982;&#32780;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23545;&#27604;&#65288;ProCo&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#29305;&#24449;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23545;&#27604;&#23545;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06726v1 Announce Type: new  Abstract: Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pa
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06398</link><description>&lt;p&gt;
&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#23485;&#24230;&#36882;&#20943;&#22238;&#25253;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Diminishing Returns of Width for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06398
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#25353;&#39034;&#24207;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#32463;&#24120;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290; &#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#20943;&#23569;&#65292;&#20294;&#23578;&#26410;&#20934;&#30830;&#21051;&#30011;&#23485;&#24230;&#21644;&#25345;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#26089;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#23485;&#24230;&#19982;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#30340;&#36951;&#24536;&#30452;&#25509;&#30456;&#20851;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#19978;&#32463;&#39564;&#24615;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#65292;&#32467;&#26524;&#26174;&#31034;&#36882;&#20943;&#22238;&#25253;&#22914;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#28165;&#26224;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05811</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Statistical Efficiency of Distributional Temporal Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#20851;&#27880;&#30340;&#26159;&#36820;&#22238;&#30340;&#23436;&#25972;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#32463;&#39564;&#25104;&#21151;&#12290;&#39046;&#22495;DRL&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#26159;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#28041;&#21450;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;pi&#30340;&#36820;&#22238;&#20998;&#24067;&#951;^pi&#12290;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#20998;&#24067;&#26102;&#38388;&#24046;&#20998;(TD)&#31639;&#27861;&#65292;&#36825;&#26159;&#32463;&#20856;RL&#25991;&#29486;&#20013;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#30340;&#24310;&#20280;&#12290;&#22312;&#34920;&#26684;&#26696;&#20363;&#20013;&#65292;citet{rowland2018analysis}&#21644;citet{rowland2023analysis}&#20998;&#21035;&#35777;&#26126;&#20102;&#20004;&#20010;&#20998;&#24067;&#24335;TD&#23454;&#20363;&#21363;&#20998;&#31867;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(CTD)&#21644;&#20998;&#20301;&#25968;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(QTD)&#30340;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;TD&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340; dis
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05811v1 Announce Type: cross  Abstract: Distributional reinforcement learning (DRL), which cares about the full distribution of returns instead of just the mean, has achieved empirical success in various domains. One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal difference (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#24418;&#24335;&#21270;&#20102;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#33322;&#22825;&#22120;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05693</link><description>&lt;p&gt;
&#22797;&#26434;&#33322;&#22825;&#22120;&#20219;&#21153;&#30340;&#23631;&#34109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#24418;&#24335;&#21270;&#20102;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#33322;&#22825;&#22120;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33322;&#22825;&#22120;&#25511;&#21046;&#36890;&#36807;&#23631;&#34109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;SDRL&#65289;&#24050;&#25104;&#20026;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23631;&#34109;&#30340;&#26500;&#24314;&#21644;&#20219;&#21153;&#30340;&#23450;&#20041;&#20173;&#19981;&#22815;&#27491;&#24335;&#65292;&#23548;&#33268;&#31574;&#30053;&#26080;&#27861;&#20445;&#35777;&#23433;&#20840;&#24182;&#32473;RL&#20195;&#29702;&#35774;&#23450;&#20102;&#27169;&#26865;&#20004;&#21487;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#65292;&#21363;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#26469;&#24418;&#24335;&#21270;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#12290;&#28982;&#21518;&#23450;&#20041;&#20102;&#19968;&#31181;&#33258;&#21160;&#20174;co-safe LTL&#35268;&#33539;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#26377;&#25928;&#35757;&#32451;SDRL&#26694;&#26550;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20026;&#33322;&#22825;&#22120;&#24212;&#29992;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#20197;&#21450;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05693v1 Announce Type: new  Abstract: Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.02524</link><description>&lt;p&gt;
&#22312;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20855;&#26377;&#20869;&#22312;&#21487;&#35266;&#27979;&#24615;&#30340;Koopman&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#23450;&#20041;&#30340;Koopman&#31639;&#23376;&#21450;&#20854;&#35889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;Jet Dynamic Mode Decomposition&#65288;JetDMD&#65289;&#65292;&#21033;&#29992;RKHS&#30340;&#20869;&#22312;&#32467;&#26500;&#21644;&#31216;&#20026;jets&#30340;&#20960;&#20309;&#27010;&#24565;&#26469;&#22686;&#24378;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#19978;&#20248;&#21270;&#20102;&#20256;&#32479;&#30340;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#25968;&#20540;&#20272;&#35745;&#26041;&#38754;&#12290;&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#29305;&#27530;&#27491;&#23450;&#20869;&#26680;&#30340;&#25910;&#25947;&#29575;&#35777;&#26126;&#20102;JetDMD&#30340;&#20248;&#36234;&#24615;&#65292;&#20026;&#20854;&#24615;&#33021;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;Koopman&#31639;&#23376;&#30340;&#35889;&#20998;&#26512;&#65292;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#12290;&#36825;&#20010;&#27010;&#24565;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#24182;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02524v1 Announce Type: cross  Abstract: This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and captu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01742</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#29992;&#20110;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-TS: Interpretable Diffusion for General Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs)&#27491;&#36880;&#28176;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#26368;&#36817;&#24050;&#22312;&#38899;&#39057;&#21512;&#25104;&#12289;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#21644;&#39044;&#27979;&#31561;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-TS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#20854;&#20013;&#20998;&#35299;&#25216;&#26415;&#25351;&#23548;Diffusion-TS&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#32780;&#21464;&#21387;&#22120;&#20174;&#22024;&#26434;&#30340;&#27169;&#22411;&#36755;&#20837;&#20013;&#25366;&#25496;&#35814;&#32454;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#37325;&#24314;&#22122;&#22768;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;Fourier&#30340;&#25439;&#22833;&#39033;&#12290;&#39044;&#26399;Diffusion-TS&#21487;&#20197;&#29983;&#25104;&#26082;&#20855;&#26377;&#35299;&#37322;&#24615;&#21448;&#30495;&#23454;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Diffusion
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
&lt;/p&gt;</description></item><item><title>VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01308</link><description>&lt;p&gt;
VBART: &#22303;&#32819;&#20854;LLM
&lt;/p&gt;
&lt;p&gt;
VBART: The Turkish LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01308
&lt;/p&gt;
&lt;p&gt;
VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VBART&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#22823;&#35821;&#26009;&#24211;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;VBART&#26159;&#22522;&#20110;BART&#21644;mBART&#27169;&#22411;&#30340;&#22909;&#24605;&#36335;&#26500;&#24314;&#30340;&#32039;&#20945;&#22411;LLMs&#65292;&#20998;&#20026;Large&#21644;XLarge&#20004;&#20010;&#23610;&#23544;&#12290;&#24494;&#35843;&#21518;&#30340;VBART&#27169;&#22411;&#22312;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;&#12289;&#26631;&#39064;&#29983;&#25104;&#12289;&#25991;&#26412;&#25913;&#20889;&#12289;&#38382;&#31572;&#21644;&#38382;&#39064;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#23427;&#20204;&#20801;&#35768;&#20026;&#26410;&#26469;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25317;&#26377;&#20026;&#22303;&#32819;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLM&#27604;&#22810;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#20026;&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#35821;&#20998;&#35789;&#22120;&#27604;OpenAI&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#26356;&#39640;&#25928;7&#20493;&#12290;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#29616;&#26377;&#39044;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.19078</link><description>&lt;p&gt;
&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth Tchebycheff Scalarization for Multi-Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#32463;&#24120;&#30456;&#20114;&#20914;&#31361;&#65292;&#19981;&#33021;&#36890;&#36807;&#21333;&#20010;&#35299;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25214;&#21040;&#24085;&#32047;&#25176;&#35299;&#65292;&#36825;&#20123;&#35299;&#20195;&#34920;&#20102;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#30340;&#19981;&#21516;&#26368;&#20339;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#33021;&#20855;&#22791;&#35299;&#20915;&#19968;&#33324;&#21487;&#24494;&#20998;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#29702;&#35770;&#23646;&#24615;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20809;&#28369;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#30340;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#23427;&#23545;&#20110;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#26159;&#26080;&#27861;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#30340;&#27425;&#20248;&#24615;&#20056;&#27861;&#22686;&#21152;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.10898</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
The Price of Adaptivity in Stochastic Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#26159;&#26080;&#27861;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#30340;&#27425;&#20248;&#24615;&#20056;&#27861;&#22686;&#21152;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#36866;&#24212;&#24615;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#12290;&#32473;&#23450;&#19968;&#32452;&#25105;&#20204;&#24076;&#26395;&#36866;&#24212;&#30340;&#38382;&#39064;&#21442;&#25968;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#8220;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#8221;&#65288;PoA&#65289;&#65292;&#31895;&#30053;&#22320;&#35828;&#65292;&#23427;&#34913;&#37327;&#20102;&#30001;&#20110;&#36825;&#20123;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#30340;&#20056;&#27861;&#22686;&#21152;&#12290;&#24403;&#21021;&#22987;&#36317;&#31163;&#26368;&#20248;&#35299;&#26410;&#30693;&#20294;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;PoA&#33267;&#23569;&#23545;&#20110;&#26399;&#26395;&#27425;&#20248;&#24615;&#26159;&#23545;&#25968;&#32423;&#21035;&#65292;&#23545;&#20110;&#20013;&#20301;&#25968;&#27425;&#20248;&#24615;&#26159;&#21452;&#23545;&#25968;&#32423;&#21035;&#12290;&#24403;&#36317;&#31163;&#21644;&#26799;&#24230;&#33539;&#25968;&#37117;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;PoA&#24517;&#39035;&#26159;&#19982;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#22810;&#39033;&#24335;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#20960;&#20046;&#19982;&#29616;&#26377;&#30340;&#19978;&#30028;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#30830;&#23450;&#20102;&#27809;&#26377;&#26080;&#21442;&#25968;&#21320;&#39184;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08268</link><description>&lt;p&gt;
&#30334;&#19975;&#38271;&#24230;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Model on Million-Length Video And Language With RingAttention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#38590;&#20197;&#29992;&#25991;&#23383;&#25551;&#36848;&#30340;&#19990;&#30028;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38271;&#31687;&#20219;&#21153;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#35270;&#39057;&#24207;&#21015;&#25552;&#20379;&#20102;&#21482;&#26377;&#35821;&#35328;&#21644;&#38745;&#24577;&#22270;&#20687;&#25152;&#19981;&#20855;&#22791;&#30340;&#23453;&#36149;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#19982;&#35821;&#35328;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#26102;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#23545;&#20154;&#31867;&#30340;&#25991;&#26412;&#30693;&#35782;&#21644;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#29702;&#35299;&#65292;&#20026;&#36741;&#21161;&#20154;&#31867;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#30334;&#19975;&#20010;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#20013;&#23398;&#20064;&#38754;&#20020;&#30528;&#35760;&#24518;&#32422;&#26463;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#26679;&#21270;&#35270;&#39057;&#21644;&#20070;&#31821;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29615;&#24418;&#27880;&#24847;&#21147;&#25216;&#26415;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#65292;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#20174;4K&#21040;1M&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;
&lt;/p&gt;
&lt;p&gt;
Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language seq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G-Retriever&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;&#22238;&#22797;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.07630</link><description>&lt;p&gt;
G-Retriever: &#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G-Retriever&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;&#22238;&#22797;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#20855;&#26377;&#25991;&#26412;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#23545;&#35805;&#30028;&#38754;&#21521;&#22270;&#24418;&#25552;&#20986;&#38382;&#39064;&#12290;&#38024;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#25991;&#26412;&#22238;&#22797;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20197;&#21508;&#31181;&#26041;&#24335;&#25972;&#21512;&#36215;&#26469;&#19981;&#21516;&#65292;&#23427;&#20204;&#22823;&#22810;&#38598;&#20013;&#22312;&#20256;&#32479;&#22270;&#20219;&#21153;(&#22914;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#20998;&#31867;)&#25110;&#32773;&#22312;&#23567;&#22411;&#25110;&#21512;&#25104;&#22270;&#19978;&#22238;&#31572;&#31616;&#21333;&#30340;&#22270;&#26597;&#35810;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#20026;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#22270;&#38382;&#39064;&#22238;&#31572;(GraphQA)&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;G-Retriever&#26041;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;GNN&#21644;LLM&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07011</link><description>&lt;p&gt;
FedImpro: &#27979;&#37327;&#21644;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
FedImpro: Measuring and Improving Client Update in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#21463;&#21040;&#24322;&#26500;&#25968;&#25454;&#24341;&#36215;&#30340;&#23458;&#25143;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#36827;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#25805;&#20316;&#29616;&#26377;&#30340;&#26799;&#24230;&#65292;&#20197;&#23454;&#29616;&#26356;&#19968;&#33268;&#30340;&#23458;&#25143;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#20998;&#26512;&#20102;&#23458;&#25143;&#28418;&#31227;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#31181;&#28418;&#31227;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#31181;&#27867;&#21270;&#36129;&#29486;&#21463;&#21040;&#19981;&#21516;&#23458;&#25143;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedImpro&#65292;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedImpro&#23558;&#27169;&#22411;&#20998;&#35299;&#20026;&#39640;&#23618;&#21644;&#20302;&#23618;&#32452;&#20214;&#65292;&#24182;&#23545;&#37325;&#26500;&#29305;&#24449;&#20998;&#24067;&#19978;&#30340;&#39640;&#23618;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#20943;&#23567;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
&lt;/p&gt;</description></item><item><title>&#24046;&#21160;&#36801;&#31227;&#20809;&#35889;&#27861;&#32467;&#21512;Alpha&#26354;&#32447;&#36830;&#32493;&#24615;&#65292;&#39318;&#27425;&#23558;&#20998;&#25955;&#22270;&#35299;&#37322;&#20026;&#24207;&#21015;&#27979;&#37327;&#65292;&#21487;&#33021;&#20026;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#30340;&#20998;&#31867;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2401.07066</link><description>&lt;p&gt;
&#22522;&#20110;Alpha&#26354;&#32447;&#36830;&#32493;&#24615;&#30340;&#24046;&#21160;&#36801;&#31227;&#20809;&#35889;&#27861;&#23545;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Volatile Organic Compounds by Differential Mobility Spectrometry Based on Continuity of Alpha Curves
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07066
&lt;/p&gt;
&lt;p&gt;
&#24046;&#21160;&#36801;&#31227;&#20809;&#35889;&#27861;&#32467;&#21512;Alpha&#26354;&#32447;&#36830;&#32493;&#24615;&#65292;&#39318;&#27425;&#23558;&#20998;&#25955;&#22270;&#35299;&#37322;&#20026;&#24207;&#21015;&#27979;&#37327;&#65292;&#21487;&#33021;&#20026;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#30340;&#20998;&#31867;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07066v2 &#20844;&#21578;&#31867;&#22411;:&#26356;&#26032; &#25688;&#35201;: &#32972;&#26223;&#65306;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOCs&#65289;&#30340;&#20998;&#31867;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#31034;&#20363;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#12289;&#29190;&#28856;&#29289;&#26816;&#27979;&#21644;&#39135;&#21697;&#36136;&#37327;&#25511;&#21046;&#12290;&#20351;&#29992;&#30005;&#23376;&#40763;&#37319;&#38598;&#30340;&#27979;&#37327;&#25968;&#25454;&#21487;&#29992;&#20110;VOCs&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#31181;&#30005;&#23376;&#40763;&#30340;&#31867;&#22411;&#65292;&#21363;&#24046;&#21160;&#36801;&#31227;&#20809;&#35889;&#27861;&#65288;DMS&#65289;&#65292;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#21457;&#23637;&#12290;DMS&#20135;&#29983;&#30340;&#27979;&#37327;&#32467;&#26524;&#34987;&#21487;&#35270;&#21270;&#20026;&#21253;&#21547;&#26354;&#32447;&#36712;&#36857;&#30340;&#20998;&#25955;&#22270;&#65292;&#20063;&#34987;&#31216;&#20026;Alpha&#26354;&#32447;&#12290;&#30446;&#21069;&#29992;&#20110;&#20998;&#26512;DMS&#20998;&#25955;&#22270;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#21033;&#29992;&#36825;&#20123;&#26354;&#32447;&#36830;&#32493;&#24615;&#20013;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#36825;&#34920;&#26126;&#24212;&#35813;&#30740;&#31350;&#26367;&#20195;&#26041;&#27861;&#12290; &#32467;&#26524;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#39318;&#27425;&#23558;&#20998;&#25955;&#22270;&#35299;&#37322;&#20026;&#19968;&#31995;&#21015;&#25353;&#39034;&#24207;&#28436;&#21464;&#30340;&#27979;&#37327;&#12290;&#22240;&#27492;&#65292;&#20551;&#35774;&#21487;&#20197;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07066v2 Announce Type: replace  Abstract: Background: Classification of volatile organic compounds (VOCs) is of interest in many fields. Examples include but are not limited to medicine, detection of explosives, and food quality control. Measurements collected with electronic noses can be used for classification and analysis of VOCs. One type of electronic noses that has seen considerable development in recent years is Differential Mobility Spectrometry (DMS). DMS yields measurements that are visualized as dispersion plots that contain traces, also known as alpha curves. Current methods used for analyzing DMS dispersion plots do not usually utilize the information stored in the continuity of these traces, which suggests that alternative approaches should be investigated.   Results: In this work, for the first time, dispersion plots were interpreted as a series of measurements evolving sequentially. Thus, it was hypothesized that time-series classification algorithms can be e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20808;&#39564;&#30340;&#36817;&#22330;3D MIMO&#25104;&#20687;&#20013;&#25391;&#24133;&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#25554;&#20837;&#21644;&#25773;&#25918;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22797;&#20540;&#21435;&#22122;&#38382;&#39064;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37325;&#24314;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.16024</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#20808;&#39564;&#30340;&#36817;&#22330;3D MIMO&#25104;&#20687;&#20013;&#30340;&#25391;&#24133;&#25554;&#20837;&#21644;&#25773;&#25918;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Regularization on Magnitude with Deep Priors for 3D Near-Field MIMO Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20808;&#39564;&#30340;&#36817;&#22330;3D MIMO&#25104;&#20687;&#20013;&#25391;&#24133;&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#25554;&#20837;&#21644;&#25773;&#25918;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22797;&#20540;&#21435;&#22122;&#38382;&#39064;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#22330;&#38647;&#36798;&#25104;&#20687;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38544;&#34109;&#27494;&#22120;&#26816;&#27979;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23545;&#25391;&#24133;&#26045;&#21152;&#27491;&#21017;&#21270;&#26469;&#37325;&#24314;&#36817;&#22330;&#22330;&#26223;&#30340;&#19977;&#32500;&#22797;&#20540;&#21453;&#23556;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#65288;ADMM&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#36870;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#36825;&#31181;&#27491;&#21017;&#21270;&#21151;&#33021;&#30456;&#20851;&#32852;&#30340;&#36817;&#20284;&#26144;&#23556;&#30340;&#19968;&#33324;&#34920;&#36798;&#24335;&#12290;&#36825;&#31561;&#25928;&#20110;&#35299;&#20915;&#19968;&#20010;&#28041;&#21450;&#25391;&#24133;&#27491;&#21017;&#21270;&#30340;&#22797;&#20540;&#21435;&#22122;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25554;&#20837;&#21644;&#25773;&#25918;&#65288;PnP&#65289;&#37325;&#24314;&#26041;&#27861;&#65292;&#21253;&#21547;&#31616;&#21333;&#30340;&#26356;&#26032;&#27493;&#39588;&#12290;&#30001;&#20110;&#25968;&#25454;&#33258;&#36866;&#24212;&#28145;&#24230;&#20808;&#39564;&#22312;&#25104;&#20687;&#20013;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;3D&#28145;&#24230;&#38477;&#22122;&#22120;&#20197;&#22312;&#20854;&#20013;&#21152;&#20197;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16024v2 Announce Type: replace-cross  Abstract: Near-field radar imaging systems are used in a wide range of applications such as concealed weapon detection and medical diagnosis. In this paper, we consider the problem of reconstructing the three-dimensional (3D) complex-valued reflectivity distribution of the near-field scene by enforcing regularization on its magnitude. We solve this inverse problem by using the alternating direction method of multipliers (ADMM) framework. For this, we provide a general expression for the proximal mapping associated with such regularization functionals. This equivalently corresponds to the solution of a complex-valued denoising problem which involves regularization on the magnitude. By utilizing this expression, we develop a novel and efficient plug-and-play (PnP) reconstruction method that consists of simple update steps. Due to the success of data-adaptive deep priors in imaging, we also train a 3D deep denoiser to exploit within the dev
&lt;/p&gt;</description></item><item><title>NM-FlowGAN&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;sRGB&#22122;&#22768;&#65292;&#24357;&#34917;&#20102;&#21333;&#19968;&#29983;&#25104;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.10112</link><description>&lt;p&gt;
NM-FlowGAN: &#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#23545;sRGB&#22122;&#22768;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10112
&lt;/p&gt;
&lt;p&gt;
NM-FlowGAN&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;sRGB&#22122;&#22768;&#65292;&#24357;&#34917;&#20102;&#21333;&#19968;&#29983;&#25104;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#21512;&#25104;&#30495;&#23454;&#30340;sRGB&#22122;&#22768;&#23545;&#20110;&#21508;&#31181;&#20302;&#32423;&#21035;&#35270;&#35273;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26500;&#24314;&#29992;&#20110;&#35757;&#32451;&#22270;&#20687;&#21435;&#22122;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#12290;&#30495;&#23454;sRGB&#22122;&#22768;&#30340;&#20998;&#24067;&#26497;&#20026;&#22797;&#26434;&#65292;&#24182;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#20351;&#24471;&#20854;&#20934;&#30830;&#24314;&#27169;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#30456;&#27604;&#20256;&#32479;&#30340;&#22122;&#22768;&#24314;&#27169;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;sRGB&#22122;&#22768;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NM-FlowGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;GAN&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#20248;&#21183;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21516;&#26102;&#37319;&#29992;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#20687;&#32032;&#32423;&#22122;&#22768;&#24314;&#27169;&#32593;&#32476;&#65292;&#20197;&#21450;&#22522;&#20110;GAN&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#24314;&#27169;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10112v2 Announce Type: replace-cross  Abstract: Modeling and synthesizing real sRGB noise is crucial for various low-level vision tasks, such as building datasets for training image denoising systems. The distribution of real sRGB noise is highly complex and affected by a multitude of factors, making its accurate modeling extremely challenging. Therefore, recent studies have proposed methods that employ data-driven generative models, such as generative adversarial networks (GAN) and Normalizing Flows. These studies achieve more accurate modeling of sRGB noise compared to traditional noise modeling methods. However, there are performance limitations due to the inherent characteristics of each generative model. To address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the strengths of both GAN and Normalizing Flows. We simultaneously employ a pixel-wise noise modeling network based on Normalizing Flows, and spatial correlation modeling networks based on GAN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#65292;&#22522;&#20110;Doob's h-transform&#65292;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.09236</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#30340;&#24212;&#29992;&#20110;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for conditional diffusion modelling with applications in motif scaffolding for protein design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#65292;&#22522;&#20110;Doob's h-transform&#65292;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#34507;&#30333;&#35774;&#35745;&#24212;&#29992;&#65292;&#22914;&#32467;&#21512;&#29289;&#25110;&#37238;&#30340;&#35774;&#35745;&#65292;&#38656;&#35201;&#20197;&#39640;&#31934;&#24230;&#25645;&#24314;&#20855;&#26377;&#32467;&#26500;&#22522;&#24207;&#30340;&#34507;&#30333;&#36136;&#12290;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#26089;&#26399;&#23454;&#39564;&#25104;&#21151;&#12290;&#22312;&#25193;&#25955;&#33539;&#24335;&#20013;&#65292;&#22522;&#24207;&#25903;&#26550;&#34987;&#35270;&#20026;&#19968;&#31181;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#26465;&#20214;&#29983;&#25104;&#21327;&#35758;&#25110;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#25991;&#29486;&#20013;&#23548;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21327;&#35758;&#22823;&#22810;&#22522;&#20110;&#21551;&#21457;&#24615;&#21160;&#26426;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30340;&#31867;&#27604;&#65292;&#24182;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#21464;&#24471;&#27169;&#31946;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#25968;&#23398;&#19978;&#29702;&#35299;&#33391;&#22909;&#30340;Doob's h-transform&#30340;&#20849;&#21516;&#26694;&#26550;&#19979;&#32479;&#19968;&#20102;&#26465;&#20214;&#35757;&#32451;&#21644;&#26465;&#20214;&#25277;&#26679;&#31243;&#24207;&#12290;&#36825;&#31181;&#26032;&#30340;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09236v2 Announce Type: replace  Abstract: Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21644;&#27979;&#37327;&#21508;&#31181;&#25351;&#30002;&#27611;&#32454;&#31649;&#30340;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#33394;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.05930</link><description>&lt;p&gt;
&#19968;&#20221;&#20840;&#38754;&#30340;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#20998;&#26512;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21644;&#27979;&#37327;&#21508;&#31181;&#25351;&#30002;&#27611;&#32454;&#31649;&#30340;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#33394;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#38236;&#26816;&#39564;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#20581;&#24247;&#29366;&#20917;&#65292;&#31361;&#20986;&#20102;&#23545;&#33258;&#21160;&#21270;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#20998;&#26512;&#31995;&#32479;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#25968;&#25454;&#38598;-&#26469;&#33258;68&#21517;&#21463;&#35797;&#32773;&#30340;321&#24352;&#22270;&#20687;&#65292;219&#27573;&#35270;&#39057;&#65292;&#38468;&#26377;&#20020;&#24202;&#25253;&#21578;&#21644;&#19987;&#23478;&#27880;&#37322;-&#36825;&#20316;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#36164;&#28304;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#27880;&#37322;&#20316;&#20026;&#30417;&#30563;&#26631;&#31614;&#23545;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#19968;&#20010;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#20998;&#26512;&#27969;&#27700;&#32447;&#20013;&#12290;&#35813;&#27969;&#27700;&#32447;&#22312;&#33258;&#21160;&#26816;&#27979;&#21644;&#27979;&#37327;&#25351;&#30002;&#30385;&#32441;&#27611;&#32454;&#31649;&#30340;&#21508;&#31181;&#22823;&#23567;&#22240;&#32032;&#12289;&#24418;&#24577;&#29305;&#24449;&#21644;&#21160;&#24577;&#26041;&#38754;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#20020;&#24202;&#25253;&#21578;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#22312;&#27979;&#37327;&#31934;&#24230;&#26041;&#38754;&#36798;&#21040;&#20102;&#20122;&#20687;&#32032;&#32423;&#21035;&#65292;&#24182;&#19988;&#26377;89.9%&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05930v2 Announce Type: replace-cross  Abstract: Nailfold capillaroscopy is widely used in assessing health conditions, highlighting the pressing need for an automated nailfold capillary analysis system. In this study, we present a pioneering effort in constructing a comprehensive nailfold capillary dataset-321 images, 219 videos from 68 subjects, with clinic reports and expert annotations-that serves as a crucial resource for training deep-learning models. Leveraging this dataset, we finetuned three deep learning models with expert annotations as supervised labels and integrated them into a novel end-to-end nailfold capillary analysis pipeline. This pipeline excels in automatically detecting and measuring a wide range of size factors, morphological features, and dynamic aspects of nailfold capillaries. We compared our outcomes with clinical reports. Experiment results showed that our automated pipeline achieves an average of sub-pixel level precision in measurements and 89.9
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26045;&#20845;&#20010;&#22238;&#24402;&#27169;&#22411;&#24182;&#20351;&#29992;&#20851;&#38190;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;37&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;27&#24180;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#65292;&#21462;&#24471;&#20102;0.94&#30340;&#30830;&#23450;&#31995;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.02254</link><description>&lt;p&gt;
&#20892;&#19994;&#39044;&#27979;&#30340;&#21019;&#26032;&#65306;&#20840;&#29699;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#22810;&#20803;&#22238;&#24402;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02254
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26045;&#20845;&#20010;&#22238;&#24402;&#27169;&#22411;&#24182;&#20351;&#29992;&#20851;&#38190;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;37&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;27&#24180;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#65292;&#21462;&#24471;&#20102;0.94&#30340;&#30830;&#23450;&#31995;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#20316;&#29289;&#20135;&#37327;&#30340;&#20840;&#29699;&#39044;&#27979;&#26159;&#20892;&#19994;&#30740;&#31350;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#35813;&#30740;&#31350;&#23454;&#26045;&#20102;6&#20010;&#22238;&#24402;&#27169;&#22411;&#65288;&#32447;&#24615;&#12289;&#20915;&#31574;&#26641;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#26799;&#24230;&#25552;&#21319;&#12289;K&#26368;&#36817;&#37051;&#21644;&#38543;&#26426;&#26862;&#26519;&#65289;&#65292;&#20197;&#39044;&#27979;37&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;27&#24180;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#12290;&#22312;&#32473;&#23450;4&#20010;&#20851;&#38190;&#30340;&#35757;&#32451;&#21442;&#25968;&#65288;&#26432;&#34411;&#21058;&#37327;&#65288;&#21544;&#65289;&#12289;&#38477;&#38632;&#37327;&#65288;mm&#65289;&#12289;&#28201;&#24230;&#65288;&#25668;&#27663;&#24230;&#65289;&#21644;&#20135;&#37327;&#65288;hg/ha&#65289;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#36798;&#21040;&#20102;0.94&#30340;&#30830;&#23450;&#31995;&#25968;&#65288;r2&#65289;&#65292;&#35823;&#24046;&#36793;&#38469;&#65288;ME&#65289;&#20026;0.03&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;&#30340;&#25968;&#25454;&#20197;&#21450;&#19990;&#30028;&#38134;&#34892;&#27668;&#20505;&#21464;&#21270;&#25968;&#25454;&#30446;&#24405;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#20102;&#27599;&#20010;&#21442;&#25968;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#24635;&#20307;&#20135;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19982;&#36890;&#24120;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30456;&#21453;&#30340;&#38750;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02254v2 Announce Type: replace-cross  Abstract: The prediction of crop yields internationally is a crucial objective in agricultural research. Thus, this study implements 6 regression models (Linear, Tree, Gradient Descent, Gradient Boosting, K Nearest Neighbors, and Random Forest) to predict crop yields in 37 developing countries over 27 years. Given 4 key training parameters, insecticides (tonnes), rainfall (mm), temperature (Celsius), and yield (hg/ha), it was found that our Random Forest Regression model achieved a determination coefficient (r2) of 0.94, with a margin of error (ME) of .03. The models were trained and tested using the Food and Agricultural Organization of the United Nations data, along with the World Bank Climate Change Data Catalog. Furthermore, each parameter was analyzed to understand how varying factors could impact overall yield. We used unconventional models, contrary to generally used Deep Learning (DL) and Machine Learning (ML) models, combined wi
&lt;/p&gt;</description></item><item><title>Receler&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#23454;&#29616;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.17717</link><description>&lt;p&gt;
Receler: &#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#21487;&#38752;&#22320;&#25830;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17717
&lt;/p&gt;
&lt;p&gt;
Receler&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#23454;&#29616;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#31105;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19982;&#30446;&#26631;&#27010;&#24565;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#24076;&#26395;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#30340;&#23646;&#24615;&#12290;&#21069;&#32773;&#38459;&#27490;&#27169;&#22411;&#20026;&#20219;&#20309;&#37322;&#20041;&#25110;&#23398;&#20064;&#25552;&#31034;&#29983;&#25104;&#19982;&#30446;&#26631;&#27010;&#24565;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#20445;&#25345;&#20854;&#29983;&#25104;&#20855;&#26377;&#38750;&#30446;&#26631;&#27010;&#24565;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#65288;Receler&#65289;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#27010;&#24565;&#25830;&#38500;&#12290;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#27233;&#30382;&#25830;&#26469;&#36827;&#34892;&#27010;&#24565;&#25830;&#38500;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20986;&#30340;&#27010;&#24565;&#23450;&#20301;&#27491;&#21017;&#21270;&#21644;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;&#26041;&#26696;&#28385;&#36275;&#19978;&#36848;&#29702;&#24819;&#29305;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#27010;&#24565;&#30340;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;Receler&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#25509;&#21463;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17717v2 Announce Type: replace-cross  Abstract: Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves its ability in generating images with non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler). It learns a lightweight Eraser to perform concept erasing while satisfying the above desirable properties by proposed concept-localized regularization and adversarial prompt learning schemes. Comprehensive experiments with various concepts verify the superiority of Receler over previous methods. Our code will be available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#22522;&#32447;&#26550;&#26500;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#38024;&#23545;&#35270;&#39057;&#37325;&#40836;&#21270;&#25216;&#26415;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26032;&#39062;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2311.11642</link><description>&lt;p&gt;
&#35270;&#39057;&#20154;&#33080;&#37325;&#40836;&#21270;&#65306;&#36808;&#21521;&#26102;&#38388;&#19968;&#33268;&#30340;&#20154;&#33080;&#37325;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#22522;&#32447;&#26550;&#26500;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#38024;&#23545;&#35270;&#39057;&#37325;&#40836;&#21270;&#25216;&#26415;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26032;&#39062;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20154;&#33080;&#37325;&#40836;&#21270;&#28041;&#21450;&#22312;&#35270;&#39057;&#20013;&#23558;&#19968;&#20010;&#20154;&#30340;&#22806;&#35266;&#24180;&#40836;&#25913;&#21464;&#21040;&#30446;&#26631;&#24180;&#40836;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#22312;&#36523;&#20221;&#21644;&#24180;&#40836;&#19978;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#37197;&#23545;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#22823;&#22810;&#25968;&#37325;&#40836;&#21270;&#26041;&#27861;&#22788;&#29702;&#27599;&#20010;&#22270;&#20687;&#26102;&#37117;&#27809;&#26377;&#32771;&#34385;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#34429;&#28982;&#19968;&#20123;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35270;&#39057;&#38754;&#37096;&#23646;&#24615;&#25805;&#20316;&#26469;&#35299;&#20915;&#26102;&#38388;&#19968;&#33268;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#24180;&#40836;&#36716;&#25442;&#26041;&#38754;&#34920;&#29616;&#19981;&#23613;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24180;&#40836;&#32452;&#30340;&#23545;&#35937;&#65307;&#65288;2&#65289;&#19968;&#20010;&#22522;&#32447;&#26550;&#26500;&#65292;&#26088;&#22312;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65307;&#20197;&#21450;&#65288;3&#65289;&#38024;&#23545;&#35780;&#20272;&#35270;&#39057;&#37325;&#40836;&#21270;&#25216;&#26415;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11642v3 Announce Type: replace-cross  Abstract: Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory performance in age transformation. To tackle the issues, we propose (1) a novel synthetic video dataset that features subjects across a diverse range of age groups; (2) a baseline architecture designed to validate the effectiveness of our proposed dataset, and (3) the development of novel metrics tailored explicitly for evaluating the temporal consistency of video re-aging techniques. Our comprehensive experiments on public datasets, inclu
&lt;/p&gt;</description></item><item><title>&#22312;&#20272;&#31639;HD&#22320;&#22270;&#26102;&#32771;&#34385;&#29616;&#26377;&#22320;&#22270;&#20449;&#24687;&#26159;&#26412;&#25991;&#30340;&#21019;&#26032;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#29992;&#30340;&#29616;&#26377;&#22320;&#22270;&#31867;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;MapEX&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#22320;&#22270;&#20803;&#32032;&#21644;&#20248;&#21270;&#21305;&#37197;&#31639;&#27861;&#26469;&#23454;&#29616;&#65292;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2311.10517</link><description>&lt;p&gt;
&#35880;&#35760;&#22320;&#22270;&#65281;&#22312;&#20174;&#20256;&#24863;&#22120;&#20272;&#31639;&#22312;&#32447;HD&#22320;&#22270;&#26102;&#32771;&#34385;&#29616;&#26377;&#22320;&#22270;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Mind the map! Accounting for existing map information when estimating online HDMaps from sensor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10517
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20272;&#31639;HD&#22320;&#22270;&#26102;&#32771;&#34385;&#29616;&#26377;&#22320;&#22270;&#20449;&#24687;&#26159;&#26412;&#25991;&#30340;&#21019;&#26032;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#29992;&#30340;&#29616;&#26377;&#22320;&#22270;&#31867;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;MapEX&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#22320;&#22270;&#20803;&#32032;&#21644;&#20248;&#21270;&#21305;&#37197;&#31639;&#27861;&#26469;&#23454;&#29616;&#65292;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39640;&#28165;&#22320;&#22270;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#20204;&#30340;&#33719;&#21462;&#21644;&#32500;&#25252;&#25104;&#26412;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#20174;&#20256;&#24863;&#22120;&#20272;&#31639;&#36825;&#20123;&#22320;&#22270;&#25215;&#35834;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#31639;&#24573;&#35270;&#20102;&#29616;&#26377;&#30340;&#39640;&#28165;&#22320;&#22270;&#65292;&#24403;&#21069;&#26041;&#27861;&#26368;&#22810;&#21482;&#26159;&#23558;&#20302;&#36136;&#37327;&#22320;&#22270;&#22320;&#29702;&#23450;&#20301;&#65292;&#25110;&#32771;&#34385;&#24050;&#30693;&#22320;&#22270;&#30340;&#36890;&#29992;&#25968;&#25454;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20272;&#31639;HD&#22320;&#22270;&#26102;&#32771;&#34385;&#25152;&#30740;&#31350;&#24773;&#20917;&#30340;&#29616;&#26377;&#22320;&#22270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;3&#31181;&#21512;&#29702;&#30340;&#26377;&#29992;&#29616;&#26377;&#22320;&#22270;&#31867;&#22411;&#65288;&#31616;&#32422;&#22411;&#12289;&#22024;&#26434;&#22411;&#21644;&#36807;&#26102;&#22411;&#65289;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MapEX&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22312;&#32447;HD&#22320;&#22270;&#20272;&#31639;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29616;&#26377;&#22320;&#22270;&#12290;MapEX&#36890;&#36807;&#23558;&#22320;&#22270;&#20803;&#32032;&#32534;&#30721;&#20026;&#26597;&#35810;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#29992;&#20110;&#35757;&#32451;&#32463;&#20856;&#22522;&#20110;&#26597;&#35810;&#30340;&#22320;&#22270;&#20272;&#31639;&#27169;&#22411;&#30340;&#21305;&#37197;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;MapEX&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10517v2 Announce Type: replace  Abstract: While HDMaps are a crucial component of autonomous driving, they are expensive to acquire and maintain. Estimating these maps from sensors therefore promises to significantly lighten costs. These estimations however overlook existing HDMaps, with current methods at most geolocalizing low quality maps or considering a general database of known maps. In this paper, we propose to account for existing maps of the precise situation studied when estimating HDMaps. We identify 3 reasonable types of useful existing maps (minimalist, noisy, and outdated). We also introduce MapEX, a novel online HDMap estimation framework that accounts for existing maps. MapEX achieves this by encoding map elements into query tokens and by refining the matching algorithm used to train classic query based map estimation models. We demonstrate that MapEX brings significant improvements on the nuScenes dataset. For instance, MapEX - given noisy maps - improves by
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.08364</link><description>&lt;p&gt;
Plum: &#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Plum: Prompt Learning using Metaheuristic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#20197;&#26469;&#65292;&#25552;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#20248;&#21270;&#21644;&#23450;&#21046;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#29305;&#27530;&#25552;&#31034;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#29978;&#33267;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#20808;&#21069;&#26410;&#30693;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#26377;&#25928;&#25552;&#31034;&#30340;&#36827;&#23637;&#32531;&#24930;&#65292;&#20419;&#20351;&#20154;&#20204;&#28212;&#26395;&#19968;&#31181;&#36890;&#29992;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24456;&#23569;&#26377;&#28385;&#36275;&#8220;&#36890;&#29992;&#8221;&#30340;&#26631;&#20934;&#65292;&#21363;&#21516;&#26102;&#20855;&#22791;&#33258;&#21160;&#12289;&#31163;&#25955;&#12289;&#40657;&#30418;&#12289;&#26080;&#26799;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20803;&#21551;&#21457;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#31163;&#25955;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#25903;&#65292;&#25317;&#26377;100&#22810;&#31181;&#36873;&#39033;&#12290;&#22312;&#25105;&#20204;&#30340;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20845;&#31181;&#20856;&#22411;&#26041;&#27861;&#65306;&#29228;&#23665;&#12289;&#27169;&#25311;&#36864;&#28779;&#12289;&#36951;&#20256;&#31639;&#27861;&#65288;&#24102;/&#19981;&#24102;&#20132;&#21449;&#65289;&#12289;&#31105;&#24524;&#25628;&#32034;&#21644;&#21644;&#35856;&#25628;&#32034;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30333;&#30418;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#21644;&#20849;&#24418;&#20195;&#25968;&#30340;Geometric Algebra Transformer&#26550;&#26500;&#65292;&#21457;&#29616;&#20849;&#24418;&#20195;&#25968;&#21644;&#25913;&#36827;&#30340;&#23556;&#24433;&#20195;&#25968;&#23450;&#20041;&#20102;&#21151;&#33021;&#24378;&#22823;&#12289;&#24615;&#33021;&#33391;&#22909;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2311.04744</link><description>&lt;p&gt;
&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#12289;&#20849;&#24418;&#65306;&#20026;&#31561;&#21464;&#25442;&#22120;&#36873;&#25321;&#20960;&#20309;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#21644;&#20849;&#24418;&#20195;&#25968;&#30340;Geometric Algebra Transformer&#26550;&#26500;&#65292;&#21457;&#29616;&#20849;&#24418;&#20195;&#25968;&#21644;&#25913;&#36827;&#30340;&#23556;&#24433;&#20195;&#25968;&#23450;&#20041;&#20102;&#21151;&#33021;&#24378;&#22823;&#12289;&#24615;&#33021;&#33391;&#22909;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Transformer&#65288;GATr&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#23556;&#24433;&#20960;&#20309;&#20195;&#25968;&#30340;&#36890;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26550;&#26500;&#27010;&#25324;&#20026;&#19968;&#20010;&#34013;&#22270;&#65292;&#20801;&#35768;&#19968;&#20010;&#26681;&#25454;&#20219;&#20309;&#20960;&#20309;&#65288;&#25110;&#20811;&#21033;&#31119;&#24503;&#65289;&#20195;&#25968;&#26469;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#21644;&#20849;&#24418;&#20195;&#25968;&#29256;&#26412;&#30340;&#36825;&#31181;&#26550;&#26500;&#65292;&#23427;&#20204;&#37117;&#36866;&#21512;&#34920;&#31034;3D&#25968;&#25454;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26368;&#31616;&#21333;&#30340;&#27431;&#20960;&#37324;&#24471;&#26550;&#26500;&#22312;&#35745;&#31639;&#19978;&#24265;&#20215;&#65292;&#20294;&#23545;&#31216;&#32676;&#36739;&#23567;&#19988;&#19981;&#22815;&#26679;&#26412;&#39640;&#25928;&#65292;&#32780;&#23556;&#24433;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#19981;&#22815;&#12290;&#20849;&#24418;&#20195;&#25968;&#21644;&#25913;&#36827;&#29256;&#26412;&#30340;&#23556;&#24433;&#20195;&#25968;&#37117;&#23450;&#20041;&#20102;&#21151;&#33021;&#24378;&#22823;&#12289;&#24615;&#33021;&#33391;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04744v2 Announce Type: replace-cross  Abstract: The Geometric Algebra Transformer (GATr) is a versatile architecture for geometric deep learning based on projective geometric algebra. We generalize this architecture into a blueprint that allows one to construct a scalable transformer architecture given any geometric (or Clifford) algebra. We study versions of this architecture for Euclidean, projective, and conformal algebras, all of which are suited to represent 3D data, and evaluate them in theory and practice. The simplest Euclidean architecture is computationally cheap, but has a smaller symmetry group and is not as sample-efficient, while the projective model is not sufficiently expressive. Both the conformal algebra and an improved version of the projective algebra define powerful, performant architectures.
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#23450;&#20219;&#21153;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.12921</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12921
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#23450;&#20219;&#21153;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35201;&#27714;&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#25110;&#32773;&#36890;&#36807;&#22823;&#37327;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#36890;&#24120;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21644;&#36890;&#29992;&#30340;&#20351;&#29992;VLM&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;VLM-RMs&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;CLIP&#30340;VLM-RMs&#26469;&#35757;&#32451;MuJoCo&#20154;&#24418;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#36330;&#19979;&#12289;&#21128;&#21449;&#21644;&#30424;&#33151;&#22352;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20165;&#25552;&#20379;&#19968;&#20010;&#25551;&#36848;&#25152;&#38656;&#20219;&#21153;&#30340;&#21333;&#20010;&#21477;&#23376;&#25991;&#26412;&#25552;&#31034;&#65292;&#20943;&#23569;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#35757;&#32451;&#20195;&#29702;&#30340;&#35270;&#39057;&#38142;&#25509;&#65306;https://sites.google.com/view/vlm-rm&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#31532;&#20108;&#20010;&#8220;&#22522;&#20934;&#8221;&#25552;&#31034;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12921v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second "baseline" prom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.07699</link><description>&lt;p&gt;
VeCLIP&#65306;&#36890;&#36807;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#25913;&#36827;CLIP&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
VeCLIP: Improving CLIP Training via Visual-enriched Captions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#23545;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#29228;&#21462;&#30340;AltTexts&#23384;&#22312;&#22266;&#26377;&#30340;&#22122;&#38899;&#21644;&#28508;&#22312;&#30340;&#19981;&#30456;&#20851;&#24615;&#65292;&#36896;&#25104;&#20102;&#31934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#23383;&#23545;&#40784;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22024;&#26434;&#26631;&#39064;&#37325;&#20889;&#30340;&#21487;&#25193;&#23637;&#27969;&#31243;&#12290;&#19982;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26631;&#39064;&#37325;&#20889;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#23567;&#22411;&#31574;&#21010;&#25968;&#25454;&#38598;&#65288;&#22914;CC3M&#21644;CC12M&#65289;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#65292;&#31216;&#20026;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#65288;VeCap&#65289;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;AltTexts&#19982;&#26032;&#29983;&#25104;&#30340;VeCap&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;CLIP&#30340;&#36866;&#24212;&#24615;&#65292;&#31216;&#20026;VeCLIP&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#36731;&#26494;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our
&lt;/p&gt;</description></item><item><title>iTransformer&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;Transformer&#26550;&#26500;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#31616;&#21333;&#24212;&#29992;&#27880;&#24847;&#21147;&#21644;&#39304;&#36865;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#20811;&#26381;&#20102;&#20854;&#20182;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#26356;&#22823;&#22238;&#28335;&#31383;&#21475;&#30340;&#31995;&#21015;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2310.06625</link><description>&lt;p&gt;
iTransformer: &#21453;&#36716;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#26377;&#25928;&#30340;
&lt;/p&gt;
&lt;p&gt;
iTransformer: Inverted Transformers Are Effective for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06625
&lt;/p&gt;
&lt;p&gt;
iTransformer&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;Transformer&#26550;&#26500;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#31616;&#21333;&#24212;&#29992;&#27880;&#24847;&#21147;&#21644;&#39304;&#36865;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#20811;&#26381;&#20102;&#20854;&#20182;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#26356;&#22823;&#22238;&#28335;&#31383;&#21475;&#30340;&#31995;&#21015;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#20852;&#36215;&#23545;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#22120;&#30340;&#26550;&#26500;&#20462;&#25913;&#30340;&#25345;&#32493;&#28909;&#24773;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#36825;&#20123;&#39044;&#27979;&#22120;&#21033;&#29992;Transformer&#26469;&#27169;&#25311;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#26631;&#35760;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#27599;&#20010;&#26102;&#38388;&#26631;&#35760;&#30001;&#30456;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#20010;&#21464;&#37327;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24615;&#33021;&#19979;&#38477;&#21644;&#35745;&#31639;&#29190;&#28856;&#65292;Transformer&#22312;&#39044;&#27979;&#20855;&#26377;&#26356;&#22823;&#22238;&#28335;&#31383;&#21475;&#30340;&#31995;&#21015;&#26102;&#21463;&#21040;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#26102;&#38388;&#26631;&#35760;&#30340;&#23884;&#20837;&#34701;&#21512;&#20102;&#20195;&#34920;&#28508;&#22312;&#24310;&#36831;&#20107;&#20214;&#21644;&#19981;&#21516;&#29289;&#29702;&#27979;&#37327;&#30340;&#22810;&#20010;&#21464;&#37327;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26080;&#27861;&#23398;&#20064;&#21464;&#37327;-centric&#34920;&#31034;&#24182;&#23548;&#33268;&#26080;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21453;&#24605;&#20102;Transformer&#32452;&#20214;&#30340;&#33021;&#21147;&#65292;&#24182;&#37325;&#26032;&#21033;&#29992;&#20102;Transformer&#26550;&#26500;&#65292;&#32780;&#27809;&#26377;&#20462;&#25913;&#22522;&#26412;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;iTransformer&#65292;&#23427;&#31616;&#21333;&#22320;&#24212;&#29992;&#20102;&#27880;&#24847;&#21147;&#21644;&#39304;&#36865;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06625v3 Announce Type: replace  Abstract: The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2310.02226</link><description>&lt;p&gt;
&#35880;&#35328;&#24910;&#34892;&#65306;&#20351;&#29992;&#26242;&#20572;&#26631;&#35760;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Think before you speak: Training Language Models With Pause Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02226
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31435;&#21363;&#36830;&#32493;&#29983;&#25104;&#19968;&#31995;&#21015;&#26631;&#35760;&#26469;&#29983;&#25104;&#21709;&#24212;: &#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#26159;&#36890;&#36807;&#25805;&#20316;&#27599;&#23618;&#30340;$K$&#20010;&#38544;&#34255;&#21521;&#37327;&#24471;&#21040;&#30340;&#65292;&#27599;&#20010;&#21521;&#37327;&#23545;&#24212;&#19968;&#20010;&#21069;&#38754;&#30340;&#26631;&#35760;&#12290;&#22914;&#26524;&#25105;&#20204;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#20043;&#21069;&#25805;&#20316;&#26356;&#22810;&#30340;&#38544;&#34255;&#21521;&#37327;&#65292;&#27604;&#22914;&#35828;$K+10$&#20010;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;$\textit{pause}$&#26631;&#35760;&#65292;&#36825;&#19968;&#31995;&#21015;&#26631;&#35760;&#38468;&#21152;&#21040;&#36755;&#20837;&#21069;&#32512;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#24310;&#36831;&#25552;&#21462;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#30452;&#21040;&#26368;&#21518;&#19968;&#20010;&#26242;&#20572;&#26631;&#35760;&#34987;&#30475;&#21040;&#65292;&#20174;&#32780;&#20801;&#35768;&#27169;&#22411;&#22312;&#20570;&#20986;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#39069;&#22806;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;1B&#21644;130M&#21442;&#25968;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;$\textit{pause-training}$&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;C4&#19978;&#36827;&#34892;&#20102;&#22240;&#26524;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#28085;&#30422;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#26222;&#36941;&#29702;&#35299;&#21644;&#20107;&#23454;&#22238;&#24518;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;infer
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#20986;&#26356;&#32039;&#20945;&#20294;&#26356;&#20855;&#30693;&#35782;&#30340;SMoE&#27169;&#22411;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#12290;</title><link>https://arxiv.org/abs/2310.01334</link><description>&lt;p&gt;
&#21512;&#24182;&#65292;&#28982;&#21518;&#21387;&#32553;&#65306;&#20174;&#20854;&#36335;&#30001;&#31574;&#30053;&#20013;&#25581;&#31034;&#39640;&#25928;&#30340;SMoE&#25216;&#26415;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#20986;&#26356;&#32039;&#20945;&#20294;&#26356;&#20855;&#30693;&#35782;&#30340;SMoE&#27169;&#22411;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;SMoE&#65289;&#26174;&#31034;&#20986;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#35832;&#22914;&#65288;a&#65289;&#39640;&#20869;&#23384;&#20351;&#29992;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#32593;&#32476;&#23618;&#30340;&#37325;&#22797;&#25104;&#20026;&#22810;&#20010;&#19987;&#23478;&#30340;&#21103;&#26412;&#65307;&#20197;&#21450;&#65288;b&#65289;&#19987;&#23478;&#20013;&#30340;&#20887;&#20313;&#65292;&#22240;&#20026;&#24120;&#35268;&#22522;&#20110;&#23398;&#20064;&#30340;&#36335;&#30001;&#31574;&#30053;&#23481;&#26131;&#20986;&#29616;&#34920;&#31034;&#24615;&#23849;&#28291;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;SMoE&#27169;&#22411;&#22312;&#20869;&#23384;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#23588;&#20854;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#19979;&#28216;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#19968;&#20010;&#32039;&#20945;&#30340;SMoE&#27169;&#22411;&#65311;&#22914;&#20309;&#23558;&#22810;&#20010;&#19987;&#23478;&#21512;&#24182;&#20026;&#26356;&#23569;&#20294;&#26356;&#26377;&#30693;&#35782;&#30340;&#19987;&#23478;&#30340;&#26368;&#20339;&#26041;&#27861;&#65311;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#26174;&#31034;&#65292;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#23545;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#24182;&#19981;&#26377;&#25928;&#12290;&#28508;&#22312;&#21407;&#22240;&#26159;&#65306;&#65288;1&#65289;&#20887;&#20313;&#20449;&#24687;&#25513;&#30422;&#20102;&#20851;&#38190;&#19987;&#23478;&#65307;&#65288;2&#65289;&#20026;&#27599;&#20010;&#19987;&#23478;&#36873;&#25321;&#36866;&#24403;&#30340;&#31070;&#32463;&#20803;&#25490;&#21015;&#26041;&#24335;&#20250;&#20002;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>&#36801;&#31227;&#23398;&#20064;&#32467;&#21512;&#36793;&#32536;&#35745;&#31639;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#35270;&#38382;&#39064;&#21644;&#30005;&#22120;&#35782;&#21035;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;CNN&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2301.03018</link><description>&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#33021;&#37327;&#20998;&#35299;&#19982;&#30005;&#22120;&#35782;&#21035;&#65306;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#36793;&#32536;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Energy Disaggregation &amp; Appliance Identification in a Smart Home: Transfer Learning enables Edge Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03018
&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#32467;&#21512;&#36793;&#32536;&#35745;&#31639;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#35270;&#38382;&#39064;&#21644;&#30005;&#22120;&#35782;&#21035;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;CNN&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#35270;&#65288;NILM&#65289;&#25110;&#33021;&#37327;&#20998;&#35299;&#26088;&#22312;&#22312;&#26234;&#33021;&#23478;&#23621;&#30340;&#24635;&#36127;&#36733;&#37197;&#32622;&#25991;&#20214;&#30340;&#22522;&#30784;&#19978;&#25552;&#21462;&#21333;&#20010;&#28040;&#36153;&#30005;&#23376;&#35774;&#22791;&#30340;&#36127;&#36733;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;NILM&#38382;&#39064;&#20197;&#21450;&#19968;&#20123;&#30456;&#20851;&#38382;&#39064;&#12290;1&#65289;&#25105;&#20204;&#22312;&#33879;&#21517;&#30340;seq2-point&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#25552;&#20986;&#30340;seq2-[3]-point CNN&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#65288;&#23478;&#24237;&#65289;NILM&#38382;&#39064;&#21644;&#31449;&#28857;NILM&#38382;&#39064;&#65288;&#22522;&#26412;&#19978;&#26159;&#22312;&#36739;&#23567;&#35268;&#27169;&#19978;&#30340;NILM&#65289;&#12290;2&#65289;&#25105;&#20204;&#36890;&#36807;&#20511;&#37492;&#26368;&#20808;&#36827;&#30340;&#65288;&#39044;&#35757;&#32451;&#65289;2D-CNN&#27169;&#22411;&#65292;&#21363;AlexNet&#12289;ResNet-18&#21644;DenseNet-121&#65292;&#35299;&#20915;&#20102;&#30005;&#22120;&#35782;&#21035;&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#36866;&#24212;&#20004;&#20010;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#30005;&#22120;&#22522;&#20110;&#23567;&#27874;&#21464;&#25442;&#21644;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#30340;2D&#30005;&#29305;&#24449;&#12290;3&#65289;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#23450;&#24615;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03018v2 Announce Type: replace-cross  Abstract: Non-intrusive load monitoring (NILM) or energy disaggregation aims to extract the load profiles of individual consumer electronic appliances, given an aggregate load profile of the mains of a smart home. This work proposes a novel deep-learning and edge computing approach to solve the NILM problem and a few related problems as follows. 1) We build upon the reputed seq2-point convolutional neural network (CNN) model to come up with the proposed seq2-[3]-point CNN model to solve the (home) NILM problem and site-NILM problem (basically, NILM at a smaller scale). 2) We solve the related problem of appliance identification by building upon the state-of-the-art (pre-trained) 2D-CNN models, i.e., AlexNet, ResNet-18, and DenseNet-121, which are fine-tuned two custom datasets that consist of Wavelets and short-time Fourier transform (STFT)-based 2D electrical signatures of the appliances. 3) Finally, we do some basic qualitative inferen
&lt;/p&gt;</description></item><item><title>Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#21463;&#21040;&#24207;&#21015;&#38271;&#24230;&#23398;&#20064;&#38382;&#39064;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#24207;&#21015;&#38271;&#24230;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#32780;&#38750;&#25991;&#26412;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2212.08399</link><description>&lt;p&gt;
&#35780;&#20272;&#24207;&#21015;&#38271;&#24230;&#23398;&#20064;&#23545;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.08399
&lt;/p&gt;
&lt;p&gt;
Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#21463;&#21040;&#24207;&#21015;&#38271;&#24230;&#23398;&#20064;&#38382;&#39064;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#24207;&#21015;&#38271;&#24230;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#32780;&#38750;&#25991;&#26412;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#20998;&#31867;&#31639;&#27861;&#22312;&#35266;&#27979;&#26469;&#33258;&#19981;&#21516;&#31867;&#30340;&#24207;&#21015;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#20998;&#24067;&#26102;&#21487;&#33021;&#21463;&#21040;&#24207;&#21015;&#38271;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#27169;&#22411;&#23558;&#24207;&#21015;&#38271;&#24230;&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#37325;&#35201;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20844;&#20849;&#25968;&#25454;&#38598;&#19981;&#21463;&#27492;&#38382;&#39064;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#21307;&#23398;&#21644;&#20445;&#38505;&#31561;&#39046;&#22495;&#30340;&#31169;&#20154;&#25317;&#26377;&#35821;&#26009;&#24211;&#21487;&#33021;&#23384;&#22312;&#36825;&#31181;&#25968;&#25454;&#20559;&#24046;&#12290;&#21033;&#29992;&#36825;&#31181;&#24207;&#21015;&#38271;&#24230;&#29305;&#24449;&#20250;&#22312;&#25972;&#20010;&#20215;&#20540;&#38142;&#20013;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20851;&#38190;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#23569;&#20854;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.08399v2 Announce Type: replace  Abstract: Classification algorithms using Transformer architectures can be affected by the sequence length learning problem whenever observations from different classes have a different length distribution. This problem causes models to use sequence length as a predictive feature instead of relying on important textual information. Although most public datasets are not affected by this problem, privately owned corpora for fields such as medicine and insurance may carry this data bias. The exploitation of this sequence length feature poses challenges throughout the value chain as these machine learning models can be used in critical applications. In this paper, we empirically expose this problem and present approaches to minimize its impacts.
&lt;/p&gt;</description></item><item><title>COMET&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#30740;&#31350;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#20851;&#38190;&#38598;&#32676;&#36164;&#28304;&#37197;&#32622;&#23545;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2211.16648</link><description>&lt;p&gt;
COMET&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#20840;&#38754;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
COMET: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.16648
&lt;/p&gt;
&lt;p&gt;
COMET&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#30740;&#31350;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#20851;&#38190;&#38598;&#32676;&#36164;&#28304;&#37197;&#32622;&#23545;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#24050;&#32463;&#21457;&#23637;&#21040;&#38656;&#35201;&#22823;&#35268;&#27169;&#19987;&#38376;&#30340;&#12289;&#39640;&#31471;&#33410;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#23567;&#12290;&#35774;&#35745;&#36825;&#26679;&#30340;&#38598;&#32676;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#24615;&#33021;&#21644;&#21033;&#29992;&#29575;--&#20197;&#25674;&#38144;&#20854;&#39640;&#26114;&#25104;&#26412;--&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20180;&#32454;&#24179;&#34913;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#32593;&#32476;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#30340;&#20247;&#22810;&#35843;&#25972;&#26059;&#38062;&#26497;&#22823;&#22320;&#24433;&#21709;&#24615;&#33021;&#65292;&#26368;&#20339;&#20540;&#24448;&#24448;&#21462;&#20915;&#20110;&#24213;&#23618;&#38598;&#32676;&#30340;&#29305;&#24449;&#65292;&#36825;&#35201;&#27714;&#36827;&#34892;&#22797;&#26434;&#30340;&#38598;&#32676;-&#24037;&#20316;&#36127;&#36733;&#21327;&#21516;&#35774;&#35745;&#36807;&#31243;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#22823;&#35268;&#27169;DL&#35757;&#32451;&#38598;&#32676;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COMET&#65292;&#36825;&#26159;&#19968;&#31181;&#32508;&#21512;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20849;&#21516;&#30740;&#31350;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#20851;&#38190;&#38598;&#32676;&#36164;&#28304;&#37197;&#32622;&#23545;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#36807;&#31243;&#65292;&#24314;&#31435;&#19968;&#31181;&#21487;&#37325;&#29992;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#21152;&#20197;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.16648v2 Announce Type: replace-cross  Abstract: Modern Deep Learning (DL) models have grown to sizes requiring massive clusters of specialized, high-end nodes to train. Designing such clusters to maximize both performance and utilization--to amortize their steep cost--is a challenging task requiring careful balance of compute, memory, and network resources. Moreover, a plethora of each model's tuning knobs drastically affect the performance, with optimal values often depending on the underlying cluster's characteristics, which necessitates a complex cluster-workload co-design process. To facilitate the design space exploration of such massive DL training clusters, we introduce COMET, a holistic cluster design methodology and workflow to jointly study the impact of parallelization strategies and key cluster resource provisioning on the performance of distributed DL training. We develop a step-by-step process to establish a reusable and flexible methodology, and demonstrate it
&lt;/p&gt;</description></item><item><title>PPAL&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#35937;&#26816;&#27979;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#25277;&#26679;&#30456;&#32467;&#21512;&#65292;&#20811;&#26381;&#20102;&#19987;&#38376;&#23545;&#35937;&#26816;&#27979;&#22120;&#26550;&#26500;&#38598;&#25104;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2211.11612</link><description>&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#30340;&#21363;&#25554;&#21363;&#29992;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Plug and Play Active Learning for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11612
&lt;/p&gt;
&lt;p&gt;
PPAL&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#35937;&#26816;&#27979;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#25277;&#26679;&#30456;&#32467;&#21512;&#65292;&#20811;&#26381;&#20102;&#19987;&#38376;&#23545;&#35937;&#26816;&#27979;&#22120;&#26550;&#26500;&#38598;&#25104;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#23545;&#35937;&#26816;&#27979;&#26631;&#27880;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#36825;&#19968;&#36127;&#25285;&#65292;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26469;&#22312;&#38480;&#23450;&#30340;&#8220;&#26631;&#27880;&#39044;&#31639;&#8221;&#20869;&#36873;&#25321;&#20449;&#24687;&#37327;&#26368;&#22823;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#12290;&#20256;&#32479;&#30340;AL&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#36827;&#34892;&#26597;&#35810;&#25277;&#26679;&#65292;&#32780;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#24320;&#21457;AL&#29305;&#23450;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#26550;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19987;&#38376;&#21270;&#26041;&#27861;&#30001;&#20110;&#38598;&#25104;&#25152;&#38656;&#30340;&#22823;&#37327;&#24037;&#31243;&#24037;&#20316;&#65292;&#19981;&#23481;&#26131;&#36866;&#24212;&#19981;&#21516;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21363;&#25554;&#21363;&#29992;&#20027;&#21160;&#23398;&#20064;&#65288;PPAL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#35937;&#26816;&#27979;AL&#31574;&#30053;&#12290;PPAL&#26159;&#19968;&#20010;&#21253;&#21547;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#25277;&#26679;&#38454;&#27573;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#38590;&#24230;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#25277;&#26679;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11612v2 Announce Type: replace-cross  Abstract: Annotating datasets for object detection is an expensive and time-consuming endeavor. To minimize this burden, active learning (AL) techniques are employed to select the most informative samples for annotation within a constrained "annotation budget". Traditional AL strategies typically rely on model uncertainty or sample diversity for query sampling, while more advanced methods have focused on developing AL-specific object detector architectures to enhance performance. However, these specialized approaches are not readily adaptable to different object detectors due to the significant engineering effort required for integration. To overcome this challenge, we introduce Plug and Play Active Learning (PPAL), a simple and effective AL strategy for object detection. PPAL is a two-stage method comprising uncertainty-based and diversity-based sampling phases. In the first stage, our Difficulty Calibrated Uncertainty Sampling leverage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23454;&#29616;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65288;SVD-PINNs&#65289;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#19968;&#31867;&#20855;&#26377;&#19981;&#21516;&#20294;&#30456;&#20284;&#21491;&#31471;&#39033;&#30340;&#39640;&#32500;PDEs&#12290;</title><link>https://arxiv.org/abs/2211.08760</link><description>&lt;p&gt;
SVD-PINNs: &#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23454;&#29616;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SVD-PINNs: Transfer Learning of Physics-Informed Neural Networks via Singular Value Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.08760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23454;&#29616;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65288;SVD-PINNs&#65289;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#19968;&#31867;&#20855;&#26377;&#19981;&#21516;&#20294;&#30456;&#20284;&#21491;&#31471;&#39033;&#30340;&#39640;&#32500;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#32531;&#35299;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#36866;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28982;&#32780;&#65292;PINNs&#30340;&#26368;&#22823;&#32570;&#28857;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#24212;&#19968;&#20010;PDE&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#19968;&#31867;PDEs&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#35768;&#22810;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#25216;&#26415;&#20063;&#36866;&#29992;&#20110;PINNs&#12290;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;PINNs&#22312;&#35299;&#20915;&#19968;&#31867;PDEs&#26102;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#22855;&#24322;&#21521;&#37327;&#21644;&#20248;&#21270;&#22855;&#24322;&#20540;&#30340;PINNs&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;SVD-PINNs&#65289;&#12290;&#23545;&#39640;&#32500;PDEs&#65288;10&#32500;&#32447;&#24615;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;10&#32500;Allen-Cahn&#26041;&#31243;&#65289;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;SVD-PINNs&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#20855;&#26377;&#19981;&#21516;&#20294;&#30456;&#20284;&#21491;&#31471;&#39033;&#30340;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.08760v2 Announce Type: replace  Abstract: Physics-informed neural networks (PINNs) have attracted significant attention for solving partial differential equations (PDEs) in recent years because they alleviate the curse of dimensionality that appears in traditional methods. However, the most disadvantage of PINNs is that one neural network corresponds to one PDE. In practice, we usually need to solve a class of PDEs, not just one. With the explosive growth of deep learning, many useful techniques in general deep learning tasks are also suitable for PINNs. Transfer learning methods may reduce the cost for PINNs in solving a class of PDEs. In this paper, we proposed a transfer learning method of PINNs via keeping singular vectors and optimizing singular values (namely SVD-PINNs). Numerical experiments on high dimensional PDEs (10-d linear parabolic equations and 10-d Allen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with different but close right-hand-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2210.17437</link><description>&lt;p&gt;
&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning New Tasks from a Few Examples with Soft-Label Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23545;&#20854;&#24494;&#35843;&#65292;&#20197;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#8220;&#26497;&#31471;&#8221;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#21482;&#38656;&#25509;&#35302;&#27599;&#20010;&#31867;&#21035;&#33267;&#23569;4&#20010;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#22522;&#20110;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#36825;&#20123;&#36719;&#26631;&#31614;&#21407;&#22411;&#20849;&#21516;&#25429;&#33719;&#20102;&#36755;&#20837;&#22495;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#20998;&#24067;&#12290;&#21463;&#21040;&#20808;&#21069;&#20851;&#20110;&#19968;&#20803;&#25110;&#31616;&#21333;&#22810;&#20803;&#65288;&#21512;&#25104;&#65289;&#25968;&#25454;&#65288;Sucholutsky&#31561;&#20154;&#65292;2021&#65289;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#26694;&#26550;&#65288;DeepSLP&#65289;&#20013;&#23398;&#20064;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#23427;&#22312;31/48&#20010;&#27979;&#35797;&#20219;&#21153;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;v&#20013;&#23398;&#20064;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;NLP&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPAR&#30340;&#20998;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23454;&#29616;&#23545;GNNs&#36827;&#34892;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#20174;&#32780;&#20445;&#25252;&#33410;&#28857;&#21450;&#20854;&#36793;&#32536;&#12290;</title><link>https://arxiv.org/abs/2210.04442</link><description>&lt;p&gt;
DPAR: &#20855;&#26377;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.04442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPAR&#30340;&#20998;&#31163;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23454;&#29616;&#23545;GNNs&#36827;&#34892;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#20174;&#32780;&#20445;&#25252;&#33410;&#28857;&#21450;&#20854;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290; &#36824;&#25552;&#20986;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26292;&#38706;&#22270;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290; &#26412;&#25991;&#26088;&#22312;&#23454;&#29616;&#23545;GNNs&#36827;&#34892;&#33410;&#28857;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65292;&#20197;&#20445;&#25252;&#33410;&#28857;&#21450;&#20854;&#36793;&#32536;&#12290; GNNs&#30340;&#33410;&#28857;DP&#22312;&#26412;&#36136;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#25152;&#26377;&#30452;&#25509;&#21644;&#22810;&#36339;&#37051;&#23621;&#36890;&#36807;&#36880;&#23618;&#28040;&#24687;&#20256;&#36882;&#21442;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#19988;&#33410;&#28857;&#21487;&#20197;&#20855;&#26377;&#22810;&#23569;&#30452;&#25509;&#21644;&#22810;&#36339;&#37051;&#23621;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#23558;&#23548;&#33268;&#24456;&#39640;&#30340;&#38544;&#31169;&#25104;&#26412;&#25110;&#30001;&#20110;&#33410;&#28857;&#25935;&#24863;&#24615;&#39640;&#32780;&#25928;&#29992;&#19981;&#20339;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#24322;&#24615;&#31169;&#20154;&#21270;&#35843;&#25972;&#39029;&#38754;&#25490;&#21517;&#65288;DPAR&#65289;&#30340;\textbf{D}ecoupled GNN&#65292;&#29992;&#20110;&#35757;&#32451;&#24102;&#26377;&#22686;&#24378;&#38544;&#31169;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.04442v2 Announce Type: replace  Abstract: Graph Neural Networks (GNNs) have achieved great success in learning with graph-structured data. Privacy concerns have also been raised for the trained models which could expose the sensitive information of graphs including both node features and the structure information. In this paper, we aim to achieve node-level differential privacy (DP) for training GNNs so that a node and its edges are protected. Node DP is inherently difficult for GNNs because all direct and multi-hop neighbors participate in the calculation of gradients for each node via layer-wise message passing and there is no bound on how many direct and multi-hop neighbors a node can have, so existing DP methods will result in high privacy cost or poor utility due to high node sensitivity. We propose a \textbf{D}ecoupled GNN with Differentially \textbf{P}rivate \textbf{A}pproximate Personalized Page\textbf{R}ank (DPAR) for training GNNs with an enhanced privacy-utility t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#34701;&#21512;LANDMARK&#29305;&#24449;&#26469;&#23454;&#29616;&#25104;&#20154;&#21644;&#20799;&#31461;&#38754;&#37096;&#34920;&#24773;&#30340;&#28145;&#24230;&#35843;&#25972;&#65292;&#20197;&#20415;&#22312;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#23089;&#20048;&#31561;&#39046;&#22495;&#20013;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2209.08614</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#22320;&#26631;&#29305;&#24449;&#23454;&#29616;&#25104;&#20154;-&#20799;&#31461;&#38754;&#37096;&#34920;&#24773;&#30340;&#28145;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#34701;&#21512;LANDMARK&#29305;&#24449;&#26469;&#23454;&#29616;&#25104;&#20154;&#21644;&#20799;&#31461;&#38754;&#37096;&#34920;&#24773;&#30340;&#28145;&#24230;&#35843;&#25972;&#65292;&#20197;&#20415;&#22312;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#23089;&#20048;&#31561;&#39046;&#22495;&#20013;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#24773;&#24863;&#25104;&#20687;&#21487;&#29992;&#20110;&#27979;&#37327;&#20799;&#31461;&#21040;&#25104;&#20154;&#38454;&#27573;&#30340;&#24515;&#29702;&#29983;&#29702;&#29305;&#24449;&#65292;&#24212;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#23089;&#20048;&#31561;&#39046;&#22495;&#12290;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25104;&#20154;&#38754;&#37096;&#34920;&#24773;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29992;&#25104;&#20154;&#22522;&#20934;&#25968;&#25454;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#27169;&#22411;&#19981;&#36866;&#21512;&#23398;&#20064;&#20799;&#31461;&#34920;&#24773;&#65292;&#22240;&#20026;&#24515;&#29702;&#29289;&#29702;&#21457;&#23637;&#23384;&#22312;&#24046;&#24322;&#12290;&#21516;&#26679;&#22320;&#65292;&#29992;&#20799;&#31461;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25104;&#20154;&#34920;&#24773;&#20998;&#31867;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21516;&#26102;&#23558;&#25104;&#20154;&#21644;&#20799;&#31461;&#34920;&#24773;&#30340;&#20998;&#24067;&#23545;&#40784;&#21040;&#20849;&#20139;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#20415;&#23545;&#20219;&#20309;&#19968;&#20010;&#39046;&#22495;&#36827;&#34892;&#20581;&#22766;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#24180;&#40836;&#21464;&#21270;&#22312;&#38754;&#37096;&#22270;&#20687;&#20013;&#21463;&#21040;&#24180;&#40836;&#19981;&#21464;&#20154;&#33080;&#35782;&#21035;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#25104;&#20154;-&#20799;&#31461;&#34920;&#24773;&#20998;&#31867;&#20013;&#20173;&#26410;&#24471;&#21040;&#21033;&#29992;&#12290;&#25105;&#20204;&#20174;&#22810;&#20010;&#39046;&#22495;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#28145;&#24230;&#33258;&#36866;&#24212;LANDMARK&#29305;&#24449;&#34701;&#21512;&#20197;&#23454;&#29616;&#31934;&#20934;&#30340;&#25104;&#20154;-&#20799;&#31461;&#38754;&#37096;&#34920;&#24773;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08614v2 Announce Type: replace-cross  Abstract: Imaging of facial affects may be used to measure psychophysiological attributes of children through their adulthood for applications in education, healthcare, and entertainment, among others. Deep convolutional neural networks show promising results in classifying facial expressions of adults. However, classifier models trained with adult benchmark data are unsuitable for learning child expressions due to discrepancies in psychophysical development. Similarly, models trained with child data perform poorly in adult expression classification. We propose domain adaptation to concurrently align distributions of adult and child expressions in a shared latent space for robust classification of either domain. Furthermore, age variations in facial images are studied in age-invariant face recognition yet remain unleveraged in adult-child expression classification. We take inspiration from multiple fields and propose deep adaptive FACial
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#20915;&#31574;&#30456;&#20851;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#65292;&#31639;&#27861;&#30340;&#24179;&#22343;&#36845;&#20195;&#19982;&#35299;&#20043;&#38388;&#30340;&#20559;&#24046;&#26159;&#27491;&#24577;&#30340;&#65292;&#24182;&#19988;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#23616;&#37096;&#36798;&#21040;&#20102;&#26368;&#20248;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2207.04173</link><description>&lt;p&gt;
&#20855;&#26377;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Decision-Dependent Distributions: Asymptotic Normality and Optimality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.04173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#20915;&#31574;&#30456;&#20851;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#65292;&#31639;&#27861;&#30340;&#24179;&#22343;&#36845;&#20195;&#19982;&#35299;&#20043;&#38388;&#30340;&#20559;&#24046;&#26159;&#27491;&#24577;&#30340;&#65292;&#24182;&#19988;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#23616;&#37096;&#36798;&#21040;&#20102;&#26368;&#20248;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#20915;&#31574;&#30456;&#20851;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#20013;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#20998;&#24067;&#27839;&#30528;&#36845;&#20195;&#24207;&#21015;&#28436;&#21464;&#12290;&#36825;&#31867;&#38382;&#39064;&#30340;&#20027;&#35201;&#20363;&#23376;&#20986;&#29616;&#22312;&#25191;&#34892;&#24615;&#39044;&#27979;&#21450;&#20854;&#22810;&#20154;&#28216;&#25103;&#25193;&#23637;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#31639;&#27861;&#30340;&#24179;&#22343;&#36845;&#20195;&#19982;&#35299;&#20043;&#38388;&#30340;&#20559;&#24046;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#27491;&#24577;&#30340;&#65292;&#21327;&#26041;&#24046;&#28165;&#26224;&#22320;&#20998;&#35299;&#20102;&#26799;&#24230;&#22122;&#22768;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;H\'ajek&#21644;Le Cam&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24102;&#26377;&#24179;&#22343;&#30340;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#22312;&#23616;&#37096;&#26159;&#23616;&#37096;&#26368;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.04173v3 Announce Type: replace-cross  Abstract: We analyze a stochastic approximation algorithm for decision-dependent problems, wherein the data distribution used by the algorithm evolves along the iterate sequence. The primary examples of such problems appear in performative prediction and its multiplayer extensions. We show that under mild assumptions, the deviation between the average iterate of the algorithm and the solution is asymptotically normal, with a covariance that clearly decouples the effects of the gradient noise and the distributional shift. Moreover, building on the work of H\'ajek and Le Cam, we show that the asymptotic performance of the algorithm with averaging is locally minimax optimal.
&lt;/p&gt;</description></item><item><title>SPI-GAN&#20351;&#29992;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23450;&#20041;&#30340;&#22686;&#24378;&#22411;GAN&#21435;&#22122;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;SGMs&#30456;&#21516;&#30340;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.14464</link><description>&lt;p&gt;
&#20351;&#29992;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23545;Denoising Diffusion GANs&#30340;SPI-GAN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.14464
&lt;/p&gt;
&lt;p&gt;
SPI-GAN&#20351;&#29992;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23450;&#20041;&#30340;&#22686;&#24378;&#22411;GAN&#21435;&#22122;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;SGMs&#30456;&#21516;&#30340;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;/&#37319;&#26679;&#22797;&#26434;&#24615;&#30001;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#21069;&#21521;/&#21518;&#21521;&#36807;&#31243;&#32780;&#26497;&#39640;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#21069;&#23545;&#23398;&#20064;&#26356;&#31616;&#21333;&#36807;&#31243;&#30340;&#20851;&#27880;&#24230;&#27491;&#36880;&#28176;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#22522;&#20110;GAN&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#31216;&#20026;SPI-GAN&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23450;&#20041;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GAN&#26550;&#26500;&#65292;&#36890;&#36807;&#30452;&#32447;&#36335;&#24452;&#21435;&#22122;&#65292;&#24182;&#19988;&#30001;&#36830;&#32493;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#34920;&#24449;&#65292;&#20197;&#27169;&#20223;&#21435;&#22122;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;SGMs&#30456;&#21516;&#30340;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;SPI-GAN&#26159;CIFAR-10&#21644;CelebA-HQ-256&#20013;&#37319;&#26679;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#20013;&#26368;&#22343;&#34913;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.14464v3 Announce Type: replace-cross  Abstract: Score-based generative models (SGMs) show the state-of-the-art sampling quality and diversity. However, their training/sampling complexity is notoriously high due to the highly complicated forward/reverse processes, so they are not suitable for resource-limited settings. To solving this problem, learning a simpler process is gathering much attention currently. We present an enhanced GAN-based denoising method, called SPI-GAN, using our proposed straight-path interpolation definition. To this end, we propose a GAN architecture i) denoising through the straight-path and ii) characterized by a continuous mapping neural network for imitating the denoising path. This approach drastically reduces the sampling time while achieving as high sampling quality and diversity as SGMs. As a result, SPI-GAN is one of the best-balanced models among the sampling quality, diversity, and time for CIFAR-10, and CelebA-HQ-256.
&lt;/p&gt;</description></item><item><title>OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2206.11104</link><description>&lt;p&gt;
OpenXAI: &#36808;&#21521;&#36879;&#26126;&#35780;&#20272;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OpenXAI: Towards a Transparent Evaluation of Model Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11104
&lt;/p&gt;
&lt;p&gt;
OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#20294;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20316;&#38750;&#24120;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenXAI&#65292;&#19968;&#20010;&#20840;&#38754;&#19988;&#21487;&#25193;&#23637;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#12290;OpenXAI&#21253;&#25324;&#20197;&#19979;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;i&#65289;&#28789;&#27963;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#29305;&#24449;&#24402;&#23646;&#26041;&#27861;&#30340;&#38598;&#21512;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#24230;&#12289;&#31283;&#23450;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#21644;&#20844;&#24179;&#24615;&#30340;&#21313;&#19968;&#31181;&#37327;&#21270;&#24230;&#37327;&#26631;&#20934;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#12290;OpenXAI&#26131;&#20110;&#25193;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#33258;&#23450;&#20041;&#35299;&#37322;&#26041;&#27861;&#24182;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#25490;&#34892;&#27036;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181; VAns (Variable Ansatz) &#21487;&#21464;&#32467;&#26500;&#26041;&#27861;&#26469;&#26500;&#24314; VQAs &#30340;&#20551;&#35774;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20197;&#19968;&#31181;&#29087;&#24713;&#30340;&#26041;&#24335;&#22686;&#38271;&#21644;&#31227;&#38500;&#37327;&#23376;&#38376;&#26469;&#25104;&#21151;&#32531;&#35299;&#20102;&#21487;&#35757;&#32451;&#24615;&#21644;&#22122;&#38899;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2103.06712</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#21487;&#21464;&#32467;&#26500;&#30340;&#21322;&#19981;&#21487;&#30693;&#20551;&#35774;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A semi-agnostic ansatz with variable structure for quantum machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2103.06712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181; VAns (Variable Ansatz) &#21487;&#21464;&#32467;&#26500;&#26041;&#27861;&#26469;&#26500;&#24314; VQAs &#30340;&#20551;&#35774;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20197;&#19968;&#31181;&#29087;&#24713;&#30340;&#26041;&#24335;&#22686;&#38271;&#21644;&#31227;&#38500;&#37327;&#23376;&#38376;&#26469;&#25104;&#21151;&#32531;&#35299;&#20102;&#21487;&#35757;&#32451;&#24615;&#21644;&#22122;&#38899;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum machine learning -- and specifically Variational Quantum Algorithms (VQAs) -- offers a powerful, flexible paradigm for programming near-term quantum computers, with applications in chemistry, metrology, materials science, data science, and mathematics. Here, one trains an ansatz, in the form of a parameterized quantum circuit, to accomplish a task of interest. However, challenges have recently emerged suggesting that deep ansatzes are difficult to train, due to flat training landscapes caused by randomness or by hardware noise. This motivates our work, where we present a variable structure approach to build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz), applies a set of rules to both grow and (crucially) remove quantum gates in an informed manner during the optimization. Consequently, VAns is ideally suited to mitigate trainability and noise-related issues by keeping the ansatz shallow. We employ VAns i
&lt;/p&gt;
&lt;p&gt;
arXiv:2103.06712v4 Announce Type: replace-cross  Abstract: Quantum machine learning -- and specifically Variational Quantum Algorithms (VQAs) -- offers a powerful, flexible paradigm for programming near-term quantum computers, with applications in chemistry, metrology, materials science, data science, and mathematics. Here, one trains an ansatz, in the form of a parameterized quantum circuit, to accomplish a task of interest. However, challenges have recently emerged suggesting that deep ansatzes are difficult to train, due to flat training landscapes caused by randomness or by hardware noise. This motivates our work, where we present a variable structure approach to build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz), applies a set of rules to both grow and (crucially) remove quantum gates in an informed manner during the optimization. Consequently, VAns is ideally suited to mitigate trainability and noise-related issues by keeping the ansatz shallow. We employ VAns i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15127</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#20851;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#30693;&#35782;&#20849;&#20139;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#26500;&#25104;&#20102;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#26426;&#36935;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#12289;GPT4all&#12289;Dolly&#12289;Stanford Alpaca&#12289;Alpaca-LoRA&#21644;Falcon&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35782;&#21035;&#24320;&#28304;&#24773;&#25253;&#20013;&#19982;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20316;&#20026;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;Twitter&#25910;&#38598;&#30340;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#26469;&#28304;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#26041;&#38754;&#65292;&#21830;&#19994;&#27169;&#22411;Chatbot GPT-4&#23454;&#29616;&#20102;&#21487;&#25509;&#21463;&#30340;F1&#20998;&#25968;0.94&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;GPT4all&#23454;&#29616;&#20102;F1&#20998;&#25968;0.90&#12290;&#28982;&#32780;&#65292;&#23601;&#32593;&#32476;&#23433;&#20840;&#23454;&#20307;&#35782;&#21035;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10474</link><description>&lt;p&gt;
LDReg: &#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#32500;&#24230;&#22349;&#32553;&#65292;&#20854;&#20013;&#23398;&#20064;&#30340;&#34920;&#31034;&#23376;&#31354;&#38388;&#32500;&#24230;&#26497;&#20302;&#65292;&#22240;&#27492;&#26080;&#27861;&#34920;&#31034;&#23436;&#25972;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#24577;&#12290;&#32500;&#24230;&#22349;&#32553;&#20063;&#34987;&#31216;&#20026;&#8220;&#22635;&#20805;&#19981;&#36275;&#8221;&#29616;&#35937;&#65292;&#26159;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30740;&#31350;&#20102;SSL&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#34920;&#31034;&#21487;&#20197;&#22312;&#20840;&#23616;&#19978;&#35206;&#30422;&#39640;&#32500;&#31354;&#38388;&#65292;&#20294;&#22312;&#23616;&#37096;&#19978;&#20250;&#22349;&#32553;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#65288;LDReg&#65289;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#26159;&#22522;&#20110;Fisher-Rao&#24230;&#37327;&#30340;&#25512;&#23548;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#20248;&#21270;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#28176;&#36827;&#23567;&#21322;&#24452;&#22788;&#30340;&#23616;&#37096;&#36317;&#31163;&#20998;&#24067;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;LDReg&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05342</link><description>&lt;p&gt;
&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#29992;&#20110;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#32454;&#32990;&#31867;&#22411;&#24182;&#29702;&#35299;&#20854;&#21151;&#33021;&#29305;&#24615;&#23545;&#25581;&#31034;&#24863;&#30693;&#21644;&#35748;&#30693;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35270;&#32593;&#33180;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#21050;&#28608;&#29289;&#26469;&#35782;&#21035;&#21151;&#33021;&#31867;&#22411;&#65292;&#20294;&#36825;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#20250;&#23545;&#20197;&#21069;&#24050;&#30693;&#30340;&#32454;&#32990;&#31867;&#22411;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#35270;&#35273;&#30382;&#23618;&#20013;&#65292;&#20173;&#28982;&#19981;&#30693;&#36947;&#23384;&#22312;&#20160;&#20040;&#21151;&#33021;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35782;&#21035;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#35270;&#32593;&#33180;&#21644;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#65288;MDS&#65289;&#26469;&#33719;&#24471;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#21644;&#32858;&#31867;&#37325;&#26032;&#20998;&#37197;&#20043;&#38388;&#30340;&#20132;&#26367;&#36827;&#34892;&#65292;&#31867;&#20284;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#25104;&#21151;&#24674;&#22797;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#36827;&#34892;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#30340;&#27604;&#29305;&#29575;&#21644;&#22797;&#26434;&#24230;&#39640;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#21644;&#39044;&#27979;&#30340;&#26368;&#23567;&#27604;&#29305;&#29575;&#26469;&#25913;&#36827;&#27604;&#29305;&#29575;&#26799;&#24230;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#22312;&#35270;&#39057;&#34892;&#19994;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#35270;&#39057;&#24182;&#20445;&#25345;&#27604;&#29305;&#29575;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03195</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#26500;&#24314;&#39640;&#25928;&#30340;&#27604;&#29305;&#29575;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features. (arXiv:2401.03195v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03195
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#30340;&#27604;&#29305;&#29575;&#21644;&#22797;&#26434;&#24230;&#39640;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#21644;&#39044;&#27979;&#30340;&#26368;&#23567;&#27604;&#29305;&#29575;&#26469;&#25913;&#36827;&#27604;&#29305;&#29575;&#26799;&#24230;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#22312;&#35270;&#39057;&#34892;&#19994;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#35270;&#39057;&#24182;&#20445;&#25345;&#27604;&#29305;&#29575;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#34892;&#19994;&#37324;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#24182;&#20445;&#25345;&#27604;&#29305;&#29575;&#30340;&#25928;&#29575;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#19968;&#20992;&#20999;&#27604;&#29305;&#29575;&#26799;&#24230;&#26041;&#26696;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#32534;&#30721;&#65292;&#20351;&#24471;&#35745;&#31639;&#26368;&#20339;&#20869;&#23481;&#24863;&#30693;&#20915;&#31574;&#25104;&#20026;&#19981;&#29616;&#23454;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#30340;&#27604;&#29305;&#29575;&#21644;&#22797;&#26434;&#24230;&#39640;&#25928;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#26041;&#27861;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26469;&#33258;&#30693;&#21517;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#26469;&#39044;&#27979;&#27604;&#29305;&#29575;&#36136;&#37327;&#34892;&#20026;&#65292;&#24182;&#38480;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65307;&#65288;2&#65289;&#36890;&#36807;&#39044;&#27979;&#39030;&#37096;&#36136;&#37327;&#30340;&#26368;&#23567;&#27604;&#29305;&#29575;&#26469;&#25913;&#36827;&#26368;&#39640;&#36136;&#37327;&#30340;&#26799;&#24230;&#25928;&#29575;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39030;&#37096;step&#12290;&#35813;&#26041;&#27861;&#22312;102&#20010;&#35270;&#39057;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;1.71%&#30340;BD-Rate&#24320;&#38144;&#19979;&#65292;&#19982;&#34542;&#21147;&#26041;&#27861;&#30456;&#27604;&#65292;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;94.1%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#22235;&#20010;&#32593;&#32476;&#21644;&#28040;&#34701;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00860</link><description>&lt;p&gt;
&#38646;&#22352;&#26631;&#31227;&#21160;&#65306;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#20248;&#21270;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#26159;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#36755;&#20986;&#30456;&#23545;&#20110;&#22352;&#26631;&#30340;&#39640;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#31216;&#20026;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#12290;ZCS&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#37327;&#20540;&#30340;&#21494;&#21464;&#37327;&#65292;&#29992;&#20110;&#27599;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#32500;&#24230;&#65292;&#36890;&#36807;&#23558;&#25152;&#38656;&#23548;&#25968;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;ZCS&#24456;&#23481;&#26131;&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#65307;&#25105;&#20204;&#20351;&#29992;DeepXDE&#36719;&#20214;&#21253;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#21644;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#29289;&#29702;&#32422;&#26463;&#30340;DeepONets&#26469;&#35299;&#20915;&#26080;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ZCS&#19968;&#30452;&#36890;&#36807;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#25552;&#20379;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20703</link><description>&lt;p&gt;
&#24378;&#21270;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#19979;&#28216;&#20219;&#21153;&#23545;&#40784;&#65292;&#21363;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26368;&#22823;&#21270;&#65288;&#21487;&#33021;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#65289;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;RFT&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#20248;&#21270;&#38556;&#30861;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#19979;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#21363;&#20351;&#26399;&#26395;&#22870;&#21169;&#36828;&#31163;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;RFT&#22522;&#20934;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23567;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#23548;&#33268;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#23475;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#26497;&#20854;&#32531;&#24930;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20811;&#26381;RFT&#20013;&#26799;&#24230;&#28040;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#23427;&#22312;RFT&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#30456;&#23545;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;SFT&#38454;&#27573;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19812</link><description>&lt;p&gt;
&#33041;&#35299;&#30721;&#65306;&#36208;&#21521;&#23454;&#26102;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20116;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#21644;&#22522;&#30784;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23545;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#35273;&#30693;&#35273;&#65292;&#29616;&#22312;&#21487;&#20197;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#35299;&#30721;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#65288;&#32422;&#20026;0.5 Hz&#65289;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#65288;&#32422;&#20026;5000 Hz&#65289;&#27979;&#37327;&#33041;&#27963;&#21160;&#30340;&#31070;&#32463;&#24433;&#20687;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;MEG&#35299;&#30721;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#21644;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#20174;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;ii&#65289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;MEG&#27169;&#22359;&#20197;&#21450;iii&#65289;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;MEG&#35299;&#30721;&#22120;&#22312;&#32463;&#20856;&#32447;&#24615;&#35299;&#30721;&#22120;&#19978;&#26174;&#31034;&#20986;7&#20493;&#30340;&#22270;&#20687;&#26816;&#32034;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#21518;&#26399;&#33041;&#37096;
&lt;/p&gt;
&lt;p&gt;
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2310.17467</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#35745;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#24605;&#24819;&#26469;&#33258;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#65292;&#20294;&#26412;&#25991;&#20013;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#26469;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#37325;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#32463;&#21382;&#20102;&#19982;&#23545;&#31216;&#24615;&#30772;&#32570;&#29616;&#35937;&#30456;&#23545;&#24212;&#30340;&#20108;&#38454;&#30456;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#65292;&#23427;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#26680;&#24515;&#65292;&#24182;&#21487;&#20197;&#29992;&#19968;&#32452;&#24179;&#22343;&#22330;&#20020;&#30028;&#25351;&#25968;&#26469;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#28909;&#21147;&#23398;&#30340;&#20844;&#24335;&#20998;&#26512;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;&#36830;&#25509;&#30340;&#26368;&#36817;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13459</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#38750;&#20984;-&#38750;&#20985;&#35757;&#32451;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#32447;&#24615;&#25554;&#20540;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20316;&#20026;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#32447;&#24615;&#25554;&#20540;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#25972;&#33539;&#22260;&#20869;&#30340;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26500;&#36896;&#21487;&#25193;&#23637;&#21040;&#32422;&#26463;&#21644;&#27491;&#21017;&#21270;&#35774;&#32622;&#12290;&#36890;&#36807;&#26367;&#25442;RAPP&#20013;&#30340;&#20869;&#37096;&#20248;&#21270;&#22120;&#65292;&#25105;&#20204;&#37325;&#26032;&#21457;&#29616;&#20102;Lookahead&#31639;&#27861;&#26063;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;Lookahead&#32487;&#25215;&#24615;&#36136;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;Lookahead&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
&lt;/p&gt;</description></item><item><title>CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.07240</link><description>&lt;p&gt;
CacheGen&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24555;&#36895;&#19978;&#19979;&#25991;&#21152;&#36733;
&lt;/p&gt;
&lt;p&gt;
CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07240
&lt;/p&gt;
&lt;p&gt;
CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25215;&#25285;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#23558;&#25972;&#21512;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#24212;&#23545;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25110;&#29992;&#25143;&#29305;&#23450;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#21709;&#24212;&#24335;&#30340;LLM&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#25152;&#26377;&#19978;&#19979;&#25991;&#34987;&#33719;&#21462;&#21644;LLM&#22788;&#29702;&#20043;&#21069;&#65292;&#26080;&#27861;&#29983;&#25104;&#20219;&#20309;&#20869;&#23481;&#12290;&#29616;&#26377;&#31995;&#32479;&#20165;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#35745;&#31639;&#24310;&#36831;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#32531;&#23384;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#20013;&#38388;&#38190;&#20540;&#29305;&#24449;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#19978;&#19979;&#25991;&#33719;&#21462;&#30340;&#32593;&#32476;&#24310;&#36831;&#26356;&#38271;&#65288;&#20363;&#22914;&#65292;&#38190;&#20540;&#29305;&#24449;&#28040;&#32791;&#30340;&#24102;&#23485;&#27604;&#25991;&#26412;&#19978;&#19979;&#25991;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CacheGen&#65292;&#20197;&#26368;&#23567;&#21270;LLM&#19978;&#19979;&#25991;&#33719;&#21462;&#21644;&#22788;&#29702;&#30340;&#24310;&#36831;&#12290;CacheGen&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#29305;&#24449;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#27604;&#29305;&#27969;&#34920;&#31034;&#65292;&#20943;&#23569;&#20102;&#20256;&#36755;&#25152;&#38656;&#30340;&#24102;&#23485;&#12290;&#32534;&#30721;&#22120;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#37327;&#21270;&#21644;......
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20855;&#26377;&#26080;&#35823;&#24046;&#19988;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#32622;&#25442;&#31561;&#21464;Transformer&#32593;&#32476;&#26469;&#25429;&#25417;&#36755;&#20837;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25490;&#24207;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.07174</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#35823;&#24046;&#30340;&#21487;&#24494;&#20998;&#20132;&#25442;&#20989;&#25968;&#30340;&#24191;&#20041;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20855;&#26377;&#26080;&#35823;&#24046;&#19988;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#32622;&#25442;&#31561;&#21464;Transformer&#32593;&#32476;&#26469;&#25429;&#25417;&#36755;&#20837;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25490;&#24207;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#26159;&#25152;&#26377;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#38500;&#20102;&#20256;&#32479;&#25490;&#24207;&#31639;&#27861;&#30340;&#38382;&#39064;&#34920;&#36848;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#25490;&#24207;&#32593;&#32476;&#32771;&#34385;&#20102;&#26356;&#25277;&#35937;&#20294;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#36755;&#20837;&#65292;&#20363;&#22914;&#22810;&#20301;&#25968;&#23383;&#22270;&#20687;&#21644;&#22270;&#20687;&#29255;&#27573;&#12290;&#20026;&#20102;&#23398;&#20064;&#20174;&#39640;&#32500;&#36755;&#20837;&#21040;&#27425;&#24207;&#21464;&#37327;&#30340;&#26144;&#23556;&#65292;&#38656;&#35201;&#20445;&#35777;&#25490;&#24207;&#32593;&#32476;&#30340;&#21487;&#24494;&#20998;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#20989;&#25968;&#23450;&#20041;&#19968;&#20010;&#26580;&#21270;&#35823;&#24046;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26080;&#35823;&#24046;&#30340;&#20132;&#25442;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#28385;&#36275;&#38750;&#20943;&#21644;&#21487;&#24494;&#20998;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#20855;&#26377;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32622;&#25442;&#31561;&#21464;Transformer&#32593;&#32476;&#65292;&#20197;&#25429;&#25417;&#32473;&#23450;&#36755;&#20837;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#20854;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#33021;&#21147;&#12290;&#22312;&#22810;&#26679;&#30340;&#25490;&#24207;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25110;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04363</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#25674;&#38144;&#38590;&#20197;&#22788;&#29702;&#30340;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#26465;&#20214;&#20998;&#24067;&#26469;&#21387;&#32553;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#35813;&#30693;&#35782;&#30340;&#21487;&#22788;&#29702;&#26597;&#35810;&#20165;&#38480;&#20110;&#20174;&#22836;&#21040;&#23614;&#30340;&#33258;&#22238;&#24402;&#25277;&#26679;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24207;&#21015;&#24310;&#32493;&#12289;&#22635;&#20805;&#21644;&#20854;&#20182;&#24418;&#24335;&#30340;&#21463;&#32422;&#26463;&#29983;&#25104;&#65292;&#37117;&#28041;&#21450;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#36825;&#20123;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36825;&#31181;&#25674;&#38144;&#36890;&#36807;&#36890;&#36807;&#23547;&#27714;&#22810;&#26679;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; - &#29983;&#25104;&#27969;&#32593;&#32476; (GFlowNets) &#26469;&#24494;&#35843; LLMs &#23454;&#29616;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;LLM&#24494;&#35843;&#30340;&#36825;&#31181;&#20998;&#24067;&#21305;&#37197;&#33539;&#24335;&#21487;&#20197;&#20316;&#20026;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#21644;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20248;&#21270;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#24605;&#32500;&#38142;&#25512;&#29702;&#35299;&#37322;&#20026;&#28508;&#21464;&#37327;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Leave-One-Out&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#31283;&#23450;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26680;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#21644;&#30041;&#19968;&#27861;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20934;&#21017;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#23494;&#24230;&#19981;&#22343;&#21248;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#21487;&#23398;&#20064;&#26435;&#37325;&#25193;&#23637;&#27169;&#22411;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.03556</link><description>&lt;p&gt;
&#20351;&#29992;Leave-One-Out&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#31283;&#23450;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective. (arXiv:2310.03556v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Leave-One-Out&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#31283;&#23450;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26680;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#21644;&#30041;&#19968;&#27861;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20934;&#21017;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#23494;&#24230;&#19981;&#22343;&#21248;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#21487;&#23398;&#20064;&#26435;&#37325;&#25193;&#23637;&#27169;&#22411;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#27010;&#29575;&#24314;&#27169;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36825;&#38656;&#35201;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#24403;&#21382;&#21490;&#25968;&#25454;&#19981;&#36275;&#26102;&#65292;&#24076;&#26395;&#23558;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24314;&#27169;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#24182;&#29983;&#25104;&#26356;&#22810;&#25968;&#25454;&#12290;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#27169;&#22411;&#26159;&#36825;&#19968;&#20219;&#21153;&#30340;&#24120;&#29992;&#36873;&#25321;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#36866;&#24212;&#23494;&#24230;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;KDE&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#26680;&#20989;&#25968;&#20855;&#26377;&#29420;&#31435;&#30340;&#24102;&#23485;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#30041;&#19968;&#27861;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#65288;LOO-MLL&#65289;&#20934;&#21017;&#65292;&#20197;&#38450;&#27490;&#24120;&#35268;&#30340;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20934;&#21017;&#20135;&#29983;&#22855;&#24322;&#35299;&#65292;&#24182;&#35777;&#26126;LOO-MLL&#21487;&#20197;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#27492;&#20445;&#35777;&#30340;&#40065;&#26834;&#24615;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20026;&#26680;&#20989;&#25968;&#20998;&#37197;&#21487;&#23398;&#20064;&#26435;&#37325;&#25193;&#23637;&#20102;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic modelling of power systems operation and planning processes depends on data-driven methods, which require sufficiently large datasets. When historical data lacks this, it is desired to model the underlying data generation mechanism as a probability distribution to assess the data quality and generate more data, if needed. Kernel density estimation (KDE) based models are popular choices for this task, but they fail to adapt to data regions with varying densities. In this paper, an adaptive KDE model is employed to circumvent this, where each kernel in the model has an individual bandwidth. The leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to prevent the singular solutions that the regular MLL criterion gives rise to, and it is proven that LOO-MLL prevents these. Relying on this guaranteed robustness, the model is extended by assigning learnable weights to the kernels. In addition, a modified expectation-maximization algorithm is employed to accelerat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#39044;&#26399;&#27969;&#32593;&#32476;&#65288;EFlowNets&#65289;&#21644;&#23545;&#25239;&#27969;&#32593;&#32476;&#65288;AFlowNets&#65289;&#65292;&#20998;&#21035;&#24212;&#29992;&#20110;&#38543;&#26426;&#29615;&#22659;&#21644;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#12290;&#22312;&#38543;&#26426;&#20219;&#21153;&#20013;&#65292;EFlowNets&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#21452;&#20154;&#28216;&#25103;&#20013;&#65292;AFlowNets&#22312;&#33258;&#25105;&#23545;&#24328;&#20013;&#25214;&#21040;&#20102;80%&#20197;&#19978;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#36229;&#36807;&#20102;AlphaZero&#12290;</title><link>http://arxiv.org/abs/2310.02779</link><description>&lt;p&gt;
&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#39044;&#26399;&#27969;&#32593;&#32476;&#21644;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Expected flow networks in stochastic environments and two-player zero-sum games. (arXiv:2310.02779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#39044;&#26399;&#27969;&#32593;&#32476;&#65288;EFlowNets&#65289;&#21644;&#23545;&#25239;&#27969;&#32593;&#32476;&#65288;AFlowNets&#65289;&#65292;&#20998;&#21035;&#24212;&#29992;&#20110;&#38543;&#26426;&#29615;&#22659;&#21644;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#12290;&#22312;&#38543;&#26426;&#20219;&#21153;&#20013;&#65292;EFlowNets&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#21452;&#20154;&#28216;&#25103;&#20013;&#65292;AFlowNets&#22312;&#33258;&#25105;&#23545;&#24328;&#20013;&#25214;&#21040;&#20102;80%&#20197;&#19978;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#36229;&#36807;&#20102;AlphaZero&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#35757;&#32451;&#29992;&#20110;&#21305;&#37197;&#32473;&#23450;&#20998;&#24067;&#30340;&#24207;&#21015;&#37319;&#26679;&#27169;&#22411;&#12290;GFlowNets&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#32467;&#26500;&#23545;&#35937;&#29983;&#25104;&#20219;&#21153;&#65292;&#33021;&#22815;&#36805;&#36895;&#37319;&#26679;&#20986;&#22810;&#26679;&#21270;&#19988;&#39640;&#22238;&#25253;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#26399;&#27969;&#32593;&#32476;&#65288;EFlowNets&#65289;&#65292;&#23558;GFlowNets&#25193;&#23637;&#21040;&#38543;&#26426;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;EFlowNets&#22312;&#38543;&#26426;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#35774;&#35745;&#65289;&#20013;&#20248;&#20110;&#20854;&#20182;GFlowNet&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;EFlowNets&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23545;&#25239;&#29615;&#22659;&#20013;&#65292;&#20026;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#25552;&#20986;&#20102;&#23545;&#25239;&#27969;&#32593;&#32476;&#65288;AFlowNets&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AFlowNets&#36890;&#36807;&#33258;&#25105;&#23545;&#24328;&#22312;Connect-4&#28216;&#25103;&#20013;&#33021;&#25214;&#21040;80%&#20197;&#19978;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#20248;&#20110;AlphaZero&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments.
&lt;/p&gt;</description></item><item><title>Delta-AI&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#22270;&#27169;&#22411;&#30340;&#25674;&#36824;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#20449;&#29992;&#20998;&#37197;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02423</link><description>&lt;p&gt;
Delta-AI: &#31232;&#30095;&#22270;&#27169;&#22411;&#30340;&#25674;&#36824;&#25512;&#29702;&#20013;&#30340;&#23616;&#37096;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Delta-AI: Local objectives for amortized inference in sparse graphical models. (arXiv:2310.02423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02423
&lt;/p&gt;
&lt;p&gt;
Delta-AI&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#22270;&#27169;&#22411;&#30340;&#25674;&#36824;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#20449;&#29992;&#20998;&#37197;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#30340;&#25674;&#36824;&#25512;&#29702;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Delta-AI&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#24403;PGM&#20013;&#30340;&#21464;&#37327;&#37319;&#26679;&#34987;&#35270;&#20026;&#19968;&#20010;&#20195;&#29702;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#24207;&#21015;&#26102;&#65292;PGM&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#23398;&#20064;&#30446;&#26631;&#33021;&#22815;&#36827;&#34892;&#23616;&#37096;&#20449;&#29992;&#20998;&#37197;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23616;&#37096;&#32422;&#26463;&#65292;&#21487;&#20197;&#36716;&#21270;&#20026;&#31867;&#20284;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#20013;&#30340;&#23616;&#37096;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31163;&#31574;&#30053;&#35757;&#32451;&#65292;&#20294;&#36991;&#20813;&#20102;&#27599;&#20010;&#21442;&#25968;&#26356;&#26032;&#38656;&#35201;&#23454;&#20363;&#21270;&#25152;&#26377;&#38543;&#26426;&#21464;&#37327;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#22823;&#22823;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;Delta-AI&#30446;&#26631;&#19982;&#19968;&#20010;&#21487;&#35745;&#31639;&#30340;&#23398;&#20064;&#37319;&#26679;&#22120;&#20013;&#30340;&#21464;&#37327;&#32473;&#23450;&#20854;&#39532;&#23572;&#21487;&#22827;&#27631;&#23376;&#30340;&#26465;&#20214;&#20998;&#24067;&#30456;&#21305;&#37197;&#65292;&#35813;&#37319;&#26679;&#22120;&#30340;&#32467;&#26500;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#22312;&#30446;&#26631;PGM&#19979;&#20855;&#26377;&#30456;&#21516;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21518;&#30340;&#37319;&#26679;&#22120;&#21487;&#20197;&#24674;&#22797;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#36793;&#38469;&#20998;&#24067;&#21644;&#26465;&#20214;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\Delta$-amortized inference ($\Delta$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The $\Delta$-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of intere
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01794</link><description>&lt;p&gt;
GNNX-BENCH: &#36890;&#36807;&#28145;&#24230;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#22120;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking. (arXiv:2310.01794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#25581;&#31034;GNN&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#25152;&#26377;&#25552;&#20986;&#30340;&#31639;&#27861;&#37117;&#21253;&#21547;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#30340;&#35810;&#38382;&#26041;&#38754;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;GNN&#35299;&#37322;&#24615;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#23545;&#20107;&#23454;&#27714;&#35777;&#25512;&#29702;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;&#12289;&#23427;&#20204;&#23545;&#19981;&#21516;GNN&#26550;&#26500;&#12289;&#22122;&#22768;&#12289;&#38750;&#20984;&#25439;&#22833;&#34920;&#38754;&#20013;&#30340;&#38543;&#26426;&#24615;&#12289;&#22312;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#21487;&#34892;&#24615;&#31561;&#31561;&#65292;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#30340;&#30740;&#31350;&#12290;&#21463;&#27492;&#38656;&#27714;&#30340;&#28608;&#21457;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#21508;&#31181;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20851;&#38190;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25152;&#26377;&#31639;&#27861;&#37117;&#21463;&#21040;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01436</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#22312;&#33258;&#21160;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;GNAS&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21171;&#21160;&#21644;&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35774;&#35745;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;GNAS&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;GPT4GNAS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;GPT-4&#35774;&#35745;&#19968;&#31867;&#26032;&#30340;&#25552;&#31034;&#65292;&#20197;&#25351;&#23548;GPT-4&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;GNAS&#30340;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31574;&#30053;&#21644;&#25628;&#32034;&#21453;&#39304;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#36816;&#34892;&#20855;&#26377;&#25552;&#31034;&#30340;GPT-4&#65292;GPT4GNAS&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;GNAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26377;&#21521;&#22270;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#23545;&#20854;&#30456;&#23545;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07030</link><description>&lt;p&gt;
&#26377;&#21521;&#21152;&#26435;&#22270;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65306;&#20197;&#32454;&#32990;&#38388;&#36890;&#35759;&#32593;&#32476;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26377;&#21521;&#22270;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#23545;&#20854;&#30456;&#23545;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27604;&#36739;&#26368;&#20248;&#36755;&#36816;&#22270;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#26368;&#20248;&#36755;&#36816;&#24341;&#36215;&#30340;&#36317;&#31163;&#26082;&#25552;&#20379;&#20102;&#22270;&#20043;&#38388;&#30340;&#21512;&#29702;&#24230;&#37327;&#65292;&#21448;&#36890;&#36807;&#36755;&#36816;&#35745;&#21010;&#30340;&#21487;&#35299;&#37322;&#25551;&#36848;&#20102;&#22270;&#20043;&#38388;&#30456;&#20851;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#23545;&#31216;&#24615;&#65292;&#36890;&#24120;&#32771;&#34385;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#20027;&#35201;&#29992;&#20110;&#26080;&#21521;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21464;&#20307;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#26377;&#21521;&#22270;&#65306;&#65288;i&#65289;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#65288;Wasserstein&#65289;&#21644;&#65288;ii&#65289;Gromov-Wasserstein&#65288;GW&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#36317;&#31163;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#22522;&#20110;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#26377;&#21521;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
&lt;/p&gt;</description></item><item><title>ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06651</link><description>&lt;p&gt;
ConR: &#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConR: Contrastive Regularizer for Deep Imbalanced Regression. (arXiv:2309.06651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06651
&lt;/p&gt;
&lt;p&gt;
ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#24067;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#24456;&#24120;&#35265;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#20102;&#32422;&#26463;&#65292;&#20197;&#34920;&#31034;&#23569;&#25968;&#31867;&#21035;&#26631;&#31614;&#24182;&#36991;&#20813;&#23545;&#22810;&#25968;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#22823;&#37327;&#30340;&#19981;&#24179;&#34913;&#26041;&#27861;&#22788;&#29702;&#20102;&#20998;&#31867;&#26631;&#31614;&#31354;&#38388;&#65292;&#20294;&#22312;&#36830;&#32493;&#26631;&#31614;&#31354;&#38388;&#30340;&#22238;&#24402;&#38382;&#39064;&#19978;&#26410;&#33021;&#26377;&#25928;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#36830;&#32493;&#26631;&#31614;&#20043;&#38388;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#32852;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26377;&#25928;&#24314;&#27169;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConR&#65292;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#23427;&#20204;&#30340;&#22810;&#25968;&#37051;&#23621;&#20013;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#30456;&#20284;&#24615;&#20316;&#20026;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#25351;&#31034;&#22120;&#65292;ConR&#21306;&#20998;&#20102;&#26631;&#31614;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23545;&#36825;&#20123;&#19981;&#19968;&#33268;&#26045;&#21152;&#24809;&#32602;&#12290;ConR&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#31574;&#30053;&#20851;&#27880;&#26631;&#31614;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategi
&lt;/p&gt;</description></item><item><title>EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03244</link><description>&lt;p&gt;
EGIC:&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#30340;&#25351;&#23548;&#19979;
&lt;/p&gt;
&lt;p&gt;
EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03244
&lt;/p&gt;
&lt;p&gt;
EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;EGIC&#65292;&#23427;&#20801;&#35768;&#20174;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26377;&#25928;&#22320;&#36941;&#21382;&#22833;&#30495;&#24863;&#30693;&#26354;&#32447;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#32534;&#30721;&#30340;&#22270;&#20687;&#25554;&#20540;&#21464;&#20307;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;MSE&#20248;&#21270;&#21644;GAN&#20248;&#21270;&#35299;&#30721;&#22120;&#36755;&#20986;&#20043;&#38388;&#30340;&#27531;&#24046;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#27531;&#24046;&#23545;&#22522;&#20110;GAN&#30340;&#37325;&#24314;&#30340;&#24433;&#21709;&#12290;&#32467;&#21512;&#25913;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#26500;&#24314;&#22359;&#65292;EGIC&#22312;&#24863;&#30693;&#23548;&#21521;&#21644;&#22833;&#30495;&#23548;&#21521;&#30340;&#22522;&#32447;&#26041;&#27861;&#65288;&#21253;&#25324;HiFiC&#65292;MRIC&#21644;DIRAC&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#22823;&#22810;&#25968;&#26041;&#27861;&#65292;&#22312;&#22833;&#30495;&#31471;&#19982;VTM-20.0&#20960;&#20046;&#30456;&#24403;&#12290;EGIC&#23454;&#29616;&#31616;&#21333;&#65292;&#38750;&#24120;&#36731;&#37327;&#32423;&#65288;&#19982;HiFiC&#30456;&#27604;&#65292;&#27169;&#22411;&#21442;&#25968;&#21482;&#26377;0.18&#20493;&#65289;&#65292;&#24182;&#25552;&#20379;&#20248;&#24322;&#30340;&#25554;&#20540;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#38024;&#23545;&#20302;&#20301;&#33539;&#22260;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.03179</link><description>&lt;p&gt;
SLiMe: &#20687;&#25105;&#19968;&#26679;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03179
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;SD&#65289;&#65292;&#22312;&#35832;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#23545;&#24212;&#21644;3D&#24418;&#29366;&#29983;&#25104;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20986;SLiMe&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#23545;&#22270;&#20687;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;SLiMe&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24352;&#35757;&#32451;&#22270;&#20687;&#21450;&#20854;&#20998;&#21106;&#25513;&#33180;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SD&#20808;&#39564;&#20013;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#8220;&#21152;&#26435;&#32047;&#31215;&#33258;&#27880;&#24847;&#21147;&#22270;&#8221;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20248;&#21270;&#31283;&#23450;&#25193;&#25955;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#20351;&#24471;&#27599;&#20010;&#23884;&#20837;&#21482;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20998;&#21106;&#21306;&#22495;&#12290;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#28982;&#21518;&#22312;&#27880;&#24847;&#21147;&#22270;&#20013;&#31361;&#20986;&#26174;&#31034;&#20998;&#21106;&#21306;&#22495;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20998;&#21106;&#22270;&#12290;&#36825;&#20351;&#24471;SLiMe&#21487;&#20197;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#20110;&#33609;&#22270;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22871;&#20214;PROMISE&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2309.02014</link><description>&lt;p&gt;
PROMISE: &#36890;&#36807;&#24341;&#20837;&#21487;&#25193;&#23637;&#26354;&#29575;&#20272;&#35745;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. (arXiv:2309.02014v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#20110;&#33609;&#22270;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22871;&#20214;PROMISE&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PROMISE&#65288;&#36890;&#36807;&#24341;&#20837;&#21487;&#25193;&#23637;&#26354;&#29575;&#20272;&#35745;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#22871;&#22522;&#20110;&#33609;&#22270;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22871;&#20214;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;PROMISE&#21253;&#25324;SVRG&#12289;SAGA&#21644;Katyusha&#30340;&#39044;&#26465;&#20214;&#29256;&#26412;&#65307;&#27599;&#20010;&#31639;&#27861;&#37117;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#26377;&#25928;&#30340;&#40664;&#35748;&#36229;&#21442;&#25968;&#20540;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#38656;&#35201;&#20180;&#32454;&#35843;&#33410;&#36229;&#21442;&#25968;&#25165;&#33021;&#25104;&#21151;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#30149;&#24577;&#26465;&#20214;&#19979;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#40664;&#35748;&#36229;&#21442;&#25968;&#20540;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30001;&#22522;&#20934;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#27719;&#32534;&#30340;51&#20010;&#23725;&#22238;&#24402;&#21644;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#19978;&#20248;&#20110;&#25110;&#19982;&#27969;&#34892;&#30340;&#35843;&#25972;&#21518;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#22120;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#26041;&#27861;&#12290;&#20351;&#29992;&#24863;&#30693;&#37327;&#21270;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08381</link><description>&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25298;&#32477;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Precision and Recall Reject Curves for Classification. (arXiv:2308.08381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#26041;&#27861;&#12290;&#20351;&#29992;&#24863;&#30693;&#37327;&#21270;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#39640;&#24230;&#30830;&#23450;&#30340;&#20998;&#31867;&#23454;&#20363;&#26159;&#21487;&#21462;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#39640;&#24230;&#30830;&#23450;&#30340;&#23454;&#20363;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20934;&#30830;&#24230;&#25298;&#32477;&#26354;&#32447;&#12290;&#25298;&#32477;&#26354;&#32447;&#20801;&#35768;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#30830;&#23450;&#24230;&#24230;&#37327;&#22312;&#25509;&#21463;&#25110;&#25298;&#32477;&#20998;&#31867;&#30340;&#19968;&#31995;&#21015;&#38408;&#20540;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#21487;&#33021;&#24182;&#19981;&#36866;&#21512;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#30456;&#21453;&#65292;&#31934;&#30830;&#24230;&#25110;&#21484;&#22238;&#29575;&#21487;&#33021;&#26356;&#21487;&#21462;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20013;&#65292;&#20363;&#22914;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#22522;&#20934;&#25968;&#25454;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#65306;&#21484;&#22238;&#29575;-&#25298;&#32477;&#26354;&#32447;&#21644;&#31934;&#30830;&#24230;-&#25298;&#32477;&#26354;&#32447;&#12290;&#36890;&#36807;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#20013;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20154;&#24037;&#22522;&#20934;&#25968;&#25454;&#19978;&#23558;&#25552;&#20986;&#30340;&#26354;&#32447;&#19982;&#20934;&#30830;&#24230;&#25298;&#32477;&#26354;&#32447;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23384;&#22312;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#22522;&#20934;&#25968;&#25454;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09912</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#33324;&#30340;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#65292;&#21253;&#25324;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#29366;&#24577;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#31995;&#32479;&#30340;&#21069;&#21521;&#20256;&#36755;&#31639;&#23376;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#31639;&#23376;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#29366;&#24577;&#25110;&#21487;&#35266;&#27979;&#37327;&#12290;&#34920;&#31034;&#36890;&#24120;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#19982;&#25237;&#24433;&#31639;&#23376;&#30456;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#19982;CCA&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#65292;&#22240;&#27492;&#36890;&#24120;&#26356;&#31283;&#23450;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;CCA&#30340;&#19968;&#20010;&#32039;&#26494;&#24347;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#26469;&#22686;&#24378;&#23427;&#65292;&#19968;&#31181;&#40723;&#21169;&#34920;&#31034;&#30340;&#20998;&#37327;&#27491;&#20132;&#65292;&#32780;&#21478;&#19968;&#31181;&#21033;&#29992;&#20102; Chapman-Kolmogorov &#26041;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.07745</link><description>&lt;p&gt;
&#26680;&#21270;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#20855;&#26377;&#22797;&#26434;&#27169;&#22411;&#21644;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#20110;&#20855;&#26377;&#23569;&#37327;&#29366;&#24577;-&#34892;&#20026;&#25110;&#31616;&#21333;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#24314;&#27169;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#65289;&#30340;&#35774;&#32622;&#12290; &#20026;&#20102;&#25512;&#23548;&#26377;&#25928;&#22788;&#29702;&#26356;&#24191;&#27867;&#20540;&#20989;&#25968;&#30340;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;RL&#31574;&#30053;&#65292;&#19968;&#20123;&#26368;&#26032;&#24037;&#20316;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;$\pi$-KRVI&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#19968;&#31181;&#20048;&#35266;&#20462;&#25913;&#65292;&#24403;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#30001;RKHS&#34920;&#31034;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#39640;&#24230;&#38750;&#20809;&#28369;&#20869;&#26680;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#20869;&#26680;&#25110;&#26576;&#20123;Mat\'ern&#20869;&#26680;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07280</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#24494;&#35843;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Controlling Text-to-Image Diffusion by Orthogonal Finetuning. (arXiv:2306.07280v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#24863;&#22270;&#20687;&#26041;&#38754;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24341;&#23548;&#25110;&#25511;&#21046;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;OFT&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#25345;&#29305;&#24449;&#23545;&#31070;&#32463;&#20803;&#22312;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#30340;&#20851;&#31995;&#25152;&#34920;&#24449;&#30340;&#36229;&#29699;&#24418;&#33021;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#23646;&#24615;&#23545;&#20110;&#20445;&#25345;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#20102;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#23427;&#23545;&#36229;&#29699;&#38754;&#26045;&#21152;&#20102;&#39069;&#22806;&#30340;&#21322;&#24452;&#32422;&#26463;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#65306;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#20984;&#32452;&#21512;&#30340;&#34920;&#36798;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#26368;&#26032;&#30340;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;IBP&#36793;&#30028;&#20043;&#38388;&#30340;&#31616;&#21333;&#20984;&#32452;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13991</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#32452;&#21512;&#30340;&#34920;&#36798;&#24615;&#25439;&#22833;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expressive Losses for Verified Robustness via Convex Combinations. (arXiv:2305.13991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20984;&#32452;&#21512;&#30340;&#34920;&#36798;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#26368;&#26032;&#30340;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;IBP&#36793;&#30028;&#20043;&#38388;&#30340;&#31616;&#21333;&#20984;&#32452;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#65288;&#25200;&#21160;&#21306;&#22495;&#30340;&#23376;&#38598;&#65289;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#38480;&#65292;&#25110;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#19978;&#24341;&#20837;&#21487;&#39564;&#35777;&#24615;&#26469;&#35757;&#32451;&#20855;&#26377;&#24050;&#39564;&#35777;&#40065;&#26834;&#24615;&#30340;&#32593;&#32476;&#12290;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20851;&#38190;&#22312;&#20110;&#25152;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#21305;&#37197;&#35757;&#32451;&#21518;&#35201;&#20351;&#29992;&#30340;&#39564;&#35777;&#22120;&#30340;&#32039;&#23494;&#24230;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#34920;&#36798;&#21147;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;IBP&#36793;&#30028;&#20043;&#38388;&#30340;&#31616;&#21333;&#20984;&#32452;&#21512;&#26469;&#28385;&#36275;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;CC-IBP&#21644;MTL-IBP&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#22343;&#21487;&#20197;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#20854;&#27010;&#24565;&#19978;&#26159;&#31616;&#21333;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;TinyImageNet&#21644;&#32553;&#23567;&#30340;ImageNet&#19978;&#65292;&#23545;&#20110;&#21322;&#24452;&#20026;$ \frac{1} {255} $&#30340;$ \ell_ \infty $&#25200;&#21160;&#65292;MTL-IBP&#21487;&#20197;&#23558;&#25991;&#29486;&#20013;&#26368;&#20339;&#26631;&#20934;&#21644;&#39564;&#35777;&#20934;&#30830;&#24615;&#20174;$1.98\%$&#25552;&#39640;&#21040;$3.92\%$&#65292;&#21516;&#26102;&#20165;&#20381;&#36182;&#20110;&#21333;&#27493;&#33258;&#36866;&#24212;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to train networks for verified adversarial robustness, previous work typically over-approximates the worst-case loss over (subsets of) perturbation regions or induces verifiability on top of adversarial training. The key to state-of-the-art performance lies in the expressivity of the employed loss function, which should be able to match the tightness of the verifiers to be employed post-training. We formalize a definition of expressivity, and show that it can be satisfied via simple convex combinations between adversarial attacks and IBP bounds. We then show that the resulting algorithms, named CC-IBP and MTL-IBP, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. In particular, for $\ell_\infty$ perturbations of radius $\frac{1}{255}$ on TinyImageNet and downscaled ImageNet, MTL-IBP improves on the best standard and verified accuracies from the literature by from $1.98\%$ to $3.92\%$ points while only relying on single-step ad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#24378;&#21270;&#23398;&#20064;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#36890;&#36947;&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#33021;&#22815;&#20256;&#36755;&#24847;&#20041;&#32780;&#38750;&#31934;&#30830;&#29256;&#26412;&#65292;&#36798;&#21040;&#20102;&#20449;&#24687;&#36895;&#29575;&#33410;&#30465;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.03571</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#27169;&#22411;&#26080;&#20851;&#35821;&#20041;&#36890;&#20449;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient. (arXiv:2305.03571v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#24378;&#21270;&#23398;&#20064;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#36890;&#36947;&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#33021;&#22815;&#20256;&#36755;&#24847;&#20041;&#32780;&#38750;&#31934;&#30830;&#29256;&#26412;&#65292;&#36798;&#21040;&#20102;&#20449;&#24687;&#36895;&#29575;&#33410;&#30465;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#22312;&#26080;&#32447;&#36890;&#20449;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#38886;&#24343;&#65288;Weaver&#65289;&#20110;1949&#24180;&#25552;&#20986;&#30340;&#35821;&#20041;&#36890;&#20449;&#27010;&#24565;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#23427;&#25171;&#30772;&#20102;&#39321;&#20892;&#32463;&#20856;&#30340;&#35774;&#35745;&#33539;&#20363;&#65292;&#26088;&#22312;&#20256;&#36755;&#28040;&#24687;&#30340;&#24847;&#20041;&#65292;&#21363;&#35821;&#20041;&#65292;&#32780;&#19981;&#26159;&#31934;&#30830;&#29256;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20449;&#24687;&#36895;&#29575;&#33410;&#30465;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#26469;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#19981;&#38656;&#35201;&#24050;&#30693;&#25110;&#21487;&#24494;&#20998;&#36890;&#36947;&#27169;&#22411;&#65292;&#36825;&#26159;&#23454;&#38469;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#26368;&#22823;&#21270;&#25509;&#25910;&#21644;&#30446;&#26631;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20986;&#21457;&#65292;&#28608;&#21457;&#20102;&#23558;SPG&#29992;&#20110;&#32463;&#20856;&#21644;&#35821;&#20041;&#36890;&#20449;&#30340;&#21160;&#26426;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#22522;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#27169;&#22411;&#24863;&#30693;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#25910;&#25947;&#36895;&#24230;&#26377;&#25152;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent success of Machine Learning tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning, i.e., semantics, of a message instead of its exact version, allowing for information rate savings. In this work, we apply the Stochastic Policy Gradient (SPG) to design a semantic communication system by reinforcement learning, not requiring a known or differentiable channel model a crucial step towards deployment in practice. Further, we motivate the use of SPG for both classic and semantic communication from the maximization of the mutual information between received and target variables. Numerical results show that our approach achieves comparable performance to a model-aware approach based on the reparametrization trick, albeit with a decreased convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#20869;&#28857;&#31639;&#27861;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#20869;&#37096;&#37051;&#22495;&#65292;&#33021;&#22815;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14907</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20869;&#28857;&#31639;&#27861;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#20869;&#28857;&#31639;&#27861;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#20869;&#37096;&#37051;&#22495;&#65292;&#33021;&#22815;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#20869;&#28857;&#31639;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#23384;&#22312;&#32422;&#26463;&#30340;&#36830;&#32493;&#21487;&#24494;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#20809;&#28369;&#65288;&#38750;&#20984;&#65289;&#20248;&#21270;&#38382;&#39064;&#26102;&#19982;&#20854;&#20182;&#20869;&#28857;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25628;&#32034;&#26041;&#21521;&#26159;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;&#23427;&#22312;&#20351;&#29992;&#21487;&#34892;&#22495;&#30340;&#20869;&#37096;&#37051;&#22495;&#65288;&#30001;&#27491;&#19988;&#28040;&#22833;&#30340;&#37051;&#22495;&#21442;&#25968;&#24207;&#21015;&#23450;&#20041;&#65289;&#30340;&#36807;&#31243;&#20013;&#20063;&#24456;&#29420;&#29305;&#65292;&#36890;&#36807;&#23558;&#36845;&#20195;&#24378;&#21046;&#20445;&#30041;&#22312;&#35813;&#37051;&#22495;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#24515;&#24179;&#34913;&#23631;&#38556;&#12289;&#27493;&#38271;&#21644;&#37051;&#22495;&#24207;&#21015;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#28385;&#36275;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#22312;&#20004;&#31181;&#35774;&#32622;&#19979;&#65292;&#25968;&#20540;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20248;&#20110;&#25237;&#24433;-&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2211.00646</link><description>&lt;p&gt;
&#20174;&#30456;&#37051;&#30340;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#23398;&#20064;&#40657;&#33394;&#32032;&#32454;&#32990;&#25513;&#33180;
&lt;/p&gt;
&lt;p&gt;
Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#30382;&#32932;&#30284;&#20043;&#19968;&#65292;&#23548;&#33268;&#22823;&#37096;&#20998;&#30382;&#32932;&#30284;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#23398;&#23478;&#23545;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#36739;&#20302;&#12290;&#30001;&#20110;&#40657;&#33394;&#32032;&#30244;&#26159;&#40657;&#33394;&#32032;&#32454;&#32990;&#30340;&#32959;&#30244;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#19982;&#30149;&#29702;&#23398;&#23478;&#30340;&#24046;&#24322;&#26080;&#20851;&#24182;&#33021;&#33258;&#21160;&#36827;&#34892;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30149;&#29702;&#23398;&#23478;&#26631;&#27880;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#37051;&#36817;&#32452;&#32455;&#20999;&#29255;&#19978;&#30340;&#20598;&#32852;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#29255;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#65292;&#34429;&#28982;&#24456;&#38590;&#26377;&#23436;&#32654;&#30340;&#26631;&#31614;&#65292;&#20294;&#36798;&#21040;&#20102;0.64&#30340;&#24179;&#22343;IOU&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&amp;E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.12191</link><description>&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#29702;&#35299;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#32534;&#30721;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#27010;&#24565;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#35782;&#21035;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#27010;&#24565;&#30340;&#19968;&#31181;&#26126;&#26174;&#26041;&#27861;&#26159;&#25628;&#32034;&#19968;&#20010;&#32447;&#24615;&#23376;&#31354;&#38388;&#65292;&#20854;&#25830;&#38500;&#20250;&#38459;&#27490;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#32447;&#24615;&#25830;&#38500;&#31639;&#27861;&#26159;&#21487;&#22788;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#26410;&#24517;&#20197;&#32447;&#24615;&#26041;&#24335;&#34920;&#31034;&#27010;&#24565;&#12290;&#20026;&#20102;&#35782;&#21035;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#21270;&#30340;&#27010;&#24565;&#25830;&#38500;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#38450;&#27490;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20445;&#25252;&#19981;&#20250;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#24443;&#24213;&#22320;&#25830;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
&lt;/p&gt;</description></item></channel></rss>